<html><head></head><body>
        

                            
                    <h1 class="header-title">Introduction to Docker Swarm</h1>
                
            
            
                
<p>In the last chapter, we introduced orchestrators. Like a conductor in an orchestra, an orchestrator makes sure that all of our containerized application services play together nicely and contribute harmoniously to a common goal. Such orchestrators have quite a few responsibilities, which we discussed in detail. Finally, we provided a short overview of the most important container orchestrators on the market.</p>
<p>This chapter introduces Docker's native orchestrator, SwarmKit. It elaborates on all of the concepts and objects SwarmKit uses to deploy and run distributed, resilient, robust, and highly available applications in a cluster on premises or in the cloud. This chapter also introduces how SwarmKit ensures secure applications by using a <strong>Software-Defined Network</strong> (<strong>SDN</strong>) to isolate containers. Additionally, this chapter demonstrates how to install a highly available Docker Swarm in the cloud. It introduces the routing mesh, which provides layer-4 routing and load balancing. Finally, it demonstrates how to deploy a first application consisting of multiple services onto the swarm.</p>
<p>These are the topics we are going to discuss in this chapter:</p>
<ul>
<li>The Docker Swarm architecture</li>
<li>Swarm nodes</li>
<li>Stacks, services, and tasks</li>
<li>Multi-host networking</li>
<li>Creating a Docker Swarm</li>
<li>Deploying a first application</li>
<li>The Swarm routing mesh</li>
</ul>
<p>After completing this chapter, you will be able to do the following:</p>
<ul>
<li style="font-weight: 400">Sketch the essential parts of a highly available Docker Swarm on a whiteboard</li>
<li style="font-weight: 400">Explain in two or three simple sentences to an interested layman what a (swarm) service is</li>
<li style="font-weight: 400">Create a highly available Docker Swarm in AWS, Azure, or GCP consisting of three manager and two worker nodes</li>
<li style="font-weight: 400">Successfully deploy a replicated service such as Nginx on a Docker Swarm</li>
<li style="font-weight: 400">Scale a running Docker Swarm service up and down</li>
<li style="font-weight: 400">Retrieve the aggregated log of a replicated Docker Swarm service</li>
<li style="font-weight: 400">Write a simple stack file for a sample application consisting of at least two interacting services</li>
<li style="font-weight: 400">Deploy a stack into a Docker Swarm</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">The Docker Swarm architecture</h1>
                
            
            
                
<p>The architecture of a Docker Swarm from a 30,000-foot view consists of two main parts—a raft consensus group of an odd number of manager nodes, and a group of worker nodes that communicate with each other over a gossip network, also called the control plane. The following diagram illustrates this architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-946 image-border" src="img/185aff48-e453-4420-a5c7-35859f604904.png" style="width:29.58em;height:20.92em;"/></p>
<p>High-level architecture of a Docker Swarm</p>
<p>The <strong>manager</strong> nodes manage the swarm while the <strong>worker</strong> nodes execute the applications deployed into the swarm. Each <strong>manager</strong> has a complete copy of the full state of the Swarm in its local raft store. Managers synchronously communicate with each other and their raft stores are always in sync.</p>
<p>The <strong>workers</strong>, on the other hand, communicate with each other asynchronously for scalability reasons. There can be hundreds if not thousands of <strong>worker</strong> nodes in a Swarm. Now that we have a high-level overview of what a Docker Swarm is, let's describe all of the individual elements of a Docker Swarm in more detail.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Swarm nodes</h1>
                
            
            
                
<p>A Swarm is a collection of nodes. We can classify a node as a physical computer or <strong>Virtual Machine</strong> (<strong>VM</strong>). Physical computers these days are often referred to as <em>bare metal</em>. People say <em>we're running on bare metal</em> to distinguish from running on a VM.</p>
<p>When we install Docker on such a node, we call this node a Docker host. The following diagram illustrates a bit better what a node and what a Docker host is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-947 image-border" src="img/45b82a12-5b32-4290-8970-944fe94532eb.png" style="width:31.25em;height:20.67em;"/></p>
<p>Bare metal and VM types of Docker Swarm nodes</p>
<p>To become a member of a Docker Swarm, a node must be a Docker host. A node in a Docker Swarm can have one of two roles. It can be a manager or it can be a worker. Manager nodes do what their name implies; they manage the Swarm. The worker nodes, in turn, execute the application workload.</p>
<p>Technically, a manager node can also be a worker node and hence run application workload—although that is not recommended, especially if the Swarm is a production system running mission-critical applications.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Swarm managers</h1>
                
            
            
                
<p>Each Docker Swarm needs to include at least one manager node. For high availability reasons, we should have more than one manager node in a Swarm. This is especially true for production or production-like environments. If we have more than one manager node, then these nodes work together using the Raft consensus protocol. The Raft consensus protocol is a standard protocol that is often used when multiple entities need to work together and always need to agree with each other as to which activity to execute next.</p>
<p>To work well, the Raft consensus protocol asks for an odd number of members in what is called the consensus group. Hence, we should always have 1, 3, 5, 7, and so on manager nodes. In such a consensus group, there is always a leader. In the case of Docker Swarm, the first node that starts the Swarm initially becomes the leader. If the leader goes away then the remaining manager nodes elect a new leader. The other nodes in the consensus group are called followers.</p>
<p>Now, let's assume that we shut down the current leader node for maintenance reasons. The remaining manager nodes will elect a new leader. When the previous leader node comes back online, it will now become a follower. The new leader remains the leader.</p>
<p>All of the members of the consensus group communicate synchronously with each other. Whenever the consensus group needs to make a decision, the leader asks all followers for agreement. If a majority of the manager nodes give a positive answer, then the leader executes the task. That means if we have three manager nodes, then at least one of the followers has to agree with the leader. If we have five manager nodes, then at least two followers have to agree.</p>
<p>Since all manager follower nodes have to communicate synchronously with the leader node to make a decision in the cluster, the decision-making process gets slower and slower the more manager nodes we have forming the consensus group. The recommendation of Docker is to use one manager for development, demo, or test environments. Use three managers nodes in small to medium size Swarms, and use five managers in large to extra large Swarms. To use more than five managers in a Swarm is hardly ever justified.</p>
<p>The manager nodes are not only responsible for managing the Swarm but also for maintaining the state of the Swarm. <em>What do we mean by that?</em> When we talk about the state of the Swarm we mean all of the information about it—for example, <em>how many nodes are in the Swarm and</em> <em>what are the properties of each node, such as name or IP address</em>. We also mean what containers are running on which node in the Swarm and more. What, on the other hand, is not included in the state of the Swarm is data produced by the application services running in containers on the Swarm. This is called application data and is definitely not part of the state that is managed by the manager nodes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-948 image-border" src="img/cf1b7c95-25a4-43a4-aa20-168b1b938059.png" style="width:38.75em;height:12.08em;"/></p>
<p>A Swarm manager consensus group</p>
<p>All of the Swarm states are stored in a high-performance key-value store (<strong>kv-store</strong>) on each <strong>manager</strong> node. That's right, each <strong>manager</strong> node stores a complete replica of the whole Swarm state. This redundancy makes the Swarm highly available. If a <strong>manager</strong> node goes down, the remaining <strong>managers</strong> all have the complete state at hand.</p>
<p>If a new <strong>manager</strong> joins the consensus group, then it synchronizes the Swarm state with the existing members of the group until it has a complete replica. This replication is usually pretty fast in typical Swarms but can take a while if the Swarm is big and many applications are running on it.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Swarm workers</h1>
                
            
            
                
<p>As we mentioned earlier, a Swarm worker node is meant to host and run containers that contain the actual application services we're interested in running on our cluster. They are the workhorses of the Swarm. In theory, a manager node can also be a worker. But, as we already said, this is not recommended on a production system. On a production system, we should let managers be managers.</p>
<p>Worker nodes communicate with each other over the so-called control plane. They use the gossip protocol for their communication. This communication is asynchronous, which means that, at any given time, it is likely that not all worker nodes are in perfect sync. </p>
<p>Now, you might ask—<em>what information do worker nodes exchange?</em> It is mostly information that is needed for service discovery and routing, that is, information about which containers are running on with nodes and more:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-952 image-border" src="img/8049bdd7-03d8-405a-9ed4-e48e4d2216b4.png" style="width:39.75em;height:24.08em;"/></p>
<p>Worker nodes communicating with each other</p>
<p>In the preceding diagram, you can see how workers communicate with each other. To make sure the gossiping scales well in a large Swarm, each <strong>worker</strong> node only synchronizes its own state with three random neighbors. For those who are familiar with Big O notation, that means that the synchronization of the <strong>worker</strong> nodes using the gossip protocol scales with O(0).</p>
<p><strong>Worker</strong> nodes are kind of passive. They never actively do something other than run the workloads that they get assigned by the manager nodes. The <strong>worker</strong> makes sure, though, that it runs these workloads to the best of its capabilities. Later on in this chapter, we will get to know more about exactly what workloads the worker nodes are assigned by the manager nodes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Stacks, services, and tasks</h1>
                
            
            
                
<p>When using a Docker Swarm versus a single Docker host, there is a paradigm change. Instead of talking of individual containers that run processes, we are abstracting away to services that represent a set of replicas of each process, and, in this way, become highly available. We also do not speak anymore of individual Docker hosts with well-known names and IP addresses to which we deploy containers; we'll now be referring to clusters of hosts to which we deploy services. We don't care about an individual host or node anymore. We don't give it a meaningful name; each node rather becomes a number to us. We also don't care about individual containers and where they are deployed any longer—we just care about having a desired state defined through a service. We can try to depict that as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-953 image-border" src="img/f1ff1173-269e-4448-a409-8279610fc9be.png" style="width:30.58em;height:7.08em;"/></p>
<p>Containers are deployed to well-known servers</p>
<p class="mce-root">Instead of deploying individual containers to well-known servers like in the preceding diagram, where we deploy the <strong>web</strong> container to the <strong>alpha</strong> server with the IP address <kbd>52.120.12.1</kbd>, and the <strong>payments</strong> container to the <strong>beta</strong> server with the IP <kbd>52.121.24.33</kbd>, we switch to this new paradigm of services and Swarms (or, more generally, clusters):</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-954 image-border" src="img/6dd5f3db-4cb9-4052-9071-a8cb8256586f.png" style="width:33.08em;height:16.33em;"/></p>
<p>Services are deployed to Swarms</p>
<p>In the preceding diagram, we see that a <strong>web</strong> service and an <strong>inventory</strong> service are both deployed to a <strong>Swarm</strong> that consists of many nodes. Each of the services has a certain number of replicas: six for <strong>web</strong> and five for <strong>inventory</strong>. We don't really care on which node the replicas will run; we only care that the requested number of replicas is always running on whatever nodes the <strong>Swarm</strong> scheduler decides to put them on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Services</h1>
                
            
            
                
<p>A Swarm service is an abstract thing. It is a description of the desired state of an application or application service that we want to run in a Swarm. The Swarm service is like a manifest describing such things as the following:</p>
<ul>
<li>Name of the service</li>
<li>Image from which to create the containers</li>
<li>Number of replicas to run</li>
<li>Network(s) that the containers of the service are attached to</li>
<li>Ports that should be mapped</li>
</ul>
<p>Having this service manifest, the Swarm manager then makes sure that the described desired state is always reconciled if ever the actual state should deviate from it. So, if for example, one instance of the service crashes, then the scheduler on the Swarm manager schedules a new instance of this particular service on a node with free resources so that the desired state is reestablished.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Task</h1>
                
            
            
                
<p>We have learned that a service corresponds to a description of the desired state in which an application service should be at all times. Part of that description was the number of replicas the service should be running. Each replica is represented by a task. In this regard, a Swarm service contains a collection of tasks. On Docker Swarm, a task is the atomic unit of deployment. Each task of a service is deployed by the Swarm scheduler to a worker node. The task contains all of the necessary information that the worker node needs to run a container based on the image, which is part of the service description. Between a task and a container, there is a one-to-one relation. The container is the instance that runs on the worker node, while the task is the description of this container as a part of a Swarm service.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Stack</h1>
                
            
            
                
<p>Now that we have a good idea about what a Swarm service is and what tasks are, we can introduce the stack. A stack is used to describe a collection of Swarm services that are related, most probably because they are part of the same application. In that sense, we could also say that a stack describes an application that consists of one to many services that we want to run on the Swarm.</p>
<p>Typically, we describe a stack declaratively in a text file that is formatted using the YAML format and that uses the same syntax as the already-known Docker Compose file. This leads to a situation where people sometimes say that a stack is described by a <kbd>docker-compose</kbd> file. A better wording would be: a stack is described in a stack file that uses similar syntax to a <kbd>docker-compose</kbd> file.</p>
<p>Let's try to illustrate the relationship between the stack, services, and tasks in the following diagram and connect it with the typical content of a stack file:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-956 image-border" src="img/34c40ca5-56fa-4ce6-a61c-aa6d5ded8230.png" style="width:82.33em;height:48.67em;"/></p>
<p>Diagram showing the relationship between stack, services, and tasks</p>
<p>In the preceding diagram, we see on the right-hand side a declarative description of a sample <strong>Stack</strong>. The <strong>Stack</strong> consists of three services called <strong>web</strong>, <strong>payments</strong>, and <strong>inventory</strong>. We also see that the <strong>web</strong> service uses the <strong>example/web:1.0</strong> image and has four replicas.</p>
<p>On the left-hand side of the diagram, we see that the <strong>Stack</strong> embraces the three services mentioned. Each service, in turn, contains a collection of <strong>Tasks</strong>, as many as there are replicas. In the case of the <strong>web</strong> service, we have a collection of four <strong>Tasks</strong>. Each <strong>Task</strong> contains the name of the <strong>Image</strong> from which it will instantiate a container once the <strong>Task</strong> is scheduled on a Swarm node.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Multi-host networking</h1>
                
            
            
                
<p>In <a href="f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml" target="_blank">Chapter 10</a>, <em>Single-Host Networking,</em> we discussed how containers communicate on a single Docker host. Now, we have a Swarm that consists of a cluster of nodes or Docker hosts. Containers that are located on different nodes need to be able to communicate with each other. Many techniques can help us to achieve this goal. Docker has chosen to implement an <strong>overlay network</strong> driver for Docker Swarm. This <strong>overlay network</strong> allows containers attached to the same <strong>overlay network</strong> to discover each other and freely communicate with each other. The following is a schema for how an <strong>overlay network</strong> works:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-957 image-border" src="img/1731fa28-e8f4-459e-b4c2-40e63b876032.png" style="width:41.00em;height:17.75em;"/></p>
<p>Overlay network</p>
<p>We have two nodes or Docker hosts with the IP addresses <kbd>172.10.0.15</kbd> and <kbd>172.10.0.16</kbd>. The values we have chosen for the IP addresses are not important; what is important is that both hosts have a distinct IP address and are connected by a physical network (a network cable), which is called the <strong>underlay network</strong>.</p>
<p>On the node on the left-hand side we have a container running with the IP address <kbd>10.3.0.2</kbd>, and on the node on the right-hand side another container with the IP address <kbd>10.3.0.5</kbd>. Now, the former container wants to communicate with the latter. <em>How can this happen?</em> In <a href="f3b1e24a-2ac4-473a-b9c8-270b97df6a8a.xhtml" target="_blank">Chapter 10</a>, <em>Single-Host Networking,</em> we saw how this works when both containers are located on the same node—by using a Linux bridge. But Linux bridges only operate locally and cannot span across nodes. So, we need another mechanism. Linux VXLAN comes to the rescue. VXLAN has been available on Linux since way before containers were a thing.</p>
<p>When the left-hand container sends a data packet, the <strong>bridge</strong> realizes that the target of the packet is not on this host. Now, each node participating in an overlay network gets a so-called <strong>VXLAN Tunnel Endpoint</strong> (<strong>VTEP</strong>) object, which intercepts the packet (the packet at that moment is an OSI layer 2 data packet), wraps it with a header containing the target IP address of the host that runs the destination container (this makes it now an OSI layer 3 data packet), and sends it over the <strong>VXLAN tunnel</strong>. The <strong>VTEP</strong> on the other side of the tunnel unpacks the data packet and forwards it to the local bridge, which in turn forwards it to the destination container.</p>
<p>The overlay driver is included in SwarmKit and is in most cases the recommended network driver for Docker Swarm. There are other multi-node-capable network drivers available from third parties that can be installed as plugins to each participating Docker host. Certified network plugins are available from the Docker store. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a Docker Swarm</h1>
                
            
            
                
<p>Creating a Docker Swarm is almost trivial. It is so easy that it seems unreal if you know what an orchestrator is all about. But it is true, Docker has done a fantastic job in making Swarms simple and elegant to use. At the same time, Docker Swarm has been proven in use by large enterprises to be very robust and scalable.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a local single node swarm</h1>
                
            
            
                
<p>So, enough imagining — let's demonstrate how we can create a Swarm. In its most simple form, a fully functioning Docker Swarm consists only of a single node. If you're using Docker for Mac or Windows, or even if you're using Docker Toolbox, then your personal computer or laptop is such a node. Hence, we can start right there and demonstrate some of the most important features of a Swarm.</p>
<p>Let's initialize a Swarm. On the command line, just enter the following command:</p>
<pre><strong>$ docker swarm init</strong></pre>
<p>And after an incredibly short time, you should see something like the following screenshot:</p>
<div><img class="alignnone size-full wp-image-865 image-border" src="img/39cd7eb9-5916-422b-a23f-a158857f0401.png" style="width:50.50em;height:8.00em;"/></div>
<p>Output of the Docker Swarm init command</p>
<p>Our computer is now a Swarm node. Its role is that of a manager and it is the leader (of the managers, which makes sense since there is only one manager at this time). Although it took only a very short time to finish <kbd>docker swarm init</kbd>, the command did a lot of things during that time. Some of them are as follows:</p>
<ul>
<li>It created a root <strong>Certificate Authority</strong> (<strong>CA</strong>).</li>
<li>It created a key-value store that is used to store the state of the whole Swarm.</li>
</ul>
<p>Now, in the preceding output, we can see a command that can be used to join other nodes to the Swarm that we just created. The command is as follows:</p>
<pre><strong>$ docker swarm join --token &lt;join-token&gt; &lt;IP address&gt;:2377</strong></pre>
<p>Here, we have the following:</p>
<ul>
<li><kbd>&lt;join-token&gt;</kbd> is a token generated by the Swarm leader at the time the Swarm was initialized.</li>
<li> <kbd>&lt;IP address&gt;</kbd> is the IP address of the leader.</li>
</ul>
<p>Although our cluster remains simple, as it consists of only one member, we can still ask the Docker CLI to list all of the nodes of the Swarm. This will look similar to the following screenshot: </p>
<div><img src="img/4519a489-42e8-42d8-9ef0-8b791ce51d14.png"/></div>
<p>Listing the nodes of the Docker Swarm</p>
<p>In this output, we first see <kbd>ID</kbd> that was given to the node. The star (<kbd>*</kbd>) that follows <kbd>ID</kbd> indicates that this is the node on which <kbd>docker node ls</kbd> was executed—basically saying that this is the active node. Then, we have the (human-readable) name of the node, its status, availability, and manager status. As mentioned earlier, this very first node of the Swarm automatically became the leader, which is indicated in the preceding screenshot. Lastly, we see which version of the Docker engine we're using.</p>
<p>To get even more information about a node, we can use the <kbd>docker node inspect</kbd> command, as shown in the following screenshot:</p>
<div><img src="img/c9182c97-d23d-4bdc-a365-38f1897f4b63.png" style="width:22.17em;height:24.67em;"/></div>
<p>Truncated output of the docker node inspect command</p>
<p>There is a lot of information generated by this command, so we only present a truncated version of the output. This output can be useful, for example, when you need to troubleshoot a misbehaving cluster node.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a local Swarm in VirtualBox or Hyper-V</h1>
                
            
            
                
<p>Sometimes, a single node Swarm is not enough, but we don't have or don't want to use an account to create a Swarm in the cloud. In this case, we can create a local Swarm in either VirtualBox or Hyper-V. Creating the Swarm in VirtualBox is slightly easier than creating it in Hyper-V, but if you're using Windows 10 and have Docker for Windows running, then you cannot use VirtualBox at the same time. The two hypervisors are mutually exclusive.</p>
<p>Let's assume we have VirtualBox and <kbd>docker-machine</kbd> installed on our laptop. We can then use <kbd>docker-machine</kbd> to list all Docker hosts that are currently defined and may be running in VirtualBox:</p>
<pre><strong>$ docker-machine ls</strong><br/>NAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORS<br/>default - virtualbox Stopped Unknown</pre>
<p>In my case, I have one VM called <kbd>default</kbd> defined, which is currently stopped. I can easily start the VM by issuing the <kbd>docker-machine start default</kbd> command. This command takes a while and will result in the following (shortened) output:</p>
<pre><strong>$ docker-machine start default</strong><br/>Starting "default"...<br/>(default) Check network to re-create if needed...<br/>(default) Waiting for an IP...<br/>Machine "default" was started.<br/>Waiting for SSH to be available...<br/>Detecting the provisioner...<br/>Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.</pre>
<p>Now, if I list my VMs again, I should see the following screenshot:</p>
<div><img src="img/e64986fa-4614-4968-9d15-b62e7e873916.png"/></div>
<p>List of all VMs running in Hyper-V</p>
<p>If we do not have a VM called <kbd>default</kbd> yet, we can easily create one using the <kbd>create</kbd> command:</p>
<pre><strong>docker-machine create --driver virtualbox default</strong></pre>
<p>This results in the following output:</p>
<div><img src="img/9a6258ef-8613-41e5-95cb-b86bfe858b8e.png" style="width:44.25em;height:22.92em;"/></div>
<p>Output of docker-machine create</p>
<p>We can see in the preceding output how <kbd>docker-machine</kbd><strong> </strong>creates the VM from an ISO image, defines SSH keys and certificates, and copies them to the VM and to the local <kbd>~/.docker/machine</kbd> directory, where we will use it later when we want to remotely access this VM through the Docker CLI. It also provisions an IP address for the new VM.</p>
<p>We're using the <kbd>docker-machine create</kbd> command with the <kbd>--driver virtualbox</kbd> parameter. The docker-machine can also work with other drivers such as Hyper-V, AWS, Azure, DigitalOcean, and many more. Please see the documentation of <kbd>docker-machine</kbd> for more information. By default, a new VM gets 1 GB of memory associated, which is enough to use this VM as a node for a development or test Swarm.</p>
<p>If you're on Windows 10 with Docker for Desktop, use the <kbd>hyperv</kbd> driver instead. To be successful though, you need to run as Administrator. Furthermore, you need to have an external virtual switch defined on Hyper-V first. You can use the Hyper-V Manager to do so. The output of the command will look very similar to the one for the <kbd>virtualbox</kbd> driver.</p>
<p>Now, let's create five VMs for a five-node Swarm. We can use a bit of scripting to reduce the manual work:</p>
<pre>$ for NODE in `seq 1 5`; do<br/>  docker-machine create --driver virtualbox "node-${NODE}"<br/>done</pre>
<p>The <kbd>docker-machine</kbd> will now create five VMs with the names <kbd>node-1</kbd> to <kbd>node-5</kbd>. This might take a few moments, so this is a good time to get yourself a hot cup of tea. After the VMs are created, we can list them:</p>
<div><img src="img/2820423c-f815-4247-a15e-251dd7791c3d.png"/></div>
<p>List of all the VMs we need for the Swarm</p>
<p>Now, we're ready to build a Swarm. Technically, we could SSH into the first VM <kbd>node-1</kbd> and initialize a Swarm and then SSH into all the other VMs and join them to the Swarm leader. But this is not efficient. Let's again use a script that does all of the hard work:</p>
<pre># get IP of Swarm leader<br/>$ export IP=$(docker-machine ip node-1)<br/># init the Swarm<br/>$ docker-machine ssh node-1 docker swarm init --advertise-addr $IP<br/># Get the Swarm join-token<br/>$ export JOIN_TOKEN=$(docker-machine ssh node-1 \<br/>    docker swarm join-token worker -q)</pre>
<p>Now that we have the join token and the IP address of the Swarm leader, we can ask the other nodes to join the Swarm as follows:</p>
<pre>$ for NODE in `seq 2 5`; do<br/>  NODE_NAME="node-${NODE}"<br/>  docker-machine ssh $NODE_NAME docker swarm join \<br/>        --token $JOIN_TOKEN $IP:2377<br/>done</pre>
<p>To make the Swarm highly available, we can now promote, for example, <kbd>node-2</kbd> and <kbd>node-3 </kbd>to become managers:</p>
<pre><strong>$ docker-machine ssh node-1 docker node promote node-2 node-3</strong><br/>Node node-2 promoted to a manager in the swarm.<br/>Node node-3 promoted to a manager in the swarm.</pre>
<p>Finally, we can list all of the nodes of the Swarm:</p>
<pre><strong>$ docker-machine ssh node-1 docker node ls</strong></pre>
<p class="mce-root CDPAlignLeft CDPAlign">We should see the following:</p>
<div><img src="img/dfb95f3f-4cbd-4275-b620-f5a6b5493e88.png"/></div>
<p>List of all of the nodes of the Docker Swarm on VirtualBox</p>
<p>This is proof that we have just created a highly available Docker Swarm locally on our laptop or workstation. Let's pull all of our code snippets together and make the whole thing a bit more robust. The script will look as follows:</p>
<pre>alias dm="docker-machine"<br/>for NODE in `seq 1 5`; do<br/>  NODE_NAME=node-${NODE}<br/>  dm rm --force $NODE_NAME<br/>  dm create --driver virtualbox $NODE_NAME<br/>done<br/>alias dms="docker-machine ssh"<br/>export IP=$(docker-machine ip node-1)<br/>dms node-1 docker swarm init --advertise-addr $IP;<br/>export JOIN_TOKEN=$(dms node-1 docker swarm join-token worker -q);<br/>for NODE in `seq 2 5`; do<br/>  NODE_NAME="node-${NODE}"<br/>  dms $NODE_NAME docker swarm join --token $JOIN_TOKEN $IP:2377<br/>done;<br/>dms node-1 docker node promote node-2 node-3</pre>
<p>The preceding script first deletes (if present) and then recreates five VMs called <kbd>node-1</kbd> to <kbd>node-5</kbd>, and then initializes a Swarm on <kbd>node-1</kbd>. After that, the remaining four VMs are added to the Swarm, and finally, <kbd>node-2</kbd> and <kbd>node-3</kbd> are promoted to manager status to make the Swarm highly available. The whole script will take less than 5 minutes to execute and can be repeated as many times as desired. The complete script can be found in the repository, in the <kbd>docker-swarm</kbd> subfolder; it is called <kbd>create-swarm.sh</kbd>.</p>
<p>It is a highly recommended best practice to always script and hence automate operations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Play with Docker to generate a Swarm</h1>
                
            
            
                
<p>To experiment with Docker Swarm without having to install or configure anything locally on our computer, we can use <strong>Play with Docker</strong> (<strong>PWD</strong>). PWD is a website that can be accessed with a browser and that offers us the ability to create a Docker Swarm consisting of up to five nodes. It is definitely a playground, as the name implies, and the time for which we can use it is limited to four hours per session. We can open as many sessions as we want, but each session automatically ends after four hours. Other than that, it is a fully functional Docker environment that is ideal for tinkering with Docker or to demonstrate some features.</p>
<p>Let's access the site now. In your browser, navigate to the website <a href="https://labs.play-with-docker.com" target="_blank">https://labs.play-with-docker.com</a>. You will be presented with a welcome and login screen. Use your Docker ID to log in. After successfully going so, you will be presented with a screen that looks like the following screenshot:</p>
<div><img class="alignnone size-full wp-image-867 image-border" src="img/3172277e-9a56-44e6-8651-cfe1d23cf525.png" style="width:152.17em;height:57.17em;"/></div>
<p>Play with Docker window</p>
<p>As we can see immediately, there is a big timer counting down from four hours. That's how much time we have left to play in this session. Furthermore, we see a + ADD NEW INSTANCE link. Click it to create a new Docker host. When you do that, your screen should look like the following screenshot:</p>
<div><img src="img/07316581-3443-4328-ba3b-5c57af1e85c8.png"/></div>
<p>PWD with one new node</p>
<p>On the left-hand side, we see the newly created node with its IP address (<kbd>192.168.0.48</kbd>) and its name (<kbd>node1</kbd>). On the right-hand side, we have some additional information about this new node in the upper half of the screen and a Terminal in the lower half. Yes, this Terminal is used to execute commands on this node that we just created. This node has the Docker CLI installed, and hence we can execute all of the familiar Docker commands on it such as <kbd>docker version</kbd>. Try it out.</p>
<p>But now we want to create a Docker Swarm. Execute the following command in the Terminal in your browser:</p>
<pre><strong>$ docker swarm init --advertise-addr=eth0</strong></pre>
<p>The output generated by the preceding command corresponds to what we already know from our previous trials with the one-node cluster on our workstation and the local cluster using VirtualBox or Hyper-V. The important information, once again, is the <kbd>join</kbd> command that we want to use to join additional nodes to the cluster we just created.</p>
<p>You might have noted that this time we specified the <kbd>--advertise-addr</kbd> parameter in the Swarm <kbd>init</kbd> command. <em>Why is that necessary here?</em> The reason is that the nodes generated by PWD have more than one IP address associated with them. We can easily verify that by executing the <kbd>ip a</kbd> command on the node. This command will show us that there are indeed two endpoints, <kbd>eth0</kbd> and <kbd>eth1</kbd>, present. We hence have to specify explicitly to the new to-be swarm manager which one we want to use. In our case, it is <kbd>eth0</kbd>.</p>
<p>Create four additional nodes in PWD by clicking four times on the + ADD NEW INSTANCE link. The new nodes will be called <kbd>node2</kbd>, <kbd>node3</kbd>, <kbd>node4</kbd>, and <kbd>node5</kbd> and will all be listed on the left-hand side. If you click on one of the nodes on the left-hand side, then the right-hand side shows the details of the respective node and a Terminal window for that node.</p>
<p>Select each node (2 to 5) and execute the <kbd>docker swarm join</kbd> command that you have copied from the leader node (<kbd>node1</kbd>) in the respective Terminal:</p>
<div><img src="img/22724adb-5938-4487-a1f2-9d491800bb51.png"/></div>
<p>Joining a node to the Swarm in PWD</p>
<p>Once you have joined all four nodes to the Swarm, switch back to <kbd>node1</kbd> and list all nodes, which, unsurprisingly, results in this:</p>
<div><img src="img/e270d3d8-8783-4eec-9ba2-2a66f9a43a08.png"/></div>
<p>List of all of the nodes of the swarm in PWD</p>
<p>Still on <kbd>node1</kbd>, we can now promote, say, <kbd>node2</kbd> and <kbd>node3</kbd>, to make the Swarm highly available:</p>
<pre><strong>$ docker node promote node2 node3</strong><br/>Node node2 promoted to a manager in the swarm.<br/>Node node3 promoted to a manager in the swarm.</pre>
<p>With this, our Swarm on PWD is ready to accept a workload. We have created a highly available Docker Swarm with three manager nodes that form a Raft consensus group and two worker nodes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a Docker Swarm in the cloud</h1>
                
            
            
                
<p>All of the Docker Swarms we have created so far are wonderful to use in development or to experiment or to use for demonstration purposes. If we want to create a Swarm that can be used as a production environment where we run our mission-critical applications, though, then we need to create a—I'm tempted to say—real Swarm in the cloud or on premises. In this book, we are going to demonstrate how to create a Docker Swarm in AWS.</p>
<p>One way to create a Swarm is by using <strong>d</strong><strong>ocker-machine</strong> (<strong>DM</strong>). DM has a driver for AWS. If we have an account on AWS, we need the AWS access key ID and the AWS secret access key. We can add those two values to a file called <kbd>~/.aws/configuration</kbd>. It should look like the following:</p>
<pre>[default]<br/>aws_access_key_id = AKID1234567890<br/>aws_secret_access_key = MY-SECRET-KEY</pre>
<p>Every time we run <kbd>docker-machine create</kbd>, DM will look up those values in that file. For more in-depth information on how to get an AWS account and how to obtain the two secret keys, please consult this link: <a href="http://dockr.ly/2FFelyT" target="_blank">http://dockr.ly/2FFelyT</a>.</p>
<p>Once we have an AWS account in place and have stored the access keys in the configuration file, we can start building our Swarm. The necessary code looks exactly the same as the one we used to create a Swarm on our local machine in VirtualBox. Let's start with the first node:</p>
<pre><strong>$ docker-machine create --driver amazonec2 \</strong><br/><strong>    --amazonec2-region us-east-1 aws-node-1</strong></pre>
<p>This will create an EC2 instance called <kbd>aws-node-1</kbd> in the requested region (<kbd>us-east-1</kbd> in my case). The output of the preceding command looks like the following screenshot:</p>
<div><img class="alignnone size-full wp-image-872 image-border" src="img/ca6013e9-a419-478f-8967-e86ac6defaab.png" style="width:166.58em;height:52.83em;"/></div>
<p>Creating a swarm node on AWS with DM</p>
<p>It looks very similar to the output we already know from working with VirtualBox. We can now configure our Terminal for remote access to that EC2 instance:</p>
<pre><strong>$ eval $(docker-machine env aws-node-1)</strong></pre>
<p>This will configure the environment variables used by the Docker CLI accordingly:</p>
<div><img class="alignnone size-full wp-image-873 image-border" src="img/2776b997-08d5-4993-8f68-e047a054969e.png" style="width:35.83em;height:7.92em;"/></div>
<p>Environment variables used by Docker to enable remote access to the AWS EC2 node</p>
<p>For security reasons, <strong>Transport Layer Security</strong> (<strong>TLS</strong>) is used for the communication between our CLI and the remote node. The certificates necessary for that were copied by DM to the path we assigned to the environment variable <kbd>DOCKER_CERT_PATH</kbd>.</p>
<p>All Docker commands that we now execute in our Terminal will be remotely executed in AWS on our EC2 instance. Let's try to run Nginx on this node:</p>
<pre><strong>$ docker container run -d -p 8000:80 nginx:alpine</strong></pre>
<p>We can use <kbd>docker container ls</kbd> to verify that the container is running. If so, then let's test it using <kbd>curl</kbd>:</p>
<pre><strong>$ curl -4 &lt;IP address&gt;:8000</strong></pre>
<p>Here, <kbd>&lt;IP address&gt;</kbd> is the public IP address of the AWS node; in my case, it would be <kbd>35.172.240.127</kbd>. Sadly, this doesn't work; the preceding command times out:</p>
<div><img class="alignnone size-full wp-image-874 image-border" src="img/0f6eb2e9-c813-4c67-b0a8-3d4e623caa0d.png" style="width:48.42em;height:5.17em;"/></div>
<p>Accessing Nginx on the AWS node times out</p>
<p>The reason for this is that our node is part of an AWS <strong>Security Group</strong> (<strong>SG</strong>). By default, access to objects inside this SG is denied. Hence, we have to find out to which SG our instance belongs and configure access explicitly. For this, we typically use the AWS console. Go to the EC2 Dashboard and select Instances on the left-hand side. Locate the EC2 instance called <kbd>aws-node-1</kbd> and select it. In the details view, under Security groups, click on the docker-machine link, as shown in the following screenshot:</p>
<div><img class="alignnone size-full wp-image-886 image-border" src="img/daacdfa4-bfb7-4c54-8333-4f42a645ed2d.png" style="width:140.58em;height:73.58em;"/></div>
<p>Locating the SG to which our Swarm node belongs</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">This will lead us to the SG page with the <kbd>docker-machine</kbd> SG pre-selected. In the details section under the Inbound tab, add a new rule for your IP address (the IP address of workstation):</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft"><img style="font-size: 1em;width:166.58em;height:71.25em;" class="alignnone size-full wp-image-887 image-border" src="img/0d0ee48e-dd73-4b8e-87ee-e5069e77183b.png"/></p>
<p>Open access to SG for our computer</p>
<p>In the preceding screenshot, the IP address <kbd>70.113.114.234</kbd> happens to be the one assigned to my personal workstation. I have enabled all inbound traffic coming from this IP address to the <kbd>docker-machine</kbd> SG. Note that in a production system you should be very careful about which ports of the SG to open to the public. Usually, it is ports <kbd>80</kbd> and <kbd>443</kbd> for HTTP and HTTPS access. Everything else is a potential invitation to hackers.</p>
<p>You can get your own IP address through a service such as <a href="https://www.whatismyip.com/" target="_blank">https://www.whatismyip.com/</a>. Now, if we execute the <kbd>curl</kbd> command again, the greeting page of Nginx is returned.</p>
<p>Before we leave the SG, we should add another rule to it. The Swarm nodes need to be able to freely communicate on ports <kbd>7946</kbd> and <kbd>4789</kbd> through TCP and UDP and on port <kbd>2377</kbd> through TCP. We could now add five rules with these requirements where the source is the SG itself, or we just define a crude rule that allows all inbound traffic inside the SG (<kbd>sg-c14f4db3</kbd> in my case):</p>
<div><img class="alignnone size-full wp-image-888 image-border" src="img/a51e4f76-db2c-4513-8dc2-d878d5e0ba30.png" style="width:166.58em;height:28.08em;"/></div>
<p>SG rule to enable intra-Swarm communication</p>
<p>Now, let's continue with the creation of the remaining four nodes. Once again, we can use a script to ease the process:</p>
<pre><strong>$</strong> <strong>for NODE in `seq 2 5`; do</strong><br/><strong>    docker-machine create --driver amazonec2 \</strong><br/><strong>        --amazonec2-region us-east-1 aws-node-${NODE}</strong><br/><strong>done</strong></pre>
<p>After the provisioning of the nodes is done, we can list all nodes with DM. In my case, I see this:</p>
<div><img src="img/9fe33d9f-01b8-4fc8-9bf7-c24f2cfaa1a3.png" style="width:46.67em;height:14.42em;"/></div>
<p>List of all the nodes created by DM</p>
<p>In the preceding screenshot, we can see the five nodes that we originally created in VirtualBox and the five new nodes that we created in AWS. Apparently, the nodes on AWS are using a new version of Docker; here, the version is <kbd>18.02.0-ce</kbd>. The IP addresses we see in the <kbd>URL</kbd> column are the public IP addresses of my EC2 instances.</p>
<p>Because our CLI is still configured for remote access to the <kbd>aws-node-1</kbd> node, we can just run the <kbd>swarm init</kbd> command as follows:</p>
<pre><strong>$ docker swarm init</strong></pre>
<p>To get the join token do the following:</p>
<pre><strong>$ export JOIN_TOKEN=$(docker swarm join-token -q worker)</strong></pre>
<p>To get the IP address of the leader use the following command:</p>
<pre><strong>$ export LEADER_ADDR=$(docker node inspect \</strong><br/><strong>    --format "{{.ManagerStatus.Addr}}" self)</strong></pre>
<p>With this information, we can now join the other four nodes to the Swarm leader:</p>
<pre><strong>$ for NODE in `seq 2 5`; do</strong><br/><strong>  docker-machine ssh aws-node-${NODE} \</strong><br/><strong>    sudo docker swarm join --token ${JOIN_TOKEN} ${LEADER_ADDR}</strong><br/><strong>done</strong></pre>
<p>An alternative way to achieve the same without needing to SSH into the individual nodes would be to reconfigure our client CLI every time we want to access a different node:</p>
<pre><strong>$ for NODE in `seq 2 5`; do</strong><br/><strong>  eval $(docker-machine env aws-node-${NODE})</strong><br/><strong>  docker swarm join --token ${JOIN_TOKEN} ${LEADER_ADDR}</strong><br/><strong>done</strong></pre>
<p>As a last step, we want to promote nodes <kbd>2</kbd> and <kbd>3</kbd> to manager:</p>
<pre><strong>$ eval $(docker-machine env node-1)</strong><br/><strong>$</strong> <strong>docker node promote aws-node-2 aws-node-3</strong></pre>
<p>We can then list all of the Swarm nodes, as shown in the following screenshot:</p>
<div><img src="img/0038c9cb-fc2f-4a9d-ac15-af1133dbf6b8.png"/></div>
<p>List of all nodes of our swarm in the cloud</p>
<p>And hence we have a highly available Docker Swarm running in the cloud. To clean up the Swarm in the cloud and avoid incurring unnecessary costs, we can use the following command:</p>
<pre><strong>$</strong> <strong>for NODE in `seq 1 5`; do</strong><br/><strong>  docker-machine rm -f aws-node-${NODE}</strong><br/><strong>done</strong></pre>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying a first application</h1>
                
            
            
                
<p>We have created a few Docker Swarms on various platforms. Once created, a Swarm behaves the same way on any platform. The way we deploy and update applications on a Swarm is not platform-dependent. It has been one of Docker's main goals to avoid vendor lock-in when using a Swarm. Swarm-ready applications can be effortlessly migrated from, say, a Swarm running on premises to a cloud-based Swarm. It is even technically possible to run part of a Swarm on premises and another part in the cloud. It works, yet we have, of course, to consider possible side effects due to the higher latency between nodes in geographically distant areas.</p>
<p>Now that we have a highly available Docker Swarm up and running, it is time to run some workloads on it. I'm using a local Swarm created with docker-machine. We'll start by first creating a single service. For this, we need to SSH into one of the manager nodes. I select <kbd>node-1</kbd>:</p>
<pre style="padding-left: 30px"><strong>$ docker-machine ssh node-1</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a service</h1>
                
            
            
                
<p>A service can be either created as part of a stack or directly using the Docker CLI. Let's first look at a sample stack file that defines a single service:</p>
<pre style="padding-left: 30px">version: "3.7"<br/>services:<br/>  whoami:<br/>    image: training/whoami:latest<br/>    networks:<br/>      - test-net<br/>    ports:<br/>      - 81:8000<br/>    deploy:<br/>      replicas: 6<br/>      update_config:<br/>        parallelism: 2<br/>        delay: 10s<br/>      labels:<br/>        app: sample-app<br/>        environment: prod-south<br/><br/>networks:<br/>  test-net:<br/>    driver: overlay</pre>
<p>In the preceding example, we see what the desired state of a service called <kbd>whoami</kbd> is:</p>
<ul>
<li>It is based on the <kbd>training/whoami:latest</kbd> image.</li>
<li>Containers of the service are attached to the <kbd>test-net</kbd> network.</li>
<li>The container port <kbd>8000</kbd> is published to port <kbd>81</kbd>.</li>
<li>It is running with six replicas (or tasks)</li>
<li>During a rolling update, the individual tasks are updated in batches of two, with a delay of 10 seconds between each successful batch.</li>
<li>The service (and its tasks and containers) is assigned the two labels <kbd>app</kbd> and <kbd>environment </kbd>with the values <kbd>sample-app</kbd> and <kbd>prod-south</kbd>, respectively</li>
</ul>
<p>There are many more settings that we could define for a service, but the preceding ones are some of the more important ones. Most settings have meaningful default values. If, for example, we do not specify the number of replicas, then Docker defaults it to <kbd>1</kbd>. The name and image of a service are, of course, mandatory. Note that the name of the service must be unique in the Swarm.</p>
<p>To create the preceding service, we use the <kbd>docker stack deploy</kbd> command. Assuming that the file in which the preceding content is stored is called <kbd>stack.yaml</kbd>, we have the following:</p>
<pre style="padding-left: 30px"><strong>$ docker stack deploy -c stack.yaml sample-stack</strong></pre>
<p>Here, we have created a stack called <kbd>sample-stack</kbd> that consists of one service, <kbd>whoami</kbd>. We can list all stacks on our Swarm, whereupon we should get this:</p>
<pre style="padding-left: 30px"><strong>$ docker stack ls</strong><br/>NAME             SERVICES<br/>sample-stack     1</pre>
<p>If we list the services defined in our Swarm, we get the following output:</p>
<div><img src="img/2598bb40-3f60-44b3-8bb8-2d23d4e5a2df.png"/></div>
<p>List of all services running in the Swarm</p>
<p>In the output, we can see that currently, we have only one service running, which was to be expected. The service has an <kbd>ID</kbd>. The format of <kbd>ID</kbd>, contrary to what you have used so far for containers, networks, or volumes, is alphanumeric (in the latter cases it was always <kbd>sha256</kbd>). We can also see that <kbd>NAME</kbd> of the service is a combination of the service name we defined in the stack file and the name of the stack, which is used as a prefix. This makes sense since we want to be able to deploy multiple stacks (with different names) using the same stack file into our Swarm. To make sure that service names are unique, Docker decided to combine service name and stack name.</p>
<p>In the third column, we see the mode, which is <kbd>replicated</kbd>. The number of <kbd>REPLICAS</kbd> is shown as <kbd>6/6</kbd>. This tells us that six out of the six requested <kbd>REPLICAS</kbd> are running. This corresponds to the desired state. In the output, we also see the image that the service uses and the port mappings of the service.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Inspecting the service and its tasks</h1>
                
            
            
                
<p>In the preceding output, we cannot see the details of the <kbd>6</kbd> replicas that have been created. To get some deeper insight into that, we can use the <kbd>docker service ps</kbd> command. If we execute this command for our service, we will get the following output:</p>
<div><img class="alignnone size-full wp-image-890 image-border" src="img/407ffc09-b33c-459b-a73c-fb17de789245.png" style="width:166.58em;height:23.25em;"/></div>
<p>Details of the whoami service </p>
<p>In the preceding output, we can see the list of six tasks that correspond to the requested six replicas of our <kbd>whoami</kbd> service. In the <kbd>NODE</kbd> column, we can also see the node to which each task has been deployed. The name of each task is a combination of the service name plus an increasing index. Also note that, similar to the service itself, each task gets an alphanumeric ID assigned.</p>
<p>In my case, apparently task 2, with the name <kbd>sample-stack_whoami.2</kbd>, has been deployed to <kbd>node-1</kbd>, which is the leader of our Swarm. Hence, I should find a container running on this node. Let's see what we get if we list all containers running on <kbd>node-1</kbd>:</p>
<div><img class="alignnone size-full wp-image-892 image-border" src="img/18d77691-b017-4ff3-9ea0-d07b10cd107a.png" style="width:166.58em;height:10.75em;"/></div>
<p>List of containers on node-1</p>
<p>As expected, we find a container running from the <kbd>training/whoami:latest</kbd> image with a name that is a combination of its parent task name and ID. We can try to visualize the whole hierarchy of objects that we generated when deploying our sample stack:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/376c4e6d-f65b-4c82-b74b-02f320141812.png" style="width:19.67em;height:18.17em;"/></p>
<p>Object hierarchy of a Docker Swarm stack</p>
<p>A <strong>stack</strong> can consist of one to many services. Each service has a collection of tasks. Each task has a one-to-one association with a container. Stacks and services are created and stored on the Swarm manager nodes. Tasks are then scheduled to Swarm worker nodes, where the worker node creates the corresponding container. We can also get some more information about our service by inspecting it. Execute the following command:</p>
<pre><strong>$ docker service inspect sample-stack_whoami</strong></pre>
<p>This provides a wealth of information about all of the relevant settings of the service. This includes those we have explicitly defined in our <kbd>stack.yaml</kbd> file, but also those that we didn't specify and that therefore got their default values assigned. We're not going to list the whole output here, as it is too long, but I encourage the reader to inspect it on their own machine. We will discuss part of the information in more detail in the <em>The swarm routing mesh</em> section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Logs of a service</h1>
                
            
            
                
<p>In an earlier chapter, we worked with the logs produced by a container. Here, we're concentrating on a service. Remember that, ultimately, a service with many replicas has many containers running. Hence, we would expect that, if we ask the service for its logs, Docker returns an aggregate of all logs of those containers belonging to the service. And indeed, that's what we get if we use the <kbd>docker service logs</kbd> command:</p>
<div><img src="img/cbe0a89f-0434-42a8-ae44-c1abdf29515f.png" style="width:35.00em;height:10.75em;"/></div>
<p>Logs of the whoami service</p>
<p>There is not much information in the logs at this point, but it is enough to discuss what we get. The first part of each line in the log always contains the name of the container combined with the node name from which the log entry originates. Then, separated by the vertical bar (<kbd>|</kbd>), we get the actual log entry. So, if we would, say, ask for the logs of the first container in the list directly, we would only get a single entry, and the value we would see in this case would be <kbd>Listening on :8000</kbd>. </p>
<p>The aggregated logs that we get with the <kbd>docker service logs</kbd> command are not sorted in any particular way. So, if the correlation of events is happening in different containers, you should add information to your log output that makes this correlation possible. Typically, this is a timestamp for each log entry. But this has to be done at the source; for example, the application that produces a log entry needs to also make sure a timestamp is added.</p>
<p>We can as well query the logs of an individual task of the service by providing the task ID instead of the service ID or name. So, querying the logs from task 2 gives us the following output:</p>
<div><img src="img/edd0bb6b-3bfc-4cb8-97f1-ddab8836e1c6.png" style="width:36.75em;height:4.33em;"/></div>
<p>Logs of an individual task of the whoami service</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reconciling the desired state</h1>
                
            
            
                
<p>We have learned that a Swarm service is a description or manifest of the desired state that we want an application or application service to run in. Now, let's see how Docker Swarm reconciles this desired state if we do something that causes the actual state of the service to be different from the desired state. The easiest way to do this is to forcibly kill one of the tasks or containers of the service.</p>
<p>Let's do this with the container that has been scheduled on <kbd>node-1</kbd>:</p>
<pre><strong>$ docker container rm -f sample-stack_whoami.2.n21e7ktyvo4b2sufalk0aibzy</strong></pre>
<p>If we do that and then do <kbd>docker service ps</kbd> right afterward, we will see the following output:</p>
<div><img class="alignnone size-full wp-image-893 image-border" src="img/2b7a9616-8a86-467d-8e73-eafd14dd9f97.png" style="width:166.58em;height:24.17em;"/></div>
<p>Docker Swarm reconciling the desired state after one task failed</p>
<p>We see that task 2 failed with exit code <kbd>137</kbd> and that the Swarm immediately reconciled the desired state by rescheduling the failed task on a node with free resources. In this case, the scheduler selected the same node as the failed tasks, but this is not always the case. So, without us intervening, the Swarm completely fixed the problem, and since the service is running in multiple replicas, at no time was the service down.</p>
<p>Let's try another failure scenario. This time we're going to shut down an entire node and are going to see how the Swarm reacts. Let's take <kbd>node-2</kbd> for this, as it has two tasks (tasks 3 and 4) running on it. For this, we need to open a new Terminal window and use <kbd>docker-machine</kbd> to stop <kbd>node-2</kbd>:</p>
<pre><strong>$ docker-machine stop node-2</strong></pre>
<p>Back on <kbd>node-1</kbd>, we can now again run <kbd>docker service ps</kbd> to see what happened:</p>
<div><img class="alignnone size-full wp-image-894 image-border" src="img/e9b37aa0-d5c0-4666-8a11-f57874d4d283.png" style="width:166.58em;height:28.08em;"/></div>
<p>Swarm reschedules all tasks of a failed node</p>
<p>In the preceding screenshot, we can see that immediately task 3 was rescheduled on <kbd>node-1</kbd> while task 4 was rescheduled on <kbd>node-3</kbd>. Even this more radical failure is handled gracefully by Docker Swarm. </p>
<p>It is important to note though that if <kbd>node-2</kbd> ever comes back online in the Swarm, the tasks that had previously been running on it will not automatically be transferred back to it. But the node is now ready for a new workload.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deleting a service or a stack</h1>
                
            
            
                
<p>If we want to remove a particular service from the Swarm, we can use the <kbd>docker service rm</kbd> command. If, on the other hand, we want to remove a stack from the Swarm, we analogously use the <kbd>docker stack rm</kbd> command. This command removes all services that are part of the stack definition. In the case of the <kbd>whoami</kbd> service, it was created by using a stack file and hence we're going to use the latter command:</p>
<div><img src="img/61fca437-abee-41f3-8cea-6a8a534a58fc.png" style="width:27.42em;height:5.58em;"/></div>
<p>Removing a stack</p>
<p>The preceding command will make sure that all tasks of each service of the stack are terminated, and the corresponding containers are stopped by first sending <kbd>SIGTERM</kbd>, and then, if not successful, <kbd>SIGKILL</kbd> after 10 seconds of timeout.</p>
<p>It is important to note that the stopped containers are not removed from the Docker host. Hence, it is advised to purge containers from time to time on worker nodes to reclaim unused resources. Use <kbd>docker container purge -f</kbd> for this purpose.</p>
<p>Question: Why does it make sense to leave stopped or crashed containers on the worker node and not automatically remove them?</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying a multi-service stack</h1>
                
            
            
                
<p>In <a href="412c6f55-a00b-447f-b22a-47b305453507.xhtml" target="_blank">Chapter 11</a>, <em>Docker Compose</em>, we used an application consisting of two services that were declaratively described in a Docker compose file. We can use this compose file as a template to create a stack file that allows us to deploy the same application into a Swarm. The content of our stack file, called <kbd>pet-stack.yaml</kbd>, looks like this:</p>
<pre>version: "3.7"<br/>services:<br/> web:<br/>   image: fundamentalsofdocker/ch11-web:2.0<br/>   networks:<br/>   - pets-net<br/>   ports:<br/>   - 3000:3000<br/>   deploy:<br/>     replicas: 3<br/> db:<br/>   image: fundamentalsofdocker/ch11-db:2.0<br/>   networks:<br/>   - pets-net<br/>   volumes:<br/>   - pets-data:/var/lib/postgresql/data<br/><br/>volumes:<br/> pets-data:<br/><br/>networks:<br/> pets-net:<br/> driver: overlay</pre>
<p class="mceNonEditable"/>
<p>We request that the <kbd>web</kbd> service has three replicas, and both services are attached to the overlay network, <kbd>pets-net</kbd>. We can deploy this application using the <kbd>docker stack deploy</kbd> command:</p>
<div><img src="img/4920e7c3-dc61-4bcf-9960-cc64dcb42e3f.png" style="width:32.00em;height:7.00em;"/></div>
<p>Deploy the pets stack</p>
<p>Docker creates the <kbd>pets_pets-net</kbd> overlay network and then the two services <kbd>pets_web</kbd> and <kbd>pets_db</kbd>. We can then list all of the tasks in the <kbd>pets</kbd> stack:</p>
<div><img src="img/2af11ff2-9cb7-4056-aa02-409bce27fed2.png"/></div>
<p>List of all of the tasks in the pets stack</p>
<p>Finally, let's test the application using <kbd>curl</kbd>. And, indeed, the application works as expected:</p>
<div><img src="img/7e7fac0b-5781-4038-a0f8-8f53d933da3f.png"/></div>
<p>Testing the pets application using curl</p>
<p>The container ID is in the output, where it says <kbd>Delivered to you by container 8b906b509a7e</kbd>. If you run the <kbd>curl</kbd> command multiple times, the ID should cycle between three different values. These are the IDs of the three containers (or replicas) that we have requested for the <kbd>web</kbd> service.</p>
<p>Once we're done, we can remove the stack with <kbd>docker stack rm pets</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The swarm routing mesh</h1>
                
            
            
                
<p>If you have been paying attention, then you might have noticed something interesting in the last section. We had the <kbd>pets</kbd> application deployed and it resulted in the fact that an instance of the <kbd>web</kbd> service was installed on the three nodes, <kbd>node-1</kbd>, <kbd>node-2</kbd>, and <kbd>node-3</kbd>. Yet, we were able to access the <kbd>web</kbd> service on <kbd>node-1</kbd> with <kbd>localhost</kbd> and we reached each container from there. <em>How is that possible?</em> Well, this is due to the so-called Swarm routing mesh. The routing mesh makes sure that when we publish a port of a service; that port is then published on all nodes of the Swarm. Hence, network traffic that hits any node of the Swarm and requests to use a specific port will be forwarded to one of the service containers by the routing mesh. Let's look at the following diagram to see how that works:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-963 image-border" src="img/6ceca300-992c-4267-a309-239315df3cd9.png" style="width:38.50em;height:25.42em;"/></p>
<p>Docker Swarm routing mesh</p>
<p>In this situation, we have three nodes, called <strong>Host A</strong> to <strong>Host C</strong>, with the IP addresses <kbd>172.10.0.15</kbd>, <kbd>172.10.0.17</kbd>, and <kbd>172.10.0.33</kbd>. In the lower-left corner of the diagram, we see the command that created a <strong>web</strong> service with two replicas. The corresponding tasks have been scheduled on <strong>Host B</strong> and <strong>Host C</strong>. Task 1 landed on <strong>Host B</strong> while task 2 landed on <strong>Host C</strong>.</p>
<p>When a service is created on Docker Swarm, it automatically gets a <strong>V</strong><strong>irtual IP</strong> (<strong>VIP</strong>) address assigned. This IP address is stable and reserved during the whole life cycle of the service. Let's assume that in our case the VIP is <kbd>10.2.0.1</kbd>.</p>
<p>If now a request for port <kbd>8080</kbd> coming from an external <strong>Load Balancer</strong> (<strong>LB</strong>) is targeted at one of the nodes of our Swarm, then this request is handled by the Linux <strong>IP Virtual Server</strong> (<strong>IPVS</strong>) service on that node. This service makes a lookup with the given port <kbd>8080</kbd> in the IP table and will find that this corresponds to the VIP of the <strong>web</strong> service. Now, since the VIP is not a real target, the IPVS service will load balance the IP addresses of the tasks that are associated with this service. In our case, it picked task 2, with the IP address, <kbd>10.2.0.3</kbd>. Finally, the <strong>ingress</strong><strong> Network</strong> (<strong>O</strong><strong>verlay</strong>) is used to forward the request to the target container on <strong>Host C</strong>.</p>
<p>It is important to note that it doesn't matter which Swarm node the external request is forwarded to by the <strong>External LB</strong>. The routing mesh will always handle the request correctly and forward it to one of the tasks of the targeted service.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we introduced Docker Swarm, which, next to Kubernetes, is the second most popular orchestrator for containers. We have looked into the architecture of a Swarm, discussed all of the types of resources running in a Swarm, such as services, tasks, and more, and we have created services in the Swarm and deployed an application that consists of multiple related services.</p>
<p>In the next chapter, we are going to explore how to deploy services or applications onto a Docker Swarm with zero downtime and automatic rollback capabilities. We are also going to introduce secrets as a means to protect sensitive information.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Questions</h1>
                
            
            
                
<p>To assess your learning progress, please answer the following questions:</p>
<ol>
<li style="font-weight: 400">How do you initialize a new Docker Swarm?<br/>
A. <kbd>docker init swarm</kbd><br/>
B. <kbd>docker swarm init --advertise-addr &lt;IP address&gt;</kbd><br/>
C. <kbd>docker swarm join --token &lt;join token&gt;</kbd></li>
</ol>
<ol start="2">
<li style="font-weight: 400">You want to remove a worker node from a Docker Swarm. What steps are necessary?</li>
<li style="font-weight: 400">How do you create an overlay network called <kbd>front-tier</kbd>? Make the network attachable.</li>
<li style="font-weight: 400">How would you create a service called <kbd>web</kbd> from the <kbd>nginx:alpine</kbd> image with five replicas, which exposes port <kbd>3000</kbd> on the ingress network and is attached to the <kbd>front-tier</kbd> network?</li>
<li style="font-weight: 400">How would you scale the web service down to three instances?</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Further reading</h1>
                
            
            
                
<p>Please consult the following link for more in-depth information about selected topics:</p>
<ul>
<li>AWS EC2 example at <a href="http://dockr.ly/2FFelyT" target="_blank">http://dockr.ly/2FFelyT</a></li>
<li>The Raft Consensus Algorithm at <a href="https://raft.github.io/" target="_blank">https://raft.github.io/</a></li>
<li>The Gossip Protocol at <a href="https://en.wikipedia.org/wiki/Gossip_protocol" target="_blank">https://en.wikipedia.org/wiki/Gossip_protocol</a></li>
<li>VXLAN and Linux at <a href="https://vincent.bernat.ch/en/blog/2017-vxlan-linux" target="_blank">https://vincent.bernat.ch/en/blog/2017-vxlan-linux</a></li>
</ul>


            

            
        
    </body></html>