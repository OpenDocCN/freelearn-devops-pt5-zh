<html><head></head><body>
		<div><h1 id="_idParaDest-176"><em class="italic"><a id="_idTextAnchor186"/>Chapter 8</em>: Pod Placement Controls</h1>
			<p>This chapter describes the various ways of controlling Pod placement in Kubernetes, as well as explaining why it may be a good idea to implement these controls in the first place. Pod placement means controlling which node a Pod is scheduled to in Kubernetes. We start with simple controls like node selectors, and then move on to more complex tools like taints and tolerations, and finish with two beta features, node affinity and inter-Pod affinity/anti-affinity.</p>
			<p>In past chapters, we've learned how best to run application Pods on Kubernetes – from coordinating and scaling them using deployments, injecting configuration with ConfigMaps and Secrets, to adding storage with persistent volumes.</p>
			<p>Throughout all of this, however, we have always relied on the Kubernetes scheduler to put Pods on the optimal node without giving the scheduler much information about the Pods in question. So far, we've added resource limits and requests to our Pods (<code>resource.requests</code> and <code>resource.limits</code> in the Pod spec). Resource requests specify a minimum level of free resources on a node that the Pod needs in order to be scheduled, while resource limits specify the maximum amount of resources a Pod is allowed to use. However, we have not put any specific requirements on which nodes or set of nodes a Pod must be run.</p>
			<p>For many applications and clusters, this is fine. However, as we'll see in the first section, there are many cases where using more granular Pod placement controls is a useful strategy.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Identifying use cases for Pod placement</li>
				<li>Using node selectors</li>
				<li>Implementing taints and tolerations </li>
				<li>Controlling Pods with node affinity</li>
				<li>Using inter-Pod affinity and anti-affinity</li>
			</ul>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor187"/>Technical requirements</h1>
			<p>In order to run the commands detailed in this chapter, you will need a computer that supports the <code>kubectl</code> command-line tool along with a working Kubernetes cluster. See <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>, for several methods for getting up and running with Kubernetes quickly, and for instructions on how to install the <code>kubectl</code> tool.</p>
			<p>The code used in this chapter can be found in the book's GitHub repository at <a href="https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter8">https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter8</a>.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor188"/>Identifying use cases for Pod placement</h1>
			<p>Pod placement<a id="_idIndexMarker364"/> controls are tools that Kubernetes gives us to decide which node to schedule a Pod on, or when to completely prevent Pod scheduling due to a lack of the nodes we want. This can be used in several different patterns, but we'll review a few major ones. To start with, Kubernetes itself implements Pod placement controls completely by default – let's see how.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor189"/>Kubernetes node health placement controls</h2>
			<p>Kubernetes uses a<a id="_idIndexMarker365"/> few default placement controls to specify which nodes are unhealthy in some way. These are generally defined using taints and tolerations, which we will review in detail later in this chapter.</p>
			<p>Some default taints (which we'll discuss in the next section) that Kubernetes uses are as follows:</p>
			<ul>
				<li><code>memory-pressure</code></li>
				<li><code>disk-pressure</code></li>
				<li><code>unreachable</code></li>
				<li><code>not-ready</code></li>
				<li><code>out-of-disk</code></li>
				<li><code>network-unavailable</code></li>
				<li><code>unschedulable</code></li>
				<li><code>uninitialized</code> (only for cloud-provider-created nodes)</li>
			</ul>
			<p>These conditions can mark nodes as unable to receive new Pods, though there is some flexibility in how these taints are handled by the scheduler, as we will see later. The purpose of these system-created placement controls is to prevent unhealthy nodes from receiving <a id="_idIndexMarker366"/>workloads that may not function properly.</p>
			<p>In addition to system-created placement controls for node health, there are several use cases where you, as a user, may want to implement fine-tuned scheduling, as we will see in the next section.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor190"/>Applications requiring different node types</h2>
			<p>In a heterogeneous<a id="_idIndexMarker367"/> Kubernetes cluster, every node is not created equal. You may have some more powerful VMs (or bare metal) and some less – or have different specialized sets of nodes.</p>
			<p>For instance, in a cluster that runs data science pipelines, you may have nodes with GPU acceleration capabilities to run deep learning algorithms, regular compute nodes to serve applications, nodes with high amounts of memory to do inference based on completed models, and more.</p>
			<p>Using Pod placement controls, you can ensure that the various pieces of your platform run on the hardware best suited for the task at hand.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor191"/>Applications requiring specific data compliance</h2>
			<p>Similar to the previous <a id="_idIndexMarker368"/>example, where application requirements may dictate the need for different types of compute, certain data compliance needs may require specific types of nodes. </p>
			<p>For instance, cloud providers such as AWS and Azure often allow you to purchase VMs with dedicated tenancy – which means that no other applications run on the underlying hardware and hypervisor. This is different from other typical cloud-provider VMs, where multiple customers may share a single physical machine.</p>
			<p>For certain data regulations, this level of dedicated tenancy is required to maintain compliance. To fulfill this need, you could use Pod placement controls to ensure that the relevant applications<a id="_idIndexMarker369"/> only run on nodes with dedicated tenancy, while reducing costs by running the control plane on more typical VMs without it.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor192"/>Multi-tenant clusters</h2>
			<p>If you are running a <a id="_idIndexMarker370"/>cluster with multiple tenants (separated by namespaces, for instance), you could use Pod placement controls to reserve certain nodes or groups of nodes for a tenant, to physically or otherwise separate them from other tenants in the cluster. This is similar to the concept of dedicated hardware in AWS or Azure.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor193"/>Multiple failure domains</h2>
			<p>Though Kubernetes<a id="_idIndexMarker371"/> already provides high availability by allowing you to schedule workloads that run on multiple nodes, it is also possible to extend this pattern. We can create our own Pod scheduling strategies that account for failure domains that stretch across multiple nodes. A great way to handle this is via the Pod or node affinity or anti-affinity features, which we will discuss later in this chapter.</p>
			<p>For now, let's conceptualize a case where we have our cluster on bare metal with 20 nodes per physical rack. If each rack has its own dedicated power connection and backup, it can be thought of as a failure domain. When the power connections fail, all the machines on the rack fail. Thus, we may want to encourage Kubernetes to run two instances or Pods on separate racks/failure domains. The following figure shows how an application could run across failure domains:</p>
			<div><div><img src="img/B14790_08_001.jpg" alt="Figure 8.1 – Failure domains"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Failure domains</p>
			<p>As you can see in the figure, as the application pods are spread across multiple failure domains, not just multiple nodes in the same failure domain, we can maintain uptime even if <em class="italic">Failure Domain 1</em> goes down. <em class="italic">App A - Pod 1</em> and <em class="italic">App B - Pod 1</em> are in the same (red) failure domain. However, if that failure domain (<em class="italic">Rack 1</em>) goes down, we will still have a replica of each application on <em class="italic">Rack 2</em>.</p>
			<p>We use the word "encourage" here because it is possible to configure some of this functionality as<a id="_idIndexMarker372"/> either a hard requirement or on a best effort basis in the Kubernetes scheduler.</p>
			<p>These examples should give you a solid understanding of some potential use cases for advanced placement controls.</p>
			<p>Let's discuss the actual implementation now, taking each placement toolset one by one. We'll start with the simplest, node selectors.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor194"/>Using node selectors and node name</h1>
			<p>Node selectors<a id="_idIndexMarker373"/> are a<a id="_idIndexMarker374"/> very simple type of placement control in Kubernetes. Each Kubernetes node can be labeled with one or more labels in the metadata block, and Pods can specify a node selector.</p>
			<p>To label an existing node, you can use the <code>kubectl label</code> command:</p>
			<pre>&gt; kubectl label nodes node1 cpu_speed=fast</pre>
			<p>In this example, we're labeling our <code>node1</code> node with the label <code>cpu_speed</code> and the value <code>fast</code>.</p>
			<p>Now, let's assume that we have an application that really needs fast CPU cycles to perform effectively. We can add a <code>nodeSelector</code> to our workload to ensure that it is only scheduled <a id="_idIndexMarker375"/>on nodes with our fast CPU speed label, as shown in the following<a id="_idIndexMarker376"/> code snippet:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-node-selector.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: speedy-app
spec:
  containers:
  - name: speedy-app
    image: speedy-app:latest
    imagePullPolicy: IfNotPresent
  nodeSelector:
    cpu_speed: fast</pre>
			<p>When deployed, as part of a Deployment or by itself, our <code>speedy-app</code> Pod will only be scheduled on nodes with the <code>cpu_speed</code> label.</p>
			<p>Keep in mind that unlike some other more advanced Pod placement options that we will review shortly, there is no leeway in node selectors. If there are no nodes that have the required label, the application will not be scheduled at all.</p>
			<p>For an even simpler (but far more brittle) selector, you can use <code>nodeName</code>, which specifies the exact node that the Pod should be scheduled on. You can use it like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-node-name.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: speedy-app
spec:
  containers:
  - name: speedy-app
    image: speedy-app:latest
    imagePullPolicy: IfNotPresent
  nodeName: node1</pre>
			<p>As you can see, this selector will only allow the Pod to be scheduled on <code>node1</code>, so if it isn't currently <a id="_idIndexMarker377"/>accepting Pods for any reason, the Pod will not be<a id="_idIndexMarker378"/> scheduled.</p>
			<p>For slightly more nuanced placement control, let's move on to taints and tolerations.</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor195"/>Implementing taints and tolerations</h1>
			<p>Taints and tolerations<a id="_idIndexMarker379"/> in <a id="_idIndexMarker380"/>Kubernetes work like reverse node selectors. Rather than nodes attracting Pods due to having the proper labels, which are then consumed by a selector, we taint nodes, which repels all Pods from being scheduled on the node, and then mark our Pods with tolerations, which allow them to be scheduled on the tainted nodes.</p>
			<p>As mentioned at the beginning of the chapter, Kubernetes uses system-created taints to mark nodes as unhealthy and prevent new workloads from being scheduled on them. For instance, the <code>out-of-disk</code> taint will prevent any new pods from being scheduled to a node with that taint.</p>
			<p>Let's take the same example use case that we had with node selectors and apply it using taints and tolerations. Since this is basically the reverse of our previous setup, let's first give our node a taint using the <code>kubectl taint</code> command:</p>
			<pre>&gt; kubectl taint nodes node2 cpu_speed=slow:NoSchedule</pre>
			<p>Let's pick apart this command. We are giving <code>node2</code> a taint called <code>cpu_speed</code> and a value, <code>slow</code>. We also mark this taint with an effect – in this case, <code>NoSchedule</code>. </p>
			<p>Once we're done with our example (don't do this quite yet if you're following along with the commands), we can remove the <code>taint</code> using the minus operator:</p>
			<pre>&gt; kubectl taint nodes node2 cpu_speed=slow:NoSchedule-</pre>
			<p>The <code>taint</code> effect lets us add in some granularity into how the scheduler handles the taints. There are three possible effect values:</p>
			<ul>
				<li><code>NoSchedule</code></li>
				<li><code>NoExecute</code></li>
				<li><code>PreferNoSchedule</code></li>
			</ul>
			<p>The first two effects, <code>NoSchedule</code> and <code>NoExecute</code>, provide hard effects – which is to say that, like node selectors, there are only two possibilities, either the toleration exists on the<a id="_idIndexMarker381"/> Pod (as we'll see momentarily) or the Pod is not <a id="_idIndexMarker382"/>scheduled. <code>NoExecute</code> adds to this base functionality by evicting all Pods on the node that do have the toleration, while <code>NoSchedule</code> lets existing pods stay put, while preventing any new Pods without the toleration from joining.</p>
			<p><code>PreferNoSchedule</code>, on the other hand, provides the Kubernetes scheduler with some leeway. It tells the scheduler to attempt to find a node for a Pod that doesn't have an untolerated taint, but if none exist, to go ahead and schedule it anyway. It implements a soft effect.</p>
			<p>In our case, we have chosen <code>NoSchedule</code>, so no new Pods will be assigned to the node – unless, of course, we provide a toleration. Let's do this now. Assume that we have a second application that doesn't care about CPU clock speeds. It is happy to live on our slower node. This is the Pod manifest:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-without-speed-requirement.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest</pre>
			<p>Right now, our <code>slow-app</code> Pod will not run on any node with a taint. We need to provide a toleration for this Pod in order for it to be scheduled on a node with a taint – which we can <a id="_idIndexMarker383"/>do<a id="_idIndexMarker384"/> like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-toleration.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
tolerations:
- key: "cpu_speed"
  operator: "Equal"
  value: "slow"
  effect: "NoSchedule"</pre>
			<p>Let's pick apart our <code>tolerations</code> entry, which is an array of values. Each value has a <code>key</code> – which is the same as our taint name. Then there is an <code>operator</code> value. This <code>operator</code> can be either <code>Equal</code> or <code>Exists</code>. For <code>Equal</code>, you can use the <code>value</code> key as in the preceding code to configure a value that the taint must equal in order to be tolerated by the Pod. For <code>Exists</code>, the taint name must be on the node, but it does not matter what the value is, as in this Pod spec:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-toleration2.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
tolerations:
- key: "cpu_speed"
  operator: "Exists"
  effect: "NoSchedule"</pre>
			<p>As you can<a id="_idIndexMarker385"/> see, we have used the <code>Exists</code> <code>operator</code> value to allow our <a id="_idIndexMarker386"/>Pod to tolerate any <code>cpu_speed</code> taint.</p>
			<p>Finally, we have our <code>effect</code>, which works the same way as the <code>effect</code> on the taint itself. It can contain the exact same values as the taint effect – <code>NoSchedule</code>, <code>NoExecute</code>, and <code>PreferNoSchedule</code>.</p>
			<p>A Pod with a <code>NoExecute</code> toleration will tolerate the taint associated with it indefinitely. However, you can add a field called <code>tolerationSeconds</code> in order to have the Pod leave the tainted node after a prescribed time has elapsed. This allows you to specify tolerations that take effect after a period of time. Let's look at an example:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-toleration3.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: slow-app
spec:
  containers:
  - name: slow-app
    image: slow-app:latest
tolerations:
- key: "cpu_speed"
  operator: "Equal"
  Value: "slow"
  effect: "NoExecute"
  tolerationSeconds: 60</pre>
			<p>In this case, the <a id="_idIndexMarker387"/>Pod already running on a node with the taint <code>slow</code> when the <a id="_idIndexMarker388"/>taint and toleration are executed will remain on the node for <code>60</code> seconds before being rescheduled to a different node.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor196"/>Multiple taints and tolerations</h2>
			<p>When there are <a id="_idIndexMarker389"/>multiple taints or tolerations on a Pod and node, the scheduler will <a id="_idIndexMarker390"/>check all of them. There is no <code>OR</code> logic operator here – if any of the taints on the node do not have a matching toleration on the Pod, it will not be scheduled on the node (with the exception of <code>PreferNoSchedule</code>, in which case, as before, the scheduler will try to not schedule on the node if possible). Even if out of six taints on the node, the Pod tolerates five of them, it will still not be scheduled for a <code>NoSchedule</code> taint, and it will still be evicted for a <code>NoExecute</code> taint. </p>
			<p>For a tool that gives us a much more subtle way of controlling placement, let's look at node affinity.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor197"/>Controlling Pods with node affinity</h1>
			<p>As you can <a id="_idIndexMarker391"/>probably tell, taints and tolerations – while <a id="_idIndexMarker392"/>much more flexible than node selectors – still leave some use cases unaddressed and in general only allow a <em class="italic">filter</em> pattern where you can match on a specific taint using <code>Exists</code> or <code>Equals</code>. There may be more advanced use cases where you want more flexible methods of selecting nodes – and <em class="italic">affinities</em> are a feature of Kubernetes that addresses this.</p>
			<p>There are two types of affinity:</p>
			<ul>
				<li><strong class="bold">Node affinity</strong></li>
				<li><strong class="bold">Inter-Pod affinity</strong></li>
			</ul>
			<p>Node affinity is a similar concept to node selectors except that it allows for a much more robust set of selection characteristics. Let's look at some example YAML and then pick apart the various pieces:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-node-affinity.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: In
            values:
            - fast
            - medium_fast
  containers:
  - name: speedy-app
    image: speedy-app:latest</pre>
			<p>As you can see, our <code>Pod</code> <code>spec</code> has an <code>affinity</code> key, and we've specified a <code>nodeAffinity</code> setting. There<a id="_idIndexMarker393"/> are two possible node affinity<a id="_idIndexMarker394"/> types:</p>
			<ul>
				<li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li>
				<li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li>
			</ul>
			<p>The functionality of these two types maps directly to how <code>NoSchedule</code> and <code>PreferNoSchedule</code> work, respectively. </p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor198"/>Using requiredDuringSchedulingIgnoredDuringExecution node affinities</h2>
			<p>For <code>requiredDuringSchedulingIgnoredDuringExecution</code>, Kubernetes will never <a id="_idIndexMarker395"/>schedule a Pod <a id="_idIndexMarker396"/>without a term matching to a node. </p>
			<p>For <code>preferredDuringSchedulingIgnoredDuringExecution</code>, it will attempt to fulfill the soft requirement but if it cannot, it will still schedule the Pod.</p>
			<p>The real capability of node affinity over node selectors and taints and tolerations comes in the actual expressions and logic that you can implement when it comes to the selector.</p>
			<p>The functionalities of the <code>requiredDuringSchedulingIgnoredDuringExecution</code> and <code>preferredDuringSchedulingIgnoredDuringExecution</code> affinities are quite different, so we will review each separately.</p>
			<p>For our <code>required</code> affinity, we have the ability to specify <code>nodeSelectorTerms</code> – which can be one or more blocks containing <code>matchExpressions</code>. For each block of <code>matchExpressions</code>, there can be multiple expressions.</p>
			<p>In the code block we saw in the previous section, we have one single node selector term, a <code>matchExpressions</code> block – which itself has only a single expression. This expression looks for <code>key</code>, which, just like with node selectors, represents a node label. Next, it has an <code>operator</code>, which gives us some flexibility on how we want to identify a match. Here are the possible values for the operator:</p>
			<ul>
				<li><code>In</code></li>
				<li><code>NotIn</code></li>
				<li><code>Exists</code></li>
				<li><code>DoesNotExist</code></li>
				<li><code>Gt</code> (Note: greater than)</li>
				<li><code>Lt</code> (Note: less than)</li>
			</ul>
			<p>In our case, we <a id="_idIndexMarker397"/>are using the <code>In</code> operator, which will <a id="_idIndexMarker398"/>check to see if the value is one of several that we specify. Finally, in our <code>values</code> section, we can list one or more values that must match, based on the operator, before the expression is true.</p>
			<p>As you can see, this gives us significantly greater granularity in specifying our selector. Let's look at our example of <code>cpu_speed</code> using a different operator:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-node-affinity2.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "5"
  containers:
  - name: speedy-app
    image: speedy-app:latest</pre>
			<p>As you can see, we<a id="_idIndexMarker399"/> are using a very <a id="_idIndexMarker400"/>granular <code>matchExpressions</code> selector. This ability to use more advanced operator matching now allows us to ensure that our <code>speedy-app</code> is only scheduled on nodes that have a high enough clock speed (in this case, 5 GHz). Instead of classifying our nodes into broad groups like <code>slow</code> and <code>fast</code>, we can be much more granular in our specifications.</p>
			<p>Next, let's look at the other node affinity type –<code>preferredDuringSchedulingIgnoredDuringExecution</code>.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor199"/>Using preferredDuringSchedulingIgnoredDuringExecution node affinities</h2>
			<p>The <a id="_idIndexMarker401"/>syntax<a id="_idIndexMarker402"/> for this is slightly different and gives us even more granularity to affect this <code>soft</code> requirement. Let's look at a Pod spec YAML that implements this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-node-affinity3.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: slow-app-affinity
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: cpu_speed
            operator: Lt
            values:
            - "3"
  containers:
  - name: slow-app
    image: slow-app:latest</pre>
			<p>This<a id="_idIndexMarker403"/> looks<a id="_idIndexMarker404"/> a bit different from our <code>required</code> syntax. </p>
			<p>For <code>preferredDuringSchedulingIgnoredDuringExecution</code>, we have the ability to assign a <code>weight</code> to each entry, with an associated preference, which can again be a <code>matchExpressions</code> block with multiple inner expressions that use the same <code>key-operator-values</code> syntax.</p>
			<p>The <code>weight</code> value is the key difference here. Since <code>preferredDuringSchedulingIgnoredDuringExecution</code> is a <code>speedy-app</code> use case:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-node-affinity4.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: speedy-app-prefers-affinity
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 90
        preference:
          matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "3"
      - weight: 10
        preference:
          matchExpressions:
          - key: memory_speed
            operator: Gt
            values:
            - "4"
  containers:
  - name: speedy-app
    image: speedy-app:latest</pre>
			<p>In our journey to ensure that our <code>speedy-app</code> runs on the best possible node, we have here decided to only implement <code>soft</code> requirements. If no fast nodes exist, we still want our app to be scheduled and run. To that end, we've specified two preferences – a node with a <code>cpu_speed</code> of over 3 (3 GHz) and a memory speed of over 4 (4 GHz). </p>
			<p>Since<a id="_idIndexMarker407"/> our<a id="_idIndexMarker408"/> app is far more CPU-bound than memory-bound, we've decided to weight our preferences appropriately. In this case, <code>cpu_speed</code> carries a <code>weight</code> of <code>90</code>, while <code>memory_speed</code> carries a <code>weight</code> of <code>10</code>. </p>
			<p>Thus, any node that satisfies our <code>cpu_speed</code> requirement will have a much higher computed score than one that only satisfies the <code>memory_speed</code> requirement – but still less than one that satisfies both. When we're trying to schedule 10 or 100 new Pods for this app, you can see how this calculation could be valuable.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor200"/>Multiple node affinities</h2>
			<p>When we're dealing<a id="_idIndexMarker409"/> with multiple node affinities, there are a few key pieces of logic to keep in mind. First off, even with a single node affinity, if it is combined with a node selector on the same Pod spec (which is indeed possible), the node selector must be satisfied before any of the node affinity logic will come into play. This is because node selectors only implement hard requirements, and there is no <code>OR</code> logical operator between the two. An <code>OR</code> logical operator would check both requirements and ensure that at least one of them is true – but node selectors do not let us do this.</p>
			<p>Secondly, for a <code>requiredDuringSchedulingIgnoredDuringExecution</code> node affinity, multiple entries under <code>nodeSelectorTerms</code> are handled in an <code>OR</code> logical operator. If one, but not all, is satisfied – the Pod will still be scheduled.</p>
			<p>Finally, for any <code>nodeSelectorTerm</code> with multiple entries under <code>matchExpressions</code>, all must be satisfied – this is an <code>AND</code> logical operator. Let's look at an example YAML <a id="_idIndexMarker410"/>of this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-node-affinity5.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: affinity-test
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cpu_speed
            operator: Gt
            values:
            - "5"
          - key: memory_speed
            operator: Gt
            values:
            - "4"
  containers:
  - name: speedy-app
    image: speedy-app:latest</pre>
			<p>In this case, if a node has a CPU speed of <code>5</code> but does not meet the memory speed requirement (or vice versa), the Pod will not be scheduled.</p>
			<p>One final thing to note about node affinity is that, as you've probably already noticed, neither of the two affinity types allows the same <code>NoExecute</code> functionality that was available to us in our taints and tolerations settings.</p>
			<p>One additional node affinity type – <code>requiredDuringSchedulingRequiredDuring execution</code> – will add this functionality in a <a id="_idIndexMarker411"/>future version. As of Kubernetes 1.19, this does not yet exist.</p>
			<p>Next, we will look at inter-pod affinity and anti-affinity, which provides affinity definitions between Pods, rather than defining rules for nodes.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor201"/>Using inter-Pod affinity and anti-affinity</h1>
			<p>Inter-Pod affinity <a id="_idIndexMarker412"/>and anti-affinity let you dictate how Pods should run based <a id="_idIndexMarker413"/>on which other Pods already exist on a node. Since the number of Pods in a cluster is typically much larger than the number of nodes, and some Pod affinity and anti-affinity rules can be somewhat complex, this feature can put quite a load on your cluster control plane if you are running many pods on many nodes. For this reason, the Kubernetes documentation does not recommend using these features with a large number of nodes in your cluster.</p>
			<p>Pod affinities and anti-affinities work fairly differently – let's look at each by itself before discussing how they can be combined.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor202"/>Pod affinities</h2>
			<p>As with node <a id="_idIndexMarker414"/>affinities, let's dive into the YAML in order to discuss the constituent parts of a Pod affinity spec:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-pod-affinity.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: not-hungry-app-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: hunger
            operator: In
            values:
            - "1"
            - "2"
        topologyKey: rack
  containers:
  - name: not-hungry-app
    image: not-hungry-app:latest</pre>
			<p>Just like with node affinity, Pod affinity lets us choose between two types:</p>
			<ul>
				<li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li>
				<li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li>
			</ul>
			<p>Again, similar to node affinity, we can have one or more selectors – which are called <code>labelSelector</code> since we are selecting Pods, not nodes. The <code>matchExpressions</code> functionality is the same as with node affinity, but Pod affinity adds a brand-new key called <code>topologyKey</code>.</p>
			<p><code>topologyKey</code> is in essence a selector that limits the scope of where the scheduler should look to see whether other Pods of the same selector are running. That means that Pod affinity doesn't only need to mean other Pods of the same type (selector) on the same node; it can mean groups of multiple nodes.</p>
			<p>Let's go back to our failure domain example at the beginning of the chapter. In that example, each rack was its own failure domain with multiple nodes per rack. To extend this concept to <code>topologyKey</code>, we could label each node on a rack with <code>rack=1</code> or <code>rack=2</code>. Then we can use the <code>topologyKey</code> rack, as we have in our YAML, to designate that the<a id="_idIndexMarker415"/> scheduler should check all of the Pods running on nodes with the same <code>topologyKey</code> (which in this case means all of the Pods on <code>Node 1</code> and <code>Node 2</code> in the same rack) in order to apply Pod affinity or anti-affinity rules.</p>
			<p>So, adding this all up, what our example YAML tells the scheduler is this: </p>
			<ul>
				<li>This Pod <em class="italic">MUST</em> be scheduled on a node with the label <code>rack</code>, where the value of the label <code>rack</code> separates nodes into groups.</li>
				<li>The Pod will then be scheduled in a group where there already exists a Pod running with the label <code>hunger</code> and a value of 1 or 2.</li>
			</ul>
			<p>Essentially, we are splitting our cluster into topology domains – in this case, racks – and prescribing to the scheduler to only schedule similar pods together on nodes that share the same topology domain. This is the opposite of our first failure domain example, where we wouldn't want pods to share the same domain if possible – but there are also reasons that you may want to keep like pods on the same domain. For example, in a multitenant setting where tenants want dedicated hardware tenancy over a domain, you could ensure that every Pod that belongs to a certain tenant is scheduled to the exact same topology domain.</p>
			<p>You can use <code>preferredDuringSchedulingIgnoredDuringExecution</code> in the same way. Before we get <a id="_idIndexMarker416"/>to anti-affinities, here's an example with Pod affinities and the <code>preferred</code> type:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-pod-affinity2.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: not-hungry-app-affinity
spec:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: Lt
              values:
              - "3"
          topologyKey: rack
  containers:
  - name: not-hungry-app
    image: not-hungry-app:latest</pre>
			<p>As before, in this code block, we have our <code>weight</code> – in this case, <code>50</code> – and our expression match – in this case, using a less than (<code>Lt</code>) operator. This affinity will induce the scheduler to try its best to schedule the Pod on a node where it is or with another node on the same rack that has a Pod running with a <code>hunger</code> of less than 3. The <code>weight</code> is used by the scheduler to compare nodes – as discussed in the section on node affinities – <em class="italic">Controlling Pods with Node Affinity</em> (see <code>pod-with-node-affinity4.yaml</code>). In this scenario specifically, the weight of <code>50</code> doesn't make any difference because there is only one entry in the affinity list.</p>
			<p>Pod anti-affinities<a id="_idIndexMarker417"/> extend this paradigm using the same selectors and topologies – let's take a look at them in detail.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor203"/>Pod anti-affinities</h2>
			<p>Pod anti-affinities <a id="_idIndexMarker418"/>allow you to prevent Pods from running on the same topology domain as pods that match a selector. They implement the opposite logic to Pod affinities. Let's dive into some YAML and explain how this works:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-pod-anti-affinity.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: hungry-app
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: In
              values:
              - "4"
              - "5"
          topologyKey: rack
  containers:
  - name: hungry-app
    image: hungry-app</pre>
			<p>Similar to Pod affinity, we use the <code>affinity</code> key as the location to specify our anti-affinity under <code>podAntiAffinity</code>. Also, as with Pod affinity, we have the ability to use either <code>preferredDuringSchedulingIgnoredDuringExecution</code> or <code>requireDuringSchedulingIgnoredDuringExecution</code>. We even use all the same syntax for the <a id="_idIndexMarker419"/>selector as with Pod affinities.</p>
			<p>The only actual difference in syntax is the use of <code>podAntiAffinity</code> under the <code>affinity</code> key.</p>
			<p>So, what does this YAML do? In this case, we are recommending to the scheduler (a <code>soft</code> requirement) that it should attempt to schedule this Pod on a node where it or any other node with the same value for the <code>rack</code> label does not have any Pods running with <code>hunger</code> label values of 4 or 5. We're telling the scheduler <em class="italic">try not to colocate this Pod in a domain with any extra hungry Pods</em>.</p>
			<p>This feature gives us a great way to separate pods by failure domain – we can specify each rack as a domain and give it an anti-affinity with a selector of its own kind. This will make the scheduler schedule clones of the Pod (or try to, in a preferred affinity) to nodes that are not in the same failure domain, giving the application greater availability in case of a domain failure.</p>
			<p>We even have the option to combine Pod affinities and anti-affinities. Let's look at how this could work.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor204"/>Combined affinity and anti-affinity</h2>
			<p>This is one of those<a id="_idIndexMarker420"/> situations where you can really put undue load on your cluster control plane. Combining Pod affinities with anti-affinities can allow incredibly nuanced rules that can be passed to the Kubernetes scheduler, which has the Herculean task of working to fulfill them. </p>
			<p>Let's look at some YAML for a Deployment spec that combines these two concepts. Remember, affinity and anti-affinity are concepts that are applied to Pods – but we normally do not specify Pods without a controller like a Deployment or a ReplicaSet. Therefore, these rules are applied at the Pod spec level in the Deployment YAML. We are only showing the Pod spec part of this deployment for conciseness, but you can find the full file on the GitHub repository:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-both-antiaffinity-and-affinity.yaml</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: hungry-app-deployment
# SECTION REMOVED FOR CONCISENESS  
     spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - other-hungry-app
            topologyKey: "rack"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - hungry-app-cache
            topologyKey: "rack"
      containers:
      - name: hungry-app
        image: hungry-app:latest</pre>
			<p>In this code block, we <a id="_idIndexMarker421"/>are telling the scheduler to treat the Pods in our Deployment as such: the Pod must be scheduled onto a node with a <code>rack</code> label such that it or any other node with a <code>rack</code> label and the same value has a Pod with <code>app=hungry-label-cache</code>.</p>
			<p>Secondly, the scheduler must attempt to schedule the Pod, if possible, to a node with the <code>rack</code> label such that it or any other node with the <code>rack</code> label and the same value does not have a Pod with the <code>app=other-hungry-app</code> label running.</p>
			<p>To boil this down, we want our Pods for <code>hungry-app</code> to run in the same topology as the<code> hungry-app-cache</code>, and we do not want them to be in the same topology as the <code>other-hungry-app</code> if at all possible.</p>
			<p>Since with great power comes great responsibility, and our tools for Pod affinity and anti-affinity are equal parts powerful and performance-reducing, Kubernetes ensures that some limits are set on the possible ways you can use both of them in order to prevent strange behavior or significant performance issues.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor205"/>Pod affinity and anti-affinity limitations</h2>
			<p>The biggest <a id="_idIndexMarker422"/>restriction<a id="_idIndexMarker423"/> on affinity and anti-affinity is that you are not allowed to use a blank <code>topologyKey</code>. Without restricting what the scheduler treats as a single topology type, some very unintended behavior can happen.</p>
			<p>The second limitation is that, by default, if you're using the hard version of anti-affinity – <code>requiredOnSchedulingIgnoredDuringExecution</code>, you cannot just use any label as a <code>topologyKey</code>. </p>
			<p>Kubernetes will only let you use the <code>kubernetes.io/hostname</code> label, which essentially means that you can only have one topology per node if you're using <code>required</code> anti-affinity. This limitation does not exist for either the <code>prefer</code> anti-affinity or either of the affinities, even the <code>required</code> one. It is possible to change this functionality, but it requires writing a custom admission controller – which we will discuss in <a href="B14790_12_Final_PG_ePub.xhtml#_idTextAnchor269"><em class="italic">Chapter 12</em></a>, <em class="italic">Kubernetes Security and Compliance</em>, and <a href="B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289"><em class="italic">Chapter 13</em></a>, <em class="italic">Extending Kubernetes with CRDs</em>.</p>
			<p>So far, our work with placement controls has not discussed namespaces. However, with Pod affinities and anti-affinities, they do hold relevance.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor206"/>Pod affinity and anti-affinity namespaces</h2>
			<p>Since Pod <a id="_idIndexMarker424"/>affinities and anti-affinities cause changes in behavior based on the <a id="_idIndexMarker425"/>location of other Pods, namespaces are a relevant piece to decide which Pods count for or against an affinity or anti-affinity.</p>
			<p>By default, the scheduler will only look to the namespace in which the Pod with the affinity or anti-affinity was created. For all our previous examples, we haven't specified a namespace so the default namespace will be used.</p>
			<p>If you want to add one or more namespaces in which Pods will affect the affinity or anti-affinity, you can do so using the following YAML:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">pod-with-anti-affinity-namespace.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: hungry-app
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: hunger
              operator: In
              values:
              - "4"
              - "5"
          topologyKey: rack
          namespaces: ["frontend", "backend", "logging"]
  containers:
  - name: hungry-app
    image: hungry-app</pre>
			<p>In this code block, the <a id="_idIndexMarker426"/>scheduler will look to the frontend, backend, and<a id="_idIndexMarker427"/> logging namespaces when trying to match the anti-affinity (as you can see on the <code>namespaces</code> key in the <code>podAffinityTerm</code> block). This allows us to constrain which namespaces the scheduler operates on when validating its rules.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor207"/>Summary</h1>
			<p>In this chapter, we learned about a few different controls that Kubernetes provides in order to enforce certain Pod placement rules via the scheduler. We learned that there are both "hard" requirements and "soft" rules, the latter of which are given the scheduler's best effort but do not necessarily prevent Pods that break the rules from being placed. We also learned a few reasons why you may want to implement scheduling controls – such as real-life failure domains and multitenancy.</p>
			<p>We learned that there are simple ways to influence Pod placement, such as node selectors and node names – in addition to more advanced methods like taints and tolerations, which Kubernetes itself also uses by default. Finally, we discovered that there are some advanced tools that Kubernetes provides for node and Pod affinities and anti-affinities, which allow us to create complex rulesets for the scheduler to follow.</p>
			<p>In the next chapter, we will discuss observability on Kubernetes. We'll learn how to view application logs and we'll also use some great tools to get a view of what is happening in our cluster in real time.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor208"/>Questions</h1>
			<ol>
				<li>What is the difference between node selectors and the Node name field?</li>
				<li>How does Kubernetes use system-provided taints and tolerations? For what reasons?</li>
				<li>Why should you be careful when using multiple types of Pod affinities or anti-affinities?</li>
				<li>How could you balance availability across multiple failure zones with colocation for performance reasons for a three-tier web application? Give an example using node or Pod affinities and anti-affinities.</li>
			</ol>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor209"/>Further reading</h1>
			<ul>
				<li>For a more in-depth explanation of the default system taints and tolerations, head to <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions">https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions</a>.</li>
			</ul>
		</div>
	</body></html>