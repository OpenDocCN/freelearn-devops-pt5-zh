<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Proxy Services"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Proxy Services</h1></div></div></div><p>We reached the point where we need something that will tie together the containers we're deploying. We need to simplify the access to the services and unify all the servers and ports our containers are (or will be) deployed on. Multiple solutions are trying to solve this problem, with <span class="strong"><strong>Enterprise Service Bus</strong></span> (<span class="strong"><strong>ESB</strong></span>) products being most commonly used. That is not to say <a class="indexterm" id="id387"/>that their only goal is redirection towards destination services. It indeed isn't, and that is one of the reasons we rejected ESB as (part of) the solution for our architecture. The significant difference in the approach is that ESBs tend to do a lot (much more than we need) while we are trying to compose our system by using very specific small components or services that do (almost) exactly what we need. Not more, not less. ESBs are an antithesis of microservices and, in a way, are betraying the initial ideas behind service-oriented architecture. With us being committed to microservices and looking for more concrete solutions, the alternative is a proxy service. It stands to reason that we should dedicate a bit more time discussing what proxy services are and which products might be able to help us in our architecture and processes.</p><p>A <span class="emphasis"><em>proxy service</em></span> is <a class="indexterm" id="id388"/>a service that acts as an intermediary between clients performing requests and services that serve those requests. A client sends a request to the proxy service that, in turn, redirects that request to the destination service thus simplifying and controlling complexity laying behind the architecture where the services reside.</p><p>There are at least three different types of proxy services:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A <span class="emphasis"><em>gateway</em></span> or <span class="emphasis"><em>tunneling service</em></span> is the kind of a proxy service that redirect requests to<a class="indexterm" id="id389"/> the destination services <a class="indexterm" id="id390"/>and responses back to the clients that made those requests.</li><li class="listitem" style="list-style-type: disc">A <span class="emphasis"><em>forward proxy</em></span> is<a class="indexterm" id="id391"/> used for retrieving data from <a class="indexterm" id="id392"/>different (mostly internet) sources.</li><li class="listitem" style="list-style-type: disc">A <span class="emphasis"><em>reverse proxy</em></span> is <a class="indexterm" id="id393"/>usually used to control and<a class="indexterm" id="id394"/> protect access to a server or services on a private network. Besides its primary function, a reverse proxy often also performs tasks<a class="indexterm" id="id395"/> such as load-balancing, decryption, caching <a class="indexterm" id="id396"/>and authentication.</li></ul></div><p>A reverse proxy is probably the best solution for the problem at hand, so we'll spend a bit more time trying to understand it better.</p><div class="section" title="Reverse Proxy Service"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec21"/>Reverse Proxy Service</h1></div></div></div><p>The main <a class="indexterm" id="id397"/>purpose of the proxy service is to hide the rest of the services as well as to redirect requests to their final destination. The same holds true for responses. Once a service responds to a request, that response goes back to the proxy service and from there is redirected to the client that initially requested it. For all purposes, from the point of view of the destination service, the request came from the proxy. In other words, neither the client that generates the request knows what is behind the proxy nor the service responding to the request knows that it originated from beyond the proxy. In other words, both clients and services know only about the existence of the proxy service.</p><p>We'll concentrate on usages of a proxy service in the context of an architecture based on (micro) services. However, most of the concepts are the same if a proxy service would be used on whole servers (except that it would be called proxy server).</p><p>Some of the main<a class="indexterm" id="id398"/> purposes of a proxy services (beyond orchestration of requests and responses) are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">While almost any applications <a class="indexterm" id="id399"/>server can provide <span class="strong"><strong>encryption</strong></span> (most commonly <span class="strong"><strong>Secure Sockets Layer</strong></span> (<span class="strong"><strong>SSL</strong></span>)), it is often easier to let the <a class="indexterm" id="id400"/>middle man be in charge of it.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Load balancing</strong></span><a class="indexterm" id="id401"/> is the process when, in this case, proxy service balances loads between multiple instances of the same service. In most cases, those instances would be scaled over multiple servers. With that combination (load balancing and scaling), especially when architecture is based on microservices, we can quickly accomplish performance improvements and avoid timeouts and downtimes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Compression</strong></span> is<a class="indexterm" id="id402"/> another candidate for a feature that is easily accomplished when centralized in a single service. Main products that act as proxy services are very efficient in compression and allow relatively easy setup. The primary reason for a compression of the traffic is a speedup of the load time. The smaller the size, the faster the load.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Caching</strong></span> is <a class="indexterm" id="id403"/>another one of the features that are easy to implement within a proxy service that (in some cases) benefits from being centralized. By caching responses, we can offload part of the work our services need to do. The gist of caching is that we set up the rules (for example, cache requests related to the products listing) and cache timeouts. From there on, the proxy service will send a request to the destination service only the first time and store the responses internally. From there on, as long as the request is the same, it will be served directly by the proxy without even sending the request to the service. That is, until the timeout is reached and the process is repeated. The are much more complicated combinations we can employ, but the most common usage is the one we described.</li><li class="listitem" style="list-style-type: disc">Most proxy services serve as a <span class="emphasis"><em>single point of entry</em></span> to the public APIs exposed through services. That in itself increases <span class="strong"><strong>security</strong></span>. In most cases only ports <code class="literal">80</code> (<code class="literal">HTTP</code>) and <code class="literal">443</code> (<code class="literal">HTTPS</code>) would be available to the public usage. All other ports required by services should be open only to the internal use.</li><li class="listitem" style="list-style-type: disc">Different types of authentication (for example OAuth) can be implemented through the proxy service. When the request does not have the user identification, the proxy service can be set to return with an appropriate response code<a class="indexterm" id="id404"/> to the caller. On the other hand, when identification is present, a proxy can choose to continue going to the destination and leave the verification of that identification to the target service or perform it itself. Of course, many variations can be used to implement the authentication. The crucial thing to note is that if a proxy is used, it will most likely be involved in this process one way or another.</li></ul></div><p>This list is by no means extensive nor final but contains some of the most commonly used cases. Many other combinations are possible involving both legal and illegal purposes. As an example, a proxy is an indispensable tool for any hacker that wants to stay anonymous.</p><p>Throughout this<a class="indexterm" id="id405"/> books, we'll focus mostly on its primary function; we'll use proxy services to act as proxies. They will be in charge of the orchestration of all traffic between microservices we'll be deploying. We'll start with simple usages used in deployments and slowly progress towards more complicated orchestration, namely <span class="emphasis"><em>blue-green deployment</em></span>.</p><p>To some, it might sound that a proxy service deviates from microservices approach since it can do (as is often the case) multiple things. However, when looking from the functional point of view, it has a single purpose. It provides a bridge between the outside world and all the services we host internally. At the same time, it tends to have a very low resource usage and can be handled with only a few configuration files.</p><p>Equipped with the basic understanding about proxy services, the time has come to take a look at some of the products we can use.</p><p>From now on, we'll refer to <span class="emphasis"><em>reverse proxy</em></span> as, simply, <span class="emphasis"><em>proxy</em></span>.</p><div class="section" title="How can Proxy Service help our project?"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec54"/>How can Proxy Service help our project?</h2></div></div></div><p>By now we managed to have a controlled way to deploy our services. Due to the nature of deployments we are trying to accomplish, those services should be deployed on ports and, potentially, servers that are unknown to us in advance. Flexibility is the key to scalable architecture, fault tolerance, and many other concepts we'll explore further on. However, that<a class="indexterm" id="id406"/> flexibility comes at a cost. We might not know in advance where will the services be deployed nor which ports they are exposing. Even if this information would be available before the deployment, we should not force users of our services to specify different ports and IPs when sending requests. The solution is to centralize all communications both from third parties as well as from internal services at a single point. The singular place that will be in charge of redirecting requests is a proxy service. We'll explore some of the tools that are at our disposal and compare their strengths and weaknesses.</p><p>As before, we'll start by creating virtual machines that we'll use to experiment with different proxy services. We'll recreate the <code class="literal">cd</code> node and use it to provision the <code class="literal">proxy</code> server with different proxy services.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd proxy</strong></span>
</pre></div><p>The first tool we'll explore is <code class="literal">nginx</code>.</p></div><div class="section" title="nginx"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec55"/>nginx</h2></div></div></div><p>The nginx (engine x) is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP<a class="indexterm" id="id407"/> proxy server. Igor Sysoev originally wrote it. In the <a class="indexterm" id="id408"/>beginning, it powered many Russian sites. Since then, it become a server of choice for some of the busiest sites in the world (NetFlix, Wordpress, and FastMail are only a few of the examples). According to Netcraft, nginx served or proxied around 23% of busiest sites in September 2015. That makes it second to Apache. While numbers provided by Netcraft might be questionable, it is clear that nginx is highly popular and probably is closer to the third place after Apache and IIS. Since everything we did by now is based on Linux, Microsoft IIS should be discarded. That leaves us with Apache as a valid candidate to be our proxy service or choice. Stands to reason that the two should be compared.</p><p>Apache has been available for many years and built a massive user base. Its huge popularity is partly thanks to Tomcat that runs on top of Apache and is one of the most popular application servers today. Tomcat is only one out of many examples of Apache's flexibility. Through its modules, it can be extended to process almost any programming language.</p><p>Being most popular does not necessarily makes something the best choice. Apache can slow down to a crawl under a heavy load due to its design deficiencies. It spawns new processes that, in turn, consume quite a lot of memory. On top of that, it creates new threads for all requests making them compete with each others for access to CPU and memory. Finally, if it reaches the configurable limit of processes, it just refuses new connections. Apache was not designed to serve as a proxy service. That function is very much an after-thought.</p><p>nginx was created to address some of the problems Apache has, in particular, the C10K problem. At the time, C10K was a challenge for web servers to begin handling ten thousand concurrent connections. nginx was released in 2004 and met the goal of the challenge. Unlike<a class="indexterm" id="id409"/> Apache, its architecture is based on asynchronous, non-blocking, event-driven architecture. Not only that it beats Apache in the number of<a class="indexterm" id="id410"/> concurrent requests it can handle, but its resource usage was much lower. It was born after Apache and designed from ground up as a solution for concurrency problems. We got a server capable of handling more requests and a lower cost.</p><p>nginx' downside is that it is designed to serve static content. If you need a server to serve content generated by Java, PHP, and other dynamic languages, Apache is a better option. In our case, this downside is of almost no importance since we are looking for a proxy service with the capability to do load balancing and few more features. We will not be serving any content (static or dynamic) directly by the proxy, but redirect requests to specialized services.</p><p>All in all, while Apache might be a good choice in a different setting, nginx is a clear winner for the task we're trying to accomplish. It will perform much better than Apache if its only task is to act as a proxy and load balancing. It's memory consumption will be minuscule and it will be capable of handling a vast amount of concurrent requests. At least, that is the conclusion before we get to other contestants for the proxy supremacy.</p><div class="section" title="Setting Up nginx"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec19"/>Setting Up nginx</h3></div></div></div><p>Before we set <a class="indexterm" id="id411"/>up the nginx proxy service, let's take a quick look at the Ansible files that we're about to run. The nginx.yml playbook is similar to those we used before. We'll be running the roles we already run before with the addition of nginx:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: proxy</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - docker</strong></span>
<span class="strong"><strong>    - docker-compose</strong></span>
<span class="strong"><strong>    - consul</strong></span>
<span class="strong"><strong>    - registrator</strong></span>
<span class="strong"><strong>    - consul-</strong></span></pre></div><p><code class="literal">oles/nginx/tasks/main.yml</code> role also doesn't contain anything extraordinary:</p><div class="informalexample"><pre class="programlisting">- name: Directories are present
  file:
    dest: "{{ item }}"
    state: directory
  with_items: directories
  tags: [nginx]
- name: Container is running
  docker:
    image: nginx
    name: nginx
    state: running
    ports: "{{ ports }}"
    volumes: "{{ volumes }}"
  tags: [nginx]
- name: Files are present
  copy:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
  with_items: files
  register: result
  tags: [nginx]
- name: Container is reloaded
  shell: docker kill -s HUP nginx
  when: result|changed
  tags: [nginx]
- name: Info is sent to Consul
  uri:
    url: http://localhost:8500/v1/kv/proxy/ip
    method: PUT
    body: "{{ ip }}"
  ignore_errors: yes
  tags: [nginx]</pre></div><p>We are creating<a class="indexterm" id="id412"/> few directories, making sure that the nginx container is running, passing few files and, if any of them changed, reloading nginx. Finally, we are putting the nginx IP to Consul in case we need it for later. The only important thing to notice is the nginx configuration file <code class="literal">roles</code>
<code class="literal">/nginx/files/services.conf</code>:</p><div class="informalexample"><pre class="programlisting">log_format upstreamlog
    '$remote_addr - $remote_user [$time_local] '
    '"$request" $status $bytes_sent '
    '"$http_referer" "$http_user_agent" "$gzip_ratio" '
    '$upstream_addr';
server {
  listen 80;
  server_name _;
  access_log /var/log/nginx/access.log upstreamlog;
  include includes/*.conf;
}
include upstreams/*.conf;</pre></div><p>For the moment, you can ignore log formatting and jump to the <code class="literal">server</code> specification. We specified that nginx should <code class="literal">listen</code> to the standard HTTP port <code class="literal">80</code> and accept requests sent to any server (<code class="literal">server_name _</code>). Next are the <code class="literal">include</code> statements. Instead of specifying all <a class="indexterm" id="id413"/>the configuration in one place, with includes we'll be able to add configuration for each service separately. That, in turn, will allow us to focus on one service at a time and make sure that the one we deploy is configured correctly. Later on, we'll explore in more depth which types of configurations go into each of those includes.</p><p>Let's run the nginx playbook and start <span class="emphasis"><em>playing</em></span> with it. We'll enter the <code class="literal">cd</code> node and execute the playbook that will provision the <code class="literal">proxy</code> node:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant ssh cd</strong></span>
<span class="strong"><strong>ansible-playbook /vagrant/ansible/nginx.yml \</strong></span>
<span class="strong"><strong>    -i /v</strong></span>
<span class="strong"><strong>agrant/ansible/hosts/proxy</strong></span>
</pre></div></div><div class="section" title="Living without a Proxy"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec20"/>Living without a Proxy</h3></div></div></div><p>Before we <a class="indexterm" id="id414"/>see nginx in action, it might be worthwhile to refresh our memory of the difficulties we are facing without a proxy service. We'll start by running the <code class="literal">books-ms</code> application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\/books-ms/master/docker-compose.yml</strong></span>
<span class="strong"><strong>export DOCKER_HOST=tcp://proxy:2375</strong></span>
<span class="strong"><strong>docker-compose up -d app</strong></span>
<span class="strong"><strong>docker-compose ps</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books</strong></span>
</pre></div><p>The output of the last command is as follows:</p><div class="informalexample"><pre class="programlisting">&lt;html&gt;
&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;
&lt;body bgcolor="white"&gt;
&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;
&lt;hr&gt;&lt;center&gt;nginx/1.9.9&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;</pre></div><p>Even though we run the application with <code class="literal">docker-compose</code> and confirmed that it is running on the <code class="literal">proxy</code> node by executing <code class="literal">docker-compose ps</code>, we observed through <code class="literal">curl</code> that the service is not accessible on the standard HTTP port 80 (there was a <code class="literal">404 Not Found</code> message served through nginx). This result was to be expected since our service is running <a class="indexterm" id="id415"/>on a random port. Even if we did specify the port (we already discussed why that is a bad idea), we could not expect users to memorize a different port for each separately deployed service. Besides, we already have service discovery with Consul in place:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://10.100.193.200:8500/v1/catalog/service/books-ms | jq '.'</strong></span>
</pre></div><p>The output of the last command is as follows:</p><div class="informalexample"><pre class="programlisting">[
  {
    "ModifyIndex": 42,
    "CreateIndex": 42,
    "Node": "proxy",
    "Address": "10.100.193.200",
    "ServiceID": "proxy:vagrant_app_1:8080",
    "ServiceName": "books-ms",
    "ServiceTags": [],
    "ServiceAddress": "10.100.193.200",
    "ServicePort": 32768,
    "ServiceEnableTagOverride": false
  }
]</pre></div><p>We can also obtain the port by inspecting the container:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>PORT=$(docker inspect \</strong></span>
<span class="strong"><strong>    --format='{{(index (index .NetworkSettings.Ports "8080/tcp") 0).HostPort}}' \</strong></span>
<span class="strong"><strong>    vagrant_app_1)</strong></span>
<span class="strong"><strong>echo $PORT</strong></span>
<span class="strong"><strong>curl http://proxy:$PORT/api/v1/books | jq '.'</strong></span>
</pre></div><p>We inspected the container, applied formatting to retrieve only the port of the service and stored that information in the <code class="literal">PORT</code> variable. Later on, we used that variable to make a proper request to the service. As expected, this time, the result was correct. Since there is no data, the service returned an empty JSON array (this time without the 404 error).</p><p>Be it as it may, while this operation was successful, it is even less acceptable one for our users. They cannot be given access to our servers only so that they can query Consul or inspect containers to obtain the information they need. Without a proxy, services are unreachable. They are running, but no one can use them:</p><div class="mediaobject"><img alt="Living without a Proxy" src="graphics/B05848_09_01.jpg"/><div class="caption"><p>Figure 9-1 – Services without proxy</p></div></div><p>Now that we felt<a class="indexterm" id="id416"/> the pain our users would feel without a proxy, let us configure nginx correctly. We'll start with manual configuration, and from there on, progress towards automated one.</p></div><div class="section" title="Manually Configuring nginx"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec21"/>Manually Configuring nginx</h3></div></div></div><p>Do you remember the first <code class="literal">includes</code> statement in the nginx configuration? Let's use it. We already<a class="indexterm" id="id417"/> have the <code class="literal">PORT</code> variable, and all that we have to do is make sure that all requests coming to nginx on port <code class="literal">80</code> and starting with the address <code class="literal">/api/v1/books</code> are redirected to the correct port. We can accomplish that by running the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo "</strong></span>
<span class="strong"><strong>location /api/v1/books {</strong></span>
<span class="strong"><strong>  proxy_pass http://10.100.193.200:$PORT/api/v1/books;</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>" | tee books-ms.conf</strong></span>
<span class="strong"><strong>scp books-ms.conf \</strong></span>
<span class="strong"><strong>    proxy:/data/nginx/includes/books-ms.conf # pass: vagrant</strong></span>
<span class="strong"><strong>docker kill -s HUP nginx</strong></span>
</pre></div><p>We created the <code class="literal">books-ms.conf</code> file that will proxy all requests for <code class="literal">/api/v1/books</code> to the correct IP and port. The <code class="literal">location</code> statement will match all requests starting with <code class="literal">/api/v1/books</code> and proxy them to the same address running on the specified IP and port. While IP was not necessary, it is a good practice to use it since, in most cases, the proxy service will run on a separate server. Further on, we used <span class="strong"><strong>secure copy</strong></span> (<span class="strong"><strong>scp</strong></span>) to transfer the file to the <code class="literal">/data/nginx/includes/</code> directory<a class="indexterm" id="id418"/> in the <code class="literal">proxy</code> node. Once the configuration was copied, all we had to do was reload nginx using <code class="literal">kill -s HUP</code> command:</p><p>Let's see whether the change we just did works correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -H 'Content-Type: application/json' -X PUT -d \</strong></span>
<span class="strong"><strong>    "{\"_id\": 1,</strong></span>
<span class="strong"><strong>    \"title\": \"My First Book\",</strong></span>
<span class="strong"><strong>    \"author\": \"John Doe\",</strong></span>
<span class="strong"><strong>    \"description\": \"Not a very good book\"}" \</strong></span>
<span class="strong"><strong>    http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
</pre></div><p>We successfully made a <code class="literal">PUT</code> request that inserted a book to the database and queried the service that returned that same book. Finally, we can make requests without worrying about the ports.</p><p>Are our problems solved? Only partly. We still need to figure out the way to make these updates to the <a class="indexterm" id="id419"/>nginx configuration automatic. After all, if we'll be deploying our microservices often, we cannot rely on human operators to continuously monitor deployments and perform configuration updates:</p><div class="mediaobject"><img alt="Manually Configuring nginx" src="graphics/B05848_09_02.jpg"/><div class="caption"><p>Figure 9-2 – Services with manual proxy</p></div></div></div><div class="section" title="Automatically Configuring nginx"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec22"/>Automatically Configuring nginx</h3></div></div></div><p>We already<a class="indexterm" id="id420"/> discussed service discovery tools and the nginx playbook we run earlier made sure that Consul, Registrator, and Consul Template are properly configured on the <a class="indexterm" id="id421"/>
<span class="emphasis"><em>proxy</em></span> node. That means that Registrator detected the service container we ran and stored that information to the Consul registry. All that is left is to make a template, feed it to Consul Template that will output the configuration file and reload nginx.</p><p>Let's make<a class="indexterm" id="id422"/> the situation a bit more complicated and scale our service by running two instances. Scaling with Docker Compose is relatively easy:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose scale app=2</strong></span>
<span class="strong"><strong>docker-compose ps</strong></span>
</pre></div><p>The output of the latter command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    Name               Command          State            Ports</strong></span>
<span class="strong"><strong>-----------------------------------------------------------------------</strong></span>
<span class="strong"><strong>vagrant_app_1   /run.sh                 Up      0.0.0.0:32768-&gt;8080/tcp</strong></span>
<span class="strong"><strong>vagrant_app_2   /run.sh                 Up      0.0.0.0:32769-&gt;8080/tcp</strong></span>
<span class="strong"><strong>vagrant_db_1    /entrypoint.sh mongod   Up      27017/tcp</strong></span>
</pre></div><p>We can observe that there are two instances of our service, both using different random ports. Concerning nginx, this means several things, most important being that we cannot proxy in the same way as before. It would be pointless to run two instances of the service and redirect all<a class="indexterm" id="id423"/> requests only to one of them. We need to combine proxy with <span class="emphasis"><em>load</em></span> <span class="emphasis"><em>balancing</em></span>.</p><p>We won't go into all possible load balancing techniques. Instead, we'll use the simplest one<a class="indexterm" id="id424"/> called <span class="emphasis"><em>round robin</em></span> that is used by nginx by default. Round robin means that the proxy will distribute requests equally among all services. As before, things closely related to a project should be stored in the repository together with the code and nginx configuration files and templates should not be an exception.take a look at the <code class="literal">nginx-includes.conf</code> configuration file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>location /api/v1/books {</strong></span>
<span class="strong"><strong>  proxy_pass http://books-ms/api/v1/books;</strong></span>
<span class="strong"><strong>  proxy_next_upstream error timeout invalid_header http_500;</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>This time, instead of specifying IP and port, we're using <code class="literal">books_ms</code>. Obviously, that domain does not exist. It is a way for us to tell nginx to proxy all requests from the location to an upstream. Additionally, we also added <code class="literal">proxy_next_upstream</code> instruction. If an error, timeout, invalid header or an error 500 is received as a service response, nginx will pass to the next upstream connection.</p><p>That is the moment when we can start using the second include statement from the main configuration file. However, since we do not know the IPs and ports the service will use, the upstream is the Consul Template file <code class="literal">nginx-upstreams.ctmpl</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>upstream books-ms {</strong></span>
<span class="strong"><strong>    {{range service "books-ms" "any"}}</strong></span>
<span class="strong"><strong>    server {{.Address}}:{{.Port}};</strong></span>
<span class="strong"><strong>    {{end}}</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>What this means<a class="indexterm" id="id425"/> is that the upstream request <code class="literal">books-ms</code> we set as the proxy upstream will be load balanced between all instances of the service and that data will be obtained from Consul. We'll see the result once we run Consul Template.</p><p>First things first. Let's download the two files we just discussed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/nginx-includes.conf</strong></span>
<span class="strong"><strong>wget http://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/nginx-upstreams.ctmpl</strong></span>
</pre></div><p>Now that the proxy configuration and the upstream template are on the <code class="literal">cd</code> server, we should run Consul Template:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>consul-template \</strong></span>
<span class="strong"><strong>    -consul proxy:8500 \</strong></span>
<span class="strong"><strong>    -template "nginx-upstreams.ctmpl:nginx-upstreams.conf" \</strong></span>
<span class="strong"><strong>    -once</strong></span>
<span class="strong"><strong>cat nginx-upstreams.conf</strong></span>
</pre></div><p>Consul Template took the downloaded template as the input and created the <code class="literal">books-ms.conf</code> upstream configuration. The second command output the result that should look similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>upstream books-ms {</strong></span>
<span class="strong"><strong>    server 10.100.193.200:32768;</strong></span>
<span class="strong"><strong>    server 10.100.193.200:32769;</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>Since we are running two instances of the same service, Consul template retrieved their IPs and ports and put them in the format we specified in the <code class="literal">books-ms.ctmpl</code> template.</p><p>Please note that we could have passed the third argument to Consul Template, and it would run any command we specify. We'll use it later on throughout the book.</p><p>Now that all the configuration files are created, we should copy them to the <code class="literal">proxy</code> node and reload nginx:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scp nginx-includes.conf \</strong></span>
<span class="strong"><strong>    proxy:/data/nginx/includes/books-ms.conf # Pass: vagrant</strong></span>
<span class="strong"><strong>scp nginx-upstreams.conf \</strong></span>
<span class="strong"><strong>    proxy:/data/nginx/upstreams/books-ms.conf # Pass: vagrant</strong></span>
<span class="strong"><strong>docker kill -s HUP nginx</strong></span>
</pre></div><p>All that's left is to double check that proxy works and is balancing requests among those two instances:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>docker logs nginx</strong></span>
</pre></div><p>After making<a class="indexterm" id="id426"/> four requests we output nginx logs that should look like following (timestamps are removed for brevity).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32768</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32769</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32768</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32769</strong></span>
</pre></div><p>While ports might be different in your case, it is obvious that the first request was sent to the port <code class="literal">32768</code>, the next one to the <code class="literal">32769</code>, then to the <code class="literal">32768</code> again, and, finally, to the <code class="literal">32769</code>. It is a success, with nginx not only acting as a proxy but also load balancing requests among all instances of the service we deployed:</p><div class="mediaobject"><img alt="Automatically Configuring nginx" src="graphics/B05848_09_03.jpg"/><div class="caption"><p>Figure 9-3 – Services with automatic proxy with Consul Template</p></div></div><p>We still haven't tested the error handling we set up with the <code class="literal">proxy_next_upstream</code> instruction. Let's<a class="indexterm" id="id427"/> remove one of the service instances and confirm that nginx handles failures correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker stop vagrant_app_2</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
</pre></div><p>We stopped one service instance and made several requests. Without the <span class="emphasis"><em>proxy_next_upstream</em></span> instruction, nginx would fail on every second request since one of the two services set as upstreams are not working anymore. However, all four requests worked correctly. We can observe what nginx did by taking a look at its logs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker logs nginx</strong></span>
</pre></div><p>The output should be similar to the following (timestamps are removed for brevity):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32768</strong></span>
<span class="strong"><strong>[error] 12#12: *98 connect() failed (111: Connection refused) while connecting to upstream, client: 172.17.42.1, server: _, request: "GET /api/v1/books HTTP/1.1", upstream: "http://10.100.193.200:32769/api/v1/books", host: "localhost"</strong></span>
<span class="strong"><strong>[warn] 12#12: *98 upstream server temporarily disabled while connecting to upstream, client: 172.17.42.1, server: _, request: "GET /api/v1/books HTTP/1.1", upstream: "http://10.100.193.200:32768/api/v1/books", host: "localhost"</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32768, 10.100.193.200:32768</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32768</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 268 "-" "curl/7.35.0" "-" 10.100.193.200:32768</strong></span>
</pre></div><p>The first request<a class="indexterm" id="id428"/> went to the port <code class="literal">32768</code> served by the instance that is still running. As expected, nginx sent the second request to the port <code class="literal">32768</code>. Since the response was <code class="literal">111</code> (Connection refused), it decided to temporarily disable this upstream and try with the next one in line. From there on, all the rest of requests were proxied to the port <code class="literal">32768</code>.</p><p>With only a few lines in configuration files, we managed to set up the proxy and combine it with load balancing and failover strategy. Later on, when we get to the chapter that will explore <span class="emphasis"><em>self-healing systems</em></span>, we'll go even further and make sure not only that proxy works <a class="indexterm" id="id429"/>only with running services, but also how to restore the whole system to a healthy state.</p><p>When nginx is combined with service discovery tools, we have an excellent solution. However, we should not use the first tool that comes along, so we'll evaluate a few more options. Let us stop the nginx container and see how <span class="emphasis"><em>HAProxy</em></span> behaves:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker stop nginx</strong></span>
</pre></div></div></div><div class="section" title="HAProxy"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec56"/>HAProxy</h2></div></div></div><p>Just like nginx, HAProxy is a free, very fast and reliable solution offering high availability, load <a class="indexterm" id="id430"/>balancing, and proxying. It is particularly suited for <a class="indexterm" id="id431"/>very high traffic websites and powers quite many of the world's most visited ones.</p><p>We'll speak about the differences later on when we compare all proxy solutions we're exploring. For now, suffice to say that HAProxy is an excellent solution and probably the best alternative to nginx.</p><p>We'll start with practical exercises and try to accomplish with HAProxy the same behavior as the one with have with nginx. Before we provision the <span class="emphasis"><em>proxy</em></span> node with HAProxy, let us take a quick look at the tasks in the Ansible role haproxy:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Directories are present</strong></span>
<span class="strong"><strong>  file:</strong></span>
<span class="strong"><strong>    dest: "{{ item }}"</strong></span>
<span class="strong"><strong>    state: directory</strong></span>
<span class="strong"><strong>  with_items: directories</strong></span>
<span class="strong"><strong>  tags: [haproxy]</strong></span>
<span class="strong"><strong>- name: Files are present</strong></span>
<span class="strong"><strong>  copy:</strong></span>
<span class="strong"><strong>    src: "{{ item.src }}"</strong></span>
<span class="strong"><strong>    dest: "{{ item.dest }}"</strong></span>
<span class="strong"><strong>  with_items: files</strong></span>
<span class="strong"><strong>  register: result</strong></span>
<span class="strong"><strong>  tags: [haproxy]</strong></span>
<span class="strong"><strong>- name: Container is running</strong></span>
<span class="strong"><strong>  docker:</strong></span>
<span class="strong"><strong>    image: million12/haproxy</strong></span>
<span class="strong"><strong>    name: haproxy</strong></span>
<span class="strong"><strong>    state: running</strong></span>
<span class="strong"><strong>    ports: "{{ ports }}"</strong></span>
<span class="strong"><strong>    volumes: /data/haproxy/config/:/etc/haproxy/</strong></span>
<span class="strong"><strong>  tags: [haproxy]</strong></span>
</pre></div><p>The <span class="emphasis"><em>haproxy</em></span> role is very similar to the one we used for nginx. We created some directories and copied some files (we'll see them later on). The major thing to note is that, unlike most other containers not built by us, we're not using the official <span class="emphasis"><em>haproxy</em></span> container. The main reason is that the official image has no way to reload HAProxy configuration. We'd need to restart the container every time we update HAProxy configuration, and that would produce some downtime. Since one of the goals is to accomplish zero-downtime, restarting the container is not an option. Therefore, we had to look at alternatives, and the user <span class="emphasis"><em>million12</em></span> has just what we need. The <code class="literal">million12/haproxy</code> container comes with <span class="emphasis"><em>inotify</em></span> (<span class="emphasis"><em>inode notify</em></span>). It is a Linux kernel subsystem that <a class="indexterm" id="id432"/>acts by extending filesystems to notice changes, and <a class="indexterm" id="id433"/>report them to applications. In our case, inotify will reload HAProxy whenever we change its configuration:</p><p>Let us proceed and provision HAProxy on the proxy node:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook /vagrant/ansible/haprox</strong></span>
<span class="strong"><strong>y.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/proxy</strong></span>
</pre></div><div class="section" title="Manually Configuring HAProxy"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec23"/>Manually Configuring HAProxy</h3></div></div></div><p>We'll start<a class="indexterm" id="id434"/> by checking whether HAProxy is running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://proxy:2375</strong></span>
<span class="strong"><strong>docker ps -a</strong></span>
<span class="strong"><strong>docker logs haproxy</strong></span>
</pre></div><p>The <code class="literal">docker ps</code> command showed that the <span class="emphasis"><em>haproxy</em></span> container has the status <code class="literal">Exited</code>, and the logs produced the output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[2015-10-16 08:55:40] /usr/local/sbin/haproxy -f /etc/haproxy/haproxy.cfg -D -p /var/run/haproxy.pid</strong></span>
<span class="strong"><strong>[2015-10-16 08:55:40] Current HAProxy config /etc/haproxy/haproxy.cfg:</strong></span>
<span class="strong"><strong>====================================================================================================</strong></span>
<span class="strong"><strong>cat: /etc/haproxy/haproxy.cfg: No such file or directory</strong></span>
<span class="strong"><strong>====================================================================================================</strong></span>
<span class="strong"><strong>[ALERT] 288/085540 (9) : Could not open configuration file /etc/haproxy/haproxy.cfg : No such file or directory</strong></span>
<span class="strong"><strong>[ALERT] 288/085540 (10) : Could not open configuration file /etc/haproxy/haproxy.cfg : No such file or directory</strong></span>
</pre></div><p>HAProxy complained that there is no <code class="literal">haproxy.cfg</code> configuration file and stopped the process. Actually, the fault is in the playbook we run. The only file we created is <span class="emphasis"><em>haproxy.cfg.orig</em></span> (more about it later) and that there is no <code class="literal">haproxy.cfg</code>. Unlike nginx, HAPRoxy cannot be run without having, at least, one proxy set. We'll set up the first proxy soon but, at the moment, we have none. Since creating the configuration without any proxy is a waste of time (HAProxy fails anyway) and we cannot provide one when provisioning the node for the first time since at that point there would be no services running, we just skipped the creation <a class="indexterm" id="id435"/>of the <span class="emphasis"><em>haproxy.cfg</em></span>.</p><p>Before we <a class="indexterm" id="id436"/>proceed with the configuration of the first proxy, let us comment another difference that might complicate the process. Unlike nginx, HAProxy does not allow includes. The complete configuration needs to be in a single file. That will pose certain problems since the idea is to add or modify only configurations of the service we are deploying and ignore the rest of the system. We can, however, simulate includes by creating parts of the configuration as separate files and concatenate them every time we deploy a new container. For this reason, we copied the <code class="literal">haproxy.cfg.orig</code> file as part of the provisioning process. Feel free to take a look at it. We won't go into details since it contains mostly the default settings and HAProxy has a decent documentation that you can consult. The important thing to note is that the <code class="literal">haproxy.cfg.orig</code> file contains settings without a single proxy being set.</p><p>We'll create the HAProxy configuration related to the service we have running in the similar way as we did before:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>PORT=$(docker inspect \</strong></span>
<span class="strong"><strong>    --format='{{(index (index .NetworkSettings.Ports "8080/tcp") 0).HostPort}}' \</strong></span>
<span class="strong"><strong>    vagrant_app_1)</strong></span>
<span class="strong"><strong>echo $PORTecho "</strong></span>
<span class="strong"><strong>frontend books-ms-fe</strong></span>
<span class="strong"><strong>    bind *:80</strong></span>
<span class="strong"><strong>    option http-server-close</strong></span>
<span class="strong"><strong>    acl url_books-ms path_beg /api/v1/books</strong></span>
<span class="strong"><strong>    use_backend books-ms-be if url_books-ms</strong></span>
<span class="strong"><strong>backend books-ms-be</strong></span>
<span class="strong"><strong>    server books-ms-1 10.100.193.200:$PORT check</strong></span>
<span class="strong"><strong>" | tee books-ms.service.cfg</strong></span>
</pre></div><p>We started by inspecting the <code class="literal">vagrant_app_1</code> container in order to assign the current port to the <code class="literal">PORT</code> variable and use it to create the <code class="literal">books-ms.service.cfg</code> file.</p><p>HAProxy uses similar logic as nginx even though things are named differently. The <span class="emphasis"><em>frontend</em></span> defines how requests should be forwarded to <span class="emphasis"><em>backends</em></span>. In a way, the <span class="emphasis"><em>frontend</em></span> is analogous to the nginx <span class="emphasis"><em>location</em></span> instruction and the <span class="emphasis"><em>backend</em></span> to the <code class="literal">upstream</code>. What we did can be translated to the following. Define a frontend called <code class="literal">books-ms-fe</code>, bind it to the port <code class="literal">80</code> and, whenever the request part starts with <code class="literal">/api/v1/books</code>, use the backend called <code class="literal">books-ms-be</code>. The backend <code class="literal">books-ms-be</code> has (at the moment) only one server defined with the IP <code class="literal">10.100.193.200</code> and the port assigned by Docker. The <code class="literal">check</code> argument has (more or less) the same meaning as in nginx and is used to skip proxying to services that are not healthy.</p><p>Now that <a class="indexterm" id="id437"/>we have the general settings in the file <code class="literal">haproxy.cfg.orig</code> and those specific to services we're deploying (named with the <code class="literal">.service.cfg</code> extension), we can concatenate them into a single <code class="literal">haproxy.cfg</code> configuration file and copy it to the <code class="literal">proxy</code> node:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat /vagrant/ansible/roles/haproxy/files/haproxy.cfg.orig \</strong></span>
<span class="strong"><strong>    *.service.cfg | tee haproxy.cfg</strong></span>
<span class="strong"><strong>scp haproxy.cfg proxy:/data/haproxy/config/haproxy.cfg</strong></span>
</pre></div><p>Since the container is not running, we'll need to start it (again), and then we can check whether the proxy is working correctly by querying the service:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>docker start haproxy</strong></span>
<span class="strong"><strong>docker logs haproxy</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
</pre></div><p>The first request returned the <code class="literal">Connection refused</code> error. We used it to confirm that no proxy is running. Then we started the <code class="literal">haproxy</code> container and saw through the container logs that the configuration file we created is valid and indeed used by the proxy service. Finally, we sent the request again, and, this time, it returned a valid response.</p><p>So far, so good. We can proceed and automate the process using Consult Template.</p></div><div class="section" title="Automatically Configuring HAProxy"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec24"/>Automatically Configuring HAProxy</h3></div></div></div><p>We'll try to <a class="indexterm" id="id438"/>do the same or very similar steps as what we did before with nginx. That way you can compare the two tools more easily.</p><p>We'll start by scaling the service:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose scale </strong></span>
<span class="strong"><strong>compose ps</strong></span>
</pre></div><p>Next we should download the <code class="literal">haproxy.ctmpl</code> template from the code repository. Before we do that, let us<a class="indexterm" id="id439"/> take a quick look at its contents:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>frontend books-ms-fe</strong></span>
<span class="strong"><strong>    bind *:80</strong></span>
<span class="strong"><strong>    option http-server-close</strong></span>
<span class="strong"><strong>    acl url_books-ms path_beg /api/v1/books</strong></span>
<span class="strong"><strong>    use_backend books-ms-be if url_books-ms</strong></span>
<span class="strong"><strong>backend books-ms-be</strong></span>
<span class="strong"><strong>    {{range service "books-ms" "any"}}</strong></span>
<span class="strong"><strong>    server {{.Node}}_{{.Port}} {{.Address}}:{{.Port}} check</strong></span>
<span class="strong"><strong>    {{end}}</strong></span>
</pre></div><p>The way we created the template follows the same pattern as the one we used with nginx. The only difference is that HAProxy needs each server to be uniquely identified so we added the service <code class="literal">Node</code> and <code class="literal">Port</code> that will serve as the server ID.</p><p>Let's download the template and run it through Consul Template:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/haproxy.ctmpl \</strong></span>
<span class="strong"><strong>    -O haproxy.ctmpl</strong></span>
<span class="strong"><strong>sudo consul-template \</strong></span>
<span class="strong"><strong>    -consul proxy:8500 \</strong></span>
<span class="strong"><strong>    -template "haproxy.ctmpl:books-ms.service.cfg" \</strong></span>
<span class="strong"><strong>    -once</strong></span>
<span class="strong"><strong>cat books-ms.service.cfg</strong></span>
</pre></div><p>We downloaded the template using <code class="literal">wget</code> and run the <code class="literal">consul-template</code> command.</p><p>Let us concatenate all the files into haproxy.cfg, copy it to the <code class="literal">proxy</code> node and take a look at <code class="literal">haproxy</code> logs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat /vagrant/ansible/roles/haproxy/files/haproxy.cfg.orig \</strong></span>
<span class="strong"><strong>    *.service.cfg | tee haproxy.cfg</strong></span>
<span class="strong"><strong>scp haproxy.cfg proxy:/data/haproxy/config/haproxy.cfg</strong></span>
<span class="strong"><strong>docker logs haproxy</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
</pre></div><p>All that's left is to double check whether the proxy balancing works with two instances:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
</pre></div><p>Unfortunately, HAProxy cannot output logs to stdout (preferred way to log Docker containers) so we cannot confirm that balancing works. We could output logs to syslog, but that is outside of the scope of this chapter.</p><p>We still haven't tested the error handling we set up with the <code class="literal">backend</code> instruction. Let's remove one <a class="indexterm" id="id440"/>of the service instances and confirm that HAProxy handles failures correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker stop vagrant_app_1</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
<span class="strong"><strong>curl http://proxy/api/v1/books | jq '.'</strong></span>
</pre></div><p>We stopped one service instance and made several requests, and all of them worked properly.</p><p>Without the possibility to include files into HAProxy configuration, our job was slightly more complicated. Not being able to log to stdout can be solved with syslog but will go astray from one of the containers best practices. There is a reason for this HAProxy behavior. Logging to stdout slows it down (noticeable only with an enormous number of requests). However, it would be better if that is left as our choice and maybe the default behavior, instead of not being supported at all. Finally, not being able to use the official HAProxy container might be considered a minor inconvenience. None of those problems are of great importance. We solved the lack of includes, could log into syslog and ended up using the container from <code class="literal">million12/haproxy</code> (we could also create our own that would extend from the official one).</p></div></div></div></div>
<div class="section" title="Proxy Tools Compared"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec22"/>Proxy Tools Compared</h1></div></div></div><p>Apache, nginx<a class="indexterm" id="id441"/> and HAProxy are by no means the only solutions we could use. There are many projects available and making a choice is harder than ever.f the open source projects worth trying out is <code class="literal">lighttpd</code> (<code class="literal">pron. lighty</code>). Just like nginx and HAProxy, it was designed for security, speed, compliance, flexibility and high performance. It features a small memory footprint and efficient management of the CPU-load.</p><p>If JavaScript is your language of preference, [node-http-proxy] could be a worthy candidate. Unlike other <a class="indexterm" id="id442"/>products we explored, node-http-proxy uses JavaScript code to define proxies and load balancing.</p><p>The VulcanD is a<a class="indexterm" id="id443"/> project to keep an eye on. It is programmable proxy and load balancer backed by etcd. A similar process that we did with Consul Template and nginx/HAProxy is incorporated inside VulcanD. It can be combined with Sidekick to provide functionality similar to <code class="literal">check</code> arguments in nginx and HAProxy.</p><p>There are many similar projects available, and it is certain that new and existing ones are into making. We can expect more <code class="literal">unconventional</code> projects to appear that will combine proxy, load balancing, and service discovery in many different ways.</p><p>However, my choice, for now, stays with nginx or HAProxy. None of the other products we spoke about has anything to add and, in turn, each of them, at least, one deficiency.</p><p>Apache is process <a class="indexterm" id="id444"/>based, making its performance when faced with a massive traffic less than desirable. At the same time, its resource usage skyrockets easily. If you need a server that will serve dynamic content, Apache is a great option, but should not be used as a proxy.</p><p>Lighttpd<a class="indexterm" id="id445"/> was promising when it appeared but faced many obstacles (memory leaks, CPU usage, and so on) that made part of its users switch to alternatives. The community maintaining it is much smaller than the one working on nginx and HAProxy. While it had its moment and many had high expectations from it, today it is not the recommended solution.</p><p>What can be said about <code class="literal">node-http-proxy</code>? Even though it does not outperform nginx and HAProxy, it is <a class="indexterm" id="id446"/>very close. The major obstacle would be its programmable configuration that is not well suited for continuously changing proxies. If your language of choice is JavaScript and proxies should be relatively static, node-http-proxy is a valid option. However, it still doesn't provide any benefit over nginx and HAProxy.</p><p>VulcanD, in <a class="indexterm" id="id447"/>conjunction with Sidekick, is a project to keep an eye on, but it is not yet production ready (at least, not at the time this text was written). It is very unlikely that it will manage to outperform main players. The potential problem with VulcanD is that it is bundled with etcd. If that's what you're already using, great. On the other hand, if your choice fell to some other type of Registry (for example Consul or Zookeeper), there is nothing VulcanD can offer. I prefer keeping proxy and service discovery separated and put the glue between them myself. Real value VulcanD provides is in a new way of thinking that combines proxy service with service discovery, and it will probably be considered as one of the pioneers that opened the door for new types of proxy services.</p><p>That leaves us with nginx and HAProxy. If you spend some more time investigating opinions, you'll see that both camps have an enormous number of users defending one over the other. There are areas where nginx outperforms HAProxy and others where it underperforms. There are some features that HAProxy doesn't have and other missing in nginx. But, the truth is that both are battle-tested, both are an excellen solution, both have a huge number of users, and both are successfully used in companies that have colossal traffic. If what you're looking for is a proxy service with load balancing, you cannot go wrong with either of them.</p><p>I am slightly more inclined towards nginx due to its better (official) Docker container (for example, it allows configuration reloads with a HUP signal), option to log to stdout and the ability to include configuration files. Excluding Docker container, HAProxy made the conscious decision not to support those features due to possible performance issues they can create. However, I prefer having the ability to choose when it's appropriate to use them and when it isn't. All those are truly preferences of no great importance and, in many cases, the <a class="indexterm" id="id448"/>choice is made depending on a particular use case one is trying to accomplish. However, there is one critical nginx feature that HAProxy does not support. HAProxy can drop traffic during reloads. If microservices architecture, continuous deployment, and blue-green processes are adopted, configuration reloads are very common. We can have several or even hundreds of reloads each day. No matter the reload frequency, with HAProxy there is a possibility of downtime.</p><p>We have to make a choice, and it falls to nginx. It will be out proxy of choice throughout the rest of the book.</p><p>With that being said, let us destroy the VMs we used in this chapter and finish the implementation of the deployment pipeline. With service discovery and the proxy, we have everything we need:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div></div></body></html>