<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Continuous Integration</h1>
                </header>
            
            <article>
                
<p>When building software, quality assessment is something that usually is pushed toward the end of the life cycle, just before the release. When the team is working in a 6-months release cycle, the drawbacks are not as obvious as when the release cycle is just a few days old (or even hours!), but from my experience, I can tell you that getting early feedback in your software is crucial in order to raise the quality to a good level we are comfortable to live with.</p>
<p>There is a misconception in the software that puts the average software project <span>in danger</span>: the software has to be perfect<em>.</em> This is totally incorrect. Think about some of these real-world systems: the engine of your car, a nuclear plant, the water purification system in major cities, and so on; human lives depend upon all of them and they fail. A fair amount of money is spent on these systems without being able to ensure the total safety, so what makes you think that the software that your company writes will be perfect?</p>
<p>Instead of investing resources in making your software perfect, the experience has taught me (through the hard path) that it is a lot better to invest the resources in building the software in a way that makes it easy for the engineers to fix the problems as quickly as possible, being able to shorten the release cycles with enough level of confidence. In this chapter, we are going to examine the key components of the continuous integration:</p>
<ul>
<li><span>Software development life cycle</span></li>
<li>Traditional CI servers:<br/>
<ul>
<li>Bamboo</li>
<li>Jenkins</li>
</ul>
</li>
<li>Modern CI servers:
<ul>
<li>Drone</li>
</ul>
</li>
</ul>
<p class="mce-root">This has the objective of building an effective continuous integration pipeline to ensure reliability and enables us to deliver faster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software development life cycle</h1>
                </header>
            
            <article>
                
<p>The software development life cycle is the diagram of our day to day activity as software engineers wait, this book is about DevOps; what are we doing talking about software engineering? Well, in theory, <span>DevOps</span> is the role of the IT activity that covers the full life cycle of a software component, from the inception to the release and further maintenance. Nowadays, many companies are hiring DevOps engineers on the basis of hiring system administrators on steroids that even though it works, it completely misses the biggest advantage of the DevOps role: having someone on the team with exposure to all the aspects of the software so that problems can be solved quickly without involving people from different teams in the majority of the cases.</p>
<p>Before proceeding further, let's take a look at how the software development life cycle works:</p>
<div class="CDPAlignCenter CDPAlign"><img height="370" width="405" class="image-border" src="assets/8fe57653-6bb9-4c79-91a2-97fe7be8e5f0.png"/></div>
<p>This is the most classical and studied software development life cycle in IT literacy, something that everyone has gone through in college and everyone has as a mental model of even if we have not seen it before. Nowadays, with the agile methodologies, people tend to think that the model is obsolete. I think it is still a very valid model, but what it has changed is the scale and the involvement of the different stakeholders through the previous diagram. Let's take a brief look at the objectives of every step in a top-to-bottom approach:</p>
<ul>
<li><strong>Requirement analysis</strong>: This is where we are going to encounter the majority of the problems. We need to find a common language between people outside of IT (accountants, marketers, farmers, and so on) and people in IT, which often leads to different problems around terminology and even business flows being captured incorrectly.</li>
<li><strong>Design</strong>: In this phase, we are going to design our flows in a language that the IT crowd can understand straight away, so they will be able to code efficiently. Often, this phase overlaps with the requirement analysis (if the stakeholder is into IT) and that is desirable as diagrams are the perfect middle language that we are looking for.</li>
<li><strong>Development</strong>: As the name suggests, this is where the software is built. This is what developers do well: build technical artifacts that work and work well--according to a potentially flawed specification. This is where we need to be clever: no matter what we do, our software is going to be imperfect, and we need to plan accordingly. When we are working in agile environments, deliver early and deliver often is the mantra followed to minimize the impact of a wrong specification so that the stakeholders can test the product before the problem is too big to be tackled. I also believe that involving the stakeholders early enough is a good strategy, but it is not a silver bullet, so no matter what we do, our software has to be modular so we can plug and play modules in order to accommodate new requirements. In order to ensure the functionality of our modules on their own, we write unit tests that can be run quickly in order to ensure that the code is doing what it is supposed to do.</li>
<li><strong>Testing</strong>: This is where continuous integration lives. Our continuous integration server will run the testing for us when appropriated and inform us about the potential problems in our application <span>as quickly as possible</span>. Depending on the complexity of our software, our testing can be very extensive in this phase, but in general, the continuous integration server focuses on running integration and acceptance (that said, the integration server usually runs all the tests as the unit tests should be inexpensive to run).</li>
<li><strong>Release</strong>: In this phase, we deliver the software to what we call production; people start using the software, and no matter how much effort we put in the previous phases, there will be bugs and that is the reason why we planned our software to be able to fix the problems quickly. In the release phase, we can create something that we will see later on in this book, called <strong>Continuous Delivery</strong> (<strong>CD</strong>) pipelines, which enables the developers to execute the build-test-deploy cycle very quickly (even a few times a day).</li>
<li><strong>Maintenance</strong>: There are two types of maintenance: evolutive and corrective. Evolutive maintenance is where we evolve our software by adding new functionalities or improving business flows to suit the business needs. Corrective maintenance is the one where we fix bugs and misconceptions. We want to minimize the latter but we cannot totally avoid it.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing types</h1>
                </header>
            
            <article>
                
<p>In the previous section, we talked about different types of tests:</p>
<ul>
<li><span><strong>Unit tests</strong>:</span> <span>What we call white box tests are what mock the dependencies and test the business flow of a particular piece of code.</span></li>
<li><strong>Integration tests</strong>: <span>These are designed to test the integration between different components of an application and they do not test the business logic extensively. Sometimes, when the software is not very complex, the integration tests are used as unit tests (</span>especially <span>in dynamic languages), but this is not the most common use case.</span></li>
<li><strong>Acceptance tests</strong>: Designed to test business assumptions, these are usually built on the principle of what we know as user stories describing situations with the style of being given an assumption.</li>
</ul>
<p>Every test has a different objective, and they work well together, but keep the following diagram in your mind:</p>
<div class="CDPAlignCenter CDPAlign"><img height="232" width="198" class="image-border" src="assets/c42d749d-a598-4031-b89b-2a6065f485bd.png"/></div>
<p>This is what I call the pyramid of testing, and there are years of experience (not only mine) behind it: your software should have a whole bunch of unit testing, fewer integration tests, and some acceptance tests. This ensures that the majority of your business logic is covered by the unit tests and the integration and acceptance tests are used for more specific functions. Also, the integration and acceptance tests are usually more expensive, so minimizing their its usage is usually something that's recommended (but not at the cost of dropping the test coverage).<br/>
When working with a CI server, usually, the developer runs the <strong>unit tests</strong> on his computer as they are quick and should spot a big amount of the potential problems, leaving the integration and acceptance tests to the CI server, which will run while the developer is working on other tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Traditional CI servers</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to walk through the most traditional CI servers:</p>
<ul>
<li>Bamboo</li>
<li>Jenkins</li>
</ul>
<p>They have been around for quite a while and even though they are heavily used in the enterprise world, they are losing some grasp against the new and more modern CI servers such as Drone or Travis (although Travis has been around for a while, it has been reinvented to work on the cloud).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bamboo</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Bamboo</strong> is a proprietary CI server that is developed by Atlassian. Atlassian is a software company that specializes in tools for developers. Products such as JIRA and Bitbucket are created by Atlassian and they are well known in the IT world. Bamboo is their proposal for CI activities, and it is quite popular as it integrates fairly well with their other products.</p>
<p class="mce-root">Let's install it. In order to do that, please visit Bamboo's home page at <a href="https://confluence.atlassian.com/bamboo/">https://confluence.atlassian.com/bamboo/</a> and follow the instructions in the quick start guide. As you can see, the installation is quite simple, and after generating the evaluation license and some steps (express installation), you should have bamboo running on your local computer:</p>
<p class="mce-root CDPAlignCenter CDPAlign CDPAlignLeft"><img class="image-border" src="assets/197335a7-5918-457e-a803-6643d215004b.png"/></p>
<p> </p>
<p class="mce-root CDPAlignCenter CDPAlign CDPAlignLeft">If you click on the button labeled <span class="packt_screen">Create your first build plan</span>, you can see how easy it is to set up jobs in Bamboo. In this case, we can use one of the open source projects that I created in the past called <strong>Visigoth</strong>, a load balancer with circuit breaking capabilities used for interconnecting microservices. The GitHub repository is located at <a href="https://github.com/dgonzalez/visigoth">https://github.com/dgonzalez/visigoth</a>.</p>
<p class="mce-root CDPAlignCenter CDPAlign CDPAlignLeft">Fork it into your GitHub repository if you want to modify it. Visigoth is a single component that does not interact with others, so only unit tests were created for it. Enter the clone URL of the repository into the appropriate field, in this case, Git repository, and submit the form.</p>
<div class="packt_tip">If you have the <strong>Time-based One-Time Password</strong> (<strong>TOTP</strong>) protection in your GitHub account, you might need to choose Git repository with no authentication instead of GitHub repository in the source part of the form to create a test plan.</div>
<p>Once you finish creating the plan, it will ask you to add tasks to your test plan, which, at the moment, is only checking out the source code from Git. In this case, Visigoth is a Node.js application, and as such, the tests are run by executing the <kbd>npm test</kbd> command. In order to run the command, we need to add two tasks of the <kbd>type</kbd> command. The first task will be used to install the dependencies of the application and the second one to run the tests. Let's add the first one:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a2b24fe9-fdae-4e5c-9d5b-7174af91496c.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">As you can see, I have added one executable by clicking on <span class="packt_screen">Add new executable by specifying the path where <kbd>NPM</kbd> is, which can be found by executing <kbd>which npm</kbd> in the terminal of the machine where you installed Bamboo.</span></p>
<div class="packt_tip">You will need to install Node.js on the same machine where you installed Bamboo in order to run the tests. The current LTS version will work fine with it, but Visigoth was tested with Node 6.x.</div>
<p>Now we are going to add the second command, which will execute <kbd>npm test</kbd> in order to run the tests. This command will only be executed if the two previous steps (checking out the code [<span class="packt_screen">Checkout Default Repository</span>) and installing the dependencies (<span class="packt_screen">NPM install</span>)] are successful:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e812b9b3-9b8b-48c5-b17b-799b0c748e1c.png"/></div>
<p>Once you save the task, we have completed all the actions that we need to execute in order to run the tests in Visigoth. Now, the only thing left is to run the job:</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft"><img src="assets/b3701c63-b5f2-45cf-a1a4-e6b9fc90aab9.png"/></p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">If everything is correct, you should get a green badge and a message of success. As you can see, my build failed in previous runs as I was adjusting the CI server to run Visigoth.</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">You can check the logs of the job to see how many tests were successful and other useful information. If you explore this even further, you can see how Bamboo also offers different types of tasks, such as mocha test runners, which allows Bamboo to understand the result of the tests. At the moment, with the current configuration, if any of the tests fails, Bamboo won't be able to understand which one failed. I'd suggest you to play around with different configurations and even different applications to get yourself familiar with it. As you can see by yourself, the interface is very friendly, and usually, it is quite simple to achieve your desired setup by creating new tasks.</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">By default, Bamboo creates something called <strong>trigger</strong>. A trigger is an action that leads to a test plan being executed. In this case, if we change the GitHub repository from where the job was created, the test plan will be triggered to verify the changes, ensuring the continuous integration of new code.</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Another interesting type of trigger is the time-based one. This type of trigger allows us to run the build overnight, so if our tests take several minutes or even hours to run, we can do it when no one is using the server. This type of trigger has saved me from bugs derived from the daylight time savings hour adjustment, causing some tests to fail due to code fragments not handling the change across different time zones <span>well</span>.</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">In general, Bamboo can deal with every situation, and it has adapted to the modern times: we can even build Docker images and push them to remote registries once the tests have passed in order to be deployed later on. Bamboo is also able to take actions in the post-build phase, for example, alerting us if the build failed overnight with an email or other communication channels.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Jenkins</h1>
                </header>
            
            <article>
                
<p class="mce-root">I have worked with Jenkins for quite a while now, and I have to say that I feel really comfortable working with it as I know it is free, open source, and also highly customizable. It has a powerful and well-documented API that enables users to automate pretty much anything related to continuous integration. In <a href="127a7b5f-4bd7-4290-bea0-3e8db867e4af.xhtml" target="_blank">Chapter 8</a>, <em>Release Management – Continuous Delivery</em>, we are going to set up a continuous delivery pipeline with Jenkins in order to be able to release new versions of an application in a transparent manner once the test results are satisfactory, enabling our team to focus on development and automating all the deployment-related activities.</p>
<p class="mce-root">Jenkins is also modular, which enables developers to write plugins to extend functionalities, for example, sending messages to a Slack channel if the build fails or running Node.js scripts as a part of a job.</p>
<p class="mce-root">On the scalability side, Jenkins, like Bamboo, can be scaled to hundreds of nodes through a master/slave configuration so that we can add more power to our CI server in order to execute some tasks in parallel.</p>
<p class="mce-root">On its own, Jenkins will be enough to provide contents for a couple of books, but we are going to visit what we need to set up automated jobs for testing our applications. It is also possible to write plugins for Jenkins, so virtually, there is no limit to what it can do.</p>
<p class="mce-root">Let's focus on the operational side of Jenkins for now. In order to run Jenkins, we have two options:</p>
<ul>
<li class="mce-root">Running it as a Docker container</li>
<li class="mce-root">Installing it as a program in your CI server</li>
</ul>
<p class="mce-root">For now, we are going to install Jenkins, using its Docker image as it is the simplest way of running it and it fits our purpose. Let's start. The first thing is running a simple instance of Jenkins from the command line:</p>
<pre><strong>docker run -p 8080:8080 -p 50000:50000 jenkins</strong></pre>
<p>This will run Jenkins, but be aware that all the information about configuration and builds executed will be stored within the container, so if you lose the container, all the data is lost as well. If you want to use a volume to store the data, the command that you need to execute is as follows:</p>
<pre><strong>docker run --name myjenkins -p 8080:8080 -p 50000:50000 -v /var/jenkins_home jenkins</strong></pre>
<p>This will create a volume that you can reuse later on when upgrading to new versions of Jenkins or even restarting the same container. After running the command, the logs will show something similar to what is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="276" width="755" class="image-border" src="assets/ff8c85aa-209a-4a0d-8848-ba6cf875f196.png"/></div>
<p>This is the initial password for Jenkins and it is necessary in order to set up the instance. After a few seconds, the logs of the container will stop, which means your Jenkins server is ready to be used. Just open the browser and go to <kbd>http://localhost:8080/</kbd>, and you will see something similar to this:</p>
<div class="CDPAlignCenter CDPAlign"><br/>
<img src="assets/233698b1-6878-42d5-b60d-72b9f6fc8f33.png"/></div>
<p>This is where you can enter <span class="packt_screen">Administrator password</span>, which we saved earlier, and click on the C<span class="packt_screen">ontinue button. The next screen will ask you whether it should install the suggested plugins or whether you want to select which plugins to install. Choose the suggested plugins. After a few minutes, it will let you create a user and that's it. Jenkins is up and running in a container:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/b5c79c48-f1d6-42ea-a9a9-cf9fa59c6064.png"/></div>
<p>Now we are going to create a new job. We are going to use the same repository as we used with Bamboo so we can compare the two integration servers. Let's click on <span class="packt_screen">Create a new project</span>. You should be presented with the following form:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/c1d6a180-4ed8-4cc3-b31e-c63a1c30d993.png"/></div>
<p>Just enter a name for the project and choose the first option: <span class="packt_screen">Freestyle project</span>. Jenkins has different types of projects. Freestyle project is a type of project where we can define the steps, as we did in Bamboo. Another interesting option is the type <span class="packt_screen">Pipeline</span> where we can, through a <strong>DSL</strong> (known as <strong>Domain Specific Language</strong>), define a set of steps and stages, creating a pipeline that can be saved as code.</p>
<p>The following screen is where we configure the project. We are going to use Git with the repository hosted at <a href="https://github.com/dgonzalez/visigoth.git">https://github.com/dgonzalez/visigoth.git</a>.<br/>
You can use your fork if you previously forked it while working with Bamboo. Your configuration should be similar to the what is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/ead57bdb-c870-47d6-9eff-669d2f8a56b6.png"/></div>
<p>Now we need to install the dependencies of Visigoth with the <kbd>npm install --development</kbd> <span>command</span> and execute the tests with the <kbd>npm test</kbd> comm<span>and,</span> but we are running Jenkins from a container and this container does not have Node.js installed. We are going to use our Docker knowledge to install it. Inspecting the Dockerfile of the Jenkins image in the Docker Hub, we can verify that it is based on Debian Jessie (it is based on OpenJDK but that is based on Debian Jessie) so we can install the required software in it. The first thing that needs to be done in order to install software is gain root access to the container. As you learned in <a href="6b4e8014-1c44-495b-b22b-e84fb1b944b8.xhtml" target="_blank">Chapter 2</a>, <em>Cloud Data Centres - The New Reality</em>, we can execute commands on the running container. Let's run the following command:</p>
<pre><strong>docker exec -u 0 -it eaaef41f221b /bin/bash</strong></pre>
<p>This command executes <kbd>/bin/bash</kbd> in the container with the ID <span><kbd>eaaef41f221b</kbd> (it will change in your system as it is unique per container) but with the user that matches the user ID <kbd>0</kbd>, in this case, root. We need to do this because the Jenkins image defines and uses a new user called</span> <kbd>jenkins</kbd> with a known UID and GID so if the <kbd>-u 0</kbd> <span>flag</span> is not passed, the <kbd>/bin/bash</kbd> <span>command</span> will be executed by the user <kbd>jenkins</kbd>.</p>
<p>Once we are root in the container, proceed to install Node.js:</p>
<pre class="language-bash"><strong>curl -sL https://deb.nodesource.com/setup_7.x <span class="token operator">| </span><span class="token function">bash</span> -</strong></pre>
<p>And once the execution of the previous command is finished, run the following one:</p>
<pre class="language-bash"><strong><span class="token function">apt-get</span> <span class="token function">install</span> -y nodejs build-essentials</strong></pre>
<p>And that's it. From now on, our Jenkins container has an installation of Node.js available to run Node.js scripts. That said, we should avoid installing software in production containers. Our containers should be <strong>immutable artifacts</strong> that do not change through their life cycle, so what we should do is commit the changes in this image and tag it as a new version in order to release it into our production container. As we don't have a production container, we are making the changes as we go.</p>
<div class="packt_tip">Our containers in production should be immutable artifacts: if we need to change their status, we create a new version of the image and redeploy it instead of modifying the running container.</div>
<p>Once Node.js is installed, we can just exit the root shell within the container and go back to Jenkins to complete our tasks. As we did with Bamboo, here are our tasks in order to run our tests:</p>
<p><img class="image-border" src="assets/fbb53be3-ffce-4470-aa06-eefbe8515c96.png"/></p>
<p> </p>
<p>In the very bottom of the job configuration, there is a section called <strong>post-build</strong> actions. This section allows you to execute actions once the job is finished. These actions include sending e-mails, adding commit messages to the Git repository, among others. As we previously mentioned, Jenkins is extensible and new actions can be added by installing new plugins.</p>
<div class="packt_tip">Jenkins can also parametrize builds with input from the user.</div>
<p>Once you have added these two steps to the build, just click on <span class="packt_screen">Save</span> and we are all set: you now <span>have</span> a fully functional Jenkins job. If we run it, it should successfully run the tests on Visigoth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Secrets Management</h1>
                </header>
            
            <article>
                
<p>One of the possibilities of the CI server is the ability to talk to third-party services that usually rely on some sort of credentials (such as access tokens or similar) to authenticate the user. Exposing these secrets would be discouraged as they could potentially cause major harm to our company.</p>
<p>Jenkins handles this in a very simple way: it provides a way to store credentials in a safe way that can be injected into the build as environment variables so that we can work with them.</p>
<p>Let's look at some examples. First, we need to create the secrets in Jenkins. In order to do that, we have to go to <span class="packt_screen">Manage Jenkins</span> from the home page.</p>
<p>Once we are there, you should see a screen very similar to this one:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/7846e9f9-5e2c-4e1f-9633-cdeb51ad4e4f.png"/></div>
<p>We are using the <span class="packt_screen">Global credentials</span> store as we just want to showcase how it works, but Jenkins allows you to box credentials so you can restrict access across different usages. In Jenkins, credentials, aside from being injected into the build context, can be connected to plugins and extensions so that they can authenticate against third-party systems.</p>
<p>Now, we click on <span class="packt_screen">Add Credentials</span> on the left-hand side:</p>
<div class="CDPAlignCenter CDPAlign"><img height="588" width="987" class="image-border" src="assets/f717e503-2da1-4b60-a474-4d1716bbc3da.png"/></div>
<p>There are some fields that we need to fill before proceeding, but they are very basic:</p>
<ul>
<li>
<p><span class="packt_screen">Kind</span>: This is the type of secret that we want to create. If you open the drop-down, there are several types, from files to certificates, walking through usernames and passwords.</p>
</li>
<li>
<p><span class="packt_screen">Scope</span>: This is the scope of our secret. The documentation is not 100% clear (at least not in the first read) but it allows us to hide the secret from certain stances. There are two options: <span class="packt_screen">Global</span> and <span class="packt_screen">System</span>. With <span class="packt_screen">Global</span>, the credentials can be exposed to any object within Jenkins and its child, whereas with <span class="packt_screen">System</span>, the credentials can be exposed <span>only</span> to Jenkins and its nodes.</p>
</li>
</ul>
<p>The rest of the fields are dependant on the type of secret. For now on, we are going to create a <kbd>Username with password</kbd> secret. Just select it in the dropdown and fill in the rest of the details. Once it is created, it should show in the list of credentials.</p>
<p>The next step is to create a job that is bound to those credentials so we can use them. Just create a new freestyle project, as we saw in the beginning of this section, but we are going to stop on the screen where we can configure the job, precisely in the <span class="packt_screen">Build Environment</span> section:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/62534e70-5f74-40f4-b828-ba2da9d6a033.png"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Now select <span class="packt_screen">Username and password (conjoined)</span>. Conjoined username and password means that we get the full secret (the username and the password) in a single variable, whereas with separated, we get the secret split in two variables: one for the username and another one for the password.</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Once we select it, the form to create the binding is fairly simple:</p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft"><img class="image-border" src="assets/7288f18b-c9c2-4911-8824-ffabdae70d27.png"/></p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">We get to choose the <span class="packt_screen">Variable</span> where we want to store the secret and we also get to choose which secret. There is a radio button that lets you choose between <span class="packt_screen">Parameter expression</span> or <span class="packt_screen">Specific credentials</span> as we can parametrize the job to get input from the user on the triggering screen. In order to showcase how well thought Jenkins is, we are going to add a <span class="packt_screen">Build</span> step that uses the secret by just echoing it into the logs:</p>
<div class="CDPAlignCenter CDPAlign"><img height="553" width="832" class="image-border" src="assets/dffe14d5-f5ec-4f76-9748-8818b39f9fa2.png"/></div>
<p>Click on the <span class="packt_screen">Save</span> button to save the job and run it. Once the job execution finishes, go to the result and click on <span class="packt_screen">Console Output</span>. If you were expecting to see the secret in here, Jenkins has a surprise for you:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/3674ed99-ede8-4ebf-a0c4-32dcafd46a35.png"/></div>
<p>The secret has been masked in order to prevent exposure to unauthorized users. This is not bullet proof, as someone could easily dump the secret from a test within an application checked out by Jenkins, but it adds some level of security in there, leaving the rest to the code reviews and processes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modern CI servers</h1>
                </header>
            
            <article>
                
<p>One thing that is clear in IT is that the market moves very fast, and every few years, a new trend breaks what was considered the perfect solution for a problem. CI software is not an exception to this. In the last few years (taking into account that this book was written in 2017), Infrastructure as Code has drawn a lot of attention to the DevOps world, but in CI, its equivalent is Pipelines as Code.</p>
<p>Jenkins and Bamboo have added support for declarative pipelines recently, but they are not built around them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Drone CI</h1>
                </header>
            
            <article>
                
<p>Drone is probably the newest CI server in the market. I decided to introduce it in this chapter as it was a big revelation to me when I found out about it working in nearForm Ltd. By that time, I was well used to Jenkins and it suited every single use case that I could come across in my professional life, from CI to continuous delivery and sometimes even as a bastion host using a feature called callback URL, where a job could be triggered by sending an HTTP request to a specific URL.</p>
<p>Drone is built around the concept of containers. Everything in Drone is a container, from the server to where the test runs, but the most interesting part is that even the plugins are containers. This makes it easy to write new plugins for executing custom actions, as the only requirement is that the containers return <kbd>0</kbd> as the exit code if it was successful and a nonzero exit code if it was not successful.</p>
<p>For Jenkins or Bamboo, writing a plugin requires a few hours of testing and reading documentation. For Drone, we just need to know how to build a Docker image and what task we want to accomplish.</p>
<p>Be aware that Drone is still in the version 0.5 and moves very quickly, so by the time you read this book, Drone might have changed significantly, but I wanted to include it as I think it is a very promising software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Drone</h1>
                </header>
            
            <article>
                
<p>In order to install Drone, we are going to use <kbd>docker-compose</kbd>, and it is going to be configured to work with GitHub.</p>
<p>Drone, like Docker, follows a client-server architecture, so we can find two differentiated components, the server and the CLI. The first part we are going to proceed with is with the server. Take a look at the following <kbd>docker-compose</kbd> file:</p>
<pre>version: '2'<br/><br/>services:<br/>drone-server:<br/>image: drone/drone:0.5<br/>ports:<br/>- 80:8000<br/>volumes:<br/>- ./drone:/var/lib/drone/<br/>restart: always<br/>environment:<br/>- DRONE_OPEN=true<br/>- DRONE_GITHUB=true<br/>- DRONE_GITHUB_CLIENT=${DRONE_GITHUB_CLIENT}<br/>- DRONE_GITHUB_SECRET=${DRONE_GITHUB_SECRET}<br/>- DRONE_SECRET=${DRONE_SECRET}<br/><br/>drone-agent:<br/>image: drone/drone:0.5<br/>command: agent<br/>restart: always<br/>depends_on: [ drone-server ]<br/>volumes:<br/>- /var/run/docker.sock:/var/run/docker.sock<br/>environment:<br/>- DRONE_SERVER=ws://drone-server:8000/ws/broker<br/>- DRONE_SECRET=${DRONE_SECRET}</pre>
<p>There are two containers running in the preceding Docker Compose file: a server and an agent. Up until version 0.4, Drone master could execute builds, but after that, an agent is needed to run builds. There are some secrets that we need to configure before proceeding that are being passed into compose via environment variables (with the <kbd>${VAR_NAME}</kbd> notation):</p>
<ul>
<li><kbd>DRONE_GITHUB_CLIENT</kbd>: As we specified earlier, we are going to use GitHub as the origin of our source code to be tested. This is provided on GitHub when registering a new OAuth application needed for Drone. You can create OAuth applications in the settings section of GitHub. Be careful; one of the parameters that you need in order to create a GitHub OAuth application is the callback URL. In this case, we are going to use <kbd>http://localhost/authorize</kbd> as we are working on our local machine.</li>
<li><kbd>DRONE_GITHUB_SECRET</kbd>: In the same way as <kbd>DRONE_GITHUB_CLIENT</kbd>, this is provided when a new OAuth application is created on GitHub.</li>
<li><kbd>DRONE_SECRET</kbd>: This is an arbitrary string shared with the agent and the master. Just create a simple string, but when running a drone in production, make sure that the string is long enough so it cannot be guessed.</li>
</ul>
<p>In order to get Drone working with the GitHub integration, we need to receive callbacks from GitHub. Once we have all the values, we just need to run the following command:<br/></p>
<pre><strong>DRONE_GITHUB_CLIENT=your-client DRONE_GITHUB_SECRET=your-secret DRONE_SECRET=my-secret docker-compose up</strong></pre>
<p>In one line, we are setting the three variables that we need, apart from running <kbd>docker-compose up</kbd>. If everything went as expected, when you browse <kbd>http://localhost</kbd>, you should see a window similar to the following one:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/5c92457f-2d38-483b-ba70-5429f8537e1e.png"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If you click on <span class="packt_screen">login</span>, Drone should redirect you to GitHub for authorization and then GitHub will redirect you to the callback URL specified when creating the OAuth application, which is your local Drone installation, <kbd>http://localhost/authorize</kbd>. Sometimes, it might require some tweaking, but in general, it is very easy to make it work. As you can see, Drone leverages the authentication to GitHub so a GitHub account is required to log in.</p>
<p>Now we are going to proceed with the CLI. It is as easy as visiting <a href="http://readme.drone.io/0.5/install/cli/">http://readme.drone.io/0.5/install/cli/</a> and choosing the right version for your platform, in my case, macOS. Just place the binary in the path and you are ready to go. In order to configure the location of the Drone server, you need to specify two environment variables:</p>
<ul>
<li>
<p><kbd>DRONE_SERVER</kbd>: This is the URL to your Drone server, in this case, <kbd>http://localhost</kbd></p>
</li>
<li>
<p><kbd>DRONE_TOKEN</kbd>: On<span>ce you are logged into Drone, navigate to</span> <span class="packt_screen">Account</span> <span>and click on <span class="packt_screen">Show token</span>. This is the value that you need</span></p>
</li>
</ul>
<p>Once you have set up the two variables, execute the following command:</p>
<pre><strong>drone info</strong></pre>
<p>This should show your GitHub username and the e-mail that you used to register.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running builds</h1>
                </header>
            
            <article>
                
<p>Drone has a different philosophy when it comes to running builds: it reacts to changes in the code on the remote repository by triggering the pipeline. Let's create a super simple repository with a very simple Node.js application. I have created it on my GitHub account in order to make everything easier: <a href="https://github.com/dgonzalez/node-example-drone/">https://github.com/dgonzalez/node-example-drone/</a>. Just fork it into your own account, and you are good to go.</p>
<p>The first thing that we need to do is activate the project in your local Drone installation. Just go to <span class="packt_screen">Account</span>, and in the list of repositories, activate <kbd>node-example-drone</kbd>. Now it should show in the home screen in a manner similar to the following screenshot:</p>
<p><img src="assets/4d391aa6-47d1-41fb-98be-6e72b96cf318.png"/><br/>
Now we are facing a small problem: Drone was created to trigger builds using a webhook delivered from GitHub into our Drone server. As we are working in a private network, we need to <span>somehow</span> expose our server to the Internet. In this case, we are going to use a service called <strong>Ngrok</strong> (<a href="http://www.ngrock.com">http://www.ngrock.com</a>) in order to expose Drone to the internet, which is not necessary when working in a production environment as it should be accessible over the internet (or at least through a proxy). Just follow the instructions, and once you run it in the Terminal, it should look very similar to what is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/2756ecbc-fdca-4bdc-8217-7e93c30b44eb.png"/></div>
<p>This specifies which host is being forwarded to your local address, in my case, <kbd>http://852cc48a.ngrok.io</kbd>. Just open it in your browser and check whether Drone is accessible from there.</p>
<p>One thing left to do is edit the webhook that Drone installed in our GitHub repository when we activated it. You will find it in the repository settings on GitHub. Just edit the webhook to change the URL from <kbd>http://localhost</kbd> to your Ngrok URL, in my case, <kbd>http://852cc48a.ngrok.io</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">
Executing pipelines</h1>
                </header>
            
            <article>
                
<p>Now the setup is complete, before doing anything else, take a look at the <kbd>.drone.yaml</kbd> <span>file</span> of the forked repository:</p>
<pre><strong>debug: true</strong><br/><strong>pipeline:</strong><br/><strong>  build:</strong><br/><strong>    image: node</strong><br/><strong>    commands:</strong><br/><strong>      - npm install --development</strong><br/><strong>      - npm test</strong></pre>
<p>This is our pipeline, and as you can guess, it gets committed alongside our code into our repository. Drone is going to execute the instructions in this pipeline when GitHub delivers the webhook into our Drone installation. As Drone works with containers, the first thing that Drone is going to do is create an image based on the node (as we specified) and run the following operations:</p>
<ul>
<li>It installs the dependencies</li>
<li>It runs the tests</li>
</ul>
<p>If the exit code of the container that executes these commands is <kbd>0</kbd>, our build is successful and you can test it by pushing some changes to your GitHub repository and watching how Drone reacts to them.</p>
<p>There is also another way to re-trigger builds (not for the first time) via the CLI interface. Open the Terminal, and after configuring the environment variables previously stated (if you haven't done it yet), run the following command:</p>
<pre><strong>drone build list dgonzalez/node-example-drone</strong></pre>
<p>This will return a list of all the previously executed builds. Just change <kbd>dgonzalez</kbd> to your username, as you can see in the web interface. In order to rerun a previous build, we can run the following command:</p>
<pre><strong>drone build run dgonzalez/node-example-drone 1</strong></pre>
<p>This command fires off a build in Drone that was already built. This is particularly useful when you suspect that the build failed due to external factors.</p>
<div class="packt_tip">Sometimes, the webhook fails (particularly with the setup that we have with Ngrok), but GitHub allows you to debug that in the webhooks section of your repository.</div>
<p>This is the simplest case of a pipeline. As mentioned earlier, Drone is based on plugins, and those plugins are also Docker images. The list is quite comprehensive and can be found at <a href="https://github.com/drone-plugins">https://github.com/drone-plugins</a>.</p>
<p>Let's assume that we want to push our image to the <strong>Google Container Registry</strong> in Google Cloud. We are going to use the plugin called <kbd>drone-gcr</kbd> from <a href="https://github.com/drone-plugins/drone-gcr">https://github.com/drone-plugins/drone-gcr</a>. Here is our pipeline:</p>
<pre>debug: true<br/>pipeline:<br/>  build:<br/>    image: node<br/>    commands:<br/>      - npm install --development<br/>      - npm test<br/>  publish:<br/>    gcr:<br/>      repo: myrepo/node-example-drone<br/>      token: &gt;<br/>         {<br/>            ...<br/>         }</pre>
<p>What we have here is a two-stage pipeline: it first executes the tests, and once they are successful, it deploys the image to Google Cloud Registry. We have different phases in the pipeline that we can use:</p>
<ul>
<li><span><strong>Build</strong>:</span> <span><span>For building the tests and the related commands</span></span></li>
<li><strong>Publish</strong>: <span><span>Used to publish the artifact in a remote repository</span></span></li>
<li><strong>Deploy</strong>: <span>Very useful for continuous integration as it allows us to deploy our software in a continuous delivery manner</span></li>
<li><strong>Notify</strong>: <span>Used to send notifications via email, slack, or any other channel</span></li>
</ul>
<p>For example, if we wanted to send a Slack notification, we would just need to add the following lines <span>to our pipeline</span>:</p>
<pre><strong><span class="hljs-attr"> notify:</span><span class="hljs-attr"><br/>   image:</span> plugins/slack<br/><span class="hljs-attr">   channel:</span> developers <br/><span class="hljs-attr">   username:</span> drone</strong></pre>
<div class="packt_tip"><br/>
Remember, YAML is sensitive to tabs and spaces so notify needs to be at the same level as publish or build.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other features</h1>
                </header>
            
            <article>
                
<p>At the time of writing this, Drone is being actively developed, with new features being added and along with some major reworks. It also offers other features, such as secret management and support services.</p>
<p>With secret management, we can inject secrets that get encrypted and stored in a database and only injected into builds that have been cryptographically signed by our drone CLI with a valid token from our Drone server.</p>
<p>Drone also offers support services, which are services that run alongside your tests. This is very helpful when our integration tests depend on a database or when we need to spin third-party software such as Hashicorp Vault or a service discovery infrastructure such as Consul or Eureka.</p>
<p>It is expected that in future, Drone will have more features, but at the moment, it is going through major changes as it is being actively developed (unlike more mature servers, such as Jenkins, that have been around for a while).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we walked through three different CI tools:</p>
<ul>
<li>Bamboo, a commercial tool</li>
<li>Jenkins, an industry standard open source tool</li>
<li>Drone, a cutting-edge technology CI server</li>
</ul>
<p>We discussed the key features of Jenkins that we are going to use going forward in this book, but we also showcased how Drone has leveraged the concept of containers into a very powerful CI system that, even though not mature yet, I expect to become the norm in the coming years.</p>
<p>The important concepts that we need to be aware of were explained, but to summarize, we use our integration server to run our tests for us so we can offload developers from doing that but also run the tests overnight in order to ensure that the daily build is stable.</p>
<p>In the next chapter, we will visit what the community has called I<strong>nfrastructure as Code:</strong> basically, a way of dealing with our infrastructure as if code was, managing resources on a very elegant way.</p>


            </article>

            
        </section>
    </body></html>