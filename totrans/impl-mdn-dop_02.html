<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Cloud Data Centers - The New Reality</h1>
                </header>
            
            <article>
                
<p>In the last few years, there has been a shift toward cloud systems, which enable the companies to scale in an easy and cheap way depending on the needs. They also enable companies to take advantage of something called <strong>Infrastructure as Code</strong> (<strong>IAC</strong>), which basically allows you to treat your physical resources (servers and routers) that previously had to be bought according to the needs as code that you can review, run, and re-run to adapt the infrastructure to your requirements.</p>
<p>In this chapter, we are going to walk through the main cloud providers, taking a look at their main strengths and weak points in order to form a clear picture of what they offer and how we <span>can</span>, as engineers, take advantage of it.</p>
<p>Out of all the providers in the market, we are going to focus on these two:</p>
<ul>
<li><strong>Amazon Web Services</strong> (<strong>AWS</strong>)</li>
<li>Google Cloud Platform</li>
</ul>
<p>We are also going to talk a bit about these:</p>
<ul>
<li>Heroku</li>
<li>Azure</li>
<li>DigitalOcean</li>
</ul>
<p>We should have an open minded attitude, as all of them can offer a different and valuable set of features, something that should not be overlooked.</p>
<p>We are going to introduce <strong>Kubernetes</strong>, which is, in my humble opinion, the answer to many problems in the modern DevOps world.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon Web Services</h1>
                </header>
            
            <article>
                
<p>Amazon is by far the biggest online retailer with an almost worldwide presence. Everyone has heard about Amazon and the possibilities that this type of store present to the busy society of the 21st century: they offer home delivery of pretty much anything that can be bought in a conventional store.</p>
<p>Amazon was founded in 1994 by Jeff Bezos, and since then, it has grown consistently every year, offering more and more products and services, but at some point, they got into the cloud computing business. It makes sense that a big company such as Amazon needs a lot of processing power, is reliable, and is able to adapt to the necessities of the business quickly.</p>
<p>Initially, the cloud services were an internal solution to satisfy the high availability needs of the business as well as have the capacity to grow in a uniform way. This created a lot of expertise within the company in building a top notch <strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>) that, at some point, they realized could be sold to customers.</p>
<p>By 2006, there was nothing in the market to compete with them, so they were in the sweet spot for a successful start.</p>
<p>I remember I was only in college when the first two services, EC2 and EC3, were introduced in a conference.</p>
<p>EC2 allowed you to create virtual machines on the cloud with an API that was manipulated through a command-line interface as well as a web interface that would act as a monitor of your resources.</p>
<p>S3 was a key value (kind of) storage that allowed you to store immense sets of data at a very low price manipulated through the command-line interface <span>as well</span>.</p>
<p>It really was a revolution. It was a complete paradigm shift: now you could ask for more resources as you need. This was as simple as an API call, and there you go: three new machines ready to be used in 2 minutes. The following screenshot is a list of services on AWS:</p>
<div class="CDPAlignCenter CDPAlign"><img height="566" width="779" class="image-border" src="assets/22f5e569-6fcb-44b2-ae21-31e98ce48d4b.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"> Catalog of services in AWS at January 2017</div>
<p>In the last few years, Amazon has been adding services very often, up until a point where it is hard to keep up with the pace. In this chapter, we are going to walk through the main services (or what I consider the most useful), showing their features and areas of application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">EC2 - computing service</h1>
                </header>
            
            <article>
                
<p>The first element that a cloud system has to provide to the users is computing power. <strong>EC2</strong> stands for <strong>Elastic Compute Cloud</strong>, and it allows you to create machines on the cloud with a few clicks.</p>
<p>This is what the EC2 interface looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img height="431" width="717" class="image-border" src="assets/42721bc2-6e22-4a08-8e95-062b2962f003.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"> EC2 interface</div>
<p>EC2 was launched on August 25, 2006 (beta version), and it has evolved a lot since then. It provides the user with different sizes of machines and is available across the globe (11 different regions as of today). This means that the user can spin up machines in different parts of the globe for high availability and latency purposes, enabling the engineers of your company to build multi-zone applications without coordinating teams across the globe.</p>
<p>They also provide different types of instances optimized for different tasks so that the users can tailor the infrastructure to their needs. In total, there are 24 different type of instances, but they are also grouped by type, which we will walk through later on in this chapter.</p>
<p>Let’s look at an example of how to launch an instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching an instance</h1>
                </header>
            
            <article>
                
<p>The first thing you need to do is go to the AWS EC2 interface.</p>
<ol>
<li>Now click on the <span class="packt_screen">Launch Instance</span> button, which will bring you to the following screen:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="502" width="696" class="image-border" src="assets/aa358a37-cb82-4374-98ea-f7cfd1b594a3.png"/></div>
<ol start="2">
<li>This is where you can choose the image to run. As you can see, the image is the operating system that will run on top of the EC2 Instance. In Amazon jargon, this image is called <strong>Amazon Machine Image</strong> (<strong>AMI</strong>), and you can create your own ones and save them for later usage, allowing you to ship prebuilt software. For now, choose Ubuntu Server 16.04 by clicking on <span class="packt_screen">Select</span>.</li>
<li>The next screen is about the size of the image. AWS offers quite a big variety of sizes and types of images. This parameter drastically affects the performance of the application regarding the network, memory, and CPU performance as well as the I/O of the machine.</li>
<li>Let’s look at the different types:</li>
</ol>
<table style="width: 838px;height: 1560px">
<tbody>
<tr>
<td>
<p><strong>Type</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p>Bursting instances</p>
</td>
<td>
<p>T2 are general-purpose instances for burst processing. They provide a baseline level of CPU for peaks of processing power, but these peaks are available on an accumulative basis: while idle, the CPU accumulates credits that can be used during high demand periods, but once these credits are used, the performance goes back to the baseline level.</p>
</td>
</tr>
<tr>
<td>
<p>General purpose</p>
</td>
<td>
<p>M3 is a general-purpose instance with dedicated resources (no burst credits). It provides a good balance between CPU, memory, and network resources, and it is the minimum instance for production applications that need solid performance.</p>
<p>M4 follows the same philosophy as M3 but with an updated hardware: <strong>Amazon Elastic Block Store </strong><span>(<strong>Amazon </strong></span><strong>EBS</strong>) optimized and a better CPU as well as enhanced networking are the highlights of this instance type. </p>
</td>
</tr>
<tr>
<td>
<p> Compute Optimized</p>
</td>
<td>
<p>The compute optimized instances in AWS are C3 and C4. In the same way as the M instances, C4 is a hardware upgrade of the C3. These types of instances are prepared for intensive CPU work, such as data processing and analysis or demanding servers. C4 also comes with an enhanced network system, which is very helpful for high networking traffic applications.</p>
</td>
</tr>
<tr>
<td>
<p>Memory Optimized </p>
</td>
<td>
<p>As you can guess, AWS also provides memory optimized instances that can be used for applications that need high memory usage. Applications based on Apache Spark (or big data in general), in memory databases and similar, benefit the most from these type of instances. In this case, the memory optimized instances are divided into two sub-families:X1: These are large scale enterprise grade instances. X1 can be used for the most demanding applications in the enterprise ecosystem and it is the flagship of the memory intensive instances and is only used for very large applications.R3/R4: Even though are more modest than X1, R instances are well capable of handling the majority of day-to-day memory intensive applications. Cache systems, in memory databases, and similar systems are the best use cases for X and R instances.</p>
</td>
</tr>
<tr>
<td>
<p>Accelerated Computing Instances </p>
</td>
<td>
<p>Some applications, such as <strong>Artificial Intelligence</strong> (<strong>AI</strong>), have specific computing requirements, such as <strong>Graphical Processing Unit</strong> (<strong>GPU</strong>) processing or reconfigurable hardware. These instances are divided into three families:P2: GPU compute instances. These are optimized to carry specific processing tasks, such as breaking passwords through brute force as well as machine learning applications (they usually rely on GPU power).G2: Graphical processing instances. Rendering videos as well as ray tracing or video streaming are the best use cases for these instances.</p>
</td>
</tr>
</tbody>
</table>
<ol start="5">
<li>As you can see, there is an instance for every necessity that the user can have. For now, we are going to choose a small instance first because we are just testing AWS and second because AWS has a free tier, which enables you to use the <kbd>t2.micro</kbd> instances for up to 1 year without any charge, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="553" width="749" class="image-border" src="assets/e706b488-60fb-4543-96de-1a646d92ca27.png"/></div>
<ol start="6">
<li>Now we have two options. Click on <span class="packt_screen"><span class="packt_screen">Review Instance Launch or <span class="packt_screen"><span class="packt_screen">Configure Instance D</span><span>etails</span></span>. In this case, we are going to click on <span><span class="packt_screen">Review Instance Launch</span></span>, but by clicking on <span class="packt_screen"><span class="packt_screen">Configure Instance D</span>etails</span>, we can configure several elements of the instance, such as networking, storage, and so on.</span></span></li>
</ol>
<p> </p>
<p> </p>
<ol start="7">
<li>Once you click on <span><span class="packt_screen">Review Instance Launch</span></span>, the review screen shows up. Click on <span class="packt_screen">Launch</span> and you should get presented with something similar to what is shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="591" width="748" class="image-border" src="assets/87e704de-af4a-4e2e-b252-d3cddee8cba3.png"/></div>
<ol start="8">
<li>Just assign a name to the key-pair name and click on the <span class="packt_screen">Download Key Pair button, which will download a <kbd>.pem</kbd> file that we will use later on to access via <kbd>ssh</kbd> to the instance.</span></li>
<li>Once you have specified the key pair, click on <span class="packt_screen">Launch Instance</span>, as shown in the preceding screenshot, and that's all. After a few checks, your image will be ready for installing the required software ( this usually takes a couple of minutes).</li>
</ol>
<p>This is the bare minimum needed to create a running instance in AWS. As you can see, the full process is very well explained on the screen and in general, if you know the basics of DevOps (<kbd>ssh</kbd>, networking, and device management), you don't really need much help creating instances.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Relational Database Service</h1>
                </header>
            
            <article>
                
<p>What we have shown in the previous section are EC2 machines that can be used to install the required software. There is another service that allows you to administer high availability databases (MySQL, PostgreSQL, Maria DB, and Aurora as well as Oracle and SQL Server) across regions. This service is called RDS and it stands for Relational Database Service.</p>
<p>One of the big headaches with relational databases is the high availability configuration: master-master configuration is something that is usually expensive and out of reach of small companies. AWS has raised the bar with RDS offering multi-region high availability databases with a few clicks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Networking in AWS and EC2</h1>
                </header>
            
            <article>
                
<p>AWS provides fine-grain control at the networking level. As with any physical data center, you can define your own networks, but AWS has a higher-level abstraction concept: The Virtual Private Cloud.</p>
<p>Amazon <strong>Virtual Private Cloud </strong>(Amazon <strong>VPC</strong>) is a segment of the AWS cloud that allows you to group and segregate your resources in subnetworks to organize and plan your infrastructure matching your requirements. It also allows you to create a VPN between AWS and your physical data center to extend the latter one, adding more resources from AWS. Also, when you create a resource in EC2, you have the possibility of creating the resource in your custom defined subnet within your VPC.</p>
<p>Before jumping into what a VPC looks like, let's first explain how AWS works regarding the geographical distribution of resources. AWS provides you with different data centers in different regions such as Europe, Asia, and the US. As an example, let's take EU West, which has three different availability zones:</p>
<div class="CDPAlignCenter CDPAlign"><img height="260" width="400" class="image-border" src="assets/421b84ce-78c3-473c-8100-bc2170eee84d.png"/></div>
<p>The concept of region in AWS is basically a geographical area where the AWS data center lives. Knowing this information enables us to build global scale applications that serve the traffic from the closest data center in order to improve latency. Another very good reason for this geographical distribution is the data protection laws in several countries. By being able to choose where our data lives, we can enforce the compliance with the laws.</p>
<p>Inside of these geographical regions, sometimes, we can find availability zones. One availability zone is basically a physically separated data center that ensures the high availability of our system, as in the case of a catastrophe in one of the data centers, we can always fall back on the other availability zones.</p>
<p>Let's see how the regions and availability zones look:</p>
<div class="CDPAlignCenter CDPAlign"><img height="208" width="379" class="image-border" src="assets/9a18dceb-63b1-4dca-9830-1b276a4e13ce.png"/></div>
<p class="CDPAlignLeft CDPAlign">Now that we understand how AWS works from the geographical perspective, let's dig deeper into what a VPC is in terms of regions and availability zones.</p>
<p class="CDPAlignLeft CDPAlign">A VPC is a logically separated segment of the AWS cloud that is private to the user, can hold resources, and spans across all the availability regions in an AWS zone. Inside of this VPC, we can define different subnets (public and privates in different availability zones) and define which machines are reachable from the Internet: AWS allows you to create routing tables, Internet gateways, and NAT gateways among other common networking resources that enable the user to build anything that they can build in a physical data center.</p>
<p class="CDPAlignLeft CDPAlign">It would take a full book just to talk about the networking in AWS. We will go deeper into some concepts in the rest of the chapters of this book, but if you really want to dive deep into the networking side of AWS, you can find more data and examples at <a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Introduction.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Introduction.html</a>.<a href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Introduction.html"/></p>
<p class="CDPAlignLeft CDPAlign">AWS also provides a very powerful element: <strong>Elastic Load Balancing </strong>(<strong>ELB</strong>). An ELB is a modern version of the classic hardware load balancer. It enables us to health-check resources and only get the healthy ones <span>into the pool</span>. Also, AWS comes in two flavors: classic load balancer and application load balancer. The first version is, as the name suggests, an application load balancer that distributes the traffic depending on health checks and does not understand the data being transmitted, whereas the application load balancer can route the traffic based on advanced policies dependent on the information of the request. ELBs can also handle the full HTTPS flow so that we can carry the SSL termination in the load balancer and allow our applications to offload the encryption/decryption to them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storage in AWS and EC2</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Up until now, we have exposed how to create machines and networking infrastructure in AWS. One important thing when building applications is the storage of the data. By default, when we launch a machine in EC2, there are two types of storage that can be associated with the machine in the root volume in order to run the operating system:</p>
<ul>
<li>
<p>Instance storage backed images</p>
</li>
<li>
<p><strong>Amazon Elastic Block Store </strong>(<strong>Amazon</strong> <strong>EBS</strong>) storage backed images</p>
</li>
</ul>
<p class="CDPAlignLeft CDPAlign">The first one, instance storage backed images, relies on the storage associated with the image to mount and run the root volume. This means that the data stored in the image will be lost once the machine is terminated (these type of images do not support the stop action; they just support termination).</p>
<p class="CDPAlignLeft CDPAlign">The second type of instances are the ones backed by EBS. Elastic Block Store is the name that AWS gives to its storage capabilities. With EBS, the user can create and destroy volumes (block devices) as needed as well as snapshots: we can create a copy of a running image before carrying a risky operation so we can restore it if something goes wrong.</p>
<p class="CDPAlignLeft CDPAlign">The type of storage can vary depending on our needs: you can create things from magnetic block devices to SSD drives as well as general-purpose units that can cover a lot of the use cases in all the applications.</p>
<p class="CDPAlignLeft CDPAlign">In general, all the instances are backed by EBS as the fact that the storage is a logically segregated from compute enables us to do things such as resizing an instance (for example, creating a more powerful instance) without losing the data.</p>
<p class="CDPAlignLeft CDPAlign">Several volumes can be mounted into the same EC2 instance that gets exposed to it as if a physical device were attached to it, so if we are using a Linux-based image (such as Ubuntu), we can use the mount <span>command</span> to mount the devices into folders.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon S3</h1>
                </header>
            
            <article>
                
<p>Amazon <strong>Simple Storage Service </strong>(Amazon <strong>S3</strong>) is, as described by its name, a simple way of storing a large amount of data on the cloud at a very low cost with a nice set of features. Unlike EC2 storage based on devices with predefined size, Amazon S3 is practically a key value storage that enables us to identify data with a key. Unlike other key value storage technologies, S3 is prepared to store from tiny to very large objects (up to 5 terabytes) with very low response times and that are accessible anywhere.</p>
<p>In the same way as EC2, Amazon S3 is a feature that has the concept of regions, but S3 does not understand availability zones: the S3 service itself manages to get the objects stored on different devices, so you don't need to worry about it. The data is stored in an abstraction called buckets that, if we try to compare S3 with a filesystem, would be the equivalent to a folder but with one catch: the bucket name has to be unique across all the regions on your AWS account so we can't create one bucket called <kbd>Documents</kbd> in two different regions.</p>
<p>Another advantage of S3 is that AWS provides a REST API to access objects in a very simple way, which makes it fairly easy to use it as storage for the modern web.</p>
<p>One of the best use cases that I've come across in my professional life for S3 is the management of a large number of documents in a financial institution. Usually, when companies are dealing with money, they have to onboard the customers to a process called <strong>Customer Due Diligence</strong> (<strong>CDD</strong>). This process ensures that the customers are who they claim to be and that the money is coming from a valid source. The company also has to keep the documents for a minimum of 6 years due to financial regulations.</p>
<p>In order to carry on this investigation, the users need to send documents to the company, and Amazon S3 is the perfect match for it: the customer uploads the documents to the website of the company, which in reality is pushing the documents to S3 buckets (one per customer) and replicating them across regions with the Amazon S3 replication feature. Also, S3 provides another interesting feature for this model: links to objects that expire within a time frame. Basically, this enables you to create a link that is valid only for a period of time so that if the person reviewing documents exposes the link to a third party, S3 will reply with an error, making it really hard to leak documents accidentally (the user could always download it).</p>
<p>Another interesting feature of S3 is the possibility of integrating it with Amazon <strong>Key Management System</strong> (Amazon <strong>KMS</strong>), another feature provided by AWS), so all our objects in S3 are encrypted by a key stored in KMS that can be transparently rotated periodically.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon ECR and ECS</h1>
                </header>
            
            <article>
                
<p>Containers are the new norm. Every single company that I've come across in the last few years is using or considering using containers for their software. This enables us to build software with the microservices principles in mind (small individual software components running independently) as it provides a decent level of abstraction from the configuration and deployment of different apps: basically, the entire configuration is stored in a container and we only need to worry about how to run it.</p>
<p>Amazon, as one of the pioneers of the microservices architectures, has created its own image registry and cluster (service).</p>
<p>As we will see in depth in <a href="fee45e6c-df39-48ae-ab43-a18911facbd8.xhtml" target="_blank">Chapter 3</a>, <em>Docker,</em> is built around two concepts: images and containers. An image is a definition of an application (configuration <em>+</em> software), whereas a container is an instance of the running <span>image</span>. The image is built through a Dockerfile (a description of the image with a very basic script language) and stored in a registry, in this case, Amazon <strong>EC2 Container Registry </strong>(<strong>ECR</strong>), our private registry in the AWS infrastructure. We don't need to worry about availability or managing resources; we just choose the region where our containers are going to run and push our images into that repository.</p>
<p>Then, from our host running Docker, the image is pulled and the container is instantiated. This is simple and effective, but there are a few considerations:</p>
<ul>
<li>What happens when our host does not have enough resources to run as many containers as we want?</li>
<li>What happens if we want to ensure the high availability of our containers?</li>
<li>How do we ensure that the containers are restarted when they fail (for some reason)?</li>
<li>How can we add more hardware resources to our system without downtime?</li>
</ul>
<p>All those questions were trickier a few years ago but are simple now: <span>A</span>mazon <strong>EC2 Container Service </strong>(Amazon <strong>ECS</strong>) will take care of it for us. ECS is basically a cluster of resources (EC2 machines) that work together to provide a runtime for our containers to be executed.</p>
<p>Within ECS, when creating a new service, we specify parameters such as how many replicas of our container should be running at the same time as well as what configuration (image) our container is supposed to use. Let's see how it works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a cluster</h1>
                </header>
            
            <article>
                
<p>First, we are going to create a cluster in the AWS console and see how it works.</p>
<ol>
<li>Go to the Amazon ECS page and click on <span class="packt_screen">Get started</span> button (the only button in the screen as you haven't created any resources yet):</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="310" width="867" class="image-border" src="assets/bb1f951f-e6fe-43e7-9c58-08e68be6cb0f.png"/></div>
<ol start="2">
<li>Make sure that the two checkboxes are ticked before continuing. We want to deploy a sample application to ECS but also we want to store the images in ECR.</li>
<li>The next screen is key: this is where we define the repository of our image, which will determine the repository URI that will be used for pushing images from our local machine using Docker.</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="329" width="772" class="image-border" src="assets/a3ce3b43-8026-4151-9ba7-6b8298935224.png"/></div>
<ol start="4">
<li>Just use <kbd>devops-test</kbd> as the repository name, and our repository URI will look very similar to the one shown in the preceding screenshot.</li>
<li>Step number 2 (out of 6) is a series of commands provided by AWS to log in into ECR and push the images of our project. In this case, we are going to use a very simple application in <kbd>Node.js</kbd>:</li>
</ol>
<pre><span class="token keyword">       var</span> express <span class="token operator">=</span> <span class="token function">require<span class="token punctuation">(</span></span><span class="token string">'express'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">       var</span> app <span class="token operator">=</span> <span class="token function">express<span class="token punctuation">(</span></span><span class="token punctuation">)</span><span class="token punctuation">;</span>

       app<span class="token punctuation">.</span><span class="token keyword">get</span><span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span>req<span class="token punctuation">,</span> res<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        res<span class="token punctuation">.</span><span class="token function">send<span class="token punctuation">(</span></span><span class="token string">'Hello World!'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">      }</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

        app<span class="token punctuation">.</span><span class="token function">listen<span class="token punctuation">(</span></span><span class="token number">3000</span><span class="token punctuation">,</span> <span class="token keyword">function</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
          console<span class="token punctuation">.</span><span class="token function">log<span class="token punctuation">(</span></span><span class="token string">'Example app listening on port 3000!'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">        }</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre>
<ol start="6">
<li>Save the code from earlier in a file called <kbd>index.js</kbd> within a folder called <kbd>devops-test</kbd> on your local machine. As we are using express, we need to install the required dependency. Just execute the following command:</li>
</ol>
<pre><strong>       npm init</strong></pre>
<ol start="7">
<li>After a few questions (just press Enter a few times and it should work), a file called <kbd>package.json</kbd> should be created. Now we need to install express for our program to run:</li>
</ol>
<pre><strong>      npm install --save express</strong></pre>
<ol start="8">
<li>And voila! Our <kbd>package.json</kbd> file should have a line describing the required dependency:</li>
</ol>
<pre>       {
            "name": "code",
            "version": "1.0.0",
            "description": "",
            "main": "index.js",
        "scripts": {
        "test": "echo "Error: no test specified" <br/>       &amp;&amp; exit 1",
        <strong>"start": "node index.js"</strong>
       },
       "author": "",
       "license": "ISC",
       "dependencies": {
       "express": "^4.14.1"
       }
     }</pre>
<ol start="9">
<li>This file allows us to reinstall the dependencies whenever required without having to do it manually; it also allows us to specify a command that will be run when we execute <kbd>npm start</kbd> (a standard way of running a Node app using npm). Add the line and highlight it, as shown in the preceding code, as we will need it later (don't forget the semicolon from the previous line).</li>
<li>Now we need to write our Dockerfile. A Dockerfile, as we will see in <a href="fee45e6c-df39-48ae-ab43-a18911facbd8.xhtml">Chapter 3</a>, <em>Docker, </em>is a file that describes what our Docker image looks like. In this case, we are going to reconstruct the steps needed to run the node application in a Docker container:</li>
</ol>
<pre><span class="token keyword">    FROM</span> node<span class="token punctuation">:latest</span>

<span class="token keyword">    RUN</span> mkdir <span class="token punctuation">-</span>p /usr/src/app
<span class="token keyword">    WORKDIR</span> /usr/src/app

<span class="token keyword">    COPY</span> package.json /usr/src/app/
<span class="token keyword">    RUN</span> npm install

<span class="token keyword">    COPY</span> . /usr/src/app

<span class="token keyword">    EXPOSE</span> 3000
<span class="token keyword">    CMD</span> <span class="token punctuation">[</span> <span class="token string">"npm"</span><span class="token punctuation">,</span> <span class="token string">"start"</span> <span class="token punctuation">]</span></pre>
<ol start="11">
<li>Don't try to understand the file; we will go deeper into this later on this book. Just save it with the name <kbd>Dockerfile</kbd> in the folder mentioned previously, <kbd>devops-test</kbd>. By now, your <kbd>devops-test</kbd> folder should look like this:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/7513e983-3dc5-4e88-8ae6-b2e3332d865f.png"/></div>
<ol start="12">
<li>Now we are ready to follow step 2 in the ECS setup. Be aware that the following image is regarding my user in AWS; your user will have different parameters, so use yours instead of copying from the preceding screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="658" width="813" class="image-border" src="assets/17a7cc60-606f-4234-ba23-a2ad9ee811fd.png"/></div>
<p style="padding-left: 90px">Once you finish it, a new version of the image with your app image should be installed in your private ECR.</p>
<ol start="14">
<li>The next step (step 3) is creating what AWS calls a task definition, which is basically the configuration for one instance of our containers: how much memory we are going to use, which image we are going to run, and what ports we are going to expose in the container. Just leave the default memory but change the port to <kbd>3000</kbd>, as it is the port that we used in the preceding example (the node application). This is typical docker parameter and we will learn more about it in the next chapter, where we will dive deeper into docker.</li>
<li>Once you are ready, click on next and we will be with step 4. This step is where we are going to configure a service. By service, we mean the number of instances of our container are we going to keep alive and how are we going to expose them: using a load balancer or just in the EC2 instances that are part of the cluster. We will also be able to specify which IAM (AWS credential system) is going to be used for registering and deregistering running instances:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="639" width="811" class="image-border" src="assets/f53dfe50-15ed-465d-913c-a4bd3193f608.png"/></div>
<ol start="16">
<li>We just leave everything by default except two parameters:
<ul>
<li>The desired number of tasks: set to <kbd>2</kbd></li>
<li>In the ELB section, we just select sample-app: <kbd>80</kbd> (or the option that isn't <span class="packt_screen">No ELB</span> so AWS provisions an ELB for us)</li>
</ul>
</li>
<li>Click on the <span class="packt_screen">Next step</span>, where we are going to define what our cluster is going to look like:
<ul>
<li>The number of nodes</li>
<li>The size of the nodes</li>
</ul>
</li>
<li>Once we are ready, just review and launch <span>the instance</span>. After a few minutes, our cluster should be up and running and ready to work with our deployed <span>task</span>.</li>
</ol>
<p>You can access the instance of the task that we created in the load balancer provisioned by the cluster itself on the port <kbd>3000</kbd>. As you can see, ECS makes the task of setting up a container cluster simple.</p>
<p>In this book, we are going to give special attention to Kubernetes and Docker Swarm mainly because they are platform agnostic technologies, but I believe that Amazon ECS is a very valid technology to be considered when building a new container-based system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other services</h1>
                </header>
            
            <article>
                
<p>As you can see, the list of services in AWS is pretty much endless. We have visited the ones that I consider the most important, and in the following chapters, we will visit some of them that are also interesting, but unfortunately, we cannot go in deep through all of them. However, AWS is pretty good in terms of the documentation, and every service always comes with quite a comprehensive explanation on how to use it.</p>
<p>In this section, we are going to touch base with some services that, even though are quite important, are not core to the development of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Route 53</h1>
                </header>
            
            <article>
                
<p>Route 53 is the DNS service in AWS. It is a global and scalable DNS service that allows you to perform some advanced operations:</p>
<ul>
<li>Register domain names</li>
<li>Transfer domain names from other registrars</li>
<li>Create traffic routing policies (such as failovers across regions)</li>
<li>Monitor the availability of your applications (and reroute the traffic to healthy instances).</li>
</ul>
<p>With Route 53, we can link domain names to AWS resources, such as load balancers, S3 buckets, and other resources, enabling us to expose a human-readable name for our resources (mainly VMs) created within our AWS instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CloudFront</h1>
                </header>
            
            <article>
                
<p>CloudFront solves one of the biggest problems that low traffic websites experience when a spike in visits happens: it provides a cache in a way that makes us wonder whether AWS is the one that serves the data and not our server. Basically, CloudFront intercepts the request to our host, renders the page, and keeps it for up to 24 hours so our site offloads the traffic to AWS. It is designed for serving static content, as the second time that the user hits the same URL, the cached version will be served instead of hitting your server again.</p>
<p>It is highly recommended that you use CloudFront for the brochure site of your company so that you can serve all the traffic with a very small machine, saving some money in resources but also being able to improve your uptime when a traffic spike hits your site.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon ElasticCache</h1>
                </header>
            
            <article>
                
<p>Amazon ElasticCache, as the name suggests, is a distributed and scalable in-memory cache system that can be used to store cached data within our applications.</p>
<p>It solves one of the biggest problems that we can face when building an application that relies on a cache for storing and retrieving data: high availability and a consistent temporary datastore.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon RDS</h1>
                </header>
            
            <article>
                
<p><strong>RDS</strong> stands for <strong>Relational Database Service</strong>. With RDS, you can provision DB instances with a few clicks that could be used to store data: Oracle, MySQL, and MariaDB are some of the options that we have for RDS. It leverages the high availability to the underlying DB system, which might be a problem if we are looking to rely on AWS for it, but it is usually acceptable as high availability in SQL databases is a complicated subject.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DynamoDB</h1>
                </header>
            
            <article>
                
<p>DynamoDB is a fine piece of engineering. It is a NoSQL database that is fine-tuned down to the millisecond of latency at any scale. It stores objects instead of rows (SQL cannot be used) and is a good candidate for storing a big amount of data in a schema-less fashion. DynamoDB, in essence, is very similar to MongoDB, but there is a basic difference: DynamoDB is a service provided by AWS and can run <span>only within AWS, whereas MongoDB is a software that can be installed anywhere, including AWS. From the functional point of view, the majority of use cases for MongoDB are valid for modeling DynamoDB databases.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Cloud Platform</h1>
                </header>
            
            <article>
                
<p>Google has always been at the top of the hill when it comes to technology. Surprisingly, Google didn't have a federated layer of services; instead, it offered every service <span>separately</span>, which was far from ideal in providing a solid platform for developers to build applications on top of it. In order to solve that, they released Google Cloud Platform, which is a collection of services (infrastructure as a service, platform as a service, containers and big data, as well as many other features) that enables developers and companies to build highly reliable and scalable systems with some of the most up-to-date features, such as Kubernetes and a set of unique machine learning APIs.</p>
<p>The interface is also one of the main points in Google Cloud: it offers you a web console where you basically have an available <kbd>ssh</kbd> Terminal that is connected to all your services, and you can operate from there without the need for any configuration on your local machine. Another good point in the interface is the fact that they use the terminology in the traditional sysadmin world, making the learning curve easy for the majority of the services.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/96e99619-c350-4e5f-8124-7c271c72cfd3.png"/></div>
<p>In the same way as AWS, Google Cloud Platform allows engineers to create resources across the globe in regions and zones in order to ensure the high availability of our systems as well as the compliance with local laws.</p>
<p>But the real jewel in the crown is their container engine. I am a big fan of container orchestration. Nowadays, everyone is gravitating toward microservices-based systems, and it is not strange to see companies hitting the wall of the operational reality of a microservices-based system: this is impossible to manage without orchestration tools. From all the potential choices on the market (Amazon ECS, Docker Swarm, and DCOS), there is one in particular that has been a game changer in my life: Kubernetes.</p>
<p>Kubernetes is the answer to the question that I raised during the writing of my first book (<em>Developling Microservices with Node.js</em>): how can we efficiently automate the operations in a microservices environment by providing a common ground between development and operations? Kubernetes has incorporated all the expertise from working with containers that Google has accumulated through the years in order to create a product that provides all the necessary components for the efficient management of deployment pipelines.</p>
<p>In this book, we are going to place a special emphasis on Kubernetes, as in my opinion, it is the solution to many of the problems that teams have today when scaling up in members and resources.</p>
<p>In order to start working with GCP, Google offers a trial version of 300 USD credit or 60 days free of charge test, which is more than enough to get your head around the majority of the services and, of course, more than enough to follow the examples of this book and play around with the majority of the concepts that we are going to be exposing. I would recommend that you activate your trial period and start playing around: once the credit is used or the 60 days are over, Google requires explicit confirmation to activate the billing so there is not going to be any extra charge in your account (this is the case at the time of writing this).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Compute Engine</h1>
                </header>
            
            <article>
                
<p>Google Compute Engine is the equivalent of EC2 in Amazon Web Services. It allows you to manage instances of machines, networks, and storage with a simplicity that I have never seen before. One of the downsides that I found when ramping up with AWS is the fact that they have created abstractions with names that are not very intuitive: Virtual Private Cloud, Elastic Block Storage, and many more. This is not a big deal as AWS is well known in the market, but Google got the message and has named its resources in a very intuitive way, facilitating the onboarding of new people into the platform with little to no effort.</p>
<p>Regarding the machine types, Google Cloud Platform provides a simplified and limited set of machines when compared to AWS but enough variety to satisfy our needs. One of the features to keep in mind with Google Cloud Platform is the fact that the hardware improves with the size of the instance, which means that the 64 cores machines get a better CPU than the two core machines.</p>
<p>Google Cloud Platform also provides a CLI tool to interact with the resources in GCP from a Terminal. In order to install it, just access this URL: <a href="https://cloud.google.com/sdk/">https://cloud.google.com/sdk/</a>.<a href="https://cloud.google.com/sdk/"/></p>
<p>Then, follow the instructions depending on your operating system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standard machine types</h1>
                </header>
            
            <article>
                
<p><span>The standard machines</span> are the most common to be used by any application. They offer a balance between CPU and memory that suits the majority of the tasks in all the projects that you can possibly imagine. These types of machines offer 3.75 GB of RAM for every single virtual CPU. Let's look at a few examples:</p>
<table>
<tbody>
<tr>
<td>
<p>Name</p>
</td>
<td>
<p>CPUs</p>
</td>
<td>
<p>Memory</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-standard-1</kbd></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>3.75 GB</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-standard-2</kbd></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>7.50 GB</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-standard-64</kbd></p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>240 GB</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As you can see, the naming convention is fairly straightforward and is easy in order to guess the machine RAM and the number of CPUs out of the canonical name.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High-memory machine types</h1>
                </header>
            
            <article>
                
<p>These machines are optimized for memory-intensive applications. They come with an extra amount of RAM for every virtual CPU that allows you to go the extra mile regarding memory power.</p>
<p>Every machine of the high-memory type comes with 6.5 GB of RAM for every single virtual CPU, and here are a few examples:</p>
<table>
<tbody>
<tr>
<td>
<p>Name</p>
</td>
<td>
<p>CPUs</p>
</td>
<td>
<p>Memory</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-highmem-2</kbd></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>13</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-highmem-8</kbd></p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>52</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-highmem-64</kbd></p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>416</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>These machines come with a massive amount of RAM and are well suited for distributed caches, databases, and many other types of applications that require a high memory consumption relative to the CPU power.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High-CPU machine types</h1>
                </header>
            
            <article>
                
<p>As the name states, high-CPU machines are instances that hold a high CPU/memory ratio with only 0.9 GB of RAM for every virtual CPU, which indicates that they are well suited for saving some money on high-intensive CPU tasks (as we cut down on a lot of memory). Here are some examples of these machines:</p>
<table>
<tbody>
<tr>
<td>
<p>Name</p>
</td>
<td>
<p>CPUs</p>
</td>
<td>
<p>Memory</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-highcpu-2</kbd></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1.8 GB</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-highcpu-8</kbd></p>
</td>
<td>
<p>8</p>
</td>
<td>
<p>7.2 GB</p>
</td>
</tr>
<tr>
<td>
<p><kbd>n1-highcpu-64</kbd></p>
</td>
<td>
<p>64</p>
</td>
<td>
<p>57.6 GB</p>
</td>
</tr>
</tbody>
</table>
<p>As you can see, the only difference between the standard or high memory machines is that these machines are built with less amount of RAM, which allows us to save money on a resource that won't be used in some applications that are able to create machines with more CPUs at the same price. High-CPU machines are well suited for applications that require high CPU and low memory consumption, such as mathematical processing or other types of calculations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shared-core machine types</h1>
                </header>
            
            <article>
                
<p>Sometimes, we really don't need a dedicated machine for our process, so Google Cloud offers shared machines that you can use for it. In my opinion, the shared-core machines are not suited for production usage, but they could well serve a prototype or experimenting with different resources. Here are the two types of machines:</p>
<table>
<tbody>
<tr>
<td>
<p>Name</p>
</td>
<td>
<p>CPUs</p>
</td>
<td>
<p>Memory</p>
</td>
</tr>
<tr>
<td>
<p><kbd>f1-micro</kbd></p>
</td>
<td>
<p>0.2</p>
</td>
<td>
<p>0.6</p>
</td>
</tr>
<tr>
<td>
<p><kbd>g1-small</kbd></p>
</td>
<td>
<p>0.5</p>
</td>
<td>
<p>1.7</p>
</td>
</tr>
</tbody>
</table>
<p>As you can see, there are only two options with different RAM and CPU power. I personally use these machines when I want to experiment with new software or new products of the Google Cloud Platform.</p>
<p>Don't forget that these are bursting machines that are only suited for short burst of intensive processing and not for sustained resource consumption as the CPU is shared across different applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom machines and GPU processing</h1>
                </header>
            
            <article>
                
<p>Sometimes, we need an extra amount of something on our machines, which is <span>usually</span> not in the predefined machine instances of other providers, but in this case, Google Cloud Platform comes to the rescue with an amazing feature: custom machine types.</p>
<p>With custom machine types in Google Cloud Platform, we can get the benefit of the upgraded hardware of the large machines in a resource-modest machine or create specific configurations that suit our needs.</p>
<p>One of the best examples that we can find for custom machines is when we want to add some GPU processing to our mix. In Google Cloud, GPUs can be attached to any non-shared (<kbd>f1</kbd> or <kbd>g1</kbd>) machine on demand. With the ability to create our custom machine types, we can define how many GPUs we want to burst our processing power in.</p>
<p>In general, when I design a system, I try to stick to the standard types <span>as much as possible</span> in order to simplify my setup, but there is nothing wrong in creating custom machine types aside from the fact that we can easily fall in the premature optimization of our system, which is probably one of the biggest problems that you can find when working in IT.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching an instance</h1>
                </header>
            
            <article>
                
<p>In Google Cloud Platform, everything is grouped in projects. In order to create resources, you need to associate them with projects, so the first step to launch an instance is to create a project. In order to do that, just select the new project button when entering the Google Cloud Platform interface the first time or in the drop-down in the top bar when you have already created one project:</p>
<ol>
<li>For the examples of this book, I am going to create a project called <kbd>Implementing Modern DevOps</kbd>, which I will to be using for running all the examples:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2f4f4f93-736a-4c28-a06a-19f235a856f1.png"/></div>
<ol start="2">
<li>Once we have created our project, we proceed to create a new VM instance. Even though it is possible to create instances with more than 64 cores (with the custom machine types), we are going to stick to the standard ones in order to save costs. Proceed to create the instance with the default values (just change the name):</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="454" width="770" class="image-border" src="assets/a83924d5-9f59-48a8-952c-f518bad5103e.png"/></div>
<ol start="3">
<li>There are two details that I really like from Google Cloud Platform:
<ul>
<li>How easy they name their resources and make everything clear to understand</li>
<li>How transparent they are with the pricing</li>
</ul>
</li>
<li>While creating a virtual machine in Google Cloud Platform, these two characteristics are present: the form to create a machine has only a   few fields, and it gives you the cost of the machine per month (so there are no surprises).</li>
<li>In the same way as AWS, Google Cloud Platform allows you to select the region and the zone (remember, a physically separated data center) where your instance is going to live in order to ensure the high availability of the overall system.</li>
<li>Also, this (not in the preceding figure) allows you a couple of clicks in two checkboxes in order to allow the <kbd>http</kbd> and <kbd>https</kbd> traffic into the instance from the outer world. This is just as simple and effective.</li>
</ol>
<ol start="7">
<li>You can also configure other things, such as networking, ssh keys, and other parameters that we are going to skip for now. Just click on the <span class="packt_screen">Create button (at the bottom of the form) and wait until the machine is fully provisioned (it might take up to few minutes), and you should see something similar to what is shown in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/65df0081-936b-4bb2-9f9d-257c56928f01.png"/></div>
<ol start="8">
<li>One of the most appealing features of Google Cloud Platform is how curated their usability is. In this case, you can see a column in your machine description called <span class="packt_screen">Connect</span> that allows you to connect to the machine in a few different ways:
<ul>
<li> SSH</li>
<li>The <kbd>gcloud</kbd> command (a command-line tool from GCP)</li>
<li>Using another ssh client</li>
</ul>
</li>
<li>We are going to select SSH (the default one) and click on the SSH <span>button</span>. A popup should appear on the screen, and after a few seconds, we should see something similar to an <kbd>ssh</kbd> Terminal, which is a Terminal in our machine:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="420" width="806" class="image-border" src="assets/7b23439c-d51b-45b2-a0fe-b55db8b59341.png"/></div>
<p>This is a very neat and useful feature that basically enables the engineer to avoid carrying a set of cryptographic keys that are always a risk as if they get leaked, your machines are exposed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Networking</h1>
                </header>
            
            <article>
                
<p>One thing I cannot stress enough about the Google Cloud Platform is how it simplifies the concepts and make them look similar to the real-world physical data center concepts. The case of the networking was not an exception: all the concepts and names can be mapped one to one to real world physical network concepts.</p>
<p>In Google Cloud, we can implement any required design that follows the principles of the IP networking (the same as AWS) with pretty much a few clicks. Another interesting feature that Google Cloud offers (along with other providers such as AWS) is the possibility of extending your data center into the cloud with a VPN network taking the benefits of the cloud products but achieving the level of security required by the most sensitive data that you could imagine.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Container Engine</h1>
                </header>
            
            <article>
                
<p>The <strong>Google Container Engine</strong> (<strong>GKE</strong>) is a proposal from Google for the container orchestration making use of one of the most powerful container clusters available in the market: Kubernetes.</p>
<p>As we will discuss further in <a href="4e6a965e-c4bf-491d-9f60-c013269350c9.xhtml" target="_blank">Chapter 7</a>, <em>Docker Swarm and Kubernetes- Clustering Infrastructure, </em>Kubernetes is a feature-full cluster used for deploying and scaling container-based applications in a controlled manner, with a special emphasis on defining the common language between development and operations: a framework that blends development and operation concepts into a common ground: a YAML (or JSON) description of resources.</p>
<p>One of the big problems of Kubernetes is ensuring high availability. When you deploy a cluster on premises or in a cloud provider, making use of the computing power (EC2 in AWS or Compute Engine in GCP), you are responsible for upgrading the cluster version and evolving it with the new releases of Kubernetes. In this case, Google Cloud Platform, through the container engine, has solved the operational problem: GCP keeps the master and is responsible for keeping it up to date and the users upgrade the nodes when a new version of Kubernetes is released, which allows us to articulate different procedures for upgrading our cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a cluster</h1>
                </header>
            
            <article>
                
<p>In <a href="4e6a965e-c4bf-491d-9f60-c013269350c9.xhtml" target="_blank">Chapter 7</a>, <em>Docker Swarm and Kubernetes- Clustering Infrastructure, </em>you are going to learn how to operate Kubernetes, but it is worth teaching you how to set up a cluster in the GKE in this chapter in order to show how easy it is before diving deep into the core concepts of Kubernetes:</p>
<ol>
<li>First, go to <span class="packt_screen">Container Engine</span> within <span class="packt_screen">Google Cloud Platform</span>:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="341" width="814" class="image-border" src="assets/f77b6e67-5289-48f8-b288-2586a052b9e0.png"/></div>
<ol start="2">
<li>As you can see, there are no clusters set up, so we have two options:
<ul>
<li><span class="packt_screen">Create a new container cluster</span></li>
<li><span class="packt_screen">Take the quickstart</span></li>
</ul>
</li>
<li>We are just going to click on <span class="packt_screen">Create a container cluster</span> and follow up the onscreen instructions (a form) in order to set up our cluster:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="536" width="706" class="image-border" src="assets/075bff92-ef20-4785-9c1c-41191c88b23a.png"/></div>
<ol start="4">
<li>Ensure that <span class="packt_screen">Zone</span> is the closest to your geographical area (even though right now it doesn't really matter) and the size is <kbd>3</kbd>. This parameter, the size, is going to ask GCP to create <kbd>3</kbd> instances in the Compute Engine in order to set up the cluster plus a master that is managed by GCP itself. Regarding the image, we have two options here, <kbd>gci</kbd> or <kbd>container-vm</kbd>. In this case, again, it doesn't really matter as it is just a test cluster, but just note that if you want to use NFS or any other advanced filesystem, you will need to use <kbd>container-vm.</kbd></li>
<li>Click on <span class="packt_screen">Create</span>, and after few minutes, you should see two things:
<ul>
<li>The cluster is created in the Google Container Engine section</li>
<li>Three new VMs are provisioned in the Compute Engine section</li>
</ul>
</li>
</ol>
<ol start="6">
<li>This is a very smart setup because with some commands using the google cloud platform command tool (<kbd>gcloud</kbd>), we can scale up or down or cluster as well as change the size of our instances in order to satisfy our needs. If you explore the cluster (by clicking on its name), you will find a <span class="packt_screen">Connect to the cluster</span>link, which leads to a screen with instructions to connect to the Kubernetes dashboard.</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img height="418" width="739" class="image-border" src="assets/0843b6c9-0cb4-4b10-9a88-08aadb91deb2.png"/></div>
<ol start="7">
<li>Sometimes, these instructions fail, and that is because <kbd>gcloud</kbd> is badly configured. If you find an error trying to configure the access to the cluster, run the following command:</li>
</ol>
<pre><strong>       gcloud auth login &lt;your email&gt;</strong></pre>
<ol start="8">
<li>Then, follow the instructions. Assuming that you have already configured the Google Cloud SDK, everything should work fine, and after running the <kbd>kubectl proxy</kbd> command, you should be able to access the Kubernetes dashboard at <kbd>http://localhost:8001/ui</kbd>.</li>
<li>In order to test whether everything works as expected, just run a simple image in Kubernetes (in this case, a <kbd>busybox</kbd> image):</li>
</ol>
<pre><strong>        kubectl run -i busybox --image=busybox</strong></pre>
<ol start="10">
<li>If we refresh the dashboard (<kbd>http://localhost:8001/ui</kbd>) while running the Kubernetes proxy (as specified earlier), we should see something similar to what is shown in the following figure in the <span class="packt_screen">Deployments</span> section:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/86e9d64a-da62-4f42-95c6-3203f82a6ef3.png"/></div>
<p>This indicates that the deployment (a Kubernetes concept that we will explore in <a href="4e6a965e-c4bf-491d-9f60-c013269350c9.xhtml" target="_blank">Chapter 7</a>, <em>Docker Swarm and Kubernetes- Clustering Infrastructure</em>) was successful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other Google Cloud Platform products</h1>
                </header>
            
            <article>
                
<p>Google Cloud platform is not only Compute Engine and Container Engine, but it is also a collection of services that are very interesting for different purposes. As things are limited in scope, we won't see the majority of them and will only focus on the ones that are more relevant to the DevOps world.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google App Engine</h1>
                </header>
            
            <article>
                
<p>Up until now, we have been working with a side of DevOps called <span><strong>IaaS</strong></span>. Google Cloud platform also offers something called <strong>Platform as a Service</strong> (<strong>PaaS</strong>). In an IaaS model, we need not worry about the underlying infrastructure: provisioning machines, installing the software, patching the software. With <strong>Google App Engine</strong> (or any other major PaaS), we forget about the ops of our infrastructure and focus on the development of our application, leveraging the underlying infrastructure to Google. Instead of launching a machine and installing Java to run our Spring Boot-based application, we just specify that we want to run a Java application, and GCP takes care of everything else.</p>
<p>This product, the Google App Engine, fits the necessity of the majority of the small to mid sized projects, but in this book, we are going to focus on the DevOps that maintaining an IaaS involves.</p>
<p>Google App Engine also provides us with features such as user management, which is a recurring problem in all the applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine Learning APIs</h1>
                </header>
            
            <article>
                
<p>Google has always been famous for its innovation across the technology products that it has released. It has changed how people use e-mail with Gmail and how people use phones with Android.</p>
<p>Regarding <strong>Machine Learning</strong>, they are also shaking up the world with an innovative set of APIs that people can use to process images (with the vision APIs), translate documents (with the translations API), and analyze large amounts of text with the natural language API.</p>
<p>One of the most amazing uses that I have seen of the vision API is a company that had to do some level of photo ID verification for its customers. There was a huge problem of people uploading invalid images (random images or even images with part of the face covered or similar), so we used the vision API to recognize images that contained a face without facial hair, hat, or any other accessories aside from glasses.</p>
<p>The result was that the people doing the ID verification focused just on valid images instead of having to classify them as valid or invalid before proceeding to the verification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Big data</h1>
                </header>
            
            <article>
                
<p>Big data is now a big thing. Everybody is trying to take the advantage of big data to explore new areas of business or unleash their potential in traditional businesses.</p>
<p>Google Cloud Platform offers a set of big data APIs that enable the users to carry on pretty much any task in large sets of data. With tools such as BigQuery, a data analyst can run queries on terabytes of information in seconds without setting up a massive scale infrastructure.</p>
<p>In general, the big data APIs from Google are what is called no-ops tools <span>in the DevOps world</span>: they don't require maintenance from users as they leverage it into Google. This means that if a big query requires a lot of processing power, Google is the one responsible for transparently offering this power to the user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other cloud providers</h1>
                </header>
            
            <article>
                
<p>Unfortunately, there is a limit to the number of concepts we can develop in a book, and in this case, we are going to focus on AWS and GCP, as they are the most feature-full cloud providers in the market.</p>
<p>I always try to adopt an open mindset regarding technology, and there are three providers that I think you should know about:</p>
<ul>
<li>DigitalOcean</li>
<li>Heroku</li>
<li>Azure</li>
</ul>
<p>They have a lot to offer and they all are up to speed with the new trends of DevOps and security.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Heroku</h1>
                </header>
            
            <article>
                
<p>Heroku's battle horse is this phrase: build apps, not infrastructure. That is a powerful message. Basically, Heroku is going full throttle with the <strong>PaaS</strong> concept <strong>Platform as a Service</strong>, allowing you to avoid maintaining the underlying infrastructure: just specify what you want to run (for example, a Node.js application) and the scale.</p>
<p>With this powerful philosophy, Heroku allows you to easily deploy instances of your application, databases, or communication buses, such as Kafka, with a few clicks and without all the hassle of having to provision them with a DevOps tool, such as Ansible, Chef, or similar.</p>
<p>Heroku is one of the cloud providers preferred by start-ups as you can save a lot of time as opposed to using AWS or Google Cloud Platform, as you just need to focus on your applications, not the infrastructure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DigitalOcean</h1>
                </header>
            
            <article>
                
<p>DigitalOcean is a provider that, even though not as well-known as AWS or GCP, offers a very interesting alternative to small to mid sized organizations to run their cloud systems. They have developed a very powerful concept: the droplet.</p>
<p>Basically, a droplet is a component that can run your software and be connected to different networks (private or public) <span>through some configuration</span>.</p>
<p>In order to assemble a droplet, we just need to define a few things:</p>
<ul>
<li>The image (the operating system or one-click images)</li>
<li>The size</li>
<li>The region</li>
</ul>
<p>And once you have chosen your configuration, the droplet starts running. This is very simple and effective, which is <span>usually what companies look for.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Azure</h1>
                </header>
            
            <article>
                
<p>Azure is the Microsoft push for cloud systems and one of the providers that has grown the most in the last couple of years. As expected, Azure is a ;particularly good platform for running Windows-based applications, but that's not to say we can overlook its capability of running Linux applications as well.</p>
<p>The catalog of products is as complete as the catalog for AWS or Google Cloud Platform, and there is absolutely no reason not to choose Azure as a cloud provider for your systems.</p>
<p>Azure is also one of the newest providers (it became widely available in 2013) in the market, so it has the advantage of being able to solve problems that other providers have presented.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Up until now, we showcased the features of AWS and GCP and introduced some other providers that are very interesting choices when building our systems. One of the advantages of having a good number of competitors in the market is the fact that each one of them has their own strong points and we can combine them by making use of VPNs, creating a big and extended virtual data center across different providers.</p>
<p>Through the rest of the book, we are going to give special attention to AWS and GCP, as they have the most interesting characteristics for a DevOps book (not to overlook the rest of them, but remember, things are limited in terms of space).</p>
<p>We are also going to take a special interest in container clusters such as Kubernetes or Docker Swarm as they are, without any kind of doubt, the future.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>