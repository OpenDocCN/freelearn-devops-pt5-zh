- en: OpenShift HA Design for Single and Multiple DCs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we briefly touched upon OpenShift HA and **h****igh
    availability** (**HA**) in general. We discussed how OpenShift provides redundancy
    in case of a failure and how you can prevent this from happening by designing
    your OpenShift cluster properly. Finally, we finished the chapter with backup
    and restore methods and procedures in OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to talk about OpenShift scenarios in single and
    multiple data centers. This chapter will also explain how to properly design OpenShift
    in a distributed and redundant configuration across one or more data centers.
  prefs: []
  type: TYPE_NORMAL
- en: 'After reading this chapter, you will have an understanding of the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift single-DC HA design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift multi-DC HA design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift single-DC HA design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we briefly covered HA and OpenShift HA in general,
    but we did'nt discuss how to practically design OpenShift in your data center
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s recall what main OpenShift components we have and how they provide redundancy:'
  prefs: []
  type: TYPE_NORMAL
- en: Openshift infrastructure nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift master nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etcd key-value store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift infrastructure nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenShift infrastructure nodes are essential components that provide access
    from the outside of an OpenShift cluster. OpenShift infrastructure nodes scale
    horizontally, which means that we can add as many nodes as we need in order to add
    network throughput. If you recall from the previous chapter, we need to consider
    which VIP method to use. We have two main VIP methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VIP using an external load balancer**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**IP failover using keepalived**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Both methods have their own pros and cons, but the one that provides better
    scalability and smooth migration to multi-DC design is the VIP using an external
    load balancer method. This method allows you to dynamically distribute the load
    among all the infra nodes and add them dynamically without any interruption. If
    you are going to distribute the load across multiple data centers, the virtual
    IP with external load balancers method allows you to implement these changes with
    minimum downtime. We are going to discuss multi-DC design later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift masters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to OpenShift infrastructure nodes, OpenShift masters require redundancy
    and high availability. Redundancy is easily achieved by the number of horizontally
    scalable master nodes, and high availability is achieved by one of the VIP methods
    that we discussed previously. For the same reason, using an external load balancer
    is a way better and scalable solution compared to the keepalived and DNS method.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift nodes do not have any specific HA requirements since they are running
    stateless pods in a redundant fashion. The only HA and redundancy consideration
    is to have enough OpenShift nodes to handle additional workload when a failure
    happens in your data center, whether a server, a rack, or a whole row of racks
    goes down. You need to distribute the workload and make sure that no matter what
    fails, you have redundant components and pods up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd key-value store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenShift etcd is a highly distributed key-value store where all critical OpenShift
    cluster-related data is kept. Etcd works in active/active configuration by default,
    which means it provides both redundancy and high availability by default. There
    is a general recommendation to install and configure your etcd cluster on dedicated
    nodes separately from OpenShift masters in a quantity of three, five, or seven
    members.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: External storage configuration and design for OpenShift persistent data is out
    of the scope of this book, but general advice is to make sure that you have your
    external storage available in a redundant and scalable fashion, meaning that if
    one or several components fails, it does not affect overall storage performance
    and is always accessible by OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Physical placement consideration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Considering that our OpenShift cluster is going to be up and running within
    a single data center, we need to take extra caution and make sure that we follow
    some simple rules:'
  prefs: []
  type: TYPE_NORMAL
- en: The same OpenShift components need to be connected to different switches and
    power circuits, and located in different racks or server rooms if possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All hardware should be running on OpenShift components connected to physical
    networking using interface teaming with LACP and MC-LAG on networking switches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An external storage cluster for persistent data should follow the same rules
    and should be connected to different switches and power circuits, and located
    in different racks or server rooms if possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use different load balancer clusters that work independently from each other
    and do not form a single point of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to provide additional reliability for an OpenShift solution, you
    can also use the server hardware RAID for OpenShift OS, ECC-enabled RAM, multiple
    networking cards, dual-socket motherboards, and SSD disks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Considering a design like this, you have to ask yourself several questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What happens if any critical component (OpenStack, network, or storage) goes
    down?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do I do if the OpenShift cluster upgrades?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do I do if external storage for OpenShift persistent data becomes unavailable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much time does it take to recover an OpenShift cluster if the whole thing
    goes down?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other questions, of course, that you have to ask yourself, but if
    you are able to answer these questions without any hesitation, then you are on
    the right track.
  prefs: []
  type: TYPE_NORMAL
- en: OpenShift multi-DC HA design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenShift multi-DC is one of the most difficult topics when it comes to OpenShift
    design and implementation in a scalable and distributed environment. This happens
    mainly because there are not that many deployments and best practices developed
    around this topic. It may be relatively easy to deploy an OpenShift cluster in
    a single data center environment, but when it comes to a multi-DC design, this
    is where things will get complicated. The reason is that now we have to consider
    all OpenShift and adjacent components, like networking and storage, to be scalable
    and highly available across multiple data centers as well. There are two main
    HA strategies for a design that involves more than one data center:'
  prefs: []
  type: TYPE_NORMAL
- en: Single OpenShift cluster across all data centers (for example, one cluster per
    three DCs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One OpenShift cluster per data center (for example, three clusters per three
    DCs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all of these strategies, we need to use active/active scenarios because
    an active/passive scenario is a waste of resources and money. And although there
    are still many companies utilizing active/passive scenarios, they usually have
    plans to migrate to active/active scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: One OpenShift cluster across all data centers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This particular design option is the most natural and the easiest to operate,
    but the most dangerous among all the other options. One data center environment
    brings one set of problems, and if you add another data center, it will give you
    twice as many problems. If you have an unreliable data center interconnect link,
    it will add up alongside the failure risks as well. Some of you may disagree,
    but this is what usually happens if you do not plan and design your solution properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering a design like the preceding, you will have the following challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I load balance the traffic across all of these data centers from the
    internet?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I distribute each and every OpenShift component across these data centers
    so that, in case of a failure, OpenShift is still able to handle all the load?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does my storage solution work across all three data centers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do I need to extend the same network subnet across all data centers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I solve asymmetric routing problems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens with the OpenShift cluster if there is a split brain scenario?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if any critical component (OpenStack, network, or storage) goes
    down in one of the data centers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much time does it take if the whole OpenShift cluster goes down?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I scale out this solution, for example, if a new data center comes out?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do I do if an OpenShift cluster upgrades?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do I do if external storage for OpenShift persistent data becomes unavailable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, as you can see, having just a single OpenShift cluster across all your data
    centers adds a lot more questions and problems compared to a single cluster in
    one data center. This solution has one main benefit—having one single OpenShift
    cluster is easier to operate. But you need to ask yourself whether you are building
    a solution that will be easy to operate or a reliable and stable solution that
    is going to be up and running during the most difficult and even catastrophic
    events.
  prefs: []
  type: TYPE_NORMAL
- en: One OpenShift cluster per data center
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the previous solution has a lot of disadvantages, there is another, not
    so popular, but very stable, predictable, and scalable solution. That solution
    is where you have as many clusters as there are data centers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The main benefit of this solution is that all of your OpenShift clusters are
    going to be independent of each other and won''t affect other OpenShift clusters
    if something goes wrong. You will still have challenges, but they are a bit different
    from single Openshift clusters across all DCs and easier to solve. Besides, this
    solution scales much better than the other ones. However, you should answer these
    questions before implementing this solution:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I load balance the traffic across all of these data centers from the
    internet?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens with the OpenShift cluster if there is a split brain scenario?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does my storage data work across all three data centers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do I need database replication across all the data center? If yes, how will
    it work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if any critical component (OpenStack, network, or storage) goes
    down in one of the data centers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much time does it take if the whole OpenShift cluster goes down?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I scale out this solution, for example, if a new data center comes out?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do I do regarding OpenShift cluster upgrades?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do I do if external storage for OpenShift persistent data becomes unavailable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following is a comparison table to consolidate the main differences in all
    **OpenShift Container Platform** (**OCP**) HA solutions that we have just discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **1xOCP-1xDC** | **1xOCP-3xDC** | **3xOCP-3xDC** |'
  prefs: []
  type: TYPE_TB
- en: '| DC redundancy | No | Yes | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Intercluster redundancy | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Intercluster storage isolation | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability | Limited | Limited | Unlimited |'
  prefs: []
  type: TYPE_TB
- en: '| Solution implementation | Easy | Moderate | Hard |'
  prefs: []
  type: TYPE_TB
- en: '| Operations | Easy | Easy | Moderate |'
  prefs: []
  type: TYPE_TB
- en: '| Troubleshooting | Easy | Hard | Moderate |'
  prefs: []
  type: TYPE_TB
- en: '| Cluster seamless Upgrade and recovery | Moderate | Hard | Easy |'
  prefs: []
  type: TYPE_TB
- en: '| Application development  | Easy | Easy | Easy |'
  prefs: []
  type: TYPE_TB
- en: '| Application deployment  | Easy | Easy | Moderate  |'
  prefs: []
  type: TYPE_TB
- en: '| Requires external custom tools | No | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: 'As you can see, each and every HA solution has their own pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1xOCP-1xDC**: This is easiest to implement, operate, and troubleshoot, but
    suffers from data center or OpenShift cluster failures and has limited scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1xOCP-3xDC**: In addition to all the benefits of the previous solution, it
    has better redundancy but adds a lot of troubleshooting effort if something goes
    wrong. This solution is also difficult to perform seamless upgrades and recoveries
    on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3xOCP-3xDC**: This is a next level solution architecture that is much harder
    to implement, operate, and troubleshoot, but is the most stable, scalable solution
    ever. This solution requires extensive experience and expertise, but ensures that
    you will always have your application up and running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to successfully implement the last solution where we have one OpenShift
    cluster per data center, we are required to take a closer look at the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main problem in networking when you are building a solution like this is
    how to properly load balance the traffic, and when failure happens, how to re-route
    the traffic to other OpenShift clusters. The main technologies we can use here
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anycast ****IP addresses**: In order to effectively load balance the traffic
    across all our data centers, we can use an anycast IP address. This will help
    not only load balance the traffic but also provide IP failover if an application
    becomes unavailable in one of the data centers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application health checks**: Application health checks are a must have in
    this solution. They will help you to identify the failure and reroute the traffic
    to other data centers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic routing protocols**: Make sure that you have IGP/BGP connectivity
    between load balancers and network HW. When failure happens, IGP/BGP will withdraw
    its IP anycast address so that the traffic goes to other data centers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL Offloading**: Depending on the implementation, you might need to configure
    SSL offloading on load balancers. This will take traffic decryption off the OpenShift
    cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00100.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Storage has always from a problem when it comes to geographically distributed
    application deployment across several platforms. Even major cloud providers have
    not solved it yet. This is the prime reason why application instances must be
    able to work independently from each other and be stateless. However, we can suggest
    a few architectures that will point you in the right direction and help solve
    the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Storage geo-replication**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can set up Multi-DC storage replication so that you have data consistency
    across all data centers. One example of this is GlusterFS geo-replication, which
    supports different scenarios that should suit your case. As we discussed earlier
    in this book, GlusterFS is a perfect match for OpenShift as a persistent storage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Database geo-replication**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In most cases, the only stateful information you have in OpenShift will be kept
    in databases. Modern databases support multi-site, multi-DC, and multi-region
    replication architectures, such as Cassandra, MongoDB, Kafka, and Hazelcast.
  prefs: []
  type: TYPE_NORMAL
- en: You will still need to take care of backup and restore procedures if the database
    gets corrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Application deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you are done with network and storage designs, the final step should be
    taken towards application deployment processes. Since we have several clusters,
    there must be a process regarding how to deliver your applications consistently
    across all OpenShift clusters and all data centers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is where external tools come into the picture. We can use external CI/CD
    software to automate the application deployment process across all OpenShift clusters,
    or we can build a separate OpenShift cluster with CI/CD to develop, build, test,
    and release applications to production OpenShift clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about OpenShift scenarios in single and multiple
    data centers. This chapter also explained how to properly design OpenShift in
    a distributed and redundant configuration across one or more data centers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to cover main network aspects while designing
    an OpenShift cluster in one or across multiple data centers. We will also cover
    commonly made mistakes, solutions, and overall guidance from a networking point
    of view.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Which OpenShift component has built-in HA and works in active/active mode? choose
    one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift Etcd key-value store
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift Masters
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift infrastructure nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenShift nodes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Which OpenShift HA solution out of the ones listed supports unlimited scalability?
    choose one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1xOSP - 3xDC
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 3xOSP - 1xDC
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 3xOSP - 3xDC
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1xOSP - 1xDC
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Anycast IP addresses ensure that application traffic is load balanced across
    several data centers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What are the two options to ensure application data consistency in geo-replicated
    OpenShift deployments? choose two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Persistent storage replication
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Application database replication
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Openshift cluster replication
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Openshift etcd key-value store replication
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a list of topics with links related to this chapter that you might
    want to deep dive into:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpeShift HA design**: [http://v1.uncontained.io/playbooks/installation/](http://v1.uncontained.io/playbooks/installation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenShift high availability**: [https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html](https://docs.openshift.com/enterprise/latest/admin_guide/high_availability.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Openshift for single and multiple DCs**: [https://blog.openshift.com/deploying-openshift-applications-multiple-datacenters/](https://blog.openshift.com/deploying-openshift-applications-multiple-datacenters/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GlusterFS geo-replication**: [https://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/](https://docs.gluster.org/en/latest/Administrator%20Guide/Geo%20Replication/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
