<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying Releases with Zero-Downtime</h1>
                </header>
            
            <article>
                
<div class="packt_tip">If we are to survive in the face of competition, we have to release features to production as soon as they are developed and tested. The need for frequent releases fortifies the need for zero-downtime deployments.</div>
<p>We learned how to deploy our applications packaged as Pods, how to scale them through ReplicaSets, and how to enable communication through Services. However, all that is useless if we cannot update those applications with new releases. That is where Kubernetes Deployments come in handy.</p>
<p>The desired state of our applications is changing all the time. The most common reasons for new states are new releases. The process is relatively simple. We make a change and commit it to a code repository. We build it, and we test it. Once we're confident that it works as expected, we deploy it to a cluster. It does not matter whether that deployment is to a development, test, staging, or production environment. We need to deploy a new release to a cluster, even when that is a single-node Kubernetes running on a laptop. No matter how many environments we have, the process should always be the same or, at least, as similar as possible.</p>
<p>The deployment must produce no downtime. It does not matter whether it is performed on a testing or a production cluster. Interrupting consumers is disruptive, and that leads to loss of money and confidence in a product. Gone are the days when users did not care if an application sometimes did not work. There are so many competitors out there that a single bad experience might lead users to another solution. With today's scale, 0.1% of failed requests is considered disastrous. While we might never be able to reach 100% availability, we should certainly not cause downtime ourselves and must minimise other factors that could cause downtime.</p>
<div class="packt_tip">Failures caused by circumstances outside of our control are things which, by definition, we can do nothing about. However, failures caused by obsolete practices or negligence are failures which should not happen. Kubernetes Deployments provide us with the tools we need to avoid such failures by allowing us to update our applications without downtime.</div>
<p>Let's explore how Kubernetes Deployments work and the benefits we gain by adopting them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Cluster</h1>
                </header>
            
            <article>
                
<p>Creating a cluster at the beginning of each chapter allows us to jump into any part of the book without worrying whether there is a requirement to meet from previous chapters. It also allows us to pause between chapters without stressing our laptops by running a VM that is not in use. The downside is that this is the boring part of every chapter. Therefore, the talk stops here. Let's get it over with.</p>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>06-deploy.sh</kbd> (<a href="https://gist.github.com/vfarcic/677a0d688f65ceb01e31e33db59a4400" target="_blank"><span class="URLPACKT">https://gist.github.com/vfarcic/677a0d688f65ceb01e31e33db59a4400</span></a>) Gist.</div>
<pre><strong>cd k8s-specs</strong>
    
<strong>git pull</strong>
   
<strong>minikube start --vm-driver=virtualbox</strong>
    
<strong>kubectl config current-context</strong>  </pre>
<p>The code was updated, the cluster is up-and-running, and we can start exploring Deployments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying new releases</h1>
                </header>
            
            <article>
                
<p>Just as we are not supposed to create Pods directly but using other controllers like ReplicaSet, we are not supposed to create ReplicaSets either. Kubernetes Deployments will create them for us. If you're wondering why, you'll have to wait a little while longer to find out. First, we'll create a few Deployments and, once we are familiar the process and the outcomes, it'll become obvious why they are better at managing ReplicaSets than we are.</p>
<p>Let's take a look at a Deployment specification for the database ReplicaSet we've been using thus far.</p>
<pre><strong>cat deploy/go-demo-2-db.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-db</strong>
<strong>spec:</strong>
<strong>  selector:</strong>
<strong>    matchLabels:</strong>
<strong>      type: db</strong>
<strong>      service: go-demo-2</strong>
<strong>  template:</strong>
<strong>    metadata:</strong>
<strong>      labels:</strong>
<strong>        type: db</strong>
<strong>        service: go-demo-2</strong>
<strong>        vendor: MongoLabs</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: db</strong>
<strong>        image: mongo:3.3</strong>
<strong>        ports:</strong>
<strong>        - containerPort: 28017</strong>  </pre>
<p>If you compare this Deployment with the ReplicaSet we created in the previous chapter, you'll probably have a hard time finding a difference. Apart from the <kbd>kind</kbd> field, they are the same.</p>
<p>Since, in this case, both the Deployment and the ReplicaSet are the same, you might be wondering what the advantage of using one over the other is.</p>
<div class="packt_infobox">We will regularly add <kbd>--record</kbd> to the <kbd>kubectl create</kbd> commands. This allows us to track each change to our resources such as a Deployments.</div>
<p>Let's create the Deployment and explore what it offers.</p>
<pre><strong>kubectl create \</strong>
<strong>    -f deploy/go-demo-2-db.yml \</strong>
<strong>    --record</strong>
    
<strong>kubectl get -f deploy/go-demo-2-db.yml</strong>  </pre>
<p>The output of the latter command is as follows:</p>
<pre><strong>NAME         DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>go-demo-2-db 1       1       1          0         7s</strong>  </pre>
<p>The Deployment was created. However, <kbd>get</kbd> does not provide us much info, so let's <kbd>describe</kbd> it.</p>
<pre><strong>kubectl describe \</strong>
<strong>    -f deploy/go-demo-2-db.yml</strong>  </pre>
<p>The output, limited to the last few lines, is as follows:</p>
<pre><strong>...</strong>
<strong>Events:</strong>
<strong>  Type   Reason            Age  From                  Message</strong>
<strong>  ----   ------            ---- ----                  -------</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled up r<br/>eplica set go-demo-2-db-75fbcbb5cd to 1</strong>  </pre>
<p>From the <kbd>Events</kbd> section, we can observe that the Deployment created a ReplicaSet. Or, to be more precise, that it scaled it. That is interesting. It shows that Deployments control ReplicaSets. The Deployment created the ReplicaSet which, in turn, created Pods. Let's confirm that by retrieving the list of all the objects:</p>
<pre><strong>kubectl get all</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>deploy/go-demo-2-db 1       1       1          1         8m</strong>
    
<strong>NAME                       DESIRED CURRENT READY AGE</strong>
<strong>rs/go-demo-2-db-75fbcbb5cd 1       1       1     8m</strong>
    
<strong>NAME                             READY STATUS  RESTARTS AGE</strong>
<strong>po/go-demo-2-db-75fbcbb5cd-k6tz9 1/1   Running 0        8m</strong>
    
<strong>NAME           TYPE      CLUSTER-IP EXTERNAL-IP PORT(S) AGE</strong>
<strong>svc/kubernetes ClusterIP 10.0.0.1   &lt;none&gt;      443/TCP 14m</strong>  </pre>
<p>All three objects were created, and you might be wondering why we created the Deployment at all. You might think that we'd have the same result if we created a ReplicaSet directly. You'd be right. So far, from the functional point of view, there is no difference between a ReplicaSet created directly or using a Deployment. The real advantage of Deployments becomes evident if we try to change some of its aspects. For example, we might choose to upgrade MongoDB to version 3.4.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7ca574b5-1c0c-406a-ba52-3f5559c0f47d.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 6-1: Deployment and its cascading effect that creates a ReplicaSet and, though it, Pods</div>
<p>Before we move onto Deployment updates, we'll go through our usual ritual of seeing the process through a sequence diagram. We won't repeat the explanation of the events that happened after the ReplicaSet object was created as those steps were already explained in the previous chapters.</p>
<ol>
<li>Kubernetes client (<kbd>kubectl</kbd>) sent a request to the API server requesting the creation of a Deployment defined in the <kbd>deploy/go-demo-2-db.yml</kbd> file</li>
<li>The deployment controller is watching the API server for new events, and it detected that there is a new Deployment object</li>
<li>The deployment controller creates a new ReplicaSet object</li>
</ol>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/0e76e8fa-76b4-4608-956e-7c5ccd76ea69.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 6-2: The sequence of events followed by request to create a deployment</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating Deployments</h1>
                </header>
            
            <article>
                
<p>Let's see what happens when we <kbd>set</kbd> a new image to the <kbd>db</kbd> Pod.</p>
<pre><strong>kubectl set image \</strong>
<strong>    -f deploy/go-demo-2-db.yml \</strong>
<strong>    db=mongo:3.4 \</strong>
<strong>    --record</strong>  </pre>
<p>It'll take a while until the new image is pulled, so you might as well fetch yourself a coffee. Once you're back, we can <kbd>describe</kbd> the Deployment by checking the events it created.</p>
<pre><strong>kubectl describe \</strong>
<strong>    -f deploy/go-demo-2-db.yml</strong></pre>
<p>The last few lines of the output are as follows:</p>
<pre><strong>...</strong>
<strong>Events:</strong>
<strong>  Type    Reason             Age   From                   Message</strong>
<strong>  ----    ------             ----  ----                   -------</strong>
<strong>  Normal  ScalingReplicaSet  19m   deployment-controller  Scaled <br/>up replica set go-demo-2-db-75fbcbb5cd to 1</strong>
<strong>  Normal  ScalingReplicaSet  5m    deployment-controller  Scaled <br/>up replica set go-demo-2-db-f8d4b86ff to 1</strong>
<strong>  Normal  ScalingReplicaSet  0s    deployment-controller  Scaled <br/>down replica set go-demo-2-db-75fbcbb5cd to 0</strong>  </pre>
<p>We can see that it created a new ReplicaSet and that it scaled the old ReplicaSet to <kbd>0</kbd>. If, in your case, the last line did not appear, you'll need to wait until the new version of the <kbd>mongo</kbd> image is pulled.</p>
<p>Instead of operating directly on the level of Pods, the Deployment created a new ReplicaSet which, in turn, produced Pods based on the new image. Once they became fully operational, it scaled the old ReplicaSet to <kbd>0</kbd>. Since we are running a ReplicaSet with only one replica, it might not be clear why it used that strategy. When we create a Deployment for the API, things will become more evident.</p>
<p>To be on the safe side, we might want to retrieve all the objects from the cluster:</p>
<pre><strong>kubectl get all</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   <br/>AGE</strong>
<strong>deploy/go-demo-2-db   1         1         1            1           <br/>3m</strong>
    
<strong>NAME                         DESIRED   CURRENT   READY     AGE</strong>
<strong>rs/go-demo-2-db-75fbcbb5cd   0         0         0         3m</strong>
<strong>rs/go-demo-2-db-f8d4b86ff    1         1         1         2m</strong>
<strong>NAME                              READY     STATUS    RESTARTS   <br/>AGE</strong>
<strong>po/go-demo-2-db-f8d4b86ff-qvhgg   1/1       Running   0          <br/>2m</strong>
    
<strong>NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   <br/>AGE</strong>
<strong>svc/kubernetes   ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP   <br/>35m</strong>  </pre>
<p>As you can see, both ReplicaSets are there. However, one is inactive (scaled to <kbd>0</kbd>).</p>
<p>You'll notice that contained within the name of the Pod is a hash which matches the hash in the name of the new ReplicaSet, namely <kbd>f8d4b86ff</kbd>. Even though it might look like it is a random value, it is not. If you destroy the Deployment and create it again, you'll notice that the hash in the Pod name and ReplicaSet name remain consistent. This value is generated by hashing the PodTemplate of the ReplicaSet. As long as the PodTemplate is the same, the hash value will be the same as well. That way a Deployment can know whether anything related to the Pods has changed and, if it does, will create a new ReplicaSet.</p>
<p>The <kbd>kubectl set image</kbd> command is not the only way to update a Deployment. We could also have used <kbd>kubectl edit</kbd> as well. The command would be as follows. <strong>Please do NOT execute it.</strong> If you do (against my advice), you'll need to type <kbd>:q</kbd> followed by the <em>Enter</em> key to exit.</p>
<pre><strong>kubectl edit -f deploy/go-demo-2-db.yml</strong>  </pre>
<p>I don't think the above <kbd>edit</kbd> command is a good way to update the definition. It is unpractical and undocumented. The <kbd>kubectl set image</kbd> is more useful if we'd like to integrate Deployment updates with one of the CI/CD tools. Since we'll have a chapter dedicated to continuous deployment, we'll continue using <kbd>kubectl set image</kbd> from now on.</p>
<p>Another alternative would be to update the YAML file and execute the <kbd>kubectl apply</kbd> command. While that is a good idea for applications that do not update frequently, it does not fit well with those that change weekly, daily, or even hourly.</p>
<p>MongoDB is one of those that might get updated with a new release only a couple of times a year so having an always up-to-date YAML file in your source code repository is an excellent practice. We used <kbd>kubectl set image</kbd> just as a way to introduce you to what's coming next when we explore frequent deployments without downtime.</p>
<p>A simple update of Pod images is far from what Deployment offers. To see its real power, we should deploy the API. Since it can be scaled to multiple Pods, it'll provide us with a much better playground.</p>
<p>Before we move on, let's finish with the database by adding a Service and, therefore, enabling internal cluster communication to it:</p>
<pre><strong>kubectl create \</strong>
<strong>    -f deploy/go-demo-2-db-svc.yml \</strong>
<strong>    --record</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Zero-Downtime Deployments</h1>
                </header>
            
            <article>
                
<p>Updating a single-replica MongoDB cannot demonstrate true power behind Deployments. We need a scalable service. It's not that MongoDB cannot be scaled (it can), but it is not as straight-forward as an application that was designed to be scalable. We'll jump to the second application in the stack and create a Deployment of the ReplicaSet that will create Pods based on the <kbd>vfarcic/go-demo-2</kbd> image. But, before we do that, we'll spend a few moments discussing the need for zero-downtime deployments.</p>
<p>On the one hand, our applications are supposed to have very high availability. Depending on the context and the goals, we usually discuss how many nines are coming after 99%. At the very least, an application must have availability of at least 99.9%. More likely, it should be something closer to 99.99 or even 99.999 percent availability. Hundred percent availability is often not possible or too expensive to accomplish. We cannot avoid all failures, but we can reduce them to acceptable limits.</p>
<p>No matter what the availability of SLA is, applications (at least when developed by us) must be scalable. Only when there are multiple replicas, can we hope for any decent availability. Scaled applications can not only spread the load across various instances but ensure that a failure of one replica will not produce downtime. Healthy instances are handling the load until the scheduler recreates failed ones.</p>
<div class="packt_tip">High availability is accomplished through fault tolerance and scalability. If either is missing, any failure might have disastrous effects.</div>
<p>The reason we're discussing failures and scalability lies in the nature of immutable deployments. If a Pod is unchangeable, the only way to update it with a new release is to destroy the old ones and put the Pods based on the new image in their place. Destruction of Pods is not much different from failures. In both cases, they cease to work. On the other hand, fault tolerance (re-scheduling) is a replacement of failed Pods.</p>
<p>The only essential difference is that new releases result in Pods being replaced with new ones based on the new image. As long as the process is controlled, new releases should not result in any downtime when multiple replicas of an application are running and when they are adequately designed.</p>
<p>We should not worry about the frequency of new releases. The process should be the same no matter whether we make releases once a month, once a week, once a day, or every few minutes. If the release process produces any downtime, we might be compelled to deploy new versions infrequently. As a matter of fact, throughout the history of software development, we were taught that releases should be limited in number. A couple a year was the norm. Part of the reasons behind such infrequent releases was due to the downtime they produce. If we can reach zero-downtime deployments, the frequency can change, and we can aim for continuous deployment. We won't go into benefits behind continuous deployment just yet. It's not relevant at this point. Instead, we'll focus on zero-downtime deployments. Given a choice, no one would choose the little-bit-of-downtime strategy, so I'll assume that everyone wants to be able to release without interruptions.</p>
<div class="packt_tip">Zero-downtime deployment is a prerequisite for higher frequency releases.</div>
<p>Let's take a look at the Deployment definition of the API:</p>
<pre><strong>cat deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-api</strong>
<strong>spec:</strong>
<strong>  replicas: 3</strong>
<strong>  selector:</strong>
<strong>    matchLabels:</strong>
<strong>      type: api</strong>
<strong>      service: go-demo-2</strong>
<strong>  minReadySeconds: 1</strong>
<strong>  progressDeadlineSeconds: 60</strong>
<strong>  revisionHistoryLimit: 5</strong>
<strong>  strategy:</strong>
<strong>   type: RollingUpdate</strong>
<strong>    rollingUpdate:</strong>
<strong>      maxSurge: 1</strong>
<strong>      maxUnavailable: 1</strong>
<strong>  template:</strong>
<strong>    metadata:</strong>
<strong>      labels:</strong>
<strong>        type: api</strong>
<strong>        service: go-demo-2</strong>
<strong>        language: go</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: api</strong>
<strong>        image: vfarcic/go-demo-2</strong>
<strong>        env:</strong>
<strong>        - name: DB</strong>
<strong>          value: go-demo-2-db</strong>
<strong>        readinessProbe:</strong>
<strong>          httpGet:</strong>
<strong>            path: /demo/hello</strong>
<strong>            port: 8080</strong>
<strong>          periodSeconds: 1</strong>
<strong>        livenessProbe:</strong>
<strong>          httpGet:</strong>
<strong>            path: /demo/hello</strong>
<strong>            port: 8080</strong>  </pre>
<p>We'll skip explaining <kbd>apiVersion</kbd>, <kbd>kind</kbd>, and <kbd>metadata</kbd>, since they always follow the same pattern.</p>
<p>The <kbd>spec</kbd> section has a few of the fields we haven't seen before, and a few of those we are familiar with. The <kbd>replicas</kbd> and the <kbd>selector</kbd> are the same as what we used in the ReplicaSet from the previous chapter.</p>
<p><kbd>minReadySeconds</kbd> defines the minimum number of seconds before Kubernetes starts considering the Pods healthy. We put the value of this field to <kbd>1</kbd> second. The default value is <kbd>0</kbd>, meaning that the Pods will be considered available as soon as they are ready and, when specified, <kbd>livenessProbe</kbd> returns OK. If in doubt, omit this field and leave it to the default value of <kbd>0</kbd>. We defined it mostly for demonstration purposes.</p>
<p>The next field is <kbd>revisionHistoryLimit</kbd>. It defines the number of old ReplicaSets we can rollback. Like most of the fields, it is set to the sensible default value of <kbd>10</kbd>. We changed it to <kbd>5</kbd> and, as a result, we will be able to rollback to any of the previous five ReplicaSets.</p>
<p>The <kbd>strategy</kbd> can be either the <kbd>RollingUpdate</kbd> or the <kbd>Recreate</kbd> type. The latter will kill all the existing Pods before an update. <kbd>Recreate</kbd> resembles the processes we used in the past when the typical strategy for deploying a new release was first to stop the existing one and then put a new one in its place. This approach inevitably leads to downtime. The only case when this strategy is useful is when applications are not designed for two releases to coexist. Unfortunately, that is still more common than it should be. If you're in doubt whether your application is like that, ask yourself the following question. Would there be an adverse effect if two different versions of my application are running in parallel? If that's the case, a <kbd>Recreate</kbd> strategy might be a good choice and <em>you must be aware that you cannot accomplish zero-downtime deployments</em>.</p>
<p>The <kbd>recreate</kbd> strategy is much better suited for our single-replica database. We should have set up the native database replication (not the same as Kubernetes ReplicaSet object), but, as explained earlier, that is out of the scope of this chapter (and probably this book).</p>
<p>If we're running the database as a single replica, we must have mounted a network drive volume. That would allow us to avoid data loss when updating it or in case of a failure. Since most databases (MongoDB included) cannot have multiple instances writing to the same data files, killing the old release before creating a new one is a good strategy when replication is absent. We'll apply it later.</p>
<p>The <kbd>RollingUpdate</kbd> strategy is the default type, for a good reason. It allows us to deploy new releases without downtime. It creates a new ReplicaSet with zero replicas and, depending on other parameters, increases the replicas of the new one, and decreases those from the old one. The process is finished when the replicas of the new ReplicaSet entirely replace those from the old one.</p>
<p>When <kbd>RollingUpdate</kbd> is the strategy of choice, it can be fine-tuned with the <kbd>maxSurge</kbd> and <kbd>maxUnavailable</kbd> fields. The former defines the maximum number of Pods that can exceed the desired number (set using <kbd>replicas</kbd>). It can be set to an absolute number (for example, <kbd>2</kbd>) or a percentage (for example, <kbd>35%</kbd>). The total number of Pods will never exceed the desired number (set using <kbd>replicas</kbd>) and the <kbd>maxSurge</kbd> combined. The default value is <kbd>25%</kbd>.</p>
<p><kbd>maxUnavailable</kbd> defines the maximum number of Pods that are not operational. If, for example, the number of replicas is set to 15 and this field is set to 4, the minimum number of Pods that would run at any given moment would be 11. Just as the <kbd>maxSurge</kbd> field, this one also defaults to <kbd>25%</kbd>. If this field is not specified, there will always be at least 75% of the desired Pods.</p>
<p>In most cases, the default values of the Deployment specific fields are a good option. We changed the default settings only as a way to demonstrate better all the options we can use. We'll remove them from most of the Deployment definitions that follow.</p>
<p>The <kbd>template</kbd> is the same <kbd>PodTemplate</kbd> we used before. Best practice is to be explicit with image tags like we did when we set <kbd>mongo:3.3</kbd>. However, that might not always be the best strategy with the images we're building. Given we employ right practices, we can rely on <kbd>latest</kbd> tags being stable. Even if we discover they're not, we can remedy that quickly by creating a new <kbd>latest</kbd> tag. However, we cannot expect the same from third-party images. They must always be tagged to a specific version.</p>
<div class="packt_tip">Never deploy third-party images based on <kbd>latest</kbd> tags. By being explicit with the release, we have more control over what is running in production, as well as what should be the next upgrade.</div>
<p>We won't always use <kbd>latest</kbd> for our services, but only for the initial Deployments. Assuming that we are doing our best to maintain the <kbd>latest</kbd> tag stable and production-ready, it is handy when setting up the cluster for the first time. After that, each new release will be with a specific tag. Our automated continuous deployment pipeline will do that for us in one of the next chapters.</p>
<div class="packt_tip">If you are confident in your ability to maintain <kbd>latest</kbd> stable, it is handy using it for the first Deployment of an application.</div>
<p>Before we explore rolling updates, we should create the Deployment and, with it, the first release of our application.</p>
<pre><strong>kubectl create \</strong>
<strong>    -f deploy/go-demo-2-api.yml \</strong>
<strong>    --record</strong>
    
<strong>kubectl get -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>We created the Deployment and retrieved the object from the Kubernetes API server.</p>
<p>The output of the latter command is as follows:</p>
<pre><strong>NAME          DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>go-demo-2-api 3       3       3          3         1m</strong>  </pre>
<p>Please make sure that the number of available Pods is <kbd>3</kbd>. Wait for a few moments, if that's not the case. Once all the Pods are up-and-running, we'll have a Deployment that created a new ReplicaSet which, in turn, created three Pods based on the latest release of the <kbd>vfarcic/go-demo-2</kbd> image.</p>
<p>Let's see what happens when we set a new image.</p>
<pre><strong>kubectl set image \</strong>
<strong>    -f deploy/go-demo-2-api.yml \</strong>
<strong>    api=vfarcic/go-demo-2:2.0 \</strong>
<strong>    --record</strong>  </pre>
<p>There are a few ways we can observe what is happening during the update. One of those is through the <kbd>kubectl rollout status</kbd> command.</p>
<pre><strong>kubectl rollout status -w \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>...</strong>
<strong>deployment "go-demo-2-api" successfully rolled out</strong>  </pre>
<p>From the last entry, we can see that the rollout of the new deployment was successful. Depending on the time that passed between setting the new image and displaying the rollout status, you might have seen other entries marking the progress. However, I think that the events from the <kbd>kubectl describe</kbd> command are painting a better picture of the process that was executed.</p>
<pre><strong>kubectl describe \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The last lines of the output are as follows:</p>
<pre><strong>...</strong>
<strong>Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unava<br/>liable</strong>
<strong>...</strong>
<strong>OldReplicaSets:  &lt;none&gt;</strong>
<strong>NewReplicaSet:   go-demo-2-api-68c75f4f5 (3/3 replicas created)</strong>
<strong>Events:</strong>
<strong>  Type   Reason            Age  From                  Message</strong>
<strong>  ----   ------            ---- ----                  -------</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled up r<br/>eplica set go-demo-2-api-68df567fb5 to 3</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled up r<br/>eplica set go-demo-2-api-68c75f4f5 to 1</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled down <br/>replica set go-demo-2-api-68df567fb5 to 2</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled up r<br/>eplica set go-demo-2-api-68c75f4f5 to 2</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled down <br/>replica set go-demo-2-api-68df567fb5 to 1</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled up r<br/>eplica set go-demo-2-api-68c75f4f5 to 3</strong>
<strong>  Normal ScalingReplicaSet 2m   deployment-controller Scaled down <br/>replica set go-demo-2-api-68df567fb5 to 0</strong>  </pre>
<p>We can see that the number of desired replicas is <kbd>3</kbd>. The same number was updated and all are available.</p>
<p>At the bottom of the output are events associated with the Deployment. The process started by increasing the number of replicas of the new ReplicaSet (<kbd>go-demo-2-api-68c75f4f5</kbd>) to <kbd>1</kbd>. Next, it decreased the number of replicas of the old ReplicaSet (<kbd>go-demo-2-api-68df567fb5</kbd>) to <kbd>2</kbd>. The same process of increasing replicas of the new, and decreasing replicas of the old ReplicaSet continued until the new one got the desired number (<kbd>3</kbd>), and the old one dropped to zero.</p>
<p>There was no downtime throughout the process. Users would receive a response from the application no matter whether they sent it before, during, or after the update. The only important thing is that, during the update, a response might have come from the old or the new release. During the update process, both releases were running in parallel.</p>
<p>Let's take a look at the rollout history:</p>
<pre><strong>kubectl rollout history \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>deployments "go-demo-2-api"</strong>
<strong>REVISION CHANGE-CAUSE</strong>
<strong>1        kubectl create --filename=deploy/go-demo-2-api.yml --rec<br/>ord=true</strong>
<strong>2        kubectl set image api=vfarcic/go-demo-2:2.0 --filename=d<br/>eploy/go-demo-2-api.yml</strong>  </pre>
<p>We can see that, so far, there were two revisions of the software. The change cause shows which command created each of those revisions.</p>
<p>How about ReplicaSets?</p>
<pre><strong>kubectl get rs</strong>  </pre>
<p>The output, limited to <kbd>go-demo-2-api</kbd>, is as follows.</p>
<pre><strong>NAME                     DESIRED CURRENT READY AGE</strong>
<strong>go-demo-2-api-68c75f4f5  3       3       3     4m</strong>
<strong>go-demo-2-api-68df567fb5 0       0       0     4m</strong>
<strong>...</strong>  </pre>
<p>We can see that the Deployment did not modify the ReplicaSet, but that it created a new one and, at the end of the process, the old one was scaled to zero replicas.</p>
<p>The diagram in the <em>Figure 6-2</em> shows the flow of the events that occurred since we executed the <kbd>kubectl set image</kbd> command. It closely depicts the events we already saw from the <kbd>kubectl describe</kbd> command.</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/43ddf2cc-9300-4176-905c-fe6f3a01ac61.png" style="width:42.92em;height:31.17em;"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 6-3: Deployment controller rolling update workflow</div>
<p>We made great progress. However, the unexpected can happen at any time, and we must be prepared to deal with it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rolling back or rolling forward?</h1>
                </header>
            
            <article>
                
<p>At this point, we are, more or less, capable of deploying new releases to production as soon as they are ready. However, there will be problems. Something unexpected will happen. A bug will sneak in and put our production cluster at risk. What should we do in such a case? The answer to that question largely depends on the size of the changes and the frequency of deployments.</p>
<p>If we are using continuous deployment process, we are deploying new releases to production fairly often. Instead of waiting until features accumulate, we are deploying small chunks. In such cases, fixing a problem might be just as fast as rolling back. After all, how much time would it take you to fix a problem caused by only a few hours of work (maybe a day) and that was discovered minutes after you committed? Probably not much. The problem was introduced by a very recent change that is still in engineer's head. Fixing it should not take long, and we should be able to deploy a new release soon.</p>
<p>You might not have frequent releases, or the amount of changes included is more than a couple of hundreds of lines of code. In such a case, rolling forward might not be as fast as it should be. Still, rolling back might not even be possible. We might not be able to revert the deployment if database schema changed, and it is not compatible with the previous versions of the back-end that uses it. The moment the first transaction enters, we might lose the option to roll-back. At least, not without losing the data generated since the new release.</p>
<div class="packt_tip">Rolling back a release that introduced database changes is often not possible. Even when it is, rolling forward is usually a better option when practicing continuous deployment with high-frequency releases limited to a small scope of changes.</div>
<p>I did my best to discourage you from rolling back. Still, in some cases that is a better option. In others, that might be the only option. Luckily, rolling back is reasonably straightforward with Kubernetes.</p>
<p>We'll imagine that we just discovered that the latest release of the <kbd>vfarcic/go-demo-2</kbd> image is faulty and that we should roll back to the previous release. The command that will do just that is as follows:</p>
<pre><strong>kubectl rollout undo \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>
    
<strong>kubectl describe \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output of the latter command, limited to the last lines, is as follows:</p>
<pre><strong>OldReplicaSets:  &lt;none&gt;</strong>
<strong>NewReplicaSet:   go-demo-2-api-68df567fb5 (3/3 replicas created)</strong>
<strong>Events:</strong>
<strong>  Type   Reason             Age             From                  <br/>Message</strong>
<strong>  ----   ------             ----            ----                  <br/>-------</strong>
<strong>  Normal ScalingReplicaSet  6m              deployment-controller <br/>Scaled up replica set go-demo-2-api-68c75f4f5 to 1</strong>
<strong>  Normal ScalingReplicaSet  6m              deployment-controller <br/>Scaled down replica set go-demo-2-api-68df567fb5 to 2</strong>
<strong>  Normal ScalingReplicaSet  6m              deployment-controller <br/>Scaled up replica set go-demo-2-api-68c75f4f5 to 2</strong>
<strong>  Normal ScalingReplicaSet  6m              deployment-controller <br/>Scaled down replica set go-demo-2-api-68df567fb5 to 1</strong>
<strong>  Normal ScalingReplicaSet  6m              deployment-controller <br/>Scaled up replica set go-demo-2-api-68c75f4f5 to 3</strong>
<strong>  Normal ScalingReplicaSet  6m              deployment-controller <br/>Scaled down replica set go-demo-2-api-68df567fb5 to 0</strong>
<strong>  Normal DeploymentRollback 1m              deployment-controller <br/>Rolled back deployment "go-demo-2-api" to revision 1</strong>
<strong>  Normal ScalingReplicaSet  1m              deployment-controller <br/>Scaled up replica set go-demo-2-api-68df567fb5 to 1</strong>
<strong>  Normal ScalingReplicaSet  1m              deployment-controller <br/>Scaled down replica set go-demo-2-api-68c75f4f5 to 2</strong>
<strong>  Normal ScalingReplicaSet  1m (x2 over 6m) deployment-controller <br/>Scaled up replica set go-demo-2-api-68df567fb5 to 3</strong>
<strong>  Normal ScalingReplicaSet  1m (x3 over 1m) deployment-controller <br/>(combined from similar events): Scaled down replica set go-demo-2<br/>-api-68c75f4f5 to0</strong>  </pre>
<p>We can see from the events section that the Deployment initiated rollback and, from there on, the process we experienced before was reversed. It started increasing the replicas of the older ReplicaSet, and decreasing those from the latest one. Once the process is finished, the older ReplicaSet became active with all the replicas, and the newer one was scaled down to zero.</p>
<p>The end result might be easier to see from the <kbd>NewReplicaSet</kbd> entry located just above <kbd>Events</kbd>. Before we undid the rollout, the value was <kbd>go-demo-2-api-68c75f4f5</kbd>, and now it's <kbd>go-demo-2-api-68df567fb5</kbd>.</p>
<p>Knowing only the current state of the latest Deployment is often insufficient, and we might need a list of the past rollouts. We can get it with the <kbd>kubectl rollout history</kbd> command.</p>
<pre><strong>kubectl rollout history \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>REVISION  CHANGE-CAUSE</strong>
<strong>2         kubectl set image api=vfarcic/go-demo-2:2.0 --filename=<br/>deploy/go-demo-2-api.yml</strong>
<strong>3         kubectl create --filename=deploy/go-demo-2-api.yml --re<br/>cord=true</strong>  </pre>
<p>If you look at the third revision, you'll notice that the change cause is the same command we used to create the Deployment the first time. Before we executed <kbd>kubectl rollout undo</kbd>, we had two revisions; <kbd>1</kbd> and <kbd>2</kbd>. The <kbd>undo</kbd> command checked the second-to-last revision (<kbd>1</kbd>). Since new deployments do no destroy ReplicaSets but scale them to <kbd>0</kbd>, all it had to do to undo the last change was to scale it back to the desired number of replicas and, at the same time, scale the current one to zero.</p>
<p>Let's fast track a bit and deploy a few new releases. That will provide us with a broader playground to explore a few additional things we can do with Deployments.</p>
<pre><strong>kubectl set image \</strong>
<strong>    -f deploy/go-demo-2-api.yml \</strong>
<strong>    api=vfarcic/go-demo-2:3.0 \</strong>
<strong>    --record</strong>
    
<strong>kubectl rollout status \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>We updated the image to <kbd>vfarcic/go-demo-2:3.0</kbd> and retrieved the rollout status. The last line of the latter command is as follows:</p>
<pre><strong>deployment "go-demo-2-api" successfully rolled out</strong>  </pre>
<p>The deployment was successfully updated and, as a result, it created a new ReplicaSet and scaled it up to the desired number of replicas. The previously active ReplicaSet was scaled to <kbd>0</kbd>. As a result, we're running tag <kbd>3.0</kbd> of the <kbd>vfarcic/go-demo-2</kbd> image.</p>
<p>We'll repeat the process with the tag <kbd>4.0</kbd>:</p>
<pre><strong>kubectl set image \</strong>
<strong>    -f deploy/go-demo-2-api.yml \</strong>
<strong>    api=vfarcic/go-demo-2:4.0 \</strong>
<strong>    --record</strong>
    
<strong>kubectl rollout status \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output of the last line of the <kbd>rollout status</kbd> confirmed that the rollout was successful.</p>
<p>Now that we deployed a few releases, we can check the current <kbd>rollout history</kbd>:</p>
<pre><strong>kubectl rollout history \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>deployments "go-demo-2-api"</strong>
<strong>REVISION CHANGE-CAUSE</strong>
<strong>2        kubectl set image api=vfarcic/go-demo-2:2.0 --filename=d<br/>eploy/go-demo-2-api.yml --record=true</strong>
<strong>3        kubectl create --filename=deploy/go-demo-2-api.yml --rec<br/>ord=true</strong>
<strong>4        kubectl set image api=vfarcic/go-demo-2:3.0 --filename=d<br/>eploy/go-demo-2-api.yml --record=true</strong>
<strong>5        kubectl set image api=vfarcic/go-demo-2:4.0 --filename=deploy/go-demo-2-api.yml --record=true</strong>  </pre>
<p>We can clearly see the commands that produced the changes and, through them, how our application progressed all the way until the current release based on the image <kbd>vfarcic/go-demo-2:4.0</kbd>.</p>
<p>You saw that we can rollback to the previous release through the <kbd>kubectl rollout undo</kbd> command. In most cases, that should be the correct action when faced with problems and without the ability to roll forward by creating a new release with the fix. However, sometimes even that is not enough, and we have to go back in time further than the previous release.</p>
<p>Let's say that we discovered not only that the current release is faulty but also that a few before it have bugs as well. Following the same narrative, we'll imagine that the last correct release was based on the image <kbd>vfarcic/go-demo-2:2.0</kbd>. We can remedy that by executing the command that follows (<strong>please do NOT run it</strong>):</p>
<pre><strong>kubectl set image \</strong>
<strong>    -f deploy/go-demo-2-api.yml \</strong>
<strong>    api=vfarcic/go-demo-2:2.0 \</strong>
<strong>    --record</strong>  </pre>
<p>While that command would certainly fix the problem, there is an easier way to accomplish the same result. We can <kbd>undo</kbd> the <kbd>rollout</kbd> by moving to the last revision that worked correctly. Assuming that we want to revert to the image <kbd>vfarcic/go-demo-2:2.0</kbd>, reviewing the change causes listed in the history tells us we should roll back to revision <kbd>2</kbd>. That can be accomplished through the <kbd>--to-revision</kbd> argument. The command is as follows:</p>
<pre><strong>kubectl rollout undo \</strong>
<strong>    -f deploy/go-demo-2-api.yml \</strong>
<strong>    --to-revision=2</strong>
    
<strong>kubectl rollout history \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>We undid the rollout by moving to revision <kbd>2</kbd>. We also retrieved the <kbd>history</kbd>.</p>
<p>The output of the latter command is as follows:</p>
<pre><strong>deployments "go-demo-2-api"</strong>
<strong>REVISION  CHANGE-CAUSE</strong>
<strong>3         kubectl create --filename=deploy/go-demo-2-api.yml --re<br/>cord=true</strong>
<strong>4         kubectl set image api=vfarcic/go-demo-2:3.0 --filename=<br/>deploy/go-demo-2-api.yml --record=true</strong>
<strong>5         kubectl set image api=vfarcic/go-demo-2:4.0 --filename=<br/>deploy/go-demo-2-api.yml --record=true</strong>
<strong>6         kubectl set image api=vfarcic/go-demo-2:2.0 --filename=<br/>deploy/go-demo-2-api.yml --record=true</strong>  </pre>
<p>Through the new revision <kbd>6</kbd>, we can see that the currently active Deployment is based on the image <kbd>vfarcic/go-demo-2:2.0</kbd>. We successfully moved back to the specific point in time. The problem is solved and, if this was the "real" application running in a production cluster, our users would continue interacting with the version of our software that actually works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rolling back failed Deployments</h1>
                </header>
            
            <article>
                
<p>Discovering a critical bug is probably the most common reason for a rollback. Still, there are others. For example, we might be in a situation when Pods cannot be created. An easy to reproduce case would be an attempt to deploy an image with a tag that does not exist.</p>
<pre><strong>kubectl set image \</strong>
<strong>    -f deploy/go-demo-2-api.yml \</strong>
<strong>    api=vfarcic/go-demo-2:does-not-exist \</strong>
<strong>    --record</strong> </pre>
<p>The output is as follows:</p>
<pre><strong>deployment "go-demo-2-api" image updated</strong>  </pre>
<p>After seeing such a message, you might be under the impression that everything is OK. However, that output only indicates that the definition of the image used in the Deployment was successfully updated. That does not mean that the Pods behind the ReplicaSet are indeed running. For one, I can assure you that the <kbd>vfarcic/go-demo-2:does-not-exist</kbd> image does not exist.</p>
<p>Please make sure that at least <kbd>60</kbd> seconds have passed since you executed the <kbd>kubectl set image</kbd> command. If you're wondering why we are waiting, the answer lies in the <kbd>progressDeadlineSeconds</kbd> field set in the <kbd>go-demo-2-api</kbd> Deployment definition. That's how much the Deployment has to wait before it deduces that it cannot progress due to a failure to run a Pod.</p>
<p>Let's take a look at the ReplicaSets.</p>
<pre><strong>kubectl get rs -l type=api</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME                     DESIRED CURRENT READY AGE</strong>
<strong>go-demo-2-api-5b49d94f9b 0       0       0     8m</strong>
<strong>go-demo-2-api-68c75f4f5  2       2       2     9m</strong>
<strong>go-demo-2-api-7cb9bb5675 0       0       0     8m</strong>
<strong>go-demo-2-api-68df567fb5 0       0       0     9m</strong>
<strong>go-demo-2-api-dc7877dcd  2       2       0     4m</strong>  </pre>
<p>By now, under different circumstances, all the Pods from the new ReplicaSet (<kbd>go-demo-2-api-dc7877dcd</kbd>) should be set to <kbd>3</kbd>, and the Pods of the previous one (<kbd>go-demo-2-api-68c75f4f5</kbd>) should have been scaled down to <kbd>0</kbd>. However, the Deployment noticed that there is a problem and stopped the update process.</p>
<p>We should be able to get more detailed information with the <kbd>kubectl rollout status</kbd> command:</p>
<pre><strong>kubectl rollout status \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>error: deployment "go-demo-2-api" exceeded its progress deadline</strong>  </pre>
<p>The Deployment realized that it shouldn't proceed. The new Pods are not running, and the limit was reached. There's no point to continue trying.</p>
<p>If you expected that the Deployment would roll back after it failed, you're wrong. It will not do such a thing. At least, not without additional add-ons. That does not mean that I would expect you to sit in front of your terminal, wait for timeouts, and check the <kbd>rollout status</kbd> before deciding whether to keep the new update or to roll back. I expect you to deploy new releases as part of your automated CDP pipeline. Fortunately, the <kbd>status</kbd> command returns <kbd>1</kbd> if the deployment failed and we can use that information to decide what to do next. For those of you not living and breathing Linux, any exit code different than <kbd>0</kbd> is considered an error. Let's confirm that by checking the exit code of the last command:</p>
<pre><strong>echo $?</strong>  </pre>
<p>The output is indeed <kbd>1</kbd>, thus confirming that the rollout failed.</p>
<p>We'll explore automated CDP pipeline soon. For now, just remember that we can find out whether Deployment updates were successful or not.</p>
<p>Now that we discovered that our last rollout failed, we should undo it. You already know how to do that, but I'll remind you just in case you're of a forgetful nature.</p>
<pre><strong>kubectl rollout undo \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>
    
<strong>kubectl rollout status \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>
<p>The output of the last command confirmed that <kbd>deployment "go-demo-2-api"</kbd> was <kbd>successfully rolled out</kbd>.</p>
<p>Now that we have learned how to rollback no matter whether the problem is a critical bug or inability to run the new release, we can take a short pause from learning new stuff and merge all the definitions we explored thus far into a single YAML file. But, before we do that, we'll remove the objects we created.</p>
<pre><strong>kubectl delete \</strong>
<strong>    -f deploy/go-demo-2-db.yml</strong>
    
<strong>kubectl delete \</strong>
<strong>    -f deploy/go-demo-2-db-svc.yml</strong>
    
<strong>kubectl delete \</strong>
<strong>    -f deploy/go-demo-2-api.yml</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Merging everything into the same YAML definition</h1>
                </header>
            
            <article>
                
<p>Consider this section a short intermezzo. We'll merge the definitions we used in this chapter into a single YAML file. You already had a similar example before, so there's no need for lengthy explanations.</p>
<pre><strong>cat deploy/go-demo-2.yml</strong>  </pre>
<p>If you start searching for differences with the previous definitions, you will find a few. The <kbd>minReadySeconds</kbd>, <kbd>progressDeadlineSeconds</kbd>, <kbd>revisionHistoryLimit</kbd>, and <kbd>strategy</kbd> fields are removed from the <kbd>go-demo-2-api</kbd> Deployment. We used them mostly as a way to demonstrate their usage. But, since Kubernetes has sensible defaults, we omitted them from this definition. You'll also notice that there are two Services even though we created only one in this chapter. We did not need the <kbd>go-demo-2-api</kbd> Service in our examples since we didn't need to access the API. But, for the sake of completeness, it is included in this definition. Finally, the strategy for deploying the database is set to <kbd>recreate</kbd>. As explained earlier, it is more suited for a single-replica database, even though we did not mount a volume that would preserve the data.</p>
<p>Let's create the objects defined in <kbd>deploy/go-demo-2.yml</kbd>.Remember, with <kbd>--save-config</kbd> we're making sure we can edit the configuration later. The alternative would be to use <kbd>kubectl apply</kbd> instead.</p>
<pre><strong>kubectl create \</strong>
<strong>    -f deploy/go-demo-2.yml \</strong>
<strong>    --record --save-config</strong>
    
<strong>kubectl get -f deploy/go-demo-2.yml</strong>  </pre>
<p>The output of the latter command is as follows:</p>
<pre><strong>NAME                DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>deploy/go-demo-2-db 1       1       1          1         15s</strong>
    
<strong>NAME             TYPE      CLUSTER-IP EXTERNAL-IP PORT(S)   AGE</strong>
<strong>svc/go-demo-2-db ClusterIP 10.0.0.125 &lt;none&gt;      27017/TCP 15s</strong>
    
<strong>NAME                 DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>deploy/go-demo-2-api 3       3       3          3         15s</strong>
    
<strong>NAME              TYPE     CLUSTER-IP EXTERNAL-IP PORT(S)        <br/>AGE</strong>
<strong>svc/go-demo-2-api NodePort 10.0.0.57  &lt;none&gt;      8080:31586/TCP <br/>15s</strong></pre>
<p>All four objects (two Deployments and two Services) were created, and we can move on and explore ways to update multiple objects with a single command.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Updating multiple objects</h1>
                </header>
            
            <article>
                
<p>Even though most of the time we send requests to specific objects, almost everything is happening using selector labels. When we updated the Deployments, they looked for matching selectors to choose which ReplicaSets to create and scale. They, in turn, created or terminated Pods also using the matching selectors. Almost everything in Kubernetes is operated using label selectors. It's just that sometimes that is obscured from us.</p>
<p>We do not have to update an object only by specifying its name or the YAML file where its definition resides. We can also use labels to decide which object should be updated. That opens some interesting possibilities since the selectors might match multiple objects.</p>
<p>Imagine that we are running several Deployments with Mongo databases and that the time has come to update them all to a newer release. Before we explore how we could do that, we'll create another Deployment so that we have at least two with the database Pods.</p>
<p>Let us first take a look at the definition:</p>
<pre><strong>cat deploy/different-app-db.yml</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>piVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: different-app-db<br/>  labels:<br/>     type: db<br/>     service: different-app<br/>     vendor: MongoLabs</strong>
<strong>spec:</strong>
<strong>  selector:</strong>
<strong>    matchLabels:</strong>
<strong>      type: db</strong>
<strong>      service: different-app</strong>
<strong>  template:</strong>
<strong>    metadata:</strong>
<strong>      labels:</strong>
<strong>        type: db</strong>
<strong>        service: different-app</strong>
<strong>        vendor: MongoLabs</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: db</strong>
<strong>        image: mongo:3.3</strong>
<strong>        ports:</strong>
<strong>        - containerPort: 28017</strong>  </pre>
<p>When compared with the <kbd>go-demo-2-db</kbd> Deployment, the only difference is in the <kbd>service</kbd> label. Both have the <kbd>type</kbd> set to <kbd>db</kbd>.</p>
<p>Let's create the deployment:</p>
<pre><strong>kubectl create \</strong>
<strong>    -f deploy/different-app-db.yml</strong>  </pre>
<p>Now that we have two deployments with the <kbd>mongo:3.3</kbd> Pods, we can try to update them both at the same time.</p>
<p>The trick is to find a label (or a set of labels) that uniquely identifies all the Deployments we want to update.</p>
<p>Let's take a look at the list of Deployments with their labels:</p>
<pre><strong>kubectl get deployments --show-labels</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME             DESIRED CURRENT UP-TO-DATE AVAILABLE AGE LABELS</strong>
<strong>different-app-db 1       1       1          1         1h  service<br/>=different-app,type=db,vendor=MongoLabs</strong>
<strong>go-demo-2-api    3       3       3          3         1h  languag<br/>e=go,service=go-demo-2,type=api</strong>
<strong>go-demo-2-db     1       1       1          1         1h  service<br/>=go-demo-2,type=db,vendor=MongoLabs</strong>  </pre>
<p>We want to update <kbd>mongo</kbd> Pods created using <kbd>different-app-db</kbd> and <kbd>go-demo-2-db</kbd> Deployments. Both are uniquely identified with the labels <kbd>type=db</kbd> and <kbd>vendor=MongoLabs</kbd>. Let's test that:</p>
<pre><strong>kubectl get deployments \</strong>
<strong>    -l type=db,vendor=MongoLabs</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>NAME             DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>different-app-db 1       1       1          1         1h</strong>
<strong>go-demo-2-db     1       1       1          1         1h</strong>  </pre>
<p>We can see that filtering with those two labels worked. We retrieved only the Deployments we want to update, so let's proceed and roll out the new release:</p>
<pre><strong>kubectl set image deployments \</strong>
<strong>    -l type=db,vendor=MongoLabs \ </strong>
<strong>    db=mongo:3.4 --record</strong>  </pre>
<p>The output is as follows:</p>
<pre><strong>deployment "different-app-db" image updated</strong>
<strong>deployment "go-demo-2-db" image updated</strong>  </pre>
<p>Finally, before we move into the next subject, we should validate that the image indeed changed to <kbd>mongo:3.4</kbd>:</p>
<pre><strong>kubectl describe \</strong>
<strong>    -f deploy/go-demo-2.yml</strong>  </pre>
<p>The output, limited to the relevant parts, is as follows:</p>
<pre><strong>...</strong>
<strong>  Containers:</strong>
<strong>   db:</strong>
<strong>    Image:        mongo:3.4</strong>
<strong>...</strong>  </pre>
<p>As we can see, the update was indeed successful, at least with that Deployment. Feel free to describe the Deployment defined in <kbd>deploy/different-app-db.yml</kbd>. You should see that its image was also updated to the newer version.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scaling Deployments</h1>
                </header>
            
            <article>
                
<p>There are quite a few different ways we can scale Deployments. Everything we do in this section is not unique to Deployments and can be applied to any Controller, like ReplicaSet, and those we did not yet explore.</p>
<p>If we decide that the number of replicas changes with relatively low frequency or that Deployments are performed manually, the best way to scale is to write a new YAML file or, even better, modify the existing one. Assuming that we store YAML files in a code repository, by updating existing files we have a documented and reproducible definition of the objects running inside a cluster.</p>
<p>We already performed scaling when we applied the definition from the <kbd>go-demo-2-scaled.yml</kbd>. We'll do something similar, but with Deployments.</p>
<p>Let's take a look at <kbd>deploy/go-demo-2-scaled.yml</kbd>.</p>
<pre><strong>cat deploy/go-demo-2-scaled.yml</strong>  </pre>
<p>We won't display the contents of the whole file since it is almost identical to <kbd>deploy/go-demo-2.yml</kbd>. The only difference is the number of replicas of the <kbd>go-demo-2-api</kbd> Deployment.</p>
<pre><strong>...</strong>
<strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
  <strong>name: go-demo-2-api</strong>
<strong>spec:</strong>
  <strong>replicas: 5</strong>
<strong>...</strong>  </pre>
<p>At the moment, we're running three replicas. Once we apply the new definition, it should increase to five.</p>
<pre><strong>kubectl apply \</strong>
<strong>    -f deploy/go-demo-2-scaled.yml</strong>  </pre>
<p>Please note that, even though the file is different, the names of the resources are the same so <kbd>kubectl apply</kbd> did not create new objects. Instead, it updated those that changed. In particular, it changed the number of replicas of the <kbd>go-demo-2-api</kbd> Deployment.</p>
<p>Let's confirm that there are indeed five replicas of the Pods controlled through the Deployment.</p>
<pre><strong>kubectl get \</strong>
<strong>    -f deploy/go-demo-2-scaled.yml</strong>  </pre>
<p>The output, limited to the <kbd>deploy/go-demo-2-api</kbd>, is as follows:</p>
<pre><strong>...</strong>
<strong>NAME                 DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>deploy/go-demo-2-api 5       5       5          5         11m</strong>
<strong>...</strong>  </pre>
<p>The result should come as no surprise. After all, we executed the same process before, when we explored ReplicaSets.</p>
<p>While scaling Deployments using YAML files (or other Controllers) is an excellent way to keep documentation accurate, it rarely fits the dynamic nature of the clusters. We should aim for a system that will scale (and de-scale) services automatically. When scaling is frequent and, hopefully, automated, we cannot expect to update YAML definitions and push them to Git. That would be too inefficient and would probably cause quite a few unwanted executions of delivery pipelines if they are triggered through repository Webhooks. After all, do we really want to push updated YAML files multiple times a day?</p>
<div class="packt_tip">The number of <kbd>replicas</kbd> should not be part of the design. Instead, they are a fluctuating number that changes continuously (or at least often), depending on the traffic, memory and CPU utilization, and so on.</div>
<p>Depending on release frequency, the same can be said for <kbd>image</kbd>. If we are practicing continuous delivery or deployment, we might be releasing once a week, once a day, or even more often. In such cases, new images would be deployed often, and there is no strong argument for the need to change YAML files every time we make a new release. That is especially true if we are deploying through an automated process (as we should).</p>
<p>We'll explore automation later on. For now, we'll limit ourselves to a command similar to <kbd>kubectl set image</kbd>. We used it to change the <kbd>image</kbd> used by Pods with each release. Similarly, we'll use <kbd>kubectl scale</kbd> to change the number of replicas. Consider this an introduction to automation that is coming later on.</p>
<pre><strong>kubectl scale deployment \</strong>
<strong>    go-demo-2-api --replicas 8 --record</strong>  </pre>
<p>We scaled the number of replicas associated with the Deployment <kbd>go-demo-2-api</kbd>. Please note that, this time, we did not use <kbd>-f</kbd> to reference a file. Since we have two Deployments specified in the same YAML, that would result in scaling of both. Since we wanted to limit it to a particular Deployment, we used its name instead.</p>
<p>Let's confirm that scaling indeed worked as expected.</p>
<pre><strong>kubectl get -f deploy/go-demo-2.yml</strong>  </pre>
<p>The output, limited to Deployments, is as follows:</p>
<pre><strong>NAME                DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>deploy/go-demo-2-db 1       1       1          1         33m</strong>
    
<strong>NAME                 DESIRED CURRENT UP-TO-DATE AVAILABLE AGE</strong>
<strong>deploy/go-demo-2-api 8       8       8          8         33m</strong>  </pre>
<p>As I mentioned earlier, we'll dedicate quite a lot of time to automation, and you won't have to scale your applications manually. However, I thought that it is useful to know that the <kbd>kubectl scale</kbd> command exists. For now, you know how to scale Deployments (and other Controllers).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What now?</h1>
                </header>
            
            <article>
                
<p>Everything we learned led us to Deployments. Pods must not be created directly, but through ReplicaSets which, similarly, must not be created directly, but through Deployments. They are the objects that allow us not only to create the ReplicaSets and Pods, but that can also be updated without producing any downtime (when applications are designed accordingly). We combined Deployments with Services so that Pods can communicate with each other, or can be accessed from outside a cluster. All in all, we have everything we need to release our services to production. That is not to say that we understand all the crucial aspects of Kubernetes. We're not even close to that point. But, we do have almost everything we need for running some types of applications in production. What we're missing is networking.</p>
<p>Before we enter the next stage of our knowledge seeking mission, we'll destroy the cluster we're running and give our laptops a break.</p>
<pre><strong>minikube delete</strong></pre>
<div class="packt_infobox">If you'd like to know more about Deployments, please explore Deployment v1 apps (<a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#deployment-v1-apps" target="_blank"><span class="URLPACKT">https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#deployment-v1-apps</span></a>) API documentation.</div>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/daf85690-4328-4ab1-9a11-5a87a9e5d3a0.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Figure 6-4: The components explored so far</div>
<p>Before we move onto the next chapter, we'll explore the differences between Kubernetes Deployments and Docker Swarm stacks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes Deployments compared to Docker Swarm stacks</h1>
                </header>
            
            <article>
                
<p>If you have already used Docker Swarm, the logic behind Kubernetes Deployments should be familiar. Both serve the same purpose and can be used to deploy new applications or update those that are already running inside a cluster. In both cases, we can easily deploy new releases without any downtime (when application architecture permits that).</p>
<p>Unlike the previous comparison between Kubernetes Pods, ReplicaSets, and Services with Docker Swarm Stacks, Kubernetes Deployments do provide a few potentially important functional differences. But, before we dive into the functional comparison, we'll take a moment to explore differences in how we define objects.</p>
<p>An example Kubernetes Deployment and Service definition is as follows:</p>
<pre><strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-db</strong>
<strong>spec:</strong>
<strong>  selector:</strong>
<strong>    matchLabels:</strong>
<strong>     type: db</strong>
<strong>      service: go-demo-2</strong>
<strong>  strategy:</strong>
<strong>    type: Recreate</strong>
<strong>  template:</strong>
<strong>    metadata:</strong>
<strong>      labels:</strong>
<strong>        type: db</strong>
<strong>        service: go-demo-2</strong>
<strong>        vendor: MongoLabs</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: db</strong>
<strong>        image: mongo:3.3</strong>
<strong>        ports:</strong>
<strong>        - containerPort: 28017</strong>
    
<strong>---</strong>
    
<strong>apiVersion: v1</strong>
<strong>kind: Service</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-db</strong>
<strong>spec:</strong>
<strong>  ports:</strong>
<strong>  - port: 27017</strong>
<strong>  selector:</strong>
<strong>    type: db</strong>
<strong>    service: go-demo-2</strong>
    
<strong>---</strong>
    
<strong>apiVersion: apps/v1beta2</strong>
<strong>kind: Deployment</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-api</strong>
<strong>spec:</strong>
<strong>  replicas: 3</strong>
<strong>  selector:</strong>
<strong>    matchLabels:</strong>
<strong>      type: api</strong>
<strong>      service: go-demo-2</strong>
<strong>  template:</strong>
<strong>    metadata:</strong>
<strong>      labels:</strong>
<strong>        type: api</strong>
<strong>        service: go-demo-2</strong>
<strong>        language: go</strong>
<strong>    spec:</strong>
<strong>      containers:</strong>
<strong>      - name: api</strong>
<strong>        image: vfarcic/go-demo-2</strong>
<strong>        env:</strong>
<strong>        - name: DB</strong>
<strong>          value: go-demo-2-db</strong>
<strong>        readinessProbe:</strong>
<strong>          httpGet:</strong>
<strong>            path: /demo/hello</strong>
<strong>            port: 8080</strong>
<strong>          periodSeconds: 1</strong>
<strong>        livenessProbe:</strong>
<strong>          httpGet:</strong>
<strong>            path: /demo/hello</strong>
<strong>            port: 8080</strong>

<strong>---</strong>
 
<strong>apiVersion: v1</strong>
<strong>kind: Service</strong>
<strong>metadata:</strong>
<strong>  name: go-demo-2-api</strong>
<strong>spec:</strong>
<strong>  type: NodePort</strong>
<strong>  ports:</strong>
<strong>  - port: 8080</strong>
<strong>  selector:</strong>
<strong>    type: api</strong>
<strong>    service: go-demo-2</strong>  </pre>
<p>An equivalent Docker Swarm stack definition is as follows:</p>
<pre><strong>version: "3"</strong>
<strong>services:</strong>
<strong>  api:</strong>
<strong>    image: vfarcic/go-demo-2</strong>
<strong>    environment:</strong>
<strong>      - DB=db</strong>
<strong>    ports:</strong>
<strong>      - 8080</strong>
<strong>    deploy:</strong>
<strong>      replicas: 3</strong>
<strong>  db:</strong>
<strong>    image: mongo:3.3</strong>  </pre>
<p>Both definitions provide, more or less, the same functionality.</p>
<p>It is evident that a Kubernetes Deployment requires a much longer definition with more a complex syntax. It is worth noting that Swarm's equivalent to <kbd>readinessProbe</kbd> and <kbd>livenessProbe</kbd> is not present in the stack because it is defined as a <kbd>HEALTHCHECK</kbd> inside the Dockerfile. Still, even if we remove them, a Kubernetes Deployment remains longer and more complicated.</p>
<div class="packt_tip">When comparing only the differences in the ways to define objects, Docker Swarm is a clear <strong>winner</strong>. Let's see what we can conclude from the functional point of view.</div>
<p>Creating the objects is reasonably straight-forward. Both <kbd>kubectl create</kbd> and <kbd>docker stack deploy</kbd> will deploy new releases without any downtime. New containers or, in case of Kubernetes, Pods will be created and, in parallel, the old ones will be terminated. So far, both solutions are, more or less, the same.</p>
<p>One of the main differences is what happens in case of a failure. A Kubernetes Deployment will not perform any corrective action in case of a failure. It will stop the update, leaving a combination of new and old Pods running in parallel. Docker Swarm, on the other hand, can be configured to rollback automatically. That might seem like another win for Docker Swarm. However, Kubernetes has something Swarm doesn't. We can use <kbd>kubectl rollout status</kbd> command to find out whether the update was a success or failure and, in case of the latter, we can <kbd>undo</kbd> the <kbd>rollout</kbd>. Even though we need a few commands to accomplish the same result, that might fare better when updates are automated. Knowing whether an update succeeded or failed allows us to not only execute a subsequent rollback action but also notify someone that there is a problem.</p>
<div class="packt_tip">Both approaches have their pros and cons. Docker Swarm's automated rollback is better suited in some cases, and Kubernetes update status works better in others. The methods are different, and there is no clear winner, so I'll proclaim it a tie.</div>
<p>Kubernetes Deployments can record history. We can use the <kbd>kubectl rollout history</kbd> command to inspect past rollout. When updates are working as expected, <kbd>history</kbd> is not very useful. But, when things go wrong, it might provide additional insight. That can be combined with the ability to rollback to a specific revision, not necessarily the previous one. However, most of the time, we rollback to the previous version. The ability to go back further in time is not very useful. Even when such a need arises, both products can do that. The difference is that Kubernetes Deployments allow us to go to a specific revision (for example, we're on the revision five, rollback to the revision two). With Docker Swarm, we'd have to issue a new update (for example, update the image to the tag 2.0). Since containers are immutable, the result is the same, so the difference is only in the syntax behind a rollback.</p>
<div class="packt_tip">The ability to rollback to a specific version or a tag exists in both products. We can argue which syntax is more straightforward or more useful. The differences are minor, and I'll proclaim that there is no winner for that functionality. It's another tie.</div>
<p>Since almost everything in Kubernetes is based on label selectors, it has a feature that Docker Swarm doesn't. We can update multiple Deployments at the same time. We can, for example, issue an update (<kbd>kubetl set image</kbd>) that uses filters to find all Mongo databases and upgrade them to a newer release. It is a feature that would require a few lines of bash scripting with Docker Swarm. However, while the ability to update all Deployments that match specific labels might sound like a useful feature, it often isn't. More often than not, such actions can produce undesirable effects. If, for example, we have five back-end applications that use Mongo database (one for each), we'd probably want to upgrade them in a more controlled fashion. Teams behind those services would probably want to test each of those upgrades and give their blessings. We probably wouldn't wait until all are finished, but upgrade a single database when the team in charge of it feels confident.</p>
<div class="packt_tip">There are the cases when updating multiple objects is useful so I must give this one to Kubernetes. It a minor win, but it still counts.</div>
<p>There are a few other things that are easier to accomplish with Kubernetes. For example, due to the way Kubernetes Services work, creating a blue-green deployment process, instead of using rolling updates, is much easier. However, such a process falls into advanced usage so I'll leave it out of this comparison. It'll (probably) come later.</p>
<p>It's difficult to say which solution provides better results. Docker Swarm continues to shine from the user-friendliness perspective. On the other hand, Kubernetes Deployments offer a few additional features.</p>
<div class="packt_tip">It is much simpler and easier to write a Docker Swarm stack file than a Kubernetes Deployment definition. Kubernetes Deployments offer a few additional functional features that Swarm does not have. However, those features are, for most use cases, of minor importance. Those that indeed matter are, more or less, the same.</div>
<p>Don't make a decision based on the differences between Kubernetes Deployments and Docker Swarm stacks. Definition syntax is where Swarm has a clear win, while on the functional front Kubernetes has a tiny edge over Swarm. If you'd make a decision only based on deployments, Swarm might be a slightly better candidate. Or not. It all depends on what matters more in your case. Do you care about YAML syntax? Are those additional Kubernetes Deployment features something you will ever use?</p>
<p>In any case, Kubernetes has much more to offer, and any conclusion based on such a limited comparison scope is bound to be incomplete. We only scratched the surface. Stay tuned for more.</p>


            </article>

            
        </section>
    </body></html>