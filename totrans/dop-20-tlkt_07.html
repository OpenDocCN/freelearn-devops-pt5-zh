<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Implementation of the Deployment Pipeline &#x2013; Intermediate Stages"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Implementation of the Deployment Pipeline – Intermediate Stages</h1></div></div></div><p>We could not complete the basic implementation of the deployment pipeline without the production server being set up. We didn't need much. At the moment, Docker is our only prerequisite for the deployment and that gave us a good excuse to make a side trip into the world of configuration management. Now, with the Ansible playbook that will set up our prod server, we <a class="indexterm" id="id279"/>can continue where <a class="indexterm" id="id280"/>we left and deploy the container to the production server:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Checkout the code - Done</li><li class="listitem">Run pre-deployment tests - Done</li><li class="listitem">Compile and/or package the code - Done</li><li class="listitem">Build the container - Done</li><li class="listitem">Push the container to the registry - Done</li><li class="listitem">Deploy the container to the production server - Pending</li><li class="listitem">Integrate the container - Pending</li><li class="listitem">Run post-deployment tests - Pending</li><li class="listitem">Push the tests container to the registry - Pending<div class="mediaobject"><img alt="Implementation of the Deployment Pipeline – Intermediate Stages" src="graphics/B05848_07_01.jpg"/><div class="caption"><p>Figure 7-1 – The initial stages of the deployment pipeline with Docker</p></div></div></li></ol></div><p>We are<a class="indexterm" id="id281"/> missing only four steps from<a class="indexterm" id="id282"/> the manual deployment pipeline.</p><div class="section" title="Deploying Containers to the Production Server"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec18"/>Deploying Containers to the Production Server</h1></div></div></div><p>Let's create and configure VMs we'll use throughout this chapter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd prod</strong></span>
<span class="strong"><strong>vagrant ssh cd</strong></span>
<span class="strong"><strong>ansible-playbook /vagrant/ansible/prod.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>
</pre></div><p>The first command brought up the <code class="literal">cd</code> and <code class="literal">prod</code> VMs while the second got us inside the <code class="literal">cd</code> VM. Finally, the last command configured the <code class="literal">prod</code> VM.</p><p>Now that the production server is properly configured, we can deploy the <code class="literal">books-ms</code> container. Even though we don't have is pulled to the destination server, we already pushed it to the Docker registry in the <code class="literal">cd</code> node (that maps into the host directory) and can retrieve it from there. What we do not have, however, is the Docker Compose configuration<a class="indexterm" id="id283"/> that specifies how the container <a class="indexterm" id="id284"/>should be run. I prefer keeping everything related to a service in the same repository and <code class="literal">**docker-compose.yml*</code> is no exception. We can retrieve it GitHub:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\/books-ms/master/docker-compose.yml</strong></span>
</pre></div><p>With the <code class="literal">docker-compo</code>
<code class="literal">se.yml</code> downloaded, let us take a quick look at it (targets that won't be used in this chapter have been excluded):</p><div class="informalexample"><pre class="programlisting">base:
  image: 10.100.198.200:5000/books-ms
  ports:- 8080environment: - SERVICE_NAME=books-ms
app:
  extends:
    service: base
  links: - db:db
db: image: mongo</pre></div><p>The <code class="literal">base</code> target contains the base definition of our container. The next target (<code class="literal">app</code>) is extending the <code class="literal">base</code> service allowing us to avoid duplicating the definitions. By extending services, we can override arguments or add new ones. The <code class="literal">app</code> target will run the container we stored in the registry on the <code class="literal">cd</code> server and is linked to the third target that represent the database required by the service. You might notice that we changed the way ports are specified. In the <code class="literal">docker-compose-d</code>
<code class="literal">ev.yml</code> we had two numbers separated by a colon (<code class="literal">8080:8080</code>). The first one was the port Docker would expose to the host while the second one is the internal port used by the server inside the container. The <code class="literal">docker-compose.yml</code> is a bit different and has only the internal port set. The reason behind that is the elimination of potential conflicts. While in the development environment we tend to run only a small number of services (those we need at the moment), in production we might run tens, hundreds, or even thousands of them at the same time. Having predefined ports can easily result in conflicts. If two of them are using the same port, the result will be a failure. For that reason, we'll let Docker expose a random port to the host.</p><p>Let us run the Docker Compose <code class="literal">app</code> target:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://prod:2375</strong></span>
<span class="strong"><strong>docker-compose up -d app</strong></span>
</pre></div><p>We exported the <code class="literal">DOCKER_HOST</code> variable that tells local Docker client to send commands to the remote one located on the <code class="literal">prod</code> node and port <code class="literal">2375</code>. The second command run the Docker Compose target <code class="literal">app</code>. Since <code class="literal">DOCKER_HOST</code> is pointing to the remote host, the <code class="literal">app</code> target and linked container <code class="literal">db</code> were deployed to the <code class="literal">prod</code> server. We did not even have to enter the destination server. The deployment was done remotely.</p><p>For security reasons, the ability to invoke remote Docker API is disabled by default. However, one of the Ansible playbook tasks was to change that behaviour by modifying the <code class="literal">/etc/default/docker</code> configuration file. Its content is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>DOCKER_OPTS="$DOCKER_OPTS --insecure-registry 10.100.198.200:5000 -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock"</strong></span>
</pre></div><p>The <code class="literal">--insecure-registry</code> allows Docker to pull images from our private registry located in the <code class="literal">cd</code> node (<code class="literal">10.100.198.200</code>). The <code class="literal">-H</code> argument tells Docker to listen to remote <a class="indexterm" id="id285"/>requests from any address (<code class="literal">0.0.0.0</code>) on the port <code class="literal">2375</code>. Please note that in the real production environment, we would <a class="indexterm" id="id286"/>need to be much more restrictive and allow only trusted addresses to access the remote Docker API.</p><p>We can confirm that both containers are indeed running on the <code class="literal">prod</code> VM by executing another remote call:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose ps</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>    Name               Command          State            Ports</strong></span>
<span class="strong"><strong>-----------------------------------------------------------------------</strong></span>
<span class="strong"><strong>vagrant_app_1   /run.sh                 Up      0.0.0.0:32770-&gt;80</strong></span>
<span class="strong"><strong>80/tcp</strong></span>
<span class="strong"><strong>vagrant_db_1    /entrypoint.sh mongod   Up      27017/tcp</strong></span>
</pre></div><p>Since Docker assigned a random port to the service's internal port <code class="literal">8080</code>, we need to find it out. That can be done with the <code class="literal">inspect</code> command.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker inspect vagrant_app_1</strong></span>
</pre></div><p>The part of the output that interests us should be similar to the following:</p><div class="informalexample"><pre class="programlisting">...
"NetworkSettings": {
    "Bridge": "",
	"EndpointID": "45a8ea03cc2514b128448...",
	"Gateway": "172.17.42.1",
	"GlobalIPv6Address": "",
	"GlobalIPv6PrefixLen": 0,
	"HairpinMode": false,
	"IPAddress": "172.17.0.4",
	"IPPrefixLen": 16,
	"IPv6Gateway": "",
    "LinkLocalIPv6Address": "",
	"LinkLocalIPv6PrefixLen": 0,
	"MacAddress": "02:42:ac:11:00:04",
	"NetworkID": "dce90f852007b489f4a2fe...",
	"PortMapping": null,
	"Ports": {
		 "8080/tcp": [
			 {
                "HostIp": "0.0.0.0",
				"HostPort": "32770"
            }
        ]
    },
    "SandboxKey": "/var/run/docker/netns/f78bc787f617",
	"SecondaryIPAddresses": null,
	"SecondaryIPv6Addresses": null
}
...</pre></div><p>The original<a class="indexterm" id="id287"/> output is much bigger than this <a class="indexterm" id="id288"/>and it contains all the info we might (or might not) need. What we are interested in right now is the <code class="literal">NetworkSettings.Ports</code> section that, in my case, gives us <code class="literal">HostPort 32770</code> mapped to the internal port <code class="literal">8080</code>. We can do better than that and use the <code class="literal">--format</code> argument:</p><div class="informalexample"><pre class="programlisting">PORT=$(docker inspect \--format='{{(index (index .NetworkSettings.Ports "8080/tcp") 0).HostPort}}' \vagrant_app_1)
echo $PORT</pre></div><p>Do not get scared by the <code class="literal">--format</code> value syntax. It uses Go's <code class="literal">text/template</code> format and indeed can be a bit daunting. The good news is that we'll use much better ways to do this once we get to <a class="link" href="ch08.html" title="Chapter 8. Service Discovery – The Key to Distributed Services">Chapter 8</a>, <span class="emphasis"><em>Service Discovery – The Key to Distributed Services</em></span> chapter. This is only the temporary workaround.</p><p>We got our port and stored it to the <code class="literal">PORT</code> variable. Now we can repeat <code class="literal">curl</code> commands we already got familiar with and confirm that the service is running and is connected to the DB:</p><div class="informalexample"><pre class="programlisting">
curl -H 'Content-Type: application/json' -X PUT -d \
"{\"_id\": 1,
\"title\": \"My First Book\",
\"author\": \"John Doe\",
\"description\": \"Not a very good book\"}" \
http://prod:$PORT/api/v1/books \
| jq '.'
curl -H 'Content-Type: application/json' -X PUT -d \
"{\"_id\": 2,
\"title\": \"My Second Book\",
\"author\": \"John Doe\",
\"description\": \"Not a bad as the first book\"}" \
http://prod:$PORT/api/v1/books \
| jq '.'
curl -H 'Content-Type: application/json' -X PUT -d \
"{\"_id\": 3,
\"title\": \"My Third Book\",
\"author\": \"John Doe\",
\"description\": \"Failed writers club\"}" \
http://prod:$PORT/api/v1/books \
| jq '.'
curl http://prod:$PORT/api/v1/books \
| jq '.'
curl http://prod:$PORT/api/v1/books/_id/1 \
| jq '.'
</pre></div><p>The output of the last command is as follows:</p><div class="informalexample"><pre class="programlisting">{
    "_id": 1,
	"author": "John Doe",
	"description": "Not a very good book",
	"title": "My First Book"
}</pre></div><p>As before, when<a class="indexterm" id="id289"/> we run the same command in<a class="indexterm" id="id290"/> the development environment, we inserted three books to the database and confirmed that they can be retrieved from the database. However, this is not an efficient way of verifying whether the service was deployed correctly. We can do better than that and run the integration tests.</p><p>The important thing to note is that we have not even entered into the <code class="literal">prod</code> node. All the deployment commands were done through the remote Docker API.</p><div class="section" title="Docker UI"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec41"/>Docker UI</h2></div></div></div><p>This might be<a class="indexterm" id="id291"/> a good opportunity to introduce a nice open source project DockerUI. It is defined as part of the <span class="emphasis"><em>docker</em></span> Ansible role so it is running on all servers where we configure Docker. We can, for example, see the instance running on the <code class="literal">prod</code> node by opening <code class="literal">http://10.100</code>
<code class="literal">.198.201:9000</code> from any browser.</p><p>Please note that all IPs created through Vagrant are set to be private, meaning that they can be accessed only from the host machine. If that happens to be your laptop, you should not have a problem to open the DockerUI address in your browser. On the other hand, if you are running the examples on one of your corporate servers, please make sure that you can access it's desktop and that a browser is installed. If you need to access that server remotely, please try one of remote desktop solutions like VNC:</p><div class="mediaobject"><img alt="Docker UI" src="graphics/B05848_07_02.jpg"/><div class="caption"><p>Figure 7-2 – DockerUI dashboard screen</p></div></div><p>While it is much <a class="indexterm" id="id292"/>more efficient to operate containers through CLI, the DockerUI provides a very useful way to gain a general overview of the system and details related to each container, network, and images. It true usefulness can be seen when a big number of containers is running in a cluster. It is very lightweight so it won't use much of your resources.</p><p>Unless specified otherwise, you'll find it running on each VM we set up.</p></div><div class="section" title="The Checklist"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec42"/>The Checklist</h2></div></div></div><p>Before we<a class="indexterm" id="id293"/> move on, let's see where we are with our basic implementation of the deployment pipeline:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Checkout the code - Done</li><li class="listitem">Run pre-deployment tests - Done</li><li class="listitem">Compile and/or package the code - Done</li><li class="listitem">Build the container - Done</li><li class="listitem">Push the container to the registry - Done</li><li class="listitem">Deploy the container to the production server - Done</li><li class="listitem">Integrate the container - Pending</li><li class="listitem">Run post-deployment tests - Pending</li><li class="listitem">Push the tests container to the registry - Pending<div class="mediaobject"><img alt="The Checklist" src="graphics/B05848_07_03.jpg"/><div class="caption"><p>Figure 7-3 – The intermediate stages of the deployment pipeline with Docker</p></div></div></li></ol></div><p>Please note that, unlike the steps we did in the previous chapter, the deployment was performed in the<a class="indexterm" id="id294"/> production environment through remote Docker API. If we deployed the second release, we would have a period which neither the old nor the new release were operational. One would need to be stopped while the other would require some time to be brought up. No matter whether this period was short or not, we would have <span class="emphasis"><em>down-time</em></span> that, in itself, would prevent us from moving towards <span class="emphasis"><em>continues deployment</em></span>. All we'll do for now is take a note of this problem. Later on, we'll explore the <span class="emphasis"><em>blue-green deployment</em></span> procedure that will help us overcome this issue and proceed towards the quest for zero-downtime deployments.</p><p>We're making progress and only three tasks are left on the checklist. However, the application is not yet integrated and, therefore, we cannot run integration tests. In order to proceed, there are two more concepts we need to explore; <span class="emphasis"><em>service discovery</em></span> and <span class="emphasis"><em>reverse proxy</em></span>.</p><p>We'll use a new set<a class="indexterm" id="id295"/> of virtual machines while experimenting with service discovery tools, so let us save some resources and destroy the VMs we're running. We'll create those that we need in the next chapter.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div></div></div></div></body></html>