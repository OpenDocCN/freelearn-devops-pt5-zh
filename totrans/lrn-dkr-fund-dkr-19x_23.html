<html><head></head><body>
        

                            
                    <h1 class="header-title">Assessments</h1>
                
            
            
                


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 1</h1>
                
            
            
                
<p class="p1">Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li class="p2">The correct answers are <strong>D</strong> and <strong>E</strong>.</li>
<li class="p2">A Docker container is to IT what a shipping container is to the transportation industry. It defines a standard on how to package goods. In this case, goods are the application(s) developers write. The suppliers (in this case, the developers) are responsible for packaging the goods into the container and making sure everything fits as expected. Once the goods are packaged into a container, it can be shipped. Since it is a standard container, the shippers can standardize their means of transportation, such as lorries, trains, or ships. The shipper doesn't really care what's in a container. Also, the loading and unloading process from one transportation means to another (for example, train to ship) can be highly standardized. This massively increases the efficiency of transportation. Analogous to this is an operations engineer in IT, who can take a software container built by a developer and ship it to a production system and run it there in a highly standardized way, without worrying about what's in the container. It will just work.</li>
<li class="p2">Some of the reasons why containers are game changers are as follows:
<ul>
<li class="p2">Containers are self-contained and thus if they run on one system, they run anywhere that a container can run.</li>
<li class="p2">Containers run on premises and in the cloud, as well as in hybrid environments. This is important for today's typical enterprises since it allows a smooth transition from on premises to the cloud.</li>
<li class="p2">Container images are built or packaged by the people who know best – the developers.</li>
<li class="p2">Container images are immutable, which is important for good release management.</li>
<li class="p2">Containers are enablers of a secure software supply chain based on encapsulation (using Linux namespaces and cgroups), secrets, content trust, and image vulnerability scanning.</li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<ol start="4">
<li>Any given container runs anywhere where containers can run for the following reasons:</li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li>
<p class="p1">Containers are self-contained black boxes. They encapsulate not only an application but all its dependencies, such as libraries and frameworks, configuration data, certificates, and so on.</p>
</li>
<li>
<p class="p1">Containers are based on widely accepted standards such as OCI.</p>
</li>
</ul>
</li>
</ul>
<ol start="5">
<li class="mce-root">The answer is <strong>B</strong>. Containers are useful for modern applications as well as to containerize traditional applications. The benefits for an enterprise when doing the latter are huge. Cost savings in the maintenance of legacy apps of 50% or more have been reported. The time between new releases of such legacy applications could be reduced by up to 90%. These numbers have been publicly reported by real enterprise customers.</li>
<li>
<p class="p1">50% or more.</p>
</li>
<li>
<p class="p1">Containers are based on Linux namespaces (network, process, user, and so on) and cgroups (control groups).</p>
</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 2</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li><kbd>docker-machine</kbd> can be used for the following scenarios:
<ol>
<li>To create a VM on various providers such as VirtualBox, Hyper-V, AWS, MS Azure, or Google Compute Engine that will serve as a Docker Host.</li>
<li>To start, stop, or kill a previously generated VM.</li>
<li>To SSH into a local or remote Docker Host VM created with this tool.</li>
<li>To re-generate certificates for the secure use of a Docker Host VM.</li>
</ol>
</li>
<li>A. True. Yes, with Docker for Windows, you can develop and run Linux containers. It is also possible, but not discussed in this book, to develop and run native Windows containers with this edition of Docker for Desktop. With the macOS edition, you can only develop and run Linux containers.</li>
</ol>
<p> </p>
<ol start="3">
<li>Scripts are used to automate processes and hence avoid human errors. Building, testing, sharing, and running Docker containers are tasks that should always be automated to increase their reliability and repeatability.</li>
<li>The following Linux distros are certified to run Docker: RedHat Linux (RHEL), CentOS, Oracle Linux, Ubuntu, and more.</li>
<li> The following Windows OS are certified to run Docker: Windows 10 Pro, Windows Server 2016, and Windows Server 2019</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 3</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>The possible states of a Docker container are as follows:
<ul>
<li><kbd>created</kbd>: A container that has been created but not started</li>
<li><kbd>restarting</kbd>: A container that is in the process of being restarted</li>
<li><kbd>running</kbd>: A currently running container</li>
<li><kbd>paused</kbd>: A container whose processes have been paused</li>
<li><kbd>exited</kbd>: A container that ran and completed</li>
<li><kbd>dead</kbd>: A container that the Docker engine tried and failed to stop</li>
</ul>
</li>
<li>We can use <kbd>docker container ls</kbd> (or the old, shorter version, <kbd>docker ps</kbd>) to list all containers that are currently running on our Docker host. Note that this will NOT list the stopped containers, for which you need the extra parameter<kbd>--all</kbd> (or <kbd>-a</kbd>).</li>
<li>To list all IDs of containers, running or stopped, we can use <kbd>docker container ls -a -q</kbd>, where <kbd>-q</kbd> stands for output ID only.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 4</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>The <kbd>Dockerfile</kbd> could look like this:</li>
</ol>
<pre style="padding-left: 60px">FROM ubuntu:19.04<br/>RUN apt-get update &amp;&amp; \<br/>    apt-get install -y iputils-ping<br/>CMD ping 127.0.0.1</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">Note that in Ubuntu, the <kbd>ping</kbd> tool is part of the <kbd>iputils-ping</kbd> package. Build the image called <kbd>pinger</kbd>—for example— with <kbd>docker image build -t my-pinger</kbd>. </p>
<p style="padding-left: 60px">2. The <kbd>Dockerfile</kbd> could look like this:</p>
<pre style="padding-left: 60px">FROM alpine:latest<br/>RUN apk update &amp;&amp; \<br/>    apk add curl</pre>
<p style="padding-left: 60px">Build the image with <kbd>docker image build -t my-alpine:1.0</kbd>. </p>
<p style="padding-left: 60px">3. The <kbd>Dockerfile</kbd> for a Go application could look like this:</p>
<pre style="padding-left: 60px">FROM golang:alpine<br/>WORKDIR /app<br/>ADD . /app<br/>RUN cd /app &amp;&amp; go build -o goapp<br/>ENTRYPOINT ./goapp</pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">You can find the full solution in the <kbd>~/fod/ch04/answer03</kbd> folder. </p>
<p style="padding-left: 60px">4. A Docker image has the following characteristics:</p>
<p style="padding-left: 90px">1. It is immutable.<br/>
2. It consists of one-to-many layers.<br/>
3. It contains the files and folders needed for the packaged application to run.</p>
<p style="padding-left: 60px">5. <strong>C.</strong> First, you need to log in to Docker Hub; then, tag your image correctly with the username; and finally, push the image.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 5</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<p>The easiest way to play with volumes is to use the Docker Toolbox because when directly using Docker for Desktop, the volumes are stored inside a (somewhat hidden) Linux VM that Docker for Desktop uses transparently.<br/>
Thus, we suggest the following:</p>
<pre><strong>$ docker-machine create --driver virtualbox volume-test</strong><br/><strong>$</strong> <strong>docker-machine ssh volume-test</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>And now that you're inside a Linux VM called <kbd>volume-test</kbd>, you can do the following exercise:</p>
<ol>
<li>To create a named volume, run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker volume create my-products</strong></pre>
<ol start="2">
<li>Execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker container run -it --rm \</strong><br/><strong>    -v my-products:/data:ro \</strong><br/><strong>    alpine /bin/sh</strong></pre>
<ol start="3">
<li>To get the path on the host for the volume, use this command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker volume inspect my-products | grep Mountpoint</strong></pre>
<p style="padding-left: 60px">This (if you're using <kbd>docker-machine</kbd> and VirtualBox) should result in this:</p>
<pre style="padding-left: 60px"><strong>"Mountpoint": "/mnt/sda1/var/lib/docker/volumes/myproducts/_data"</strong></pre>
<p style="padding-left: 60px">Now execute the following command:</p>
<pre style="padding-left: 60px"><strong>$ sudo su</strong><br/><strong>$ cd /mnt/sda1/var/lib/docker/volumes/my-products/_data</strong><br/><strong>$ echo "Hello world" &gt; sample.txt</strong><br/><strong>$ exit</strong></pre>
<ol start="4">
<li>Execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker run -it --rm -v my-products:/data:ro alpine /bin/sh</strong><br/><strong>/ # cd /data</strong><br/><strong>/data # cat sample.txt</strong></pre>
<p style="padding-left: 60px">In another terminal, execute this command:</p>
<pre style="padding-left: 60px"><strong>$ docker run -it --rm -v my-products:/app-data alpine /bin/sh</strong><br/><strong>/ # cd /app-data</strong><br/><strong>/app-data # echo "Hello other container" &gt; hello.txt</strong><br/><strong>/app-data # exit</strong></pre>
<ol start="5">
<li>Execute a command such as this:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container run -it --rm \</strong><br/><strong>    -v $HOME/my-project:/app/data \</strong><br/><strong>    alpine /bin/sh</strong></pre>
<ol start="6">
<li>Exit both containers and then, back on the host, execute this command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker volume prune</strong></pre>
<ol start="7">
<li>The answer is B. Each container is a sandbox and thus has its very own environment.</li>
<li>Collect all environment variables and their respective values in a configuration file, which you then provide to the container with the <kbd>--env-file</kbd> command-line parameter in the <kbd>docker run</kbd> command, like so:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker container run --rm -it \</strong><br/><strong>    --env-file ./development.config \</strong><br/><strong>    alpine sh -c "export"</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 6</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>Possible answers: a) Volume mount your source code in the container; b) use a tool that automatically restarts the app running inside the container when code changes are detected; c) configure your container for remote debugging.</li>
<li>You can mount the folder containing the source code on your host in the container.</li>
<li>If you cannot cover certain scenarios easily with unit or integration tests and if the observed behavior of the application cannot be reproduced when the application runs on the host. Another scenario is a situation where you cannot run the application on the host directly due to the lack of the necessary language or framework.</li>
<li>Once the application is running in production, we cannot easily gain access to it as developers. If the application shows unexpected behavior or even crashes, logs are often the only source of information we have to help us reproduce the situation and pinpoint the root cause of the bug.</li>
</ol>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 7 </h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>Pros and cons:
<ul>
<li>Pro: We don't need to have the particular shell, tool, or language required by the task installed on our host.</li>
<li>Pro: We can run on any Docker host, from Raspberry Pi to a mainframe computer; the only requirement is that the host can run containers.</li>
<li>Pro: After a successful run, the tool is removed without leaving any traces from the host when the container is removed.</li>
<li>Con: We need to have Docker installed on the host.</li>
<li>Con: The user needs to have a basic understanding of Docker containers.</li>
<li>Con: Use of the tool is a bit more indirect than when using it natively.</li>
</ul>
</li>
<li>Running tests in a container has the following advantages:
<ul>
<li>They run equally well on a developer machine than on a test or CI system.</li>
<li>It is easier to start each test run with the same initial conditions.</li>
<li>All developers working with the code use the same setup, for example, versions of libraries and frameworks.</li>
</ul>
</li>
</ol>
<ol start="3">
<li>Here, we expect a diagram that shows a developer writing code and checking it in, for example, GitHub. We then want to see an automation server such as Jenkins or TeamCity in the picture that is either periodically polling GitHub for changes or the GitHub triggers the automation server (with an HTTP callback) to create a new build. The diagram should also show that the automation server then runs all tests against the built artifacts and, if they all succeed, deploys the application or service to an integration system where it is again tested, for example, with a few smoke tests. Once again, if those tests succeed, the automation server should either ask a human for approval to deploy to production (this equals to continuous delivery) or the automation server should automatically deploy to production (continuous deployment).</li>
</ol>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 8</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>You could be working on a workstation with limited resources or capabilities, or your workstation could be locked down by your company so that you are not allowed to install any software that is not officially approved. Sometimes, you might need to do proof of concepts or experiments using languages or frameworks that are not yet approved by your company (but might be in the future if the proof of concept is successful).</li>
<li>Bind-mounting a Docker socket into a container is the recommended method when a containerized application needs to automate some container-related tasks. This can be an application such as an automation server such as Jenkins that you are using to build, test, and deploy Docker images.</li>
</ol>
<ol start="3">
<li>Most business applications do not need root-level authorizations to do their job. From a security perspective, it is hence strongly recommended to run such applications with the least necessary access rights to their job. Any unnecessary elevated privileges could possibly be exploited by hackers in a malicious attack. By running the application as a non-root user, you make it more difficult for potential hackers to compromise your system.</li>
<li>Volumes contain data and the lifespan of data most often needs to go far beyond the life cycle of a container or an application, for that matter. Data is often mission-critical and needs to be stored safely for days, months, even years. When you delete a volume, you irreversibly delete the data associated with it. Hence, make sure you know what you're doing when deleting a volume.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 9</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>In a distributed application architecture, every piece of the software and infrastructure needs to be redundant in a production environment, where the continuous uptime of the application is mission-critical. A highly distributed application consists of many parts and the likelihood of one of the pieces failing or misbehaving increases with the number of parts. It is guaranteed that, given enough time, every part will eventually fail. To avoid outages of the application, we need redundancy in every part, be it a server, a network switch, or a service running on a cluster node in a container.</li>
</ol>
<p> </p>
<ol start="2">
<li>In highly distributed, scalable, and fault-tolerant systems, individual services of the application can move around due to scaling needs or due to component failures. Thus, we cannot hardwire different services with each other. Service A, which needs access to Service B, should not have to know details such as the IP address of Service B. It should rely on an external provider of this information. DNS is such a provider of location information. Service A just tells it that it wants to talk to Service B and the DNS service will figure out the details.</li>
<li>A circuit breaker is a means to avoid cascading failures if a component in a distributed application is failing or misbehaving. Similar to a circuit breaker in electric wiring, a software-driven circuit breaker cuts the communication between a client and a failed service. The circuit breaker will directly report an error back to the client component if the failed service is called. This gives the system the opportunity to recover or heal from failure.</li>
<li>A monolithic application is easier to manage that a multi-service application since it consists of a single deployment package. On the other hand, a monolith is harder to scale to account for increased demand. In a distributed application, each service can be scaled individually and each service can run on optimized infrastructure, while a monolith needs to run on infrastructure that is OK for all or most of the features implemented in it. Maintaining and updating a monolith is much harder than a multi-service application, where each service can be updated and deployed independently. The monolith is often a big, complex, and tightly coupled pile of code. Minor modifications can have unexpected side effects. (Micro-) Services, on the other hand, are self-contained, simple components that behave like black boxes. Dependent services know nothing about the inner workings of the service and thus do not depend on it.</li>
<li>A blue-green deployment is a form of software deployment that allows for zero downtime deployments of new versions of an application or an application service. If, say, Service A needs to be updated with a new version, then we call the currently running version blue. The new version of the service is deployed into production, but not yet wired up with the rest of the application. This new version is called green. Once the deployment succeeds and smoke tests have shown it's ready to go, the router that funnels traffic to blue is reconfigured to switch to green. The behavior of green is observed for a while and if everything is OK, blue is decommissioned. On the other hand, if green causes difficulties, the router can simply be switched back to blue and green can be fixed and later redeployed. </li>
</ol>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 10</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>The three core elements are <strong>sandbox</strong>, <strong>endpoint</strong>, and <strong>network</strong>.</li>
<li>Execute this command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker network create --driver bridge frontend</strong></pre>
<ol start="3">
<li> Run this command:</li>
</ol>
<pre style="padding-left: 60px"><br/><strong>$ docker container run -d --name n1 \</strong><br/><strong>    --network frontend -p 8080:80 nginx:alpine</strong><br/><strong>$ docker container run -d --name n2 \</strong><br/><strong>    --network frontend -p 8081:80 nginx:alpine</strong></pre>
<p style="padding-left: 60px">Test that both NGINX instances are up and running:</p>
<pre style="padding-left: 60px"><strong>$ curl -4 localhost:8080</strong><br/><strong>$</strong> <strong>curl -4 localhost:8081</strong></pre>
<p style="padding-left: 60px">You should be seeing the welcome page of NGINX in both cases.</p>
<ol start="4">
<li>To get the IPs of all attached containers, run this command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker network inspect frontend | grep IPv4Address</strong></pre>
<p style="padding-left: 60px">You should see something similar to the following:</p>
<pre style="padding-left: 60px"><strong>"IPv4Address": "172.18.0.2/16",</strong><br/><strong>"IPv4Address": "172.18.0.3/16",</strong></pre>
<p style="padding-left: 60px">To get the subnet used by the network, use the following (for example):</p>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker network inspect frontend | grep subnet</strong></pre>
<p style="padding-left: 60px">You should receive something along the lines of the following (obtained from the previous example):</p>
<pre style="padding-left: 60px"><strong>"Subnet": "172.18.0.0/16",</strong></pre>
<ol start="5">
<li>The <kbd>host</kbd> network allows us to run a container in the networking namespace of the host.</li>
<li>Only use this network for debugging purposes or when building a system-level tool. Never use the <kbd>host</kbd> network for an application container running a production environment!</li>
</ol>
<p> </p>
<ol start="7">
<li>The <kbd>none</kbd> network is basically saying that the container is not attached to any network. It should be used for containers that do not need to communicate with other containers and do not need to be accessed from outside.</li>
<li>The <kbd>none</kbd> network could, for example, be used for a batch process running in a container that only needs access to local resources such as files that could be accessed via a host mounted volume.</li>
<li>Traefik can be used to provide Layer 7 or application-level routing. This is especially useful if you want to break out functionality from a monolith with a well-defined API. In this case, you have a need to reroute certain HTTP calls to the new container/service. This is just one of the possible usage scenarios, but it's also the most important one. Another one could be to use Traefik as a load balancer.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 11</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>The following code can be used to run the application in detached or daemon mode:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker-compose up -d</strong></pre>
<ol start="2">
<li>Execute the following command to display the details of the running service:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker-compose ps</strong></pre>
<p style="padding-left: 60px">This should result in the following output:</p>
<pre style="padding-left: 60px"><strong>Name               Command               State  Ports</strong><br/><strong>-------------------------------------------------------------------</strong><br/><strong>mycontent_nginx_1  nginx -g daemon off;  Up     0.0.0.0:3000-&gt;80/tcp</strong></pre>
<ol start="3">
<li>The following command can be used to scale up the web service:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker-compose up --scale web=3</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 12</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>A mission-critical, highly available application that is implemented as a highly distributed system of interconnected application services that are just too complex to manually monitor, operate, and manage. Container orchestrators help in this regard. They automate most of the typical tasks, such as reconciling a desired state, or collecting and aggregating key metrics of the system. Humans cannot react quick enough to make such an application elastic or self-healing. Software support is needed for this in the form of the mentioned container orchestrators.</li>
<li>A container orchestrator frees us from mundane and cumbersome tasks such as the following:
<ul>
<li>Scaling services up and down</li>
<li>Load balancing requests</li>
<li>Routing requests to the desired target</li>
<li>Monitoring the health of service instances</li>
<li>Securing a distributed application</li>
</ul>
</li>
<li>The winner in this space is Kubernetes, which is open sourced and owned by the CNCF. It was originally developed by Google. We also have Docker Swarm, which is proprietary and has been developed by Docker. AWS offers a container service called ECS, which is also proprietary and tightly integrated into the AWS ecosystem. Finally, Microsoft offers AKS, which has the same pros and cons as AWS ECS.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 13</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>The correct answer is as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker swarm init [--advertise-addr &lt;IP address&gt;]</strong></pre>
<p style="padding-left: 60px">The <kbd>--advertise-addr</kbd> is optional and is only needed if you the host have more than one IP address.</p>
<ol start="2">
<li>On the worker node that you want to remove, execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong> $ docker swarm leave</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">On one of the master nodes, execute the command <kbd>$ docker node rm -f&lt;node ID&gt;</kbd>, where &lt;<kbd>node ID&gt;</kbd> is the ID of the worker node to remove.</p>
<ol start="3">
<li>The correct answer is as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker network create \</strong><br/><strong>    --driver overlay \</strong><br/><strong>    --attachable \</strong><br/><strong>    front-tier</strong></pre>
<ol start="4">
<li>The correct answer is as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker service create --name web \</strong><br/><strong>    --network front-tier \</strong><br/><strong>    --replicas 5 \</strong><br/><strong>    -p 3000:80 \</strong><br/><strong>    nginx:alpine</strong></pre>
<ol start="5">
<li>The correct answer is as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$</strong> <strong>docker service update --replicas 3 web</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 14</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>Zero-downtime deployment means that a new version of a service in a distributed application is updated to a new version without the application needing to stop working. Usually, with Docker SwarmKit or Kubernetes (as we will see), this is done in a rolling fashion. A service consists of multiple instances and those are updated in batches so that the majority of the instances are up and running at all times.</li>
<li>By default, Docker SwarmKit uses a rolling updated strategy to achieve zero-downtime deployments.</li>
<li>Containers are self-contained units of deployment. If a new version of a service is deployed and does not work as expected, we (or the system) need to only roll back to the previous version. The previous version of the service is also deployed in the form of self-contained containers. Conceptually, there is no difference in rolling forward (update) or backward (rollback). One version of a container is replaced by another one. The host itself is not affected by such changes in any way.</li>
</ol>
<p> </p>
<ol start="4">
<li>Docker secrets are encrypted at rest. They are only transferred to the services and containers that use the secrets. Secrets are transferred encrypted due to the fact that the communication between swarm nodes uses mutual TLS. Secrets are never physically stored on a worker node.</li>
<li>The command to achieve this is as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ docker service update --image acme/inventory:2.1 \</strong><br/><strong>    --update-parallelism 2 \</strong><br/><strong>    --update-delay 60s \</strong><br/><strong>    inventory</strong></pre>
<p style="padding-left: 60px">6. First, we need to remove the old secret from the service, and then we need to add the new version to it (directly updating a secret is not possible):</p>
<pre style="padding-left: 60px"><strong>$ docker service update \</strong><br/><strong>     --secret-rm MYSQL_PASSWORD \</strong><br/><strong>    inventory</strong><br/><strong>$ docker service update \</strong><br/><strong>    --secret-add source=MYSQL_PASSWORD_V2, target=MYSQL_PASSWORD \</strong><br/><strong>    inventory</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 15</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>The Kubernetes master is responsible for managing the cluster. All requests to create objects, reschedule pods, manage ReplicaSets, and more happen on the master. The master does not run the application workload in a production or production-like cluster.</li>
<li>On each worker node, we have the kubelet, the proxy, and container runtime.</li>
<li>The answer is A. <strong>Yes</strong>. You cannot run standalone containers on a Kubernetes cluster. Pods are the atomic units of deployment in such a cluster.</li>
<li>All containers running inside a pod share the same Linux kernel network namespace. Thus, all processes running inside those containers can communicate with each other through <kbd>localhost</kbd> in a similar way to how processes or applications directly running on the host can communicate with each other through <kbd>localhost</kbd>.</li>
</ol>
<p> </p>
<ol start="5">
<li>The <kbd>pause</kbd> container's sole role is to reserve the namespaces of the pod for containers that run in it.</li>
<li>This is a bad idea since all containers of a pod are co-located, which means they run on the same cluster node. Also, if multiple containers run in the same pod, they can only be scaled up or down all at once. However, the different components of the application (that is, <kbd>web</kbd>, <kbd>inventory</kbd>, and <kbd>db</kbd>) usually have very different requirements with regard to scalability or resource consumption. The <kbd>web</kbd> component might need to be scaled up and down depending on the traffic and the <kbd>db</kbd> component, in turn, has special requirements regarding storage that the others don't have. If we do run every component in its own pod, we are much more flexible in this regard.</li>
<li>We need a mechanism in order to run multiple instances of a pod in a cluster and make sure that the actual number of pods running always corresponds to the desired number, even when individual pods crash or disappear due to network partition or cluster node failures. The ReplicaSet is the mechanism that provides scalability and self-healing to any application service.</li>
<li>We need deployment objects whenever we want to update an application service in a Kubernetes cluster without causing downtime to the service. Deployment objects add rolling updates and rollback capabilities to ReplicaSets.</li>
<li>Kubernetes service objects are used to make application services participate in service discovery. They provide a stable endpoint to a set of pods (normally governed by a ReplicaSet or a deployment). Kube services are abstractions that define a logical set of pods and a policy regarding how to access them. There are four types of Kube service:
<ul>
<li><strong>ClusterIP</strong>: Exposes the service on an IP address that's only accessible from inside the cluster; this is a virtual IP (VIP).</li>
<li><strong>NodePort</strong>: Publishes a port in the range 30,000–32,767 on every cluster node.</li>
<li><strong>LoadBalancer</strong>: This type exposes the application service externally using a cloud provider's load balancer, such as ELB on AWS.</li>
<li><strong>ExternalName</strong>: Used when you need to define a proxy for a cluster's external service such as a database.</li>
</ul>
</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 16</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>Assuming we have a Docker image in a registry for the two application services, the web API and Mongo DB, we then need to do the following:
<ul>
<li>Define a deployment for Mongo DB using a StatefulSet; let's call this deployment <kbd>db-deployment</kbd>. The StatefulSet should have one replica (replicating Mongo DB is a bit more involved and is outside the scope of this book).</li>
<li>Define a Kubernetes service called <kbd>db</kbd> of the <kbd>ClusterIP</kbd> type for <kbd>db-deployment</kbd>.</li>
<li>Define a deployment for the web API; let's call it <kbd>web-deployment</kbd>. Let's scale this service to three instances.</li>
<li>Define a Kubernetes service called <kbd>api</kbd> of the <kbd>NodePort</kbd> type for <kbd>web-deployment</kbd>.</li>
<li>If we use secrets, then define those secrets directly in the cluster using kubectl.</li>
<li>Deploy the application using kubectl.</li>
</ul>
</li>
<li>To implement layer 7 routing for an application, we ideally use an IngressController. The IngressController is a reverse proxy such as Nginx that has a sidecar listening on the Kubernetes Server API for relevant changes and updating the reverse proxy's configuration and restarting it if such a change has been detected. Then, we need to define Ingress resources in the cluster that define the routing, for example, from a context-based route such as <kbd>https://example.com/pets to &lt;a service name&gt;/&lt;port&gt;</kbd> or a pair such as <kbd>api/32001</kbd>. The moment Kubernetes creates or changes this Ingress object, the IngressController's sidecar picks it up and updates the proxy's routing configuration.</li>
<li>Assuming this is a cluster internal inventory service, then we do the following:
<ul>
<li>When deploying version 1.0, we define a deployment called <kbd>inventory-deployment-blue</kbd> and label the pods with a label of <kbd>color: blue</kbd>.</li>
<li>We deploy the Kubernetes service of the <kbd>ClusterIP</kbd> type called inventory for the preceding deployment with the selector containing <kbd>color: blue</kbd>.</li>
<li>When we're ready to deploy the new version of the payments service, we define a deployment for version 2.0 of the service and call it <kbd>inventory-deployment-green</kbd>. We add a label of <kbd>color: green</kbd> to the pods.</li>
<li>We can now smoke test the "green" service and when everything is OK, we can update the inventory service so that the selector contains <kbd>color: green</kbd>.</li>
</ul>
</li>
<li>Some forms of information that are confidential and thus should be provided to services through Kubernetes secrets include passwords, certificates, API key IDs, API key secrets, and tokens.</li>
<li>
<p class="mce-root">Sources for secret values can be files or base64-encoded values.</p>
</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 17</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>
<p>We cannot do any live debugging on a production system for performance and security reasons. This includes interactive or remote debugging. Yet application services can show unexpected behavior to code defects or other infrastructure-related issues such as network glitches or external services that are not available. To quickly pinpoint the reason for the misbehavior or failure of a service, we need as much logging information as possible. This information should give us a clue about, and guide us to, the root cause of the error. When we instrument a service, we do exactly this — we produce as much information as reasonable in the form of log entries and published metrics.</p>
</li>
<li>Prometheus is a service that is used to collect functional or non-functional metrics that are provided by other infrastructure services and most importantly by application services. Since Prometheus itself is pulling those metrics periodically from all configured services, the services themselves do not have to worry about sending data. Prometheus also defines the format in which the metrics are to be presented by the producers.</li>
</ol>
<p> </p>
<ol start="3">
<li>To instrument a Node.js-based application service we need to do the following four steps:
<ol>
<li>Add a Prometheus adapter to the project. The maintainers of Prometheus recommend the library called <kbd>siimon/prom-client</kbd>.</li>
<li>Configure the Prometheus client during startup of the application. This includes the definition of a metrics registry.</li>
<li>Expose an HTTP GET endpoint/metrics where we return the collection of metrics defined in the metrics registry.</li>
<li>Finally, we define custom metrics of the <kbd>counter</kbd>, <kbd>gauge</kbd>, or <kbd>histogram</kbd> type, and use them in our code; for example, we increase a metric of the <kbd>counter</kbd> type each time a certain endpoint is called. </li>
</ol>
</li>
<li>Normally in production, a Kubernetes cluster node only contains a minimal OS to keep its attack surface as limited as possible and to not waste precious resources. Thus we cannot assume that the tools typically used to troubleshoot applications or processes are available on the respective host. A powerful and recommended way to troubleshoot is to run a special tools or troubleshoot container as part of an ad hoc pod. This container can then be used as a bastion from which we can investigate network and other issues with the troubled service. A container that has been successfully used by many Docker field engineers at their customers site is <kbd>netshoot</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Chapter 18</h1>
                
            
            
                
<p>Here are some sample answers to the questions presented in this chapter:</p>
<ol>
<li>To install UCP in AWS, we do the following:
<ul>
<li>Create a VPC with subnets and an SG.</li>
<li>Then, provision a cluster of Linux VMs, possibly as part of an ASG. Many Linux distributions are supported, such as CentOS, RHEL, and Ubuntu.</li>
<li>Next, install Docker on each VM.</li>
<li>Finally, select one VM on which to install UCP using the <kbd>docker/ucp</kbd> image.</li>
<li>Once UCP is installed, join the other VMs to the cluster either as worker nodes or manager nodes.</li>
</ul>
</li>
</ol>
<ol start="2">
<li>Here are a few reasons to consider a hosted Kubernetes offering:
<ul>
<li>You do not want to, or do not have the resources to, install and manage a Kubernetes cluster.</li>
<li>You want to concentrate on what brings value to your business, which in most cases is the applications that are supposed to run on Kubernetes and not Kubernetes itself.</li>
<li>You prefer a cost model where you pay only for what you need.</li>
<li>The nodes of your Kubernetes cluster are automatically patched and updated.</li>
<li>Upgrading the version of Kubernetes with zero downtime is easy and straightforward.</li>
</ul>
</li>
<li>The two main reasons to host container images on the cloud provider's container registry (such as ACR on Microsoft Azure) are these:
<ul>
<li>The images are geographically close to your Kubernetes cluster and thus the latency and transfer network costs are minimal.</li>
<li>Production or production-like clusters are ideally sealed from the internet, and thus the Kubernetes cluster nodes cannot access Docker Hub directly.</li>
</ul>
</li>
</ol>


            

            
        
    </body></html>