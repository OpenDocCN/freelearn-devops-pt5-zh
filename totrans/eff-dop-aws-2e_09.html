<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Assessment</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 1: The Cloud and DevOps Revolution</h1>
                </header>
            
            <article>
                
<ol>
<li><span>DevOps is a framework and a methodology concerned with adopting the right culture for developers and the operations team to work together.</span></li>
<li><span>DevOps – IaC stands for </span><strong>DevOps – Infrastructure as Code</strong><span>, where we should treat and manage our </span><span>vertical</span><span> </span><span>infrastructure in the form of code, helping us with repeatable, scalable, and manageable infrastructure.</span></li>
<li><span>The key characteristics of a DevOps culture</span><br/>
<ul>
<li>Source controlling everything</li>
<li>Automated testing</li>
<li>Automated provisioning</li>
<li>Configuration management</li>
<li>Automated deployment</li>
<li>Measuring</li>
<li>Adaptation to virtualization (public/private cloud)</li>
</ul>
</li>
<li><span>The three major service models in the cloud:</span>
<ul>
<li><strong>Infrastructure as a Service</strong><span> </span>(<span><strong>IaaS</strong>) </span></li>
<li><strong>Platform as a Service</strong><span> </span>(<span><strong>PaaS</strong>) </span></li>
<li><strong>Software as a Service</strong><span> </span>(<span><strong>SaaS</strong>) </span></li>
</ul>
</li>
<li><span>AWS is the largest public cloud service platform available today. AWS offers multiple services, from computing and storage to machine learning and analytics, all of which are highly scalable and reliable. The most important part of using AWS is the </span><em>pay-per-use model</em><span>. You need not invest in any hardware. Instead, deploy the services, and pay for them</span><span> </span><span>until you are using the services. The day you shut down and remove the services, no charges will be applicable - which is great.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 2: Deploying Your First Web Application</h1>
                </header>
            
            <article>
                
<ol>
<li><span>If you don't have an AWS cloud account,  go to </span><a href="https://aws.amazon.com/">www.aws.amazon.com</a><span> and create a free-tier account. Follow the step-by-step instructions at </span><a href="https://aws.amazon.com/">https://aws.amazon.com/</a><span>. You need to provide your credit or debit card details in order to create an AWS account.</span></li>
<li><span>Go to </span><a href="https://us-east-1.signin.aws.amazon.com/oauth?SignatureVersion=4&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAJMOATPLHVSJ563XQ&amp;X-Amz-Date=2018-08-27T09%3A42%3A05.017Z&amp;X-Amz-Signature=9a2851741438a5ac794ebce02b2f9dac6adf96b92ec4e336cc5a35322ede9064&amp;X-Amz-SignedHeaders=host&amp;client_id=arn%3Aaws%3Aiam%3A%3A015428540659%3Auser%2Fhomepage&amp;redirect_uri=https%3A%2F%2Fconsole.aws.amazon.com%2Fconsole%2Fhome%3Fstate%3DhashArgs%2523%26isauthcode%3Dtrue&amp;response_type=code&amp;state=hashArgs%23">console.aws.amazon.com</a><span> and choose </span><span class="packt_screen">AWS compute services</span><span> to create your first EC2 instance. Click on the </span><span class="packt_screen">Launch Instance</span><span> button on the console and follow the steps to select an AMI, instance type (select free-tier in this case), followed by instance details, storage details, tags, and security group. For this exercise, you can select default options as our AIM is just to get familiar with the console portal so that we can automate this process using DevOps practices.</span></li>
<li><span>Follow the step-by-step instructions provided under the </span><em>Creating our first web server</em><span> </span><span>section in the chapter </span><span>to create your first AWS instance using AWS CLI.</span></li>
<li class="mce-root"><span>Follow the steps mentioned in the</span><span> </span><em>Creating a simple Hello World web application</em><span> </span><span>section in the chapter. You can download the sample code of the application from the following links:</span><br/>
<ul>
<li><a href="https://raw.githubusercontent.com/yogeshraheja/Effective-DevOps-with-AWS/master/Chapter02/helloworld.js" target="_blank">https://raw.githubusercontent.com/yogeshraheja/Effective-DevOps-with-AWS/master/Chapter02/helloworld.js</a>.<a href="https://raw.githubusercontent.com/yogeshraheja/Effective-DevOps-with-AWS/master/Chapter02/helloworld.js" target="_blank"/></li>
<li><a href="https://raw.githubusercontent.com/yogeshraheja/Effective-DevOps-with-AWS/master/Chapter02/helloworld.conf" target="_blank">https://raw.githubusercontent.com/yogeshraheja/Effective-DevOps-with-AWS/master/Chapter02/helloworld.conf</a>.</li>
</ul>
</li>
<li><span>Find the instance ID of your AWS instance using </span><kbd>ec2-metadata --instance-id</kbd><span> and then execute the mentioned command by amending your instance ID: </span><kbd>aws ec2 terminate-instances --instance-ids &lt;YOUR AWS INSTANCE ID&gt;</kbd><span>.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 3: Treating Your Infrastructure as Code</h1>
                </header>
            
            <article>
                
<ol>
<li><span>IaC stands for Infrastructure as Code. This is a process of treating your infrastructure objects, such as  EC2 instances, VPC network, subnets, load balancers, storage, application deployment and orchestration, and in the form of infrastructure codes. IaC allows the infrastructure vertical to change, replicate, and roll back changes in the entire environment in a very short space of time.</span></li>
</ol>
<ol start="2">
<li style="color: black">Open the CloudFormation template at <a href="https://console.aws.amazon.com/cloudformation">https://console.aws.amazon.com/cloudformation</a><span> </span>and click on<span> </span><span class="packt_screen">Create Stack</span><span> </span>button. Now create a<span> </span><kbd>helloworld-cf.template</kbd> template file, using the Python file located at<span> </span><a href="https://raw.githubusercontent.com/yogeshraheja/Effective-DevOps-with-AWS/master/Chapter03/EffectiveDevOpsTemplates/helloworld-cf-template-part-1.py">https://raw.githubusercontent.com/yogeshraheja/Effective-DevOps-with-AWS/master/Chapter03/EffectiveDevOpsTemplates/helloworld-cf-template-part-1.py</a>. After doing this, upload a template to Amazon S3. Provide a name to your stack, followed by an SSH key-pair, and other additional information that can be taken as default here. Now review the information and click on<span> </span><span class="packt_screen">Create</span>. When the creation of the template is complete, click on the<span> </span><span class="packt_screen">Outputs</span><span> </span>tab and click on<span> </span><span class="packt_screen">Weburl</span>, which will take you to the application home page.</li>
</ol>
<p style="padding-left: 90px">Hint: Generate the CloudFormation template by saving the output of the script in the<span> </span><kbd>python helloworld-cf-template.py &gt; helloworld-cf.template</kbd> <span>file.</span><span> </span></p>
<ol start="3">
<li><span>There are multiple SCM offerings available on the market, including GitLab, BitBucket, GitHub, and even SCM offerings by public clouds. Here, we will use one of the most popular SCM offerings: GitHub. Create your free account on Github at </span><a href="https://github.com">https://github.com</a><span>. Once you have done this, log into your GitHub account and create your first public repository with the name </span><kbd>helloworld</kbd><span>.</span></li>
<li><span>Install a Git package for your supported platform and clone the previously created GitHub repository here using </span><kbd>git clone &lt;github repository URL&gt;</kbd><span>, which you can find from the GitHub console for your repository. Now copy your </span><kbd>helloworld-cf.template</kbd><span> in the repository followed by the </span><kbd>git add</kbd><span> and </span><kbd>git commit</kbd><span> operations. Now you are in a position to push your local repository file to your GitHub account. To do this, execute </span><kbd>git push</kbd><span> to push your committed file and confirm this by checking your GitHub repository.</span></li>
<li>
<p style="color: black">Ansible is a simple, powerful, and easy-to-learn configuration management tool used by the system/cloud engineers and DevOps engineers to automate their regular repetitive tasks. The installation of Ansible is very simple and works as an agentless model.</p>
<p style="color: black">In Ansible, modules are the fundamental building blocks for creating Ansible code files written in YAML. These files, written in YAML, are called Ansible Playbooks. Multiple Ansible playbooks are arranged in well defined directory structures, called <kbd>roles</kbd> in Ansible, where roles are the structure directories for Ansible codes that contain Ansible playbooks, variables, static/dynamic files, and so on. There are also a number of other objects in Ansible, including Ansible Vault, Ansible Galaxy, and a GUI for Ansible called <strong>Ansible Tower</strong>. You can further explore these objects at <a href="https://docs.ansible.com/">https://docs.ansible.com</a>.</p>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 4:  Infrastructure as Code with Terraform</h1>
                </header>
            
            <article>
                
<ol>
<li><span>Terraform is a high level infrastructure tool that is primarily used for building, changing, and versioning infrastructure safely and efficiently. Terraform is not a configuration management tool as it focuses on the infrastructure layer and allows tools such as Puppet, Chef, Ansible, and Salt to perform application deployment and orchestration.</span></li>
<li><span>HashiCorp does not provide native packages for operating systems. Terraform is distributed as a single binary, packaged inside a ZIP archive, which can be downloaded from </span><a href="https://www.terraform.io/downloads.html">https://www.terraform.io/downloads.html</a><span>. Once downloaded, extract the </span><kbd>.zip</kbd><span> file and place it under the </span><kbd>/usr/bin Linux</kbd><span> binary path. Once this is done, run </span><kbd>terraform -v</kbd><span> to confirm the installed Terraform version.</span></li>
<li><span>In order to provision AWS instances using Terraform, you need to initialize the AWS provider by creating a  </span><kbd>provider</kbd><span> block inside the </span><kbd>.tf</kbd><span> file. You then have to run </span><kbd>terraform init</kbd><span>. Upon successful initialization, you need to proceed by developing a Terraform template with </span><kbd>resources</kbd><span>. In this case, you need to use the </span><kbd>aws_instance</kbd><span> resource type with the appropriate attribute. Once this is done, validated, and planned, apply your Terraform template to create your first AWS instance.</span></li>
<li><span>In order to configure Terraform with Ansible, you need to use a </span><strong>provider</strong><span>, to initialize the platform; </span><strong>resources</strong><span>, to create the platform-related services; and finally </span><strong>provisioner</strong><span>, to establish a connection with the created service to install Ansible and to run </span><kbd>ansible-pull</kbd><span> to run Ansible code on the system. You may refer to the following link for a sample Terraform template: </span><a href="https://raw.githubusercontent.com/yogeshraheja/EffectiveDevOpsTerraform/master/fourthproject/helloworldansiblepull.tf">https://raw.githubusercontent.com/yogeshraheja/EffectiveDevOpsTerraform/master/fourthproject/helloworldansiblepull.tf</a><span>.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 5: Adding Continuous Integration and Continuous Deployment</h1>
                </header>
            
            <article>
                
<ol>
<li><span class="fontstyle0">The terms CI, CD and continuous delivery can be defined as follows:</span>
<ul>
<li><span class="fontstyle0"><strong>Continuous Integration</strong>: A CI pipeline will allow us to test proposed code changes automatically and continuously. This will free up the time of developers and QAs who no longer have to carry out as much manual testing. It also makes the integration of code changes much easier.</span></li>
<li><strong>Continuous Deployment</strong>: <span>In CD, </span><span class="fontstyle0">you drastically accelerate the feedback loop process that DevOps provides. Releasing new code to production at high speed lets you collect real customer metrics, which often leads to exposing new and unexpected issues.</span></li>
<li><strong>Continuous Delivery</strong>: <span class="fontstyle0">In order to build our continuous delivery pipeline, we are first going to create a CloudFormation stack for a production environment. We will then add a new deployment group in CodeDeploy, which will provide us with the ability to deploy code to the new CloudFormation stack. Finally, we will upgrade the pipeline to include an approval process to deploy our code to production and the production deployment stage itself.</span></li>
</ul>
</li>
<li><span class="fontstyle0">Jenkins is one of the most widely used integration tools to run our CI pipeline. With over 10 years of development, Jenkins has been the leading open-source solution to practice continuous integration for a long time. Famous for its rich plugin ecosystem, Jenkins has gone through a major new release (Jenkins 2.x), which has put the spotlight on a number of very DevOps centric features, including the ability to create native delivery pipelines that can be checked in and version-controlled. It also provides better integration with source control systems such as GitHub</span></li>
<li><span class="fontstyle0">In order to implement our continuous deployment pipeline, we are going to look at two new AWS services</span><span class="fontstyle2">—</span><span class="fontstyle3">CodePipeline</span> <span class="fontstyle0">and</span> <span class="fontstyle3">CodeDeploy</span><span class="fontstyle0">:</span>
<ul>
<li><span class="fontstyle0"><span class="fontstyle0"><strong>CodePipeline</strong> lets create our deployment pipeline. We will tell it to take our code from GitHub, like we did before, and send it to Jenkins to run CI testing on it. Instead of simply returning the result to GitHub, however, we will then take the code and deploy it to our EC2 instance with the help of AWS CodeDeploy.</span></span></li>
<li><span class="fontstyle0"><strong>CodeDeploy</strong> is a service that lets us properly deploy code to our EC2 instances. By adding a certain number of configuration files and scripts, we can use CodeDeploy to deploy and test our code reliably. Thanks to CodeDeploy, we don't have to worry about any kind of complicated logic when it comes to sequencing our deployment. It is tightly integrated with EC2 and knows how to perform rolling updates across multiple instances and, if needed, perform a rollback.</span></li>
</ul>
</li>
</ol>
<p style="padding-left: 60px">For more details, please refer to <em><span class="fontstyle0">Building a continuous deployment pipeline</span></em> section of this chapter</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 6: Scaling Your Infrastructure</h1>
                </header>
            
            <article>
                
<ol>
<li>No, it is not always the best choice because a multi-level application means more components to manage. If your application works well as a monolith, you can<span> </span>accept<span> </span>a short period of downtime and the traffic will not increase over time. You can also consider letting it run as it is.</li>
<li>In the multi-level approach used in this book, all software is in one ZIP file,<span> </span>instead in a microservices and more in the serverless approach it is broken in multiple parts.<span> </span>For example, in an e-commerce software (the software used to show the content to the users in one service), the part<span> </span>to manage the backend to place a new product is in one service, while the<span> </span>part<span> </span>to manage the payment is in another service, and so on. </li>
<li>If you are not familiar with the service, it can be difficult. However, AWS is full of documentation and video. Furthermore, in this book we demonstrated how to use a set of basic services to break the classic monolith approach in<span> </span>multi-level.</li>
<li>This is true for an NLB but you need to pre-warm it if you use an ALB or a CLB. You must also do this if your traffic goes up to<span> more than 50 percent every five minutes. </span></li>
<li>Using the Certificate Manager is free unless you want to<span> </span><span class="packt_screen">Request a private certificate</span>, a classic SSL * certificate can also cost 500 dollars a year.  </li>
<li>Each AWS Region is organized in AZs and each zone is a separate datacenter. Consequently, it is rare that there are issues in one zone<span> </span>but it is not likely multiple issues in the same moment.<span> </span>Each subnet can belong to only one zone so it is convenient to place each component in at least <span>two, or preferably three,</span> zones.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 7: Running Containers in AWS</h1>
                </header>
            
            <article>
                
<ol>
<li class="CDPAlignLeft CDPAlign">Docker is a container platform to build, ship and run containerized applications. The four important components of Docker Engine are as follows:</li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Containers</strong>: A read write template</li>
<li><strong>Images</strong>: A read only template</li>
<li><strong>Network</strong>: A virtual network for containers</li>
<li><strong>Volumes</strong>: A persistent storage for containers</li>
</ul>
</li>
</ul>
<ol start="2">
<li class="CDPAlignLeft CDPAlign">Docker CE can be installed on many platforms including Linux, Windows, and MacOS. Refer to<span> </span><span><a href="https://docs.docker.com/install/">https://docs.docker.com/install/</a>, the official Docker link, click on your choice of platform, and follow the instructions to install and configure the latest version of Docker CE on your system.</span></li>
</ol>
<p style="padding-left: 60px">Confirm the installed Docker CE version by running<span> </span><kbd>docker --version</kbd><span> </span>command.</p>
<ol start="3">
<li><span>Use a Dockerfile </span><a href="https://github.com/yogeshraheja/helloworld/blob/master/Dockerfile">https://github.com/yogeshraheja/helloworld/blob/master/Dockerfile</a><span> and create an image using </span><kbd>docker build</kbd><span> command. This newly created image is an image for Hello World application. Create a container by exposing the port outside using </span><kbd>docker run -d -p 3000:3000 &lt;image-name&gt;</kbd><span>. Once done, check and confirm the webserver outputs either using </span><kbd>curl</kbd><span> or using your public IP with port </span><kbd>3000</kbd><span> from the web browse</span><span>r.</span></li>
<li><span>Login to your AWS account using your credentials and select </span><span class="packt_screen">ECS service</span><span> from the services tab. There you will find options to Create Amazon ECS Cluster and Amazon ECR repository. At this point, click on </span><span class="packt_screen">Repository</span><span> and create your first ECR repository. The screen will also display some of the commands that you can use to perform an operation on ECR. Similarly, click on the </span><span class="packt_screen">Cluster</span><span> tab followed by </span><span class="packt_screen">create cluster</span><span> on the ECS screen. From here, select your choice of cluster for Windows or Linux or Network only, click 'next step', and fill in the details of your choice. These details include cluster name, provisioning model, EC2 instance type, number of instance, and so on. To complete the process, click </span><span class="packt_screen">Create</span><span>. Once a few minutes have passed, your ECS cluster will be ready to use. In this chapter, we have demonstrated </span>this<span> using CloudFormation. If you are interested in setting up an ECS cluster using the same process, feel free to follow the steps provided in the chapter in </span><em>Creating an ECS cluster</em><span> section.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 8: Hardening the Security of Your AWS Environment</h1>
                </header>
            
            <article>
                
<ol>
<li><span>Before starting to build your infrastructure, it is strongly recommended that you </span><em>lock in</em><span> your root account (that is, the account bound to your registration email). Then, create IAM users and groups with the necessary privileges, and use MFA (instead of just usernames and passwords) for root and IAM users.</span></li>
<li><span>You should enable CloudTrail for registering IAM users and role actions, and VPC Flow Logs for monitoring and logging network traffic. </span></li>
<li><span>No; there is also WAF, an application firewall that works at level 7 of the TPC/IP protocol. </span></li>
<li><span>You have to follow some best practices to configure your application, expose the least possible surface of the app to the internet and scale up and down. There are also WAF rate rules that help to limit malicious DDoS attacks.</span></li>
<li><span>In theory, you can, but it is convenient to split them between private and public subnets, to expose only the necessary resources to the internet. Anything else should stay private. Also, it is a best practice to spread parts of your application over multiple availability zones. This means, in practice, using multiple data centers. For these reasons, and also because one subnet can be in a single AZ, you have to use multiple subnets.</span></li>
</ol>


            </article>

            
        </section>
    </body></html>