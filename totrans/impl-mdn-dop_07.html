<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Docker Swarm and Kubernetes - Clustering Infrastructure</h1>
                </header>
            
            <article>
                
<p>So far, we have seen how powerful Docker is but we have not unleashed the full potential of containers. You have learned how to run containers on a single host with the local resources without the possibility of clustering our hardware resources in a way that allows us to uniformly use them as one big host. This has a lot of benefits, but the most obvious one is that we provide a middleware between developers and ops engineers that acts as a common language so that we don't need to go to the ops team and ask them for a machine of a given size. we just provide the definition of our service and the Docker clustering technology will take care of it.</p>
<p>In this chapter, we are going to dive <span>deep into</span> deploying and managing applications on Kubernetes, but we will also take a look at how Docker Swarm works.</p>
<p>People usually tend to see <span>Kubernetes and Docker Swarm</span> as competitors, but in my experience, they solve different problems:</p>
<ul>
<li><strong>Kubernetes</strong> is focused on advanced microservices topologies that offer all the potential of years of experience running containers in Google</li>
<li><strong>Docker Swarm</strong> offers the most straightforward clustering capabilities for running applications in a very simple way</li>
</ul>
<p>In short, Kubernetes is more suited for advanced applications, whereas Docker Swarm is a version of Docker on steroids.</p>
<p>This comes at a cost: managing a Kubernetes cluster can be very hard, whereas managing a Docker Swarm cluster is fairly straightforward.</p>
<p>There are other clustering technologies that are used in the current DevOps ecosystem, such as DC/OS or Nomad, but unfortunately, we need to focus on the ones that are, in my opinion, the most suited for DevOps and focus specifically on Kubernetes that, in my opinion, is eating the DevOps market.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why clustering ?</h1>
                </header>
            
            <article>
                
<p>In <a href="0b33ce27-d21e-4021-87ad-6865c65b7e33.xhtml">Chapter 1</a>, <em>DevOps in the Real World,</em> you learned about organizational alignment and why is important to shift roles in a company to accommodate DevOps tools. It is not okay anymore to just be a developer or a sysadmin; now you need to be a full stack DevOps engineer in order to get success in any project. Full stack DevOps means that you need to understand the business and the technology used in the organisation. Think about it; if you became a civil engineer instead of an <span>IT engineer</span>, it is mandatory to know the local rules (the business) plus the commercial names of the tools used to build roads and bridges (the technology) but also be able to coordinate their building (ops). Maybe not every engineer needs to know everything but they need to be aware of in the full picture in order to ensure the success of the project.</p>
<p>Coming back to containers and DevOps, making concepts simple for everyone to understand is something that's mandatory nowadays. You want to ensure that all the engineers in your project are able to trace the software from conception (requirements) to deployment (ops) but also have in mind predictability so that the business people that barely speak tech are able to plan strategies around the products that you build.</p>
<p>One of the keys to achieving the flow described here is predictability, and the way to achieve predictability is making uniform and repeatable use of your resources. As you learned earlier, cloud data centers such as Amazon Web Services or Google Cloud Platform provide us with a virtually unlimited pool of resources that can be used to build our systems in a traditional way:</p>
<ul>
<li>Define the size of the VMs</li>
<li>Provision VMs</li>
<li>Install the software</li>
<li>Maintain it</li>
</ul>
<p>Or, if we want to draw a diagram so we can understand it better, it would be similar to the next one:</p>
<div class="CDPAlignCenter CDPAlign"><img height="290" width="486" class="image-border" src="assets/a68c76ed-810f-4527-bbbc-c2226fefe7fc.png"/></div>
<p>Here are a few considerations:</p>
<ul>
<li>Clear separation between Development and Operations (this may vary depending on the size of your company</li>
<li>Software components owned by Development and deployments and configuration owned by Operations</li>
<li>Some servers might be relatively underutilized (<strong>Server 1</strong>) and on a very low load</li>
</ul>
<p>This has been the picture for 40 odd years of software development, and it is still the picture if we are running Docker containers, but there are few problems in it:</p>
<ul>
<li>If a problem arises in <strong>Component 3</strong> in production, who is responsible for it?</li>
<li>If there is a configuration mismatch, who will fix it if developers are not supposed to see what is going on in production?</li>
<li><strong>Server 1</strong> is running a software component that might be called only once or twice a day (imagine an authentication server for workstations); do we need a full VM just for it?</li>
<li>How do we scale our services in a transparent manner?</li>
</ul>
<p>These questions can be answered, but usually, they get an answer too late in the game plus "the hidden requirements" are only seen once the problems arise at the worst possible time:</p>
<ul>
<li>Service discovery</li>
<li>Load balancing</li>
<li>Self-healing infrastructure</li>
<li>Circuit breaking</li>
</ul>
<p>During college years, one of the things in common across all the different subjects was reusability and extensibility. Your software should be extensible and reusable so that we can potentially build libraries of components creating the <span>engineering</span> sweet spot (not just software development): build once, use everywhere.</p>
<p>This has been completely overlooked in the operations part of the software development until recent years. If you get a job as a Java developer in a company, there is a set of accepted practices that every single Java developer in the world knows and makes use of so you can nearly hit the ground running without too many problems (in theory). Now let's raise a question: if all the Java apps follow the same practices and set of common patterns, why does every single company deploy them in a different way?</p>
<p>A continuous delivery pipeline has the same requirements in pretty much every company in the IT world, but I have seen at least three different ways of organizing it with a huge amount of custom magic happening that only one or two people within the company know of.</p>
<p>Clusters are here to save us. Let's reshuffle the image from before:</p>
<div class="CDPAlignCenter CDPAlign"><img height="533" width="696" class="image-border" src="assets/9e0e0ad8-b84a-468c-bc50-e7d0bd7f2627.png"/></div>
<p>In this case, we have solved few of our problems:</p>
<ul>
<li>Now development and ops are connected via a middleware: the cluster.</li>
<li>Components can be replicated (refer to component 1 and component 1') without provisioning extra hardware.</li>
<li>DevOps engineers are the glue between the two teams (development and ops), making things happen at a fast pace.</li>
<li>The stability of the full system does not depend on a single server (or component) as the cluster is built in a way that can accept some level of failure by just degrading performance or taking down the less critical services: it is okay to sacrifice e-mailing in order to keep the accounting processes of the company running.</li>
</ul>
<p>And about the hidden requirements. Well, this is where we need to make a decision about which clustering technology we want to use as they approach the service discovery, load balancing, and auto-scaling from different angles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Docker Swarm</h1>
                </header>
            
            <article>
                
<p>As we have seen in previous chapters, Docker is a fantastic tool that follows the most modern architectural principles used for running applications packed as containers. In this case, Docker Swarm runs <span>only</span> Docker containers, ignoring other technologies that, at the moment, are not suitable for production, such as Rkt. Even Docker is quite new to the scene up to a point that some companies hesitate in deploying it in their production systems, as there is not so much expertise in the market as well as many doubts about security or how Docker works in general.</p>
<p><strong>Docker Swarm</strong> is the clustered version of Docker, and it solves the problem described in the previous section in a very simple manner: pretty much all the docker commands that you learned in the Docker chapter works in Docker Swarm so that we can federate our hardware without actually taking care of the hardware itself. Just add nodes to the pool of resources and Swarm will take care of them, leveraging the way we build our systems to purely containers.</p>
<p>Docker Swarm is not something that we need to install aside from the Docker engine: it comes embedded into it and it is a mode rather than a server itself.</p>
<p>Docker Swarm is evolving quite quickly and it is dragging Docker itself along as more and more features are being baked into it due to its usage in the Swarm mode. The most interesting part of this is how we can leverage our Docker knowledge into it without any extra as the swarm mode of our Docker engine takes care of the resources.</p>
<p>This is also a problem: we are limited by the Docker API, whereas with Kubernetes (we will come back to it in a second), we are not only limited by the Docker API, but we can also extend the Kubernetes API to add new objects to fulfill our needs.</p>
<p>Docker Swarm can be operated through <kbd>docker-compose</kbd> (up to a certain extent), which provides a decent approach to infrastructure as code but is not very comprehensive when our application is somehow complex.</p>
<p>In the current IT market, Kubernetes seems to be the clear winner of the orchestration battle, and as such, we are going to focus on it, but if you want to learn more about Docker Swarm, the official documentation can be found at <a href="https://docs.docker.com/engine/swarm/">https://docs.docker.com/engine/swarm/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes</h1>
                </header>
            
            <article>
                
<p>Kubernetes is the jewel of the crown of the containers orchestration. The product itself was vamped by Google leveraging years of knowledge on how to run containers in production. Initially, it was an internal system used to run Google services, but at some point, it became a public project. Nowadays, it is an open source project maintained by few companies (Red Hat, Google, and so on) and is used by thousands of companies.</p>
<p>At the time of writing this, the demand for Kubernetes engineers has skyrocketed up to a point that companies are willing to hire people without expertise in the field but with a good attitude to learn new technologies.</p>
<p>Kubernetes has become so popular due to, in my opinion, the following factors:</p>
<ul>
<li>It solves all the deployment problems</li>
<li>It automates micro services' operations</li>
<li>It provides a common language to connect ops and development with a clean interface</li>
<li>Once it is setup, it is very easy to operate</li>
</ul>
<p>Nowadays, one of the biggest problems in companies that want to shorten the delivery life cycle is the <strong>red tape that has grown around the delivery process</strong>. Quarter releases are not acceptable anymore in a market where a company of five skilled engineers can overtake a classic bank due to the fact that they can cut the red tape and streamline a delivery process that allows them to release multiple times a day.</p>
<p>One of my professional activities is to speak at conferences (meet-ups in Dublin, RebelCon in Cork, <strong>Google Developer Groups</strong> (<strong>GDGs</strong>) in multiple places, Google IO Extended) and I always use the same words in all the talks: release management should stop being a big bang event that stops the world for three hours in order to release a new version of your company's application and <strong>s</strong>tart being a painless process that can be rolled back at any time so that we remove the majority of the stress from it by providing the tools to manage a faulty release.</p>
<p>This (not just this, but mainly this) is Kubernetes: a set of tools and virtual objects that will provide the engineers with a framework that can be used to streamline all the operations around our apps:</p>
<ul>
<li>Scale up</li>
<li>Scale down</li>
<li>Zero downtime rollouts</li>
<li>Canary deployments</li>
<li>Rollbacks</li>
<li>Secret management</li>
</ul>
<p>Kubernetes is built in a technology-agnostic way. Docker is the main container engine, but all the components were designed with interchangeability in mind: once Rkt is ready, it will be easy to switch to Rkt from Docker, which gives an interesting perspective to the users as they don't get tied to a technology in particular so that avoiding vendor locking becomes easier. This applies to the software defined network and other Kubernetes components as well.</p>
<p>One of the pain points is the steep learning curve for setting it up as well as for using it.</p>
<p>Kubernetes is very complex, and being skilled in its API and operations can take any smart engineer a few weeks, if not months, but once you are proficient in it, the amount of time that you can save completely pays off all the time spent learning it.</p>
<p>On the same way, setting up a cluster is not easy up to a point that companies have started selling Kubernetes as a service: they care about maintaining the cluster and you care about using it.</p>
<p>One of the (once again, in my opinion) most advanced providers for Kubernetes is the <strong>Google Container Engine</strong> (<strong>GKE</strong>), and it is the one that we are going to use for the examples in this book.</p>
<p>When I was planning the contents of this chapter, I had to make a decision between two items:</p>
<ul>
<li>Setting up a cluster</li>
<li>Showing how to build applications around Kubernetes</li>
</ul>
<p>I was thinking about it for a few days but then I realized something: there is a lot of information and about half a dozen methods to set up a cluster and none of them are official. Some of them are supported by the official Kubernetes GitHub repository, but there is no (at the time of writing this) official and preferred way of setting up a Kubernetes instance either on premises or in the cloud, so the method chosen to explain how to deploy the cluster might be obsolete by the time this book hits the market. The following options are the most common ways of setting up a Kubernetes cluster currently:</p>
<ul>
<li><strong>Kops</strong>: The name stands for Kubernetes operations and it is a command-line interface for operating clusters: creating, destroying, and scaling them with a few commands.</li>
<li><strong>Kubeadm</strong>: Kubeadm is alpha at the moment and breaking changes can be integrated at any time into the source code. It brings the installation of Kubernetes to the execution of a simple command in every node that we want to incorporate to the cluster in the same way as we would do if it was Docker Swarm.</li>
<li><strong>Tectonic</strong>: Tectonic is a product from CoreOS to install Kubernetes in a number of providers (AWS, Open Stack, Azure) pretty much painlessly. It is free for clusters up to nine nodes and I would highly recommend that, at the very least, you play around it to learn about the cluster topology itself.</li>
<li><strong>Ansible</strong>: Kubernetes' official repository also provides a set of playbooks to install a Kubernetes cluster on any VM provider as well as on bare metal.</li>
</ul>
<p>All of these options are very valid to set up a cluster from scratch as they automate parts of Kubernetes architecture by hiding the details and the full picture. If you really want to learn about the internals of Kubernetes, I would recommend a guide written by Kelsey Hightower called Kubernetes the hard way, which basically shows you how to set up everything around Kubernetes, from the etcd cluster needed to share information across nodes to the certificates used to communicate with <kbd>kubectl</kbd>, the remote control for Kubernetes. This guide can be found at <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a>.</p>
<p>And it is maintained and up to date with new versions of Kubernetes.</p>
<p>As you can guess from this explanation, in this chapter, you are going to learn about the architecture of Kubernetes, but mainly, we will focus on how to deploy and operate applications on Kubernetes so that by the end of this chapter, we have a good understanding of how we can benefit from an already running cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes logical architecture</h1>
                </header>
            
            <article>
                
<p>The first problem that you will find once you start playing with Kubernetes is creating a mental map on how and where everything runs in Kubernetes as well as how everything is connected.</p>
<p>In this case, it took me few weeks to fully understand how it all was wiring up, but once I had the picture in my mind, I drew something similar to what is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img height="292" width="441" class="image-border" src="assets/02b1c383-6951-4f06-a713-c3934fe10ace.png"/></div>
<p>This is Kubernetes on a very high level: a master node that orchestrates the running of containers grouped in pods across different Nodes (they used to be called minions but not anymore).</p>
<p>This mental map helps us understand how everything is wired up and brings up a new concept: the pod. A pod is basically a set of one or more containers running in orchestration to achieve a single task. For example, think about a cache and a cache warmer: they can run in different containers but on the same pod so that the cache warmer can be packed as an individual application. We will come back to this later on.</p>
<p>With this picture, we are also able to identify different physical components:</p>
<ul>
<li>Master</li>
<li>Nodes</li>
</ul>
<p>The master is the node that runs all support services such as DNS (for service discovery) as well as the API server that allows us to operate the cluster. Ideally, your cluster should have more than one master, but in my opinion, being able to recover a master quickly is more important than having a high availability configuration. After all, if the master goes down, usually, it is possible to keep everything running until we recover the master that usually is as simple as spawning a new VM (on the cloud) with the same template as the old master was using.</p>
<div class="packt_tip">It is also possible to have a master running with the IP Tables blocking connections to key ports so that it does not join the cluster and remove the IP Tables rules once you want the master to become the lead of your cluster.</div>
<p>The nodes are basically workers: they follow instructions from the master in order to deploy and keep applications <span>alive</span> as per the specified configuration. They use a software called Kubelet, which is basically the Kubernetes agent that orchestrates the communication with the master.</p>
<p>Regarding the networking, there are two layers of network in here:</p>
<ul>
<li>Hardware network</li>
<li>Software network</li>
</ul>
<p>The hardware network is what we all know and that is used to interconnect the VMs on the cluster. It is defined in our cloud provider (AWS, Google Cloud Platform, and so on), and there is nothing special about it, just bear in mind that ideally, this network should be a high profile network (Gigabyte Ethernet) as the inter-node traffic can be quite high.</p>
<p>The software network (or <strong>Software Defined Network</strong>, <strong>SDN</strong>) is a network that runs on top of Kubernetes middleware and is shared between all the nodes via <strong>etcd</strong>, which is basically a distributed key value storage that is used by Kubernetes as a coordination point to share information about several components.</p>
<p>This SDN is used to interconnect the pods: the IPs are virtual IPs that do not really exist in the external network and only the nodes (and master) know about. They are used to rout the traffic across different nodes so that if an app on the node 1 needs to reach a pod living in the <strong>Node 3</strong>, with this network, the application will be able to reach it using the standard <kbd>http/tcp</kbd> stack. This network would look similar to what is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="467" width="348" class="image-border" src="assets/6ff1c1a6-6e62-44c8-aeed-57a30964dedd.png"/></div>
<p>Let's explain this a bit:</p>
<ul>
<li>The <span>addresses on the network 192.168.0.0/16</span> are the physical addresses. They are used to interconnect the VMs that compound the cluster.</li>
<li>The <span>addresses on the network 10.0.0.0/24</span> are the software defined network addresses. They are not reachable from outside the cluster and only the nodes are able to resolve these addresses and forward the traffic to the right target.</li>
</ul>
<p>Networking is a fairly important topic in Kubernetes, and currently, the most common bottleneck in performance is that traffic forwarding is common across nodes (we will come back to this later on in this chapter), and this causes extra inter-node traffic that might cause a general slowdown of the applications running in Kubernetes.</p>
<p>In general and for now, this is all we need to know about the Kubernetes architecture. The main idea behind Kubernetes is to provide a uniform set of resources that can be used as a single computing unit with easy zero downtime operations. As of now, we really don't know how to use it, but the important thing is that we have a mental model of the big picture in a Kubernetes cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up a cluster in GCP</h1>
                </header>
            
            <article>
                
<p>The first thing we need to start playing with in Kubernetes is a cluster. There are several options, but we are going to use GKE as we have already signed for the trial and there should be enough credit in there for going through the full book.</p>
<p>Another option if you did not sign for the trial on GCP is Minikube. Minikube is an out-of-the-box, easy-to-install local cluster that runs on VMs and is a very good tool for experimenting with new features without being afraid of breaking something.</p>
<p>The Minikube project can be found at <a href="https://github.com/kubernetes/minikube">https://github.com/kubernetes/minikube</a>.</p>
<p>Its documentation is fairly comprehensive.</p>
<p>In order to create a cluster in GCP, the first thing we need to do is open the container engine in the online console in GCP that will show something similar to what is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/317e21d3-388e-4dc9-b6bc-fc66f39a85c1.png"/></div>
<p>This means that you have no clusters at the moment. Click on the <span class="packt_screen">Create a container cluster</span> button and fill in the following form:</p>
<div class="CDPAlignCenter CDPAlign"><img height="418" width="431" class="image-border" src="assets/44a48f26-1eee-4542-b5fb-5ac6df3c3ce5.png"/></div>
<p>Just a few considerations here:</p>
<ul>
<li>Give a comprehensive name <span>to the cluster</span>. In my case, I named it <kbd>testing-cluster</kbd>.</li>
<li>Choose a zone that is close to you geographically, in my case, <kbd>europe-west1-c</kbd>.</li>
<li>Regarding the cluster version, choose the default one. This is the version of Kubernetes that you want your cluster to run. It can be seamlessly upgraded later. Also, be aware that Kubernetes releases a new version every 2 months (apporximately), so by the time you are reading this book, it is most likely that there will be a more modern version available.</li>
<li>The machine type should also be the standard one (1 vCPU 3.75 GB of RAM).</li>
<li>Size is the number of machines that we want to use in our cluster. Three is a good number for testing and it can also be increased (or decreased later on).</li>
</ul>
<p>Everything else should be default. Auto-upgrade and auto-repair are beta functionalities that I would hesitate to use in a production cluster yet. These two options allow GCP to take actions if there is a new version of Kubernetes available or one of the nodes breaks for some reason.</p>
<p>Once the form is completed click on <span class="packt_screen">Create Cluster</span>, and that is everything. Now Google is provisioning a cluster for us. In order to check what is going on, open the tab of the Compute Engine in the GCP and you should see something similar to what is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2890b604-2b63-4fd3-97db-22e6d31a8fca.png"/></div>
<p>Three machines have been created in the compute engine with the prefix "<kbd>gke-</kbd>", which means that they belong to the GKE, <strong>K</strong> is for <strong>Kubernetes</strong>. They are regular machines, and there's nothing special about them aside from the fact that Google has provisioned all the software required to set up a Kubernetes Node, but where is the master?</p>
<p>Here is the interesting thing about running Kubernetes in Google Cloud Platform: they look after your master so there is no need to worry about the high availability or upgrading it as it is done automatically.</p>
<p>The master of our cluster is hosting one of the key components of our whole cluster: the API server. All the operations in Kubernetes are done via the API server with a component called <kbd>kubectl</kbd>. Kubectl stands for Kubernetes Control and is basically a terminal program that you can install on your local machine (or in a continuous integration server), add the configuration for a given cluster, and start issuing commands to our cluster.</p>
<p>First, we are going to install <kbd>kubectl</kbd>. In the previous chapters, we already installed the Google Cloud SDK (<kbd>gcloud</kbd> command), which can be used to install <kbd>kubectl</kbd> with the following command:</p>
<pre><strong>gcloud components install kubectl</strong></pre>
<p>And that's it. Now we can use <kbd>kubectl</kbd> in our system as if it was any other command, but we need to add our cluster configuration. As of now, <kbd>kubectl</kbd> is not configured to operate our cluster, so the first thing we need to do is fetch the required configuration. Google Cloud Platform makes it very easy. If you open the Google Container Engine tab, it now should look similar to the following one:</p>
<div class="CDPAlignCenter CDPAlign"><img height="166" width="624" class="image-border" src="assets/78bc4e64-e930-49b3-bd60-83a16b3d782b.png"/></div>
<p>As you can see, there is a button called <span class="packt_screen">Connect</span> on the right-hand side of the screen. By clicking on it, you will be presented with the following form:</p>
<div class="CDPAlignCenter CDPAlign"><img height="264" width="363" class="image-border" src="assets/4e2ca1a1-c476-4fe8-8241-9aacb15e9323.png"/></div>
<p>The date in the form will be slightly different (as the name of your cluster and project will be different), but there are two commands presented in there:</p>
<ul>
<li>A <kbd>gcloud</kbd> command to get the configuration of our Kubernetes cluster in our local machine</li>
<li>A <kbd>kubectl</kbd> command to start the proxy into the Kuberentes Dashboard UI</li>
</ul>
<p>The first command is easy. Just execute it:</p>
<pre><strong>gcloud container clusters get-credentials testing-cluster --zone europe-west1-c --project david-on-microservices</strong></pre>
<p>And the output will be similar to the following one:</p>
<pre><strong>Fetching cluster endpoint and auth data.</strong><br/><strong>kubeconfig entry generated for testing-cluster.</strong></pre>
<p>So, what happened here is that <kbd>gcloud</kbd> fetched the configuration and installed it locally for us to operate the cluster. You can try this by running the following command:</p>
<pre><strong>kubectl get nodes</strong></pre>
<p>This will output the list of nodes in your cluster. Kubectl is a very extensive command-line tool. With it, we can do pretty much anything inside the cluster, as we will learn in the rest of this chapter.</p>
<p>The second command in the preceding screenshot is used to start a proxy in Kubernetes:</p>
<pre><strong>kubectl proxy</strong></pre>
<p>This will output the following:</p>
<pre><strong>Starting to serve on 127.0.0.1:8001</strong></pre>
<p>Let's explain what happened here. Kubernetes makes heavy usage of client certificates. In order to communicate with the master, our machine needs to proxy the requests sending the certificate to validate them.</p>
<p>So, if we browse to the URL in the preceding screenshot now, <kbd>http://localhost:8001/ui</kbd>, we get presented with the Kubernetes dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img height="465" width="893" class="image-border" src="assets/6f4986fd-fc75-4ae3-82d9-4d605efc0768.png"/></div>
<p>The dashboard is basically a nice way of presenting all the information of our running cluster to the end users. It is also possible to operate the cluster up to a certain extent from the dashboard, but my recommendation will be to master <kbd>kubectl</kbd> as it is way more powerful. On the dashboard, we can see a lot of information, such as the state of the nodes, the items deployed into the cluster (Pods, Replica Sets, Daemon Sets, and so on), and the namespaces as well as many other elements.</p>
<p>Explore around a bit and get yourself familiar with the dashboard as it is a nice tool to actually see things happening in your cluster.</p>
<p>Kubernetes divides the workloads into namespaces. A namespace is a virtual cluster that allows the engineers to segregate resources (up to a point) across different teams. It is also used by Kubernetes to run its own internal components. This is important because Kubernetes spreads the key components across different nodes to ensure high availability. In this case, we have three components that are running on every node:</p>
<ul>
<li>The Kubernetes dashboard</li>
<li>Kubernetes proxy (<kbd>kube-proxy</kbd>)</li>
<li>Kubernetes DNS (<kbd>kube-dns</kbd>)</li>
</ul>
<p>The Kubernetes dashboard is what we just have seen: a user interface to represent the information within the Kubernetes cluster.</p>
<p>Kubernetes proxy is a proxy that the nodes use to resolve IP addresses in the SDN from Pods addresses to node addresses so that the cluster is able to redirect the traffic to the right Node.</p>
<p>The Kubernetes DNS is basically a load balancing and service discovery mechanism. In the next section, you will learn about the building blocks that we can use for deploying applications to Kubernetes. In particular, Services are strongly coupled with this DNS service in a way that in order to locate an application within Kubernetes, we just need to know its name and the configuration of the Service that groups the Pods compounding the given application.</p>
<p>The fact that we are running these components in every node enables Kubernetes to enter into an autopilot mode in case of a master going down: applications will continue working (in the majority of the cases) even without a master, so losing a master is not a catastrophic event.</p>
<p>Once we have configured <kbd>kubectl</kbd> in our machines, it is time to learn about the building blocks that we can use in Kubernetes in order to build extremely robust applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes building blocks</h1>
                </header>
            
            <article>
                
<p>In the preceding section, you learned about the cluster topology, but now we need the tools to run applications on it. We have already introduced one of the Kubernetes building blocks: the Pod. In this section, we are going to look at some of the most important API objects (building blocks) that Kubernetes provide in order to build our applications.</p>
<p>When I started learning Kubernetes, I was working in the second company that was deploying applications in a continuous delivery way, and I always had a question in mind: why are different companies trying to solve the same problem in different ways?</p>
<p>Then I realized why: The element missing was the domain-specific language for continuous delivery. The lack of a common standard and well understood way of rolling out applications was preventing them to work efficiently and deliver value early in the chain. Everybody knows what a load balancer is or a proxy or many other elements that are involved in the deployment of a new version of an app, but the way people uses the in, say, imaginative ways is where the problem lies. If you hire a new engineer, their previous knowledge of continuous delivery becomes obsolete as they need to learn your way of doing things.</p>
<p>Kubernetes solves this problem with a set of objects (Pods, ReplicaSets, DameonSets, and so on) that are described in YAML files (or JSON). Once we finish this section, we will already have enough knowledge to be able to, from the <span>YAML</span> or JSON files defining our resources, build a diagram about what the system looks like. These files, alongside the Docker images, are enough for Kubernetes to run our system, and we will look at a few examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pods</h1>
                </header>
            
            <article>
                
<p>Pods are the most basic element of the Kubernetes API. A Pod basically is a set of containers that work together in order to provide a service or part of it. The concept of Pod is something that can be misleading. The fact that we can run several containers working together suggests that we should be sticking the frontend and backend of our application on a single pod as they work together. Even though we can do this, it is a practice that I would strongly suggest you avoid. The reason for this is that by bundling together the frontend and the backend, we are losing a lot of flexibility that Kubernetes is providing us with, such as autoscaling, load balancing, or canary deployments.</p>
<p>In general, pods contain a single container and it is, by far, the most common use case, but there are few legitimate use cases for multi-container pods:</p>
<ul>
<li>Cache and cache warmer</li>
<li>Precalculating and serving HTML pages</li>
<li>File upload and file processing</li>
</ul>
<p>As you can see, all of these are activities that are strongly coupled together, but if the feeling is that the containers within a pod are working toward different tasks (such as backend and frontend), it might be worth placing them in different Pods.</p>
<p>There are two options for communication between containers inside a pod:</p>
<ul>
<li>Filesystem</li>
<li>Local network interface</li>
</ul>
<p>As Pods are indivisible elements running on a single machine, volumes mounted in all the containers of a pod are shared: files created in a container within a pod can be accessed from other containers mounting the same volume.</p>
<p>The local network interface or loopback is what we commonly know as localhost. Containers inside a pod share the same network interface; therefore, they can communicate via localhost (or <kbd>127.0.0.1</kbd>) on the exposed ports.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying a pod</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, Kubernetes relies <span>heavily</span> on <strong>Yet Another Markup Language</strong> (<span><strong>YAML</strong>)</span> files to configure API elements. In order to deploy a pod, we need to create a yaml file, but first, just create a folder called <strong>deployments</strong>, where we are going to create all the descriptors that we will be created on this section. Create a file called <kbd>pod.yaml</kbd> (or <kbd>pod.yml</kbd>) with the following content:</p>
<pre><strong>apiVersion: v1</strong><br/><strong>kind: Pod</strong><br/><strong>metadata:</strong><br/><strong>  name: nginx</strong><br/><strong>  labels:</strong><br/><strong>    name: nginx</strong><br/><strong>spec:</strong><br/><strong>  containers:</strong><br/><strong>  - name: nginx</strong><br/><strong>     image: nginx</strong><br/><strong>     ports:</strong><br/><strong>      - containerPort: 80</strong><br/><strong>     resources:</strong><br/><strong>       requests:</strong><br/><strong>         memory: "64Mi"</strong><br/><strong>         cpu: "250m"</strong></pre>
<p>As you can see, the preceding <kbd>yaml</kbd> is fairly descriptive, but some points need clarification:</p>
<ul>
<li><kbd>apiVersion</kbd>: This is the version of the Kubernetes API that we are going to use to define our resource (in this case, pod). Kuberentes is a living project that evolves very quickly. The version is the mechanism used to avoid deprecating resources with new releases. In general, Kuberentes works with three branches: alpha, beta, and stable. In the preceding case, we are using the stable version. More information can be found at <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/">https://kubernetes.io/docs/concepts/overview/kubernetes-api/</a>.</li>
<li><kbd>metadata</kbd>: In this section, we are defining one of the most powerful discovery mechanisms that I have ever seen: the pattern matching. The section label, specifically, will be used later on to expose pods with certain <strong>l</strong>abels to the outer world.</li>
<li><kbd>spec</kbd>: This is where we define our container. In this case, we are deploying an <kbd>nginx</kbd> instance so that we can easily see how everything works without focusing too much on the application itself. As expected, the image and the exposed port have been specified. We have also defined the CPU and memory limitations for this Pod, so we prevent an outbreak in resource consumption (note that the YAML file is requesting the resources; they might not be available so the pod will operate with lower profile resources).</li>
</ul>
<p>This is the simplest configuration for an item that we can create in Kubernetes. Now it's time to deploy the resource in our cluster:</p>
<pre><strong>kubectl apply -f pod.yml</strong></pre>
<p>This will produce an output similar to the following one:</p>
<pre><strong>pod "nginx" created.</strong></pre>
<p>Disclaimer: there are several ways of creating a resource, but in this book, I will use <kbd>apply</kbd> as much as possible. Another possibility would be to use <kbd>create</kbd>:</p>
<pre><strong>kubectl create -f pod.yml</strong></pre>
<p>The advantage that <kbd>apply</kbd> has over create is that apply does a three-way diff between the previous version, the current version, and the changes that you want to apply and decides how is best to update the resource. This is letting Kubernetes do what it does best: automate container orchestration.</p>
<p>With create, Kubernetes does not save the state of the resource, and if we want to run apply afterward in order to gracefully change the state of a resource, a warning is produced:</p>
<pre><strong>Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</strong><br/><strong>pod "nginx" configured</strong></pre>
<p>This means that we can push our system to an unstable state for few seconds, which might not be acceptable depending on your use case.</p>
<p>Once we have applied our YAML file, we can use <kbd>kubectl</kbd> to see what is going on in Kubernetes. Execute the following command:</p>
<pre><strong>kubectl get pods</strong></pre>
<p>This will output our pod:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/2a6b0b4e-706a-4686-b11f-43592c818f96.png"/></div>
<p>We can do this for other elements of our cluster, such as the nodes:</p>
<pre><strong>kubectl get nodes</strong></pre>
<p>And this will output the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/d4b67f6b-c276-4f97-917e-e1e95b2eeb2b.png"/></div>
<p>The <kbd>kubectl get</kbd> works for all the workflows in Kubernetes and the majority of the API objects.</p>
<p>Another way of seeing what is going on in Kubernetes is using the dashboard. Now that we have created a pod, open the dashboard at <kbd>http://localhost:8001/ui</kbd> and navigate to the pods section on the left-hand side.</p>
<div class="packt_infobox">Remember that in order to access the dashboard, first, you need to execute <kbd>kubectl proxy</kbd> on a Terminal.</div>
<p>There; you will see the list of the current deployed pods, in this case, just <span class="packt_screen">nginx</span>. Click on it and the screen should look very similar to what is shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img height="800" width="1147" class="image-border" src="assets/15fd80e2-8bb2-4d68-830f-ffb7d6fb62a8.png"/></div>
<p>Here, we get a ton of information, from the memory and CPU that the pod is consuming to the node where it is running and a few other valuable items, such as the annotations applied to the pod. We can get this using the '<kbd>describe</kbd>' command of <kbd>kubectl</kbd>, as follows:</p>
<pre><strong>kubectl describe pod nginx</strong></pre>
<p>Annotations are a new concept and are the data around our API element, in this case, our pod. If you click on L<span class="packt_screen">ast applied configuration</span> in the Details section, you can see the data from the YAML file, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/9577ed4e-2e88-4681-aeaf-42ca6d67379e.png"/></div>
<p>And this relates to the three-way diff that was explained earlier and is used by Kubernetes to decide the best way of upgrading a resource without getting into an inconsistent state.</p>
<p>As of now, our pod is running in Kubernetes but is not connected to the outer world; therefore, there is no way to open a browser and navigate to the <span class="packt_screen">nginx</span> home page from outside the cluster. One thing that we can do is open a remote session to a bash Terminal in the container inside the pod in a manner similar to what we would do with Docker:</p>
<pre><strong>kubectl exec -it nginx bash</strong></pre>
<p>And we are in. Effectively, we have gained access to a root terminal inside our container and we can execute any command. We will use this functionality later on.</p>
<p>Once we have seen how pods work, you might have a few questions abound what Kubernetes is supposed to do:</p>
<ul>
<li>How can we scale pods?</li>
<li>How can we roll out new versions of an application?</li>
<li>How can we access our application?</li>
</ul>
<p>We will answer all these questions, but first, we need to know other 'building blocks'.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Replica Sets</h1>
                </header>
            
            <article>
                
<p>So far, we know how to deploy applications in pods. The sole concept of pod is very powerful, but it lacks robustness. It is actually impossible to define scaling policies or even make sure that the pods remain alive if something happens (such as a node going down). This might be okay in some situations, but here is an interesting question. If we are biting the bullet on the overhead of maintaining a Kubernetes cluster, why don't we take the benefits of it?</p>
<p>In order to do that, we need to work with <strong>Replica Sets</strong>. A Replica Set is like a traffic cop in a road full of pods: they make sure that the traffic flows and everything works without crashing and moving the pods around so that we make the best use of the road (our cluster, in this case).</p>
<p>Replica Sets are actually an update of a much older item: the Replication Controller. The reason for the upgrade is the labeling and selecting of resources, which we will see visit when we dive <span>deep</span> into the API item called Service.</p>
<p>Let's take a look at a Replica Set:</p>
<pre><strong>apiVersion: extensions/v1beta1</strong><br/><strong>kind: ReplicaSet</strong><br/><strong>metadata:</strong><br/><strong>   name: nginx-rs</strong><br/><strong>spec:</strong><br/><strong>   replicas: 3</strong><br/><strong>   template:</strong><br/><strong>      metadata:</strong><br/><strong>         labels:</strong><br/><strong>            app: nginx</strong><br/><strong>            tier: frontend</strong><br/><strong>      spec:</strong><br/><strong>         containers:</strong><br/><strong>         - name: nginx</strong><br/><strong>            image: nginx</strong><br/><strong>            resources:</strong><br/><strong>               requests:</strong><br/><strong>                  cpu: 256m</strong><br/><strong>                  memory: 100Mi</strong><br/><strong>           ports:</strong><br/><strong>           - containerPort: 80</strong></pre>
<p>Again, this a YAML file that is basically fairly easy to understand but might require some explanation:</p>
<ul>
<li>In this case, we have used the extensions API on the version <kbd>v1beta1</kbd>. If you remember from the pod section (previously), Kubernetes has three branches: stable, alpha, and beta. The complete reference can be found in the official documentation, and it is very likely to change often as Kubernetes is a vibrant and always evolving project.</li>
<li>In the spec section is where the important things happen: we have defined a set of labels for the Replica Set, but we have also defined a pod (in this case, with a single container) and specified that we want three instances of it (replicas: three).</li>
</ul>
<p>Simple and effective. Now we have defined a resource called Replica Set, which allows us to deploy a pod and keep it alive as per configuration.</p>
<p>Let's test it:</p>
<pre><strong>kubectl apply -f replicaset.yml</strong></pre>
<p>Once the command returns, we should see the following message:</p>
<pre><strong>replicaset "nginx-rs" created</strong></pre>
<p>Let's verify it using <kbd>kubectl</kbd>:</p>
<pre><strong>kubectl get replicaset nginx-rs</strong></pre>
<p>As the output of the preceding command, you should see the Replica Set explaining that there are three desired pods, three actually deployed, and three ready. Note the difference between current and ready: a pod might be deployed but still not ready to process requests.</p>
<p>We have specified that our <kbd>replicaset</kbd> should keep three pods <span>alive</span>. Let's verify this:</p>
<pre><strong>kubectl get pods</strong></pre>
<p>No surprises here: our <kbd>replicaset</kbd> has created three pods, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="83" width="343" class="image-border" src="assets/f1055e82-5872-477b-91a8-6d200347d72b.png"/></div>
<p>We have four pods:</p>
<ul>
<li>One created in the preceding section</li>
<li>Three created by the Replica Set</li>
</ul>
<p>Let's kill one of the pods and see what happens:</p>
<pre><strong>kubectl delete pod nginx-rs-0g7nk</strong></pre>
<p>And now, query how many pods are running:</p>
<div class="CDPAlignCenter CDPAlign"><img height="83" width="371" class="image-border" src="assets/508833b4-48b0-4dc0-a0e4-119ead927a8e.png"/></div>
<p>Bingo! Our <kbd>replicaset</kbd> has created a new pod (you can see which one in the AGE column). This is immensely powerful. We have gone from a world where a pod (an application) being killed wakes you up at 4 a.m. in the morning to take action to a world where when one of our application dies, Kubernetes revives it for us.</p>
<p>Let's take a look at what happened in the dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/5bf89559-f168-4880-bac5-c038679168b7.png"/></div>
<p>As you can expect, the Replica Set has created the pods for you. You can try to kill them from the interface as well (the period icon to the very right of every pod will allow you to do that), but the Replica Set will re-spawn them for you.</p>
<p>Now we are going to do something that might look like it's from out of this world: we are going to scale our application with a single command, but first, edit <kbd>replicaset.yml</kbd> and change the <kbd>replicas</kbd> <span>field</span> from three to five.</p>
<p>Save the file and execute this:</p>
<pre><strong>kubectl apply -f replicaset.yml</strong></pre>
<p>Now take a look at the dashboard again:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/8f92bebd-c741-495b-a01b-3b9be7fb475d.png"/></div>
<p>As you can see, Kubernetes is creating pods for us following the instructions of the Replica Set, <kbd>nginx-rs</kbd>. In the preceding sreenshot, we can see one pod whose icon is not green, and that is because its status is <span class="packt_screen">Pending</span>, but after a few seconds, the status becomes <span class="packt_screen">Ready</span>, just like any other pod.</p>
<p>This is also very powerful, but there is a catch: who scales the application if the load spike happens at 4 a.m. in the morning? Well, Kubernetes provides a solution for this: <span class="packt_screen">Horizontal Pod Autoscalers</span>.</p>
<p>Let's execute the following command:</p>
<pre><strong>kubectl autoscale replicaset nginx-rs --max=10</strong></pre>
<p>With the preceding command, we have specified that Kubernetes should attach a <span class="packt_screen">Horizontal Pod Autoscalers</span> to our Replica Set. If you browse the Replica Set in the dashboard again, the situation has changed dramatically:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/6f948bb0-7e82-4b40-b53e-dc9d12fac20f.png"/></div>
<p>Let's explain what happened here:</p>
<ul>
<li>We have attached an <span class="packt_screen">Horizontal Pod Autoscalers</span> to our Replica Set: minimum <kbd>1</kbd> pod, maximum <kbd>10</kbd>, and the trigger for creating or destroying pods is the CPU utilization going over <kbd>80%</kbd> on a given pod.</li>
<li>The Replica Set has scaled down to one pod because there is no load on the system, but it will scale back to up to 10 nodes if required and stay there for as long as the burst of requests is going on and scale back to the minimum required resources.</li>
</ul>
<p>Now this is actually the dream of any sysadmin: no-hassle autoscaling and self-healing infrastructure. As you can see, Kubernetes starts making sense altogether, but there is one thing disturbing in the autoscaler part. It was a command that we ran in the terminal, but it is captured nowhere. So how can we keep track of our infrastructure (yes, an Horizontal Pod Autoscaler is part of the infrastructure)?</p>
<p>Well, there is an alternative; we can create a YAML file that describes our Horizontal Pod Autoscaler:</p>
<pre><strong>apiVersion: autoscaling/v1</strong><br/><strong>kind: HorizontalPodAutoscaler</strong><br/><strong>metadata:</strong><br/><strong>   name: nginx-hpa</strong><br/><strong>spec:</strong><br/><strong>   maxReplicas: 10</strong><br/><strong>   minReplicas: 1</strong><br/><strong>   scaleTargetRef:</strong><br/><strong>      kind: ReplicaSet</strong><br/><strong>      name: nginx-rs</strong><br/><strong>   targetCPUUtilizationPercentage: 80</strong></pre>
<p>First, from the dashboard, remove <kbd>HorizontalPodAutoscaler</kbd> created from the previous example. Then, write the preceding content into a file called <kbd>horizontalpodautoscaler.yml</kbd> and run the following command:</p>
<pre><strong>kubectl apply -f horizontalpodautoscaler.yml</strong></pre>
<p>This should have the same effect as the <kbd>autoscale</kbd> command but with two obvious benefits:</p>
<ul>
<li>We can control more parameters, such as the name of the HPA, or add metadata to it, such as labels</li>
<li>We keep our infrastructure as code within reach so we know what is going on</li>
</ul>
<p>The second point is extremely important: we are in the age of the infrastructure as code and Kubernetes leverages this powerful concept in order to provide traceability and readability. Later on, in <a href="127a7b5f-4bd7-4290-bea0-3e8db867e4af.xhtml" target="_blank">Chapter 8</a>, <em>Release Management – Continuous Delivery</em>, you will learn how to create a continuous delivery pipeline with Kubernetes in a very easy way that works on 90% of the software projects.</p>
<p>Once the preceding command returns, we can check on the dashboard and see that effectively, our Replica Set has attached an Horizontal Pod Autoscaler as per our configuration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployments</h1>
                </header>
            
            <article>
                
<p>Even though the Replica Set is a very powerful concept, there is one part of it that we have not talked about: what happens when we apply a new configuration to a Replica Set in order to upgrade our applications? How does it handle the fact that we want to keep our application alive 100% of the time without service interruption?</p>
<p>Well, the answer is simple: it doesn't. If you apply a new configuration to a Replica Set with a new version of the image, the Replica Set will destroy all the Pods and create newer ones without any guaranteed order or control. In order to ensure that our application is always up with a guaranteed minimum amount of resources (Pods), we need to use Deployments.</p>
<p>First, take a look at what a deployment looks like:</p>
<pre><strong>apiVersion: apps/v1beta1</strong><br/><strong>kind: Deployment</strong><br/><strong>metadata:</strong><br/><strong>   name: nginx-deployment</strong><br/><strong>spec:</strong><br/><strong>   strategy:</strong><br/><strong>      type: RollingUpdate</strong><br/><strong>      rollingUpdate:</strong><br/><strong>         maxUnavailable: 0</strong><br/><strong>         maxSurge: 1</strong><br/><strong>   replicas: 3</strong><br/><strong>   template:</strong><br/><strong>      metadata:</strong><br/><strong>         labels:</strong><br/><strong>            app: nginx</strong><br/><strong>      spec:</strong><br/><strong>         containers:</strong><br/><strong>         - name: nginx</strong><br/><strong>            image: nginx</strong><br/><strong>            resources:</strong><br/><strong>               requests:</strong><br/><strong>                  cpu: 256m</strong><br/><strong>                  memory: 100Mi</strong><br/><strong>            ports:</strong><br/><strong>            - containerPort: 80</strong></pre>
<p>As you can see, it is very similar to a Replica Set, but there is a new section: strategy. In strategy, we are defining how our <kbd>rollout</kbd> is going to work, and we have two options:</p>
<ul>
<li><kbd>RollingUpdate</kbd></li>
<li><kbd>Recreate</kbd></li>
</ul>
<p><kbd>RollingUpdate</kbd> is the default option as it seems the most versatile in modern 24/7 applications: It coordinates two replica sets and starts shutting down pods from the old replica set at the same time that it is creating them in the new Replica Set. This is very powerful because it ensures that our application always stays up. Kubernetes decides what is best to coordinate the pods' rescheduling, but you can influence this decision with two parameters:</p>
<ul>
<li><kbd>maxUnavailable</kbd></li>
<li><kbd>maxSurge</kbd></li>
</ul>
<p>The first one defines how many pods we <span>can</span> loose from our Replica Set in order to perform a <kbd>rollout</kbd>. As an example, if our Replica Set has three replicas, a <kbd>rollout</kbd> with <span>the</span> <kbd>maxUnavailable</kbd> value of <kbd>1</kbd> will allow Kubernetes to transition to the new Replica Set with only two pods in the status <kbd>Ready</kbd> at some point. In this example, <kbd>maxUnavailable</kbd> is <kbd>0</kbd>; therefore, Kubernetes will always keep three pods alive.</p>
<p><kbd>MaxSurge</kbd> is similar to maxUnavailable, but it goes the other way around: it defines how many pods above the replicas can be scheduled by Kubernetes. In the preceding example, with three replicas with <kbd>maxSurge</kbd> set on <kbd>1</kbd>, the maximum amount of pods at a given time in our <kbd>rollout</kbd> will be <kbd>4</kbd>.</p>
<p>Playing with these two parameters as well as the replicas' number, we can achieve quite interesting effects. For example, by specifying three replicas with <kbd>maxSurge 1</kbd> and <kbd>maxUnavailable 1</kbd>, we are forcing Kubernetes to move the pods one by one in a very conservative way: we might have four pods during the <kbd>rollout</kbd>, but we will never go below three available pods.</p>
<p>Coming back to the strategies, Recreate basically destroys all the pods and creates them again with the new configuration without taking uptime <span>into account</span>. This might be indicated in some scenarios, but I would strongly suggest that you use <kbd>RollingUpdate</kbd> when possible (pretty much always) as it leads to smoother deployments.</p>
<p>It is also possible to attach a Horizontal Pod Autoscaler to a Deployment in the same way that we would do with a Replica Set.</p>
<p>Let's test our deployment. Create a file called <kbd>deployment.yml</kbd> and apply it to our cluster:</p>
<pre><strong>kubectl apply -f deployment.yml --record</strong></pre>
<p>Once the command returns, we can go to the Kubernetes dashboard (<kbd>localhost:8001/ui</kbd> with the proxy active) and check what happened in the <span class="packt_screen">Deployments</span> section in the menu on the left-hand side:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/da68eada-ed69-491f-b51a-851d95ceb92f.png"/></div>
<p>We have a new <span class="packt_screen">Deployment</span> called <kbd>nginx-deployment</kbd>, which has created a Replica Set that also contains the specified pods. In the preceding command, we have passed a new parameter: <kbd>--record</kbd>. This saves the command in the <kbd>rollout</kbd> history of our deployment so that we can query the <kbd>rollout</kbd> history of a given deployment to see the changes applied to it. In this case, just execute the following:</p>
<pre><strong>kubectl rollout history deployment/nginx-deployment</strong></pre>
<p>This will show you all the actions that altered the status of a deployment called <kbd>nginx-deployment</kbd>. Now, let's execute some change:</p>
<pre><strong>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1</strong></pre>
<p>We have used <kbd>kubectl</kbd> to change the version of the <kbd>nginx</kbd> container back to version 1.9.1 (<kbd>kubectl</kbd> is very versatile; the official documentation offers shortcuts for pretty much everything), and a few things happened. The first one is that a new Replica Set has been created and the pods have been moved over to it from the old replica set. We can verify this in the <span class="packt_screen">Replica Sets</span> section of the menu on the left-hand side of the dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/914506b4-95bc-4bb3-a8f7-cbc5da1cc2d1.png"/></div>
<p>As you can see, the old replica set has 0 pods, whereas the new one that took over has three pods. This all happened without you noticing it, but it is a very clever workflow with a lot of work from the Kubernetes community and the companies behind it.</p>
<p>The second thing that happened was that we have a new entry in our rollout history. Let's check it out:</p>
<pre><strong>kubectl rollout history deployment/nginx-deployment</strong></pre>
<p>Which one should produce an output similar to the following one:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/b1947f72-a1f1-410f-b4e1-8d946788f00e.png"/></div>
<p>Now we have two entries that describe the changes applied to our deployment.</p>
<p>If you have been into IT for few years, by now, you have reached the conclusion that a rollback strategy is always necessary because bugs flowing into production are the reality no matter how good our QA is. I am a big fan of building the systems in a way that deployments are unimportant events (from a technical point of view), as shown with Kuberentes, and the engineers always have an easy way out if things start to fail in production. Deployments offer an easy rollback if something goes wrong:</p>
<pre><strong>kubectl rollout undo deployment/nginx-deployment</strong> </pre>
<p>Execute the preceding and browse back to the dashboard on the <strong>Replica Sets</strong> <span>section again</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/18a385bd-ca4c-44b3-8b36-6a0508f690cc.png"/></div>
<p>That's right. In a matter of seconds, we have gone from instability (a broken build) to the safety of the old known version without interrupting the service and without involving half of the IT department: a simple command brings back the stability to the system. The rollback command has a few configurations, and we can even select the revision where we want to jump to.</p>
<p>This is how powerful Kubernetes is and this is how simple our life becomes by using Kubernetes as the middleware of our enterprise: a modern CD pipeline assembled in a few lines of configuration that works in the same way in all the companies in the world by facilitating command <kbd>rollouts</kbd> and rollbacks. That's it...simple and efficient.</p>
<p>Right now, it feels like we know enough to move our applications to Kubernetes, but there is one thing missing. So far, up until now, we have just run predefined containers that are not exposed to the outer world. In short, there is no way to reach our application <span>from outside the cluster</span>. You are going to learn how to do that in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Services</h1>
                </header>
            
            <article>
                
<p>Up until now, we were able to deploy containers into Kubernetes and keep them alive by making use of pods, Replica Sets, and Horizontal Pods Autoscalers as well as Deployments, but so far, you have not learned how to expose applications to the outer world or make use of service discovery and balancing within Kubernetes.</p>
<p><strong>Services</strong> are responsible for all of the above. A Service in Kubernetes is not an element as we are used to it. A Service is an abstract concept used to give entity to a group of pods through pattern matching and expose them to different channels via the same interface: a set of labels attached to a Pod that get matched against a selector (another set of labels and rules) in order to group them.</p>
<p>First, let's create a service on top of the deployment created in the previous section:</p>
<pre><strong>kind: Service</strong><br/><strong>apiVersion: v1</strong><br/><strong>metadata:</strong><br/><strong>   name: nginx-service</strong><br/><strong>spec:</strong><br/><strong>   selector:</strong><br/><strong>      app: nginx</strong><br/><strong>   ports:</strong><br/><strong>      - protocol: TCP</strong><br/><strong>         port: 80</strong><br/><strong>         targetPort: 80</strong></pre>
<p>Easy and straightforward, but there's one detail: the selector section has a hidden message for us. The selectors are the mechanisms that Kubernetes uses to connect components via pattern matching algorithms. Let's explain what pattern matching is. In the preceding Service, we are specifying that we want to select all the Pods that have a label with the <kbd>app</kbd> <span>key</span> and the <kbd>nginx</kbd> <span>value</span>. If you go back to the previous section, you'll understand our deployment has these labels in the pod specification. This is a match; therefore, our service will select these pods. We can check this by browsing in the dashboard in the <span class="packt_screen">Services</span> section and clicking on <kbd>nginx-service</kbd>, but first, you need to create the <kbd>service</kbd>:</p>
<pre><strong>kubectl apply -f service.yml</strong></pre>
<p>Then, check out the dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img height="451" width="940" class="image-border" src="assets/8e9b3f8f-cae8-4dee-8969-95fad1d7814e.png"/></div>
<p>As you can see, there are three pods selected, and they all belong to the deployment <kbd>nginx</kbd> that we created in the preceding section.</p>
<div class="packt_tip">Don't remove the deployment from the previous section; otherwise, there will be no pods to select by our service.</div>
<p>This screen has a lot of interesting information. The first piece of information is that the service has an IP: this IP is denominated as <kbd>clusterIP</kbd>. Basically, it is an IP within the cluster that can be reached by our pods and other elements in Kubernetes. There is also a field called <kbd>Type</kbd>, which allows us to chose the service type. There are three types:</p>
<ul>
<li><kbd>ClusterIP</kbd></li>
<li><kbd>NodePort</kbd></li>
<li><kbd>LoadBalancer</kbd></li>
</ul>
<p><kbd>ClusterIP</kbd> is what we just created and explained.</p>
<p><kbd>NodePort</kbd> is another type of service that is rarely used in Cloud but is very common on premises. It allocates a port on all the nodes to expose our application. This allows Kubernetes to define the ingress of the traffic into our pods. This is challenging for two reasons:</p>
<ul>
<li>It generates extra traffic in our internal network as the nodes need to forward the traffic across to reach the pods (imagine a cluster of 100 nodes that has an app with only three pods, it is very unlikely to hit the node that is running one of them).</li>
<li>The ports are allocated randomly so you need to query the Kubernetes API to know the allocated port.</li>
</ul>
<p><kbd>LoadBalancer</kbd> is the jewel in the crown here. When you create a service of type <kbd>LoadBalancer</kbd>, a cloud load balancer is provisioned so that the client applications hit the load balancer that redirects the traffic into the correct nodes. As you can imagine, for a cloud environment where infrastructure is created and destroyed in matter of seconds, this is the ideal situation.</p>
<p>Coming back to the previous screenshot, we can see another piece of interesting information: the internal endpoints. This is the service discovery mechanism that Kubernetes is using to locate our applications. What we have done here is connect the pods of our application to a name: <kbd>nginx-service</kbd>. From now on, no matter what happens, the only thing that our apps need to know in order to reach our <kbd>nginx</kbd> pods is that there is a service called <kbd>nginx</kbd> that knows how to locate them.</p>
<p>In order to test this, we are going to run an instance of a container called <kbd>busybox</kbd>, which is <span>basically</span> the Swiss army knife of command-line tools. Run the following command:</p>
<pre><strong>kubectl run -i --tty busybox --image=busybox --restart=Never -- sh</strong></pre>
<p>The preceding command will present us with a shell inside the container called <kbd>busybox</kbd> running in a pod so we are inside the Kubernetes cluster and, more importantly, inside the network so that we can see what is going on. Be aware that the preceding command runs just a pod: no deployment or replica set is created, so once you exit the shell, the pod is finalized and resources are destroyed.</p>
<p>Once we get the prompt inside <kbd>busybox</kbd>, run the following command:</p>
<pre><strong>nslookup nginx-service</strong></pre>
<p>This should return something similar to the following:</p>
<pre class="mce-root"><strong>Server: 10.47.240.10</strong><br/><strong>Address 1: 10.47.240.10 kube-dns.kube-system.svc.cluster.local</strong><br/><br/><strong>Name: nginx-service</strong><br/><strong>Address 1: 10.47.245.73 nginx-service.default.svc.cluster.local</strong></pre>
<p>Okay, what happened here? When we created a service, we assigned a name to it: <kbd>nginx-service</kbd>. This name has been used to register it in an internal DNS for service discovery. As mentioned earlier, the DNS service is running on Kubernetes and is reachable from all the Pods so that it is a centralised repository of common knowledge. There is another way that the Kubernetes engineers have created in order to carry on with the service discovery: the environment variables. In the same prompt, run the following command:</p>
<pre><strong>env</strong></pre>
<p>This command outputs all the environment variables, but there are few that are relevant to our recently defined service:</p>
<pre><strong>NGINX_SERVICE_PORT_80_TCP_ADDR=10.47.245.73</strong><br/><strong>NGINX_SERVICE_PORT_80_TCP_PORT=80</strong><br/><strong>NGINX_SERVICE_PORT_80_TCP_PROTO=tcp</strong><br/><strong>NGINX_SERVICE_SERVICE_PORT=80</strong><br/><strong>NGINX_SERVICE_PORT=tcp://10.47.245.73:80</strong><br/><strong>NGINX_SERVICE_PORT_80_TCP=tcp://10.47.245.73:80</strong><br/><strong>NGINX_SERVICE_SERVICE_HOST=10.47.245.73</strong></pre>
<p>These variables, injected by Kubernetes at creation time, define where the applications can find our service. There is one problem with this approach: the environment variables are injected at creation time, so if our service changes during the life cycle of our pods, these variables become obsolete and the pod has to be restarted in order to inject the new values.</p>
<p>All this magic happens through the selector mechanism on Kubernetes. In this case, we have used the equal selector: a label must match in order for a pod (or an object in general) to be selected. There are quite a few options, and at the time of writing this, this is still evolving. If you want to learn more about selectors, here is the official documentation: <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/</a>.</p>
<p>As you can see, services are used in Kubernetes to glue our applications together. Connecting applications with services allows us to build systems based on microservices by coupling REST endpoints in the API with the name of the service that we want to reach on the DNS.</p>
<p>Up until now, you have learned how to expose our applications to the rest of our cluster, but how do we expose our applications to the outer world? You have also learned that there is a type of service that can be used for this: <kbd>LoadBalancer</kbd>. Let's take a look at the following definition:</p>
<pre>kind: Service<br/>apiVersion: v1<br/>metadata:<br/>   name: nginx-service<br/>spec:<br/><strong>   type: LoadBalancer</strong><br/>   selector:<br/>      app: nginx<br/> ports:<br/>   - protocol: TCP<br/>      port: 80<br/>      targetPort: 80</pre>
<p>There is one change in the preceding definition: the service type is now <kbd>LoadBalancer</kbd>. The best way to explain what this causes is by going to the <span class="packt_screen">Services</span> <span>section</span> of the dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8f694bd9-47e9-49dd-9f6d-1b0d2fdfd334.png"/></div>
<p>As you can see, our newly created service got assigned an external endpoint. If you browse it, bingo! The <kbd>nginx</kbd> default page is rendered.</p>
<p>We have created two services, <kbd>nginx-service</kbd> and <kbd>nginx-service-lb</kbd>, of the type <kbd>ClusterIP</kbd> and <kbd>LoadBalancer</kbd>, respectively, which both point to the same pods that belong to a deployment and are managed through a replica set. This can be a bit confusing, but the following diagram will explain it better:</p>
<div class="CDPAlignCenter CDPAlign"><img height="327" width="297" class="image-border" src="assets/1647789c-12c0-459c-866b-7956a0919600.png"/></div>
<p>The preceding diagram is the perfect explanation of what we've built in this section. As you can see, the load balancer is outside of Kubernetes, but everything else is inside our cluster as virtual elements of an API.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other Building Blocks</h1>
                </header>
            
            <article>
                
<p>In the previous sections, you learned the basics needed to deploy applications into Kubernetes successfully. The API objects that we visited are as follows:</p>
<ul>
<li>Pod</li>
<li>ReplicaSet</li>
<li>Deployment</li>
<li>Service</li>
</ul>
<p>In Kubernetes, there are many other building blocks that can be used to build more advanced applications; every few months, the Kubernetes engineers add new elements to improve or add functionality.</p>
<p>One example of these additions is the ReplicaSet that was designed to replace another item called ReplicationController. The main difference between the ReplicationController and the ReplicaSet is that the latter one has a more advance semantics label selection for the Pods that were recently re-engineered in Kubernetes.</p>
<p>As a new product, Kuberentes is constantly changing (in fact, it is possible that by the time that you read this book, the core elements might have changed), so the engineers try to keep the compatibility across different versions so that people are not urged to upgrade in a short period of time.</p>
<p>Other examples of more advanced building blocks are the following:</p>
<ul>
<li>DaemonSet</li>
<li>PetSets</li>
<li>Jobs and CronJobs</li>
<li>CronJobs</li>
</ul>
<p>In order to go in deep to the full stack in Kubernetes, we would need a full book (or more!). Let's visit some of them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Daemon Sets</h1>
                </header>
            
            <article>
                
<p>Daemon Sets are an API element used to <strong>ensure that a Pod is running in all (or some) nodes</strong>. One of the assumptions in Kubernetes is that the pod should not worry about which node is being run, but that <span>said</span>, there might be a situation where we want to ensure that we run at least one pod on each node for a number of reasons:</p>
<ul>
<li>Collect logs</li>
<li>Check the hardware</li>
<li>Monitoring</li>
</ul>
<p>In order to do that, Kubernetes provides an API element called Daemon Set. Through a combination of labels and selectors, we can define something called <strong>affinity</strong>, which can be used to run our pods on certain nodes (we might have specific hardware requirements that only a few nodes are able to provide so that we can use tags and selectors to provide a hint to the pods to relocate to certain nodes).</p>
<p>Daemon Sets have several ways to be contacted, from the DNS through a headless service (a service that works as a load balancer instead of having a cluster IP assigned) to the node IP, but Daemon Sets work best when they are the initiators of the communication: something happens (an event) and a Daemon Set sends an event with information about that event (for example, a node is running low on space).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PetSets</h1>
                </header>
            
            <article>
                
<p><strong>PetSets</strong> are an interesting concept within Kubernetes: they are strong named resources whose naming is supposed to stay the same for a long term. As of now, a pod does not have a strong entity within a Kubernetes cluster: you need to create a service in order to locate a pod as they are ephemeral. Kubernetes can reschedule them at any time without prior notice for changing their name, as we have seen before. If you have a deployment running in Kubernetes and kill one of the pods, its name changes from (for example) <em>pod-xyz</em> to <em>pod-abc i</em>n an unpredictable way. so we cannot know which names to use in our application to connect to them beforehand.</p>
<p>When working with a Pet Set, this changes completely. A pet set has an ordinal order, so it is easy to guess the name of the pod. Let's say that we have deployed a Pet Set called mysql, which defines pods running a MySQL server. If we have three replicas, the naming will be as follows:</p>
<ul>
<li><kbd>mysql-0</kbd></li>
<li><kbd>mysql-1</kbd></li>
<li><kbd>mysql-2</kbd></li>
</ul>
<p>So, we can bake this knowledge in our application to reach them. This is suboptimal but good enough: we are still coupling services by name (DNS service discovery has this limitation), but it works in all cases and is a sacrifice that is worth paying for because in return, we get a lot of flexibility. The ideal situation in service discovery is where our system does not need to know even the name of the application carrying the work: just throw the message into the ether (the network) and the appropriated server will pick it up and respond accordingly.</p>
<p>Pet Sets have been replaced in later versions of Kubernetes with another item called <strong>Stateful Set.</strong> The Stateful Set is an improvement over the Pet Set mainly in how Kubernetes manages the <strong>master knowledge to avoid a split brain situation</strong>: where two different elements think that they are in control.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Jobs</h1>
                </header>
            
            <article>
                
<p>A <strong>Job</strong> in Kubernetes is basically an element that spawns the defined number of pods and waits for them to finish before completing its life cycle. It is very useful when there is a need to run a one-off task, such as rotating logs or migrating data across databases.</p>
<p>Cron jobs have the same concept as Jobs, but they get triggered by time instead of a one-off process.</p>
<p>Both in combination are very powerful tools to keep any system running. If you think about how we rotate logs without Kubernetes via ssh, it is quite risky: there is no control (by default) over who is doing what, and usually, there is no review process in the ssh operations carried by an individual.</p>
<p>With this approach, it is possible to create a Job and get other engineers to review it before running it for extra safety.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Secrets and configuration management</h1>
                </header>
            
            <article>
                
<p>On Docker in general, as of today, secrets are being passed into containers via environment variables. This is very insecure: first, there is no control over who can access what, and second, environment variables are not designed to act as secrets and a good amount of commercial software (and open source) outputs them into the standard output as part of bootstrapping. Needless to say, that's rather inconvenient.</p>
<p>Kubernetes has solved this problem quite gracefully: instead of passing an environment variable to our container, a volume is mounted with the secret on a file (or several) ready to be consumed.</p>
<p>By default, Kubernetes injects a few secrets related to the cluster into our containers so that they can interact with the API and so on, but it is also possible to create your own secrets.</p>
<p>There are two ways to create secrets:</p>
<ul>
<li>Using <kbd>kubectl</kbd></li>
<li>Defining an API element of type secret and using <kbd>kubectl</kbd> to deploy it</li>
</ul>
<p>The first way is fairly straightforward. Create a folder called <em>secrets</em> in your current work folder and execute the following commands inside it:</p>
<pre>echo -n "This is a secret" &gt; ./secret1.txt<br/>echo -n "This is another secret" &gt; ./secret2.txt</pre>
<p>This creates two files with two strings (simple strings as of now). Now it is time to create the secret in Kubernetes using <kbd>kubectl</kbd>:</p>
<pre><strong>kubectl create secret generic my-secrets --from-file=./secret1.txt --from-file=./secret2.txt</strong></pre>
<p>And that's it. Once we are done, we can query the secrets using <kbd>kubectl</kbd>:</p>
<pre><strong>kubectl get secrets</strong></pre>
<p>This, in my case, returns two secrets:</p>
<ul>
<li>A service account token injected by the cluster</li>
<li>My newly created secret (<kbd>my-secrets</kbd>)</li>
</ul>
<p>The second way of creating a secret is by defining it in a <kbd>yaml</kbd> file and deploying it via <kbd>kubectl</kbd>. Take a look at the following definition:</p>
<pre><strong>apiVersion: v1</strong><br/><strong>kind: Secret</strong><br/><strong>metadata:</strong><br/><strong>   name: my-secret-yaml</strong><br/><strong>type: Opaque</strong><br/><strong>data:</strong><br/><strong>   secret1: VGhpcyBpcyBhIHNlY3JldA==</strong><br/><strong>   secret2: VGhpcyBpcyBhbm90aGVyIHNlY3JldA==</strong></pre>
<p>First, the values for <kbd>secret1</kbd> and <kbd>secret2</kbd>, seem to be encrypted, but they are not; they are just encoded in <kbd>base64</kbd>:</p>
<pre><strong>echo -n "This is a secret" | base64</strong><br/><strong>echo -n "This is another secret" | base64</strong></pre>
<p>This will return the values that you can see here. The type of the secret is Opaque, which is the default type of secret, and the rest seems fairly straightforward. Now create the secret with kubectl (save the preceding content in a file called <kbd>secret.yml</kbd>):</p>
<pre><strong>kubectl create -f secret.yml</strong></pre>
<p>And that's it. If you query the secrets <span>again</span>, note that there should be a new one called <kbd>my-secret-yaml</kbd>. It is also possible to list and see the secrets in the dashboard on the <span class="packt_screen">Secrets</span> link in the menu on left-hand side.</p>
<p>Now it is time to use them. In order to use the secret, two things need to be done:</p>
<ul>
<li>Claim the secret as a volume</li>
<li>Mount the volume from the secret</li>
</ul>
<p>Let's take a look at a <kbd>Pod</kbd> using a secret:</p>
<pre>{<br/>   "apiVersion": "v1",<br/>   "kind": "Pod",<br/>   "metadata": {<br/>      "name": "test-secrets",<br/>      "namespace": "default"<br/>   },<br/>   "spec": {<br/>      "containers": [{<br/>         "name": "pod-with-secret",<br/>         "image": "nginx",<br/>         "volumeMounts": [{<br/>            "name": "secrets",<br/>            "mountPath": "/secrets",<br/>            "readOnly": true<br/>         }]<br/>      }],<br/>      "volumes": [{<br/>          "name": "secrets",<br/>          "secret": {<br/>              "secretName": "my-secret"<br/>          }<br/>      }]<br/>   }<br/>}</pre>
<p>So, you have learned a new thing here: <kbd>kubectl</kbd> also understands JSON. If you don't like YAML, it is possible to write your definitions in JSON without any side-effects.</p>
<p>Now, looking at the JSON file, we can see how first, the secret is declared as a volume and then how the secret is mounted in the path/secrets.</p>
<p>If you want to verify this, just run a command in your container to check it:</p>
<pre><strong>kubectl exec -it test-secrets ls /secrets</strong></pre>
<p>This should list the two files that we have created, <kbd>secret1.txt</kbd> and <kbd>secret2.txt</kbd>, containing the data that we have also specified.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes- moving on</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned enough to run simple applications in Kubernetes, but even though we cannot claim ourselves to be experts, we got the head start in becoming experts. Kubernetes is a project that evolves at the speed of light, and the best thing that you can do to keep yourself updated is follow the project on GitHub at <a href="https://github.com/kubernetes">https://github.com/kubernetes</a>.</p>
<p>The Kubernetes community is very responsive with issues raised by the users and are also very keen on getting people to contribute to the source code and documentation.</p>
<p>If you keep working with Kubernetes, some help will be required. The official documentation is quite complete, and even though it feels like it needs a reshuffle sometimes, it is usually enough to keep you going.</p>
<p>The best way that I've found to learn Kubernetes is by experimenting in Minikube (or a test cluster) before jumping into a bigger commitment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at a good amount of concepts required to deploy an application on Kubernetes. As mentioned earlier, it is impossible to cover everything abound Kubernetes in a single chapter, but with the amount of knowledge from this chapter, we are going to be able to set up a continuous delivery pipeline in the following chapter in a way that we automate zero downtime deployments without the big bang effect (the big deployment that stops the world), enabling our organization to move faster.</p>


            </article>

            
        </section>
    </body></html>