- en: Chapter 6. Multinode Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 多节点应用
- en: 'In this chapter, we are going to start playing with the really cool stuff.
    We are going to use all the skills we have learned in the book so far. We are
    really going to step it up by a notch. In this chapter, we are going to deploy
    four servers. We will look at how to cluster Consul, which will give us a perfect
    opportunity to further our modules'' functionality. In this chapter, we are going
    to look at two ways to network our containers. First, by using the standard host
    IP network, that our Consul cluster will communicate on. We will also install
    the **ELK** (**Elasticsearch**, **Logstash**, **and Kibana**) stack ([https://www.elastic.co/](https://www.elastic.co/)).
    To do this, we will be writing a module for each of the products. Because Elasticsearch
    is our data store in this solution, we want to hide it so that only Logstash and
    Kibana can access the application. We will accomplish this using the native Docker
    networking stack and isolate Elasticsearch using VXLAN. As you can see, we are
    going to get through a lot in this chapter. We will cover the following topics
    in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我们将开始使用一些非常酷的东西。我们将利用到目前为止在书中学到的所有技能，并在此基础上进一步提升。在这一章，我们将部署四台服务器。我们将研究如何将
    Consul 集群化，这将为我们提供一个完美的机会来进一步扩展我们的模块功能。在这一章中，我们将研究两种容器网络配置方式。首先是使用标准的主机 IP 网络，让我们的
    Consul 集群在该网络上进行通信。我们还将安装 **ELK**（**Elasticsearch**、**Logstash** 和 **Kibana**）堆栈
    ([https://www.elastic.co/](https://www.elastic.co/))。为此，我们将为每个产品编写一个模块。因为 Elasticsearch
    是我们解决方案中的数据存储，我们希望将其隐藏，只允许 Logstash 和 Kibana 访问该应用程序。我们将通过使用本地 Docker 网络堆栈，并通过
    VXLAN 隔离 Elasticsearch 来实现这一点。正如你所看到的，我们将在这一章中完成很多内容。我们将在这一章中涵盖以下主题：
- en: The design of our solution
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们解决方案的设计
- en: Putting it all together
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一切整合起来
- en: The design of our solution
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们解决方案的设计
- en: As there are a lot of moving parts in the solution, it would be best to visualize
    what we are going to be coding for. As this will be a big step up from our last
    chapter, we will break down the solution. In the first topic, we will look at
    the design for our Consul cluster.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于解决方案中涉及很多动态部分，最好可视化我们将要编写的代码。由于这是从上一章迈出的重要一步，我们将分解解决方案。在第一个主题中，我们将查看 Consul
    集群的设计。
- en: The Consul cluster
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Consul 集群
- en: 'In the design, we are going to use four servers: **node-01**, **node-02**,
    **node-03**, and **node-04**. We will use **node-01** to bootstrap our Consul
    cluster. We will add the other three nodes to the cluster as servers. They will
    be able to join the conciseness, vote, and replicate the key/value store. We will
    set up an IP network that is on the `172.17.8.0/24` network. We will map our container
    ports to the host ports that sit on the `172.17.8.0/24` network. The following
    image will show the network flow:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本设计中，我们将使用四台服务器：**node-01**、**node-02**、**node-03** 和 **node-04**。我们将使用 **node-01**
    来启动我们的 Consul 集群。然后将其他三台节点作为服务器添加到集群中。它们将能够加入集群、进行投票，并复制键值存储。我们将设置一个位于 `172.17.8.0/24`
    网络的 IP 网络，并将我们的容器端口映射到位于 `172.17.8.0/24` 网络上的主机端口。下图将展示网络流向：
- en: '![The Consul cluster](img/B05201_06_1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_1.jpg)'
- en: The ELK stack
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ELK 堆栈
- en: 'Now that we have our Consul cluster, we can look at what our ELK stack is going
    to look like. First, we will walk through the design of the network. The stack
    will be connected to the native Docker network. Note that I have not listed the
    IP addresses of our Docker network. The reason for this is that we will let the
    Docker daemon select the networking address range. For this solution, we are not
    going to route any traffic out of this network, so letting the daemon choose the
    IP range is fine. So you will also note that Elasticsearch is only connected to
    our Docker network. This is because we only want Logstash and Kibana. This is
    to keep other applications from being able to send requests or queries to Elasticsearch.
    You will note that both Logstash and Kibana are connected to both the Docker network
    and the host network. The reason for this is that we want applications to send
    their logs to Logstash, and we will want to access the Kibana web application:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了Consul集群，我们可以看看我们的ELK栈将是什么样子的。首先，我们将详细了解网络的设计。该栈将连接到本地Docker网络。请注意，我没有列出我们的Docker网络的IP地址。这样做的原因是我们将让Docker守护进程选择网络地址范围。对于这个解决方案，我们不打算将任何流量路由到这个网络之外，所以让守护进程选择IP范围是可以的。你还会注意到Elasticsearch只连接到我们的Docker网络。这是因为我们只希望Logstash和Kibana连接。这样做是为了防止其他应用程序能够向Elasticsearch发送请求或查询。你会注意到，Logstash和Kibana都连接到了Docker网络和主机网络。这样做的原因是我们希望应用程序将日志发送到Logstash，并且我们希望能够访问Kibana的Web应用程序。
- en: '![The ELK stack](img/B05201_06_2.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![ELK栈](img/B05201_06_2.jpg)'
- en: To get the full picture of the architecture, we just need to overlay both the
    diagrams. So, let's start coding!!!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面了解架构，我们只需要将两个图表叠加在一起。所以，让我们开始编写代码吧！
- en: Putting it all together
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合起来
- en: Now that we have looked at the design, let's put it all together. We will look
    at the changes to the plumbing of our Vagrant repo. Then, we will code the extra
    functionality into our `consul` module. We will then run our Vagrant repo to make
    sure that our Consul cluster is up and running. After we have completed that task,
    we will build the ELK stack, and we will build a module for each of the products.
    We will also set up Logstash to forward logs to node-03, just so we can test to
    make sure that our ELK stack is correct. Let's get some light on this.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了设计，接下来我们将把所有内容整合起来。我们将查看Vagrant仓库的管道更改。然后，我们会将额外的功能编码到`consul`模块中。接着，我们将运行Vagrant仓库，以确保Consul集群正常运行。在完成这项任务后，我们将构建ELK栈，并为每个产品构建一个模块。我们还将设置Logstash将日志转发到`node-03`，以便我们可以测试确保我们的ELK栈是正确的。让我们来深入了解一下这个。
- en: The server setup
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务器设置
- en: 'We will now look at the changes we are going to make to our new Vagrant repo.
    The first file that we are going to look at is our `servers.yaml` file. The first
    thing we need to do is change our base box. As we are going to be connecting containers
    to the native Docker network, our host machines must run a kernel version above
    3.19\. I have created a prebuilt vagrant box with just this configuration. It
    is the Puppetlabs box that we have been using in all the other chapters with the
    kernel updated to version 4.4:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看我们要对新的Vagrant仓库所做的更改。我们要查看的第一个文件是`servers.yaml`文件。我们需要做的第一件事是更改我们的基础盒子。由于我们将容器连接到本地Docker网络，我们的主机必须运行版本高于3.19的内核。我已经创建了一个预构建的vagrant盒子，正是这一配置。它是我们在所有其他章节中使用的Puppetlabs盒子，内核已更新至4.4版本：
- en: '![The server setup](img/B05201_06_3.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![服务器设置](img/B05201_06_3.jpg)'
- en: As you can note in the preceding screenshot, the other change that we have made
    to the `servers.yaml` file is that we have added entries to the `/etc/hosts` directory.
    We have done this to simulate a traditional DNS infrastructure. If this had been
    in your production environment, we wouldn't have needed to add that configuration.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的截图中看到的，我们对`servers.yaml`文件所做的另一个更改是我们已向`/etc/hosts`目录添加了条目。我们这样做是为了模拟传统的DNS基础设施。如果这是在你的生产环境中，我们就不需要添加这个配置。
- en: 'Now, we have to add the other three servers. The following screenshot will
    show exactly how it should look:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要添加另外三个服务器。以下截图将准确显示它应该是什么样子：
- en: '![The server setup](img/B05201_06_4.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![服务器设置](img/B05201_06_4.jpg)'
- en: So, the ports that we will hit once all our servers are built are `8500` on
    `node-01` (the Consul web UI `127.0.0.1:8500`) and `8081` (the Kibana web UI `127.0.0.1:8081`).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一旦所有服务器构建完成，我们将访问的端口是`8500`（在`node-01`上，即Consul的Web UI `127.0.0.1:8500`）和`8081`（Kibana的Web
    UI `127.0.0.1:8081`）。
- en: The Consul cluster
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Consul集群
- en: 'We are well aquatinted with the `consul` module now, but we are going to take
    it to the next level. For this chapter, we are just going to use the compose version.
    The reason why is because when you start getting into more complex applications
    or need to use an `if` statement to add logic, an `.erb` file gives us that freedom.
    There are a fair few changes to this module. So, let''s start again at our `params.pp`
    file:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经非常熟悉`consul`模块，但我们将把它提升到一个新的水平。在这一章中，我们将只使用compose版本。原因是当你开始处理更复杂的应用程序或需要使用`if`语句来添加逻辑时，`.erb`文件为我们提供了这样的自由。这一模块有相当多的更改。那么，让我们从`params.pp`文件重新开始：
- en: '![The Consul cluster](img/B05201_06_5.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_5.jpg)'
- en: As you can see, we have added two new parameters. The first one is `$consul_master_ip`
    and the other parameter is `$consul_is_master`. We will use this to define which
    server will bootstrap our Consul cluster and which server will join the cluster.
    We have hardcoded the hostname of **node-01**. If this was a production module,
    I would not hardcode a hostname that should be a parameter that is looked up in
    Hiera ([https://docs.puppetlabs.com/hiera/3.0/](https://docs.puppetlabs.com/hiera/3.0/)).
    We will pick up on this again when we look at our `docker-compose.yml.erb` file.
    The other parameters should look very familiar to you.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们添加了两个新参数。第一个是`$consul_master_ip`，另一个是`$consul_is_master`。我们将使用它来定义哪个服务器将引导我们的Consul集群，哪个服务器将加入集群。我们已将**node-01**的主机名硬编码。如果这是一个生产模块，我不会硬编码主机名，它应该是一个参数，可以在Hiera中查找（[https://docs.puppetlabs.com/hiera/3.0/](https://docs.puppetlabs.com/hiera/3.0/)）。当我们查看`docker-compose.yml.erb`文件时，我们会再次提到这一点。其他参数应该对您来说很熟悉。
- en: 'Next, let''s look at our `init.pp` file:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下我们的`init.pp`文件：
- en: '![The Consul cluster](img/B05201_06_6.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_6.jpg)'
- en: As you can see here, we have not changed this file much, as we have added a
    Boolean (`$consul_is_master`). However, we will want to validate the input. We
    do this by calling the stdlib function, `validate_bool`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这里看到的，我们没有对这个文件做太多更改，因为我们仅添加了一个布尔值（`$consul_is_master`）。然而，我们仍然需要验证输入。我们通过调用标准库函数`validate_bool`来完成这项工作。
- en: 'Let''s quickly browse through the `install.pp` file:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下`install.pp`文件：
- en: '![The Consul cluster](img/B05201_06_7.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_7.jpg)'
- en: 'Now, let''s look at the `network.pp` file:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下`network.pp`文件：
- en: '![The Consul cluster](img/B05201_06_8.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_8.jpg)'
- en: 'Finally, we will look at the `package.pp` file:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将查看`package.pp`文件：
- en: '![The Consul cluster](img/B05201_06_9.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_9.jpg)'
- en: As you can see, we have not made any changes to these files. Now, we can look
    at the file that will really have the logic for deploying our container. We will
    then move to our `templates` folder and look at our `docker-compose.yml.erb` file.
    This is where most of the changes in the module have been made.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们并没有对这些文件进行更改。现在，我们可以查看将真正部署容器逻辑的文件。接下来，我们将转到`templates`文件夹，查看我们的`docker-compose.yml.erb`文件。这是模块中大多数更改的地方。
- en: 'So, let''s look at the contents of the file, which are shown in the following
    screenshot:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来看一下文件的内容，如下图所示：
- en: '![The Consul cluster](img/B05201_06_10.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_10.jpg)'
- en: 'So as you can see, the code in this file has doubled. Let''s break it down
    into three pieces, as shown in the following screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，这个文件中的代码已被复制。我们将其分为三部分，如下图所示：
- en: '![The Consul cluster](img/B05201_06_11.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_11.jpg)'
- en: 'In the first block of code, the first change that you will note is the `if`
    statement. This is a choice to determine whether the node will be a master for
    bootstrapping Consul or a server in the cluster. If you remember from our `params.pp`
    file, we set `node-01` as our master. When we apply this class to our node, if
    its `node-01`, it will bootstrap the cluster. The next line that we want to pay
    attention to is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一块代码中，您会注意到的第一个变化是`if`语句。这是一个用于确定节点是否为Consul引导主节点或集群中的服务器的选择。如果您还记得我们的`params.pp`文件，我们将`node-01`设置为我们的主节点。当我们将这个类应用到节点时，如果是`node-01`，它将引导集群。我们接下来要关注的行如下：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We should just take note to compare the same line in the next block of code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意对比下一块代码中的相同一行：
- en: '![The Consul cluster](img/B05201_06_12.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_12.jpg)'
- en: 'First, we can see that this is `elsif`, the second half of our `if` statement.
    So, this will be the block of code that will install Consul on the other three
    nodes. They will still be servers in the cluster. They will just not have the
    job of bootstrapping the cluster. We can tell this from the following line:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到这是`elsif`，是`if`语句的后半部分。因此，这将是安装 Consul 到其他三个节点的代码块。它们仍然是集群中的服务器，只是没有启动集群的任务。我们可以通过以下一行代码来判断这一点：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Remember earlier that we looked at the same line from the first line of code.
    You see the difference? In block one, we declare `-bootstrap-expect <%= @consul_bootstrap_expect
    %>`, and in the second block, we declare `-join <%= @consul_master_ip %>`. By
    looking at the code, this is how we can tell the bootstrap order. Lastly, we can
    see that we are declaring `<% end -%>` to close the `if` statement.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们之前看过代码的第一行吗？你看到了区别吗？在第一块代码中，我们声明了`-bootstrap-expect <%= @consul_bootstrap_expect
    %>`，而在第二块代码中，我们声明了`-join <%= @consul_master_ip %>`。通过查看代码，我们可以判断启动顺序。最后，我们可以看到，我们声明了`<%
    end -%>`来结束`if`语句。
- en: 'Now, let''s look at the last block of code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看最后一段代码：
- en: '![The Consul cluster](img/B05201_06_13.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_13.jpg)'
- en: 'As you can see, it''s going to deploy the `registrator` container. As this
    sits outside the `if` statement, this container will be deployed on any node that
    the `consul` class is applied to. We have made a lot of progress till now. We
    should check the module changes before moving on to creating our new elastic modules.
    The one last thing we need to change is our `default.pp` manifest file, which
    is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它将部署`registrator`容器。由于这个容器位于`if`语句之外，它将部署到所有应用了`consul`类的节点上。到目前为止，我们已经取得了很大进展。在继续创建新的弹性模块之前，我们应该先检查一下模块的更改。我们最后需要更改的是`default.pp`清单文件，内容如下：
- en: '![The Consul cluster](img/B05201_06_14.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_14.jpg)'
- en: As you can see, we have a node declaration for each node and have applied the
    `consul` class. Now, let's open our terminal change directory to the root of our
    Vagrant repo and issue the `vagrant up` command. This time, it will download a
    new base box from Hashicloud. So, depending on your Internet connection, this
    could take some time. Remember that the reason we need this new box is that it
    has an updated kernel to take advantage of the native Docker networking. In the
    last chapter, we were able to create a network, but we weren't able to connect
    containers to it. In this chapter, we will. Also, we are going to build four servers,
    so running Vagrant should take about 5 minutes. As soon as our first machine is
    up, we can log on to the consul web UI. There we can watch the progress as each
    node is joined.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个节点都有一个节点声明，并且都应用了`consul`类。现在，让我们打开终端，切换到 Vagrant 仓库的根目录，并执行`vagrant
    up`命令。这一次，它将从 Hashicloud 下载一个新的基础框。根据你的网络连接情况，可能需要一些时间。请记住，我们需要这个新框的原因是，它具有更新的内核，可以利用本地的
    Docker 网络。在上一章中，我们能够创建一个网络，但无法将容器连接到它。在本章中，我们将能做到这一点。同时，我们将构建四台服务器，所以运行 Vagrant
    应该需要大约 5 分钟。一旦我们的第一台机器启动，我们就可以登录到 Consul Web UI。在那里，我们可以看到每个节点加入的进度。
- en: 'As you can see in the following screenshot, our cluster is bootstrapped:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们的集群已经启动：
- en: '![The Consul cluster](img/B05201_06_15.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_15.jpg)'
- en: 'We can also check whether all our services are up and stable on the **SERVICES**
    tabs, as shown in this screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过查看**SERVICES**标签，检查所有服务是否已启动并稳定，如下图所示：
- en: '![The Consul cluster](img/B05201_06_16.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_16.jpg)'
- en: 'As you can see in the following screenshot, our second node has checked in:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们的第二个节点已成功加入：
- en: '![The Consul cluster](img/B05201_06_17.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_17.jpg)'
- en: 'The following screenshot shows what the screen looks like when we go to our
    **SERVICES** tab:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了当我们进入**SERVICES**标签时屏幕的样子：
- en: '![The Consul cluster](img/B05201_06_18.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_18.jpg)'
- en: As you can see, our services have doubled. So things are looking good.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的服务数量已经翻倍。因此，情况看起来不错。
- en: 'Now, the `vagrant up` command is complete, and our terminal output should look
    like the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`vagrant up`命令已完成，我们的终端输出应该如下图所示：
- en: '![The Consul cluster](img/B05201_06_19.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_19.jpg)'
- en: 'Let''s log back into our web browser to our Consul UI (`127.0.0.1:8500`). Under
    our **NODES** tab, we should now see all four nodes:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新登录到我们的浏览器，访问 Consul UI（`127.0.0.1:8500`）。在**NODES**标签下，我们现在应该能看到所有四个节点：
- en: '![The Consul cluster](img/B05201_06_20.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_20.jpg)'
- en: 'We can see that our cluster is in a good state as all four nodes have the same
    amount of services, which is **10**, and that all services are green. The last
    thing that we need to check is our DNS service discovery. So lets login to one
    of our boxes. We will choose **node-03**. So in our terminal, let''s issue the
    `vagrant ssh node-03` command. We need to specify the node now as we have more
    than one vagrant box. We are going to ping the Consul service 8500\. So, we just
    issue the `ping consul-8500.service.consul` command. The terminal output should
    look like the following screenshot:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的集群状态良好，因为所有四个节点的服务数量相同，都是**10**个，并且所有服务都是绿色的。我们需要检查的最后一项是我们的 DNS 服务发现。现在让我们登录到其中一台机器。我们选择**node-03**。在终端中，我们输入命令`vagrant
    ssh node-03`。我们需要指定节点，因为我们有多个 vagrant 主机。接下来，我们将 ping Consul 服务 8500。我们只需输入命令`ping
    consul-8500.service.consul`。终端输出应该像下面的截图一样：
- en: '![The Consul cluster](img/B05201_06_21.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_21.jpg)'
- en: 'This is now working perfectly. So, let''s check one more thing now. We need
    to make sure that our Docker network is configured. For that, we will need to
    change the directory to root (`sudo -i`) and then issue the `docker network ls`
    command, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它工作得非常完美。那么，让我们再检查一件事。我们需要确保我们的 Docker 网络已配置好。为此，我们需要切换到 root 目录（`sudo -i`），然后输入`docker
    network ls`命令，如下所示：
- en: '![The Consul cluster](img/B05201_06_22.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Consul 集群](img/B05201_06_22.jpg)'
- en: Now that everything is up and running, let's move on to our ELK stack.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都已启动并运行，让我们继续进行 ELK 堆栈的配置。
- en: The ELK stack
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ELK 堆栈
- en: One of the focuses I had when planning this book was to use examples that could
    be ported to the real world so that readers get some real value. The ELK stack
    is no exception. The ELK stack is a very powerful stack of applications that lets
    you collate all your application logs to see the health of your application. For
    more reading on the ELK stack, visit [https://www.elastic.co/](https://www.elastic.co/).
    It has great documentation on all the products. Now, let's start our first new
    module.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在规划这本书时，我的一个重点是使用能够应用于现实世界的例子，以便读者能够获得一些实际价值。ELK 堆栈也不例外。ELK 堆栈是一套非常强大的应用程序，可以帮助你汇总所有的应用程序日志，以查看应用程序的健康状况。想了解更多
    ELK 堆栈的信息，请访问[https://www.elastic.co/](https://www.elastic.co/)。该网站有关于所有产品的优秀文档。现在，让我们开始我们的第一个新模块。
- en: As per our design, there is an order in which we need to install the ELK stack.
    Seeing as both Logstash and Kibana depend on Elasticsearch, we will build it first.
    All the images that we are going to use in our modules are built, maintained,
    and released by Elasticsearch. So we can be sure that the quality is good. The
    first thing that we need to do is create a new module called `<AUTHOR>-elasticsearch`.
    We covered how to create a module in the previous chapter, so if you are unsure
    how to do this, go back and read that chapter again. Now that we have our module,
    let's move it into the modules directory in the root of our Vagrant repo.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，安装 ELK 堆栈是有顺序的。由于 Logstash 和 Kibana 都依赖于 Elasticsearch，我们将首先构建 Elasticsearch。我们将在模块中使用的所有镜像都是由
    Elasticsearch 构建、维护并发布的，因此我们可以确保质量是可靠的。我们首先需要做的是创建一个名为`<AUTHOR>-elasticsearch`的新模块。我们在上一章中介绍了如何创建模块，如果你不确定如何操作，可以回去阅读那一章。现在我们已经有了模块，让我们把它移到
    Vagrant 仓库根目录下的模块目录中。
- en: 'Seeing as the containers are already built by elastic, these modules are going
    to be short and sweet. We are just going to add code to the `init.pp` file:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些容器已经由 Elasticsearch 构建，这些模块将会简短明了。我们只需要向`init.pp`文件添加代码：
- en: '![The ELK stack](img/B05201_06_23.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![ELK 堆栈](img/B05201_06_23.jpg)'
- en: As you can see, we are calling the `docker::image` class to download `ealsticsearch`.
    In the `docker::run` class, we are calling our `elasticearch` container, and we
    are going to bind our container only to the `docker-internal` Docker network.
    You will note that we are not binding any ports. This is because, by default,
    this container will expose port 9200\. We only want to expose port 9200 on the
    Docker network. Docker is smart enough to allow the exposed ports automatically
    on a Docker native network. In the next resource we are declaring just the host
    network for `elasticsearch`. We are specifying `0.0.0.0` as we don't know the
    IP that the container is going to get from the Docker network. As this service
    will be hidden from the outside world, this configuration will be fine. We will
    then map a persistent drive to keep our data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们正在调用 `docker::image` 类来下载 `elasticsearch`。在 `docker::run` 类中，我们调用我们的
    `elasticsearch` 容器，并且我们将只将容器绑定到 `docker-internal` Docker 网络。你会注意到我们没有绑定任何端口。这是因为，默认情况下，这个容器将暴露
    9200 端口。我们只希望在 Docker 网络上暴露 9200 端口。Docker 足够智能，能够自动允许在 Docker 本地网络上暴露端口。在下一个资源中，我们只声明了
    `elasticsearch` 的主机网络。我们指定了 `0.0.0.0`，因为我们不知道容器从 Docker 网络中获得的 IP 地址。由于该服务将对外界隐藏，因此这个配置是没问题的。然后，我们将映射一个持久化的驱动器来保存我们的数据。
- en: 'The next thing that we need to do now is to add `elasticsearch` to a node.
    As per our design, we will add `elasticsearch` to `node-02`. We do this in our
    `default.pp` file in our `manifests` directory, as shown in the following screenshot:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是将 `elasticsearch` 添加到一个节点中。根据我们的设计，我们将把 `elasticsearch` 添加到 `node-02`。我们在
    `manifests` 目录下的 `default.pp` 文件中进行此操作，如下图所示：
- en: '![The ELK stack](img/B05201_06_24.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![ELK 堆栈](img/B05201_06_24.jpg)'
- en: You will note that I have used `contain` instead of `include`. This is because
    I want to make sure that the `consul` class is applied before the `elasticsearch`
    class, as we need our Docker network to be there before `elasticsearch` comes
    up. If the network is not there, our catalogue will fail as the container will
    not build.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我使用了 `contain` 而不是 `include`。这是因为我希望确保 `consul` 类在 `elasticsearch` 类之前被应用，因为我们需要在
    `elasticsearch` 启动之前，确保 Docker 网络已存在。如果网络不存在，我们的目录将无法构建，因为容器无法启动。
- en: 'The next module we are going to write is `logstash`. Our `logstash` module
    is going to be a little more complicated, as it will be on both the Docker network
    and the host network. The reason we want it on both is because we want applications
    to forward their logs to `logstash`. We also need `logstash` to talk to `elasticsearch`.
    Thus, we add `logstash` to the Docker network as well. We will create the module
    in the same way we did for `elasticsearch`. We will call our `<AUTHOR>-logstash`
    module. So, let''s look at our code in our `init.pp` file, which is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要编写的模块是 `logstash`。我们的 `logstash` 模块将稍微复杂一些，因为它既会在 Docker 网络上，也会在主机网络上。我们之所以要在这两个网络上都部署它，是因为我们希望应用程序能将日志转发到
    `logstash`。我们还需要 `logstash` 与 `elasticsearch` 进行通信。因此，我们还将 `logstash` 添加到 Docker
    网络中。我们将以与 `elasticsearch` 相同的方式创建该模块。我们将把模块命名为 `<AUTHOR>-logstash`。接下来，让我们看一下
    `init.pp` 文件中的代码，如下所示：
- en: '![The ELK stack](img/B05201_06_25.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![ELK 堆栈](img/B05201_06_25.jpg)'
- en: Here, the first thing you will note is that we are creating a directory. This
    is to map to the container and will contain our `logstash.conf` file. The next
    declaration is a file type. This is our `logstah.conf` file, and as we can see,
    it's a template from the code. So, let's come back to it after looking at the
    rest of the code in our `init.pp` file. The next line of code will pull our `logstash`
    image from Docker Hub. In the `docker::run` class we will call our `logstash`
    container, use the `logstash` image, and attach the container to our `docker-internal`
    Docker network.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，首先你会注意到我们正在创建一个目录。这是为了映射到容器，并且将包含我们的 `logstash.conf` 文件。接下来的声明是文件类型。这是我们的
    `logstash.conf` 文件，正如我们所看到的，它是来自代码的模板。那么，在查看完 `init.pp` 文件中的其余代码后，我们再回来看它。下一行代码将从
    Docker Hub 拉取我们的 `logstash` 镜像。在 `docker::run` 类中，我们将调用我们的 `logstash` 容器，使用 `logstash`
    镜像，并将容器附加到我们的 `docker-internal` Docker 网络。
- en: The next line of code will tell `logstash` to start using our `logstash.conf`
    file; we will then mount the directory we created earlier in our `init.pp` file
    to the container. Now, you can see in this module that we've exposed ports to
    the host network. In the last line, we tell `logstash` about our Elasticsearch
    host and Elasticsearch port. How does Logstash know where Elasticsearch is? We
    are not linking the containers like we did in previous chapters. This works in
    the same way to when we named our Elasticsearch container `elasticsearch`, and
    our Docker network has an inbuilt DNS server that lives at the address `127.0.0.11`.
    Any container that joins that network will register itself as its container name.
    This is how services on the `docker-internal` network find each other.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码将告诉`logstash`开始使用我们的`logstash.conf`文件；然后，我们会将之前在`init.pp`文件中创建的目录挂载到容器中。现在，您可以在这个模块中看到我们已经将端口暴露到主机网络。在最后一行，我们告诉`logstash`我们的Elasticsearch主机和端口。Logstash是如何知道Elasticsearch的位置的呢？我们并没有像前面章节中那样连接容器。这与我们将Elasticsearch容器命名为`elasticsearch`的方式是一样的，我们的Docker网络内置了一个DNS服务器，地址是`127.0.0.11`。任何加入该网络的容器都会以其容器名称注册自己。这就是`docker-internal`网络上的服务如何互相发现的方式。
- en: The last thing we need to look at is our template file for our `logstash.conf`
    file that we declared in our `init.pp` file. So, create a new folder called `templates`
    in the root of our module and then a file called `logstash.conf.erb`. We will
    add the following configuration to accept logs from syslog and Docker.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要查看的最后一件事是我们在`init.pp`文件中声明的`logstash.conf`文件的模板文件。所以，在我们模块的根目录中创建一个名为`templates`的新文件夹，然后创建一个名为`logstash.conf.erb`的文件。我们将添加以下配置，以接受来自syslog和Docker的日志。
- en: 'Lastly, at the bottom, we put our Elasticsearch configuration, as shown in
    this screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在底部，我们将我们的Elasticsearch配置放在这里，如下截图所示：
- en: '![The ELK stack](img/B05201_06_26.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_26.jpg)'
- en: Now, let's add our `logstash` module to `node-03` in the same way that we did
    with our `elastcsearch` module.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按照之前在`elasticsearch`模块中做过的方式将`logstash`模块添加到`node-03`中。
- en: '![The ELK stack](img/B05201_06_27.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_27.jpg)'
- en: Again, we will use `contain` instead of `include`. Now it's time to move on
    to our last module. We will create this in the same way as we have done for the
    last two modules. We will call this module `<AUTHOR>-kibana`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用`contain`而不是`include`。现在是时候继续创建我们的最后一个模块了。我们将像前两个模块一样创建它。我们将这个模块命名为`<AUTHOR>-kibana`。
- en: 'In Kibana, we will only be adding code to the `init.pp` file, as shown in the
    following screenshot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kibana中，我们只会向`init.pp`文件中添加代码，如下截图所示：
- en: '![The ELK stack](img/B05201_06_28.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_28.jpg)'
- en: As you can see, we are downloading the `kibana` image. In the `docker::run`
    class, we are calling our `kibana` container using the `kibana` image, attaching
    the container to our local Docker network. In the next line, we are mapping the
    container port `5601` (Kibana's default port) to port `80` on the host. This is
    just for ease of use for our lab. In the last line, we are telling `kibana` how
    to connect to `elasticsearch`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们正在下载`kibana`镜像。在`docker::run`类中，我们使用`kibana`镜像调用我们的`kibana`容器，并将该容器连接到本地Docker网络。在下一行，我们将容器端口`5601`（Kibana的默认端口）映射到主机的`80`端口。这只是为了方便我们实验室的使用。在最后一行，我们告诉`kibana`如何连接到`elasticsearch`。
- en: 'Let''s add `kibana` to `node-04` again using `contain` instead of `include`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用`contain`而不是`include`，将`kibana`添加到`node-04`：
- en: '![The ELK stack](img/B05201_06_29.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_29.jpg)'
- en: We are now ready to run our Vagrant environment. Let's open our terminal and
    change the directory to the root of our Vagrant repo. We will build this completely
    from scratch, so let's issue the `vagrant destroy -f && vagrant up` command.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好运行我们的Vagrant环境了。让我们打开终端并将目录切换到Vagrant仓库的根目录。我们将从零开始构建，所以让我们发出`vagrant
    destroy -f && vagrant up`命令。
- en: 'This will take about 5 minutes or so to build, depending on your Internet connection,
    so please be patient. Once the build is complete, our terminal should have no
    errors and look like the following screenshot:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程大约需要5分钟左右，具体取决于您的互联网连接速度，请耐心等待。一旦构建完成，我们的终端应该没有错误，并且看起来像以下截图：
- en: '![The ELK stack](img/B05201_06_30.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_30.jpg)'
- en: 'The next thing we will check is our Consul web UI (`127.0.0.1:8500`):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要检查的是我们的Consul Web UI（`127.0.0.1:8500`）：
- en: '![The ELK stack](img/B05201_06_31.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_31.jpg)'
- en: 'In the preceding screenshot, you can see that our Logstash and Kibana services
    are there, but where is Elasticsearch ? Don''t worry, Elasticsearch is there,
    but we can''t see it in Consul as we have not forwarded any ports to the host
    network. Registrator will only register services with exposed ports. We can make
    sure that our ELK stack is configured by logging in to our Kibana web UI (`127.0.0.1:8080`):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的截图中，你可以看到我们的Logstash和Kibana服务已经运行，但Elasticsearch在哪里呢？别担心，Elasticsearch已经在那儿，只是因为我们没有将任何端口转发到主机网络，所以在Consul中看不到它。Registrator只会注册具有暴露端口的服务。我们可以通过登录到Kibana的Web
    UI (`127.0.0.1:8080`) 来确认我们的ELK堆栈是否配置正确：
- en: '![The ELK stack](img/B05201_06_32.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_32.jpg)'
- en: 'The next thing we need to do is click on the **Create** button. Then, if we
    go to the **Discover** tab, we can see the logs from Logstash:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是点击**创建**按钮。然后，如果我们进入**发现**标签页，我们可以看到来自Logstash的日志：
- en: '![The ELK stack](img/B05201_06_33.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![ELK堆栈](img/B05201_06_33.jpg)'
- en: Logstash's logs
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash的日志
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at how to use Puppet to deploy containers across
    multiple nodes. We took advantage of the native Docker networking to hide services.
    This is a good security practice when working with production environments. The
    only issue with this chapter is that we don't have any failover or resiliency
    in our applications. This is why container schedulers are so important.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了如何使用Puppet在多个节点上部署容器。我们利用了本地的Docker网络来隐藏服务。这在处理生产环境时是一种很好的安全实践。本章唯一的问题是我们的应用程序没有任何故障转移或弹性处理。这就是容器调度器如此重要的原因。
- en: In the next chapter, we will drive into three different schedulers to arm you
    with the knowledge that you will need to make sound design choices in the future.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨三种不同的调度器，为你提供未来做出合理设计决策所需的知识。
