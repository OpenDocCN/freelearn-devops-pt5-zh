- en: Chapter 6. Multinode Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to start playing with the really cool stuff.
    We are going to use all the skills we have learned in the book so far. We are
    really going to step it up by a notch. In this chapter, we are going to deploy
    four servers. We will look at how to cluster Consul, which will give us a perfect
    opportunity to further our modules'' functionality. In this chapter, we are going
    to look at two ways to network our containers. First, by using the standard host
    IP network, that our Consul cluster will communicate on. We will also install
    the **ELK** (**Elasticsearch**, **Logstash**, **and Kibana**) stack ([https://www.elastic.co/](https://www.elastic.co/)).
    To do this, we will be writing a module for each of the products. Because Elasticsearch
    is our data store in this solution, we want to hide it so that only Logstash and
    Kibana can access the application. We will accomplish this using the native Docker
    networking stack and isolate Elasticsearch using VXLAN. As you can see, we are
    going to get through a lot in this chapter. We will cover the following topics
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The design of our solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The design of our solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As there are a lot of moving parts in the solution, it would be best to visualize
    what we are going to be coding for. As this will be a big step up from our last
    chapter, we will break down the solution. In the first topic, we will look at
    the design for our Consul cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Consul cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the design, we are going to use four servers: **node-01**, **node-02**,
    **node-03**, and **node-04**. We will use **node-01** to bootstrap our Consul
    cluster. We will add the other three nodes to the cluster as servers. They will
    be able to join the conciseness, vote, and replicate the key/value store. We will
    set up an IP network that is on the `172.17.8.0/24` network. We will map our container
    ports to the host ports that sit on the `172.17.8.0/24` network. The following
    image will show the network flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ELK stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have our Consul cluster, we can look at what our ELK stack is going
    to look like. First, we will walk through the design of the network. The stack
    will be connected to the native Docker network. Note that I have not listed the
    IP addresses of our Docker network. The reason for this is that we will let the
    Docker daemon select the networking address range. For this solution, we are not
    going to route any traffic out of this network, so letting the daemon choose the
    IP range is fine. So you will also note that Elasticsearch is only connected to
    our Docker network. This is because we only want Logstash and Kibana. This is
    to keep other applications from being able to send requests or queries to Elasticsearch.
    You will note that both Logstash and Kibana are connected to both the Docker network
    and the host network. The reason for this is that we want applications to send
    their logs to Logstash, and we will want to access the Kibana web application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To get the full picture of the architecture, we just need to overlay both the
    diagrams. So, let's start coding!!!
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have looked at the design, let's put it all together. We will look
    at the changes to the plumbing of our Vagrant repo. Then, we will code the extra
    functionality into our `consul` module. We will then run our Vagrant repo to make
    sure that our Consul cluster is up and running. After we have completed that task,
    we will build the ELK stack, and we will build a module for each of the products.
    We will also set up Logstash to forward logs to node-03, just so we can test to
    make sure that our ELK stack is correct. Let's get some light on this.
  prefs: []
  type: TYPE_NORMAL
- en: The server setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now look at the changes we are going to make to our new Vagrant repo.
    The first file that we are going to look at is our `servers.yaml` file. The first
    thing we need to do is change our base box. As we are going to be connecting containers
    to the native Docker network, our host machines must run a kernel version above
    3.19\. I have created a prebuilt vagrant box with just this configuration. It
    is the Puppetlabs box that we have been using in all the other chapters with the
    kernel updated to version 4.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The server setup](img/B05201_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can note in the preceding screenshot, the other change that we have made
    to the `servers.yaml` file is that we have added entries to the `/etc/hosts` directory.
    We have done this to simulate a traditional DNS infrastructure. If this had been
    in your production environment, we wouldn't have needed to add that configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to add the other three servers. The following screenshot will
    show exactly how it should look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The server setup](img/B05201_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, the ports that we will hit once all our servers are built are `8500` on
    `node-01` (the Consul web UI `127.0.0.1:8500`) and `8081` (the Kibana web UI `127.0.0.1:8081`).
  prefs: []
  type: TYPE_NORMAL
- en: The Consul cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are well aquatinted with the `consul` module now, but we are going to take
    it to the next level. For this chapter, we are just going to use the compose version.
    The reason why is because when you start getting into more complex applications
    or need to use an `if` statement to add logic, an `.erb` file gives us that freedom.
    There are a fair few changes to this module. So, let''s start again at our `params.pp`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have added two new parameters. The first one is `$consul_master_ip`
    and the other parameter is `$consul_is_master`. We will use this to define which
    server will bootstrap our Consul cluster and which server will join the cluster.
    We have hardcoded the hostname of **node-01**. If this was a production module,
    I would not hardcode a hostname that should be a parameter that is looked up in
    Hiera ([https://docs.puppetlabs.com/hiera/3.0/](https://docs.puppetlabs.com/hiera/3.0/)).
    We will pick up on this again when we look at our `docker-compose.yml.erb` file.
    The other parameters should look very familiar to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at our `init.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see here, we have not changed this file much, as we have added a
    Boolean (`$consul_is_master`). However, we will want to validate the input. We
    do this by calling the stdlib function, `validate_bool`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s quickly browse through the `install.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the `network.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we will look at the `package.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have not made any changes to these files. Now, we can look
    at the file that will really have the logic for deploying our container. We will
    then move to our `templates` folder and look at our `docker-compose.yml.erb` file.
    This is where most of the changes in the module have been made.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s look at the contents of the file, which are shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So as you can see, the code in this file has doubled. Let''s break it down
    into three pieces, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the first block of code, the first change that you will note is the `if`
    statement. This is a choice to determine whether the node will be a master for
    bootstrapping Consul or a server in the cluster. If you remember from our `params.pp`
    file, we set `node-01` as our master. When we apply this class to our node, if
    its `node-01`, it will bootstrap the cluster. The next line that we want to pay
    attention to is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We should just take note to compare the same line in the next block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we can see that this is `elsif`, the second half of our `if` statement.
    So, this will be the block of code that will install Consul on the other three
    nodes. They will still be servers in the cluster. They will just not have the
    job of bootstrapping the cluster. We can tell this from the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Remember earlier that we looked at the same line from the first line of code.
    You see the difference? In block one, we declare `-bootstrap-expect <%= @consul_bootstrap_expect
    %>`, and in the second block, we declare `-join <%= @consul_master_ip %>`. By
    looking at the code, this is how we can tell the bootstrap order. Lastly, we can
    see that we are declaring `<% end -%>` to close the `if` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the last block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, it''s going to deploy the `registrator` container. As this
    sits outside the `if` statement, this container will be deployed on any node that
    the `consul` class is applied to. We have made a lot of progress till now. We
    should check the module changes before moving on to creating our new elastic modules.
    The one last thing we need to change is our `default.pp` manifest file, which
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have a node declaration for each node and have applied the
    `consul` class. Now, let's open our terminal change directory to the root of our
    Vagrant repo and issue the `vagrant up` command. This time, it will download a
    new base box from Hashicloud. So, depending on your Internet connection, this
    could take some time. Remember that the reason we need this new box is that it
    has an updated kernel to take advantage of the native Docker networking. In the
    last chapter, we were able to create a network, but we weren't able to connect
    containers to it. In this chapter, we will. Also, we are going to build four servers,
    so running Vagrant should take about 5 minutes. As soon as our first machine is
    up, we can log on to the consul web UI. There we can watch the progress as each
    node is joined.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following screenshot, our cluster is bootstrapped:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also check whether all our services are up and stable on the **SERVICES**
    tabs, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the following screenshot, our second node has checked in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows what the screen looks like when we go to our
    **SERVICES** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our services have doubled. So things are looking good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the `vagrant up` command is complete, and our terminal output should look
    like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s log back into our web browser to our Consul UI (`127.0.0.1:8500`). Under
    our **NODES** tab, we should now see all four nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that our cluster is in a good state as all four nodes have the same
    amount of services, which is **10**, and that all services are green. The last
    thing that we need to check is our DNS service discovery. So lets login to one
    of our boxes. We will choose **node-03**. So in our terminal, let''s issue the
    `vagrant ssh node-03` command. We need to specify the node now as we have more
    than one vagrant box. We are going to ping the Consul service 8500\. So, we just
    issue the `ping consul-8500.service.consul` command. The terminal output should
    look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is now working perfectly. So, let''s check one more thing now. We need
    to make sure that our Docker network is configured. For that, we will need to
    change the directory to root (`sudo -i`) and then issue the `docker network ls`
    command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Consul cluster](img/B05201_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that everything is up and running, let's move on to our ELK stack.
  prefs: []
  type: TYPE_NORMAL
- en: The ELK stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the focuses I had when planning this book was to use examples that could
    be ported to the real world so that readers get some real value. The ELK stack
    is no exception. The ELK stack is a very powerful stack of applications that lets
    you collate all your application logs to see the health of your application. For
    more reading on the ELK stack, visit [https://www.elastic.co/](https://www.elastic.co/).
    It has great documentation on all the products. Now, let's start our first new
    module.
  prefs: []
  type: TYPE_NORMAL
- en: As per our design, there is an order in which we need to install the ELK stack.
    Seeing as both Logstash and Kibana depend on Elasticsearch, we will build it first.
    All the images that we are going to use in our modules are built, maintained,
    and released by Elasticsearch. So we can be sure that the quality is good. The
    first thing that we need to do is create a new module called `<AUTHOR>-elasticsearch`.
    We covered how to create a module in the previous chapter, so if you are unsure
    how to do this, go back and read that chapter again. Now that we have our module,
    let's move it into the modules directory in the root of our Vagrant repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Seeing as the containers are already built by elastic, these modules are going
    to be short and sweet. We are just going to add code to the `init.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we are calling the `docker::image` class to download `ealsticsearch`.
    In the `docker::run` class, we are calling our `elasticearch` container, and we
    are going to bind our container only to the `docker-internal` Docker network.
    You will note that we are not binding any ports. This is because, by default,
    this container will expose port 9200\. We only want to expose port 9200 on the
    Docker network. Docker is smart enough to allow the exposed ports automatically
    on a Docker native network. In the next resource we are declaring just the host
    network for `elasticsearch`. We are specifying `0.0.0.0` as we don't know the
    IP that the container is going to get from the Docker network. As this service
    will be hidden from the outside world, this configuration will be fine. We will
    then map a persistent drive to keep our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing that we need to do now is to add `elasticsearch` to a node.
    As per our design, we will add `elasticsearch` to `node-02`. We do this in our
    `default.pp` file in our `manifests` directory, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You will note that I have used `contain` instead of `include`. This is because
    I want to make sure that the `consul` class is applied before the `elasticsearch`
    class, as we need our Docker network to be there before `elasticsearch` comes
    up. If the network is not there, our catalogue will fail as the container will
    not build.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next module we are going to write is `logstash`. Our `logstash` module
    is going to be a little more complicated, as it will be on both the Docker network
    and the host network. The reason we want it on both is because we want applications
    to forward their logs to `logstash`. We also need `logstash` to talk to `elasticsearch`.
    Thus, we add `logstash` to the Docker network as well. We will create the module
    in the same way we did for `elasticsearch`. We will call our `<AUTHOR>-logstash`
    module. So, let''s look at our code in our `init.pp` file, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the first thing you will note is that we are creating a directory. This
    is to map to the container and will contain our `logstash.conf` file. The next
    declaration is a file type. This is our `logstah.conf` file, and as we can see,
    it's a template from the code. So, let's come back to it after looking at the
    rest of the code in our `init.pp` file. The next line of code will pull our `logstash`
    image from Docker Hub. In the `docker::run` class we will call our `logstash`
    container, use the `logstash` image, and attach the container to our `docker-internal`
    Docker network.
  prefs: []
  type: TYPE_NORMAL
- en: The next line of code will tell `logstash` to start using our `logstash.conf`
    file; we will then mount the directory we created earlier in our `init.pp` file
    to the container. Now, you can see in this module that we've exposed ports to
    the host network. In the last line, we tell `logstash` about our Elasticsearch
    host and Elasticsearch port. How does Logstash know where Elasticsearch is? We
    are not linking the containers like we did in previous chapters. This works in
    the same way to when we named our Elasticsearch container `elasticsearch`, and
    our Docker network has an inbuilt DNS server that lives at the address `127.0.0.11`.
    Any container that joins that network will register itself as its container name.
    This is how services on the `docker-internal` network find each other.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we need to look at is our template file for our `logstash.conf`
    file that we declared in our `init.pp` file. So, create a new folder called `templates`
    in the root of our module and then a file called `logstash.conf.erb`. We will
    add the following configuration to accept logs from syslog and Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, at the bottom, we put our Elasticsearch configuration, as shown in
    this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's add our `logstash` module to `node-03` in the same way that we did
    with our `elastcsearch` module.
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Again, we will use `contain` instead of `include`. Now it's time to move on
    to our last module. We will create this in the same way as we have done for the
    last two modules. We will call this module `<AUTHOR>-kibana`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kibana, we will only be adding code to the `init.pp` file, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we are downloading the `kibana` image. In the `docker::run`
    class, we are calling our `kibana` container using the `kibana` image, attaching
    the container to our local Docker network. In the next line, we are mapping the
    container port `5601` (Kibana's default port) to port `80` on the host. This is
    just for ease of use for our lab. In the last line, we are telling `kibana` how
    to connect to `elasticsearch`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add `kibana` to `node-04` again using `contain` instead of `include`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are now ready to run our Vagrant environment. Let's open our terminal and
    change the directory to the root of our Vagrant repo. We will build this completely
    from scratch, so let's issue the `vagrant destroy -f && vagrant up` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will take about 5 minutes or so to build, depending on your Internet connection,
    so please be patient. Once the build is complete, our terminal should have no
    errors and look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next thing we will check is our Consul web UI (`127.0.0.1:8500`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, you can see that our Logstash and Kibana services
    are there, but where is Elasticsearch ? Don''t worry, Elasticsearch is there,
    but we can''t see it in Consul as we have not forwarded any ports to the host
    network. Registrator will only register services with exposed ports. We can make
    sure that our ELK stack is configured by logging in to our Kibana web UI (`127.0.0.1:8080`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next thing we need to do is click on the **Create** button. Then, if we
    go to the **Discover** tab, we can see the logs from Logstash:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The ELK stack](img/B05201_06_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logstash's logs
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to use Puppet to deploy containers across
    multiple nodes. We took advantage of the native Docker networking to hide services.
    This is a good security practice when working with production environments. The
    only issue with this chapter is that we don't have any failover or resiliency
    in our applications. This is why container schedulers are so important.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will drive into three different schedulers to arm you
    with the knowledge that you will need to make sound design choices in the future.
  prefs: []
  type: TYPE_NORMAL
