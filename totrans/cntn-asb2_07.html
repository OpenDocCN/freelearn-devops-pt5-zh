<html><head></head><body>
        

                            
                    <h1 class="header-title">Deploying Your First Project</h1>
                
            
            
                
<p>Throughout this book so far, we have looked at the various ways we can run builds and containers using the Ansible Container workflow. We learned about running containers in a local Docker daemon, pushing built containers to a remote Docker image repository, managing container images, and even running containers at scale using container orchestration tools such as Kubernetes and OpenShift. We have almost come full circle, demonstrating the rich capabilities of Ansible Container and how it can be leveraged as a fully functional tool for building, running, and testing container images throughout an application's life cycle.</p>
<p>However, there is one aspect of the Ansible Container workflow we have not yet looked at in depth. In previous chapters, we alluded to the <kbd>deploy</kbd> command, and how <kbd>deploy</kbd> can be leveraged to run containers in production environments, or on remote systems. Now that we have covered a lot of the basics of how Docker, Kubernetes, and OpenShift work, it is time we turned our attention to the final Ansible Container workflow component: <kbd>ansible-container deploy</kbd>. It is my goal that, by reading through this chapter and following along with the examples, it will become evident to the reader that Ansible Container is more than a tool used to build and run container images locally. It is a robust tool for complex containerized application deployments across a variety of popular container platforms.</p>
<p>Throughout this chapter, we will cover the following topics:</p>
<ul>
<li class="mce-root">Overview of ansible-container deploy</li>
<li class="mce-root">Deploying containers to Kubernetes</li>
<li class="mce-root">Deploying containers to OpenShift</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Overview of ansible-container deploy</h1>
                
            
            
                
<p>The <kbd>ansible-container deploy</kbd> command is the component of the Ansible Container workflow that is responsible for, you guessed it, deploying containers to remote container service engines. At the time of writing, these engines include Docker, Kubernetes, and OpenShift. By leveraging configuration in the <kbd>container.yml</kbd> file, Ansible Container has the ability to authenticate to these services and leverage API calls to start containers according to the configuration specified by the user. Deployment with Ansible Container is a two-step process. First, Ansible Container pushes the built container images to a remote image registry, similar to Docker Hub or <kbd>Quay.io</kbd>. This enables the remote container runtime service to have access to the containers during the deployment process. Second, Ansible Container generates deployment playbooks that can be executed locally and performs the deployment using the <kbd>ansible-container run</kbd> command. Working through the deploy process can be a little confusing at first. The following flowchart demonstrates the deployment process after first building and running a project locally:</p>
<div><img height="120" width="376" src="img/fa204a2c-74c0-4408-9f86-a0518a29f1af.png"/></div>
<p>Figure 1: ansible-container deploy workflow</p>
<p>We are going to start out by looking at examples using our simple NGINX container project. Later, we are going to look at deploying examples to Kubernetes and OpenShift using the MariaDB project we built in <a href="747fd1c6-46e4-424a-be59-5bbf20deb5ed.xhtml" target="_blank" rel="noopener noreferrer">Chapter 4</a>, <em>What's in a Role?</em></p>


            

            
        
    

        

                            
                    <h1 class="header-title">ansible-container deploy</h1>
                
            
            
                
<p>Before we start looking at <kbd>ansible-container deploy</kbd>, let's first rebuild the NGINX project we created earlier. In your Ubuntu Vagrant lab VM, navigate to the <kbd>/vagrant/AnsibleContainer/nginx_demo</kbd> directory; or, if you built this example yourself in another directory, navigate to it and run the <kbd>ansible-container build</kbd> command. This will make sure that the lab VM has a fresh build of the project:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/AnsibleContainer/nginx_demo$ ansible-container build  </strong><br/><strong>                                                                  
Building Docker Engine context...                                                                                                              
Starting Docker build of Ansible Container Conductor image (please be patient)...                                                              
Parsing conductor CLI args.                                                                                                                    
Docker™ daemon integration engine loaded. Build starting.       project=nginx_demo                                                             
Building service...     project=nginx_demo service=webserver                                                                                   
                                                                                                                                               
PLAY [webserver] ***************************************************************                                                               
                                                                                                                                               
TASK [Gathering Facts] *********************************************************                                                               
ok: [webserver]                                                                                                                                
                                                                                                                                               
TASK [ansible.nginx-container : Install epel-release] **************************                                                               
changed: [webserver]

TASK [ansible.nginx-container : Install nginx] *********************************
changed: [webserver] =&gt; (item=[u'nginx', u'rsync'])</strong>

<strong>TASK [ansible.nginx-container : Install dumb init] *****************************
changed: [webserver]

TASK [ansible.nginx-container : Update nginx user] *****************************
changed: [webserver]

TASK [ansible.nginx-container : Put nginx config] ******************************
changed: [webserver]</strong></pre>
<p>You can validate that the project has successfully been built and the container images are cached by running the <kbd>docker images</kbd> command:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/AnsibleContainer/nginx_demo$ docker images
</strong><br/><strong>REPOSITORY  TAG   IMAGE ID  CREATED  SIZE
aric49/nginx_demo-webserver 20171022202358 09f7b7cc3e3e 10 minutes ago  268MB</strong></pre>
<p>Now that we have the container cached locally, we can use the <kbd>ansible-container deploy</kbd> command to simulate project deployment. Without providing any arguments about what engine we will deploy our container to, <kbd>ansible-container deploy</kbd> will generate playbooks that can be used to deploy our project onto a local or remote host that is running Docker. It will also push our project to the registries configured in the <kbd>container.yml</kbd> file located in the <kbd>root</kbd> directory of our project. Due to the fact that <kbd>deploy</kbd> leverages much of the same functionality as <kbd>ansible-container push</kbd>, we will provide <kbd>deploy</kbd> with the same flags we would provide the <kbd>push</kbd> command concerning our container image registry. In this case, we will tell it to push to our Docker Hub registry, as we will provide the username for our account and any tags we want to use to differentiate this version of the container from previous versions. For the purposes of demonstration, we will use the <kbd>deploy</kbd> tag:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/AnsibleContainer/nginx_demo$ ansible-container deploy --push-to docker --username aric49 --tag deploy
</strong><br/><strong>Enter password for aric49 at Docker Hub:                                                                                                       
Parsing conductor CLI args.                                                                                                                    
Engine integration loaded. Preparing push.      engine=Docker™ daemon                                                                          
Tagging aric49/nginx_demo-webserver                                                                                                            
Pushing aric49/nginx_demo-webserver:deploy...                                                                                          
The push refers to a repository [docker.io/aric49/nginx_demo-webserver]                                                                        
Preparing                                                                                                                                      
Pushing                                                                                                                                        
Pushed                                                                                                                                         
Pushing                                                                                                                                        
Pushed                                                                                                                                         
20171022202358: digest: sha256:74948d56b3289009a6329c0c2035e3217d0e83479dfaee3da3d8ae6444b04165 size: 741                                      
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=4c7c43d090654e62869185458434941cb7718e257eeed80f03e846d460eae24f save_contain
er=False                                                                                                                                       
Parsing conductor CLI args.                                                                                                                    
Engine integration loaded. Preparing deploy.    engine=Docker™ daemon                                                                          
Verifying image for webserver                                                                                                                  
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=acf8f1ec2adc821c33b2a341bc1404346b6d41f6ef18de5fb8dce7f98ddaea3f save_container=False                   </strong>                                                                                                      </pre>
<p>The deploy process, in a similar fashion to the push process, will prompt you for the password for your Docker Hub account. Upon successful authentication, it will push your container image layers to the container image registry. So far, this might look exactly identical to the push process. However, you may notice that, in the <kbd>root</kbd> directory of your project, a new directory called <kbd>ansible-deployment</kbd> now exists. Within this directory, you will find a single Ansible playbook that is named identically to that of your project, <kbd>nginx_demo</kbd>. Here is a sample of what this playbook looks like:</p>
<pre class="western">  - name: Deploy nginx_demo
    hosts: localhost
    gather_facts: false
    tasks:
      - docker_service:
            definition:
                services: &amp;id001
                    webserver:
                        image: docker.io/aric49/nginx_demo-webserver:deploy
                        command: [/usr/bin/dumb-init, nginx, -c, /etc/nginx/nginx.conf]
                        ports:
                          - 80:8000
                        user: nginx
                version: '2'
            state: present
            project_name: nginx_demo
        tags:
          - start
      - docker_service:
            definition:
                services: *id001
                version: '2'
            state: present
            project_name: nginx_demo
            restarted: true
        tags:
          - restart
TRUNCATED</pre>
<p>You may have to ensure the <kbd>image</kbd> line reflects the image path in this format, <kbd>docker.io/username/containername:tag</kbd>, as some versions of Ansible Container supply the wrong path as input in the playbook. If this is the case, simply modify the playbook in a text editor.</p>
<p>The deploy playbook works by making calls to the <kbd>docker_service</kbd> module running on the target hosts. By default, the container uses <kbd>localhost</kbd> as the target host for deployment. However, you can easily provide a standard Ansible inventory file to have this project run on remote hosts. The deploy playbook supports full Docker life cycle application management, such as starting the container, restarting, and ultimately destroying the project by providing a series of playbook tags to conditionally execute the desired functionality. You may notice that the playbook inherits many of the settings we configured in the <kbd>container.yml</kbd> file. Ansible Container uses these settings so that the playbooks can be executed independently of the project itself.</p>
<p>Since we have already looked at using <kbd>ansible-container run</kbd> to run our containers locally throughout this book,  let's try executing the playbook directly to start the container. This mimics the same process used if you want to manually run a deployment outside of the Ansible Container workflow. This can be accomplished by using the <kbd>ansible-playbook</kbd> command with the <kbd>start</kbd> tag to deploy a project on our localhost. You may notice that this process is exactly the same process as running the <kbd>ansible-container run</kbd> command. It is important to note that any of the core Ansible Container functionality (run, restart, stop, and destroy) can be executed independently of Ansible Container by running playbooks directly and supplying the appropriate tag according to the functionality you are trying to achieve:</p>
<pre class="western"><strong>ubuntu@node01:$ ansible-playbook nginx_demo.yml --tags start
 </strong><br/><strong>[WARNING]: Host file not found: /etc/ansible/hosts

 [WARNING]: provided hosts list is empty, only localhost is available


PLAY [Deploy nginx_demo] ************************************************

TASK [docker_service] ************************************************
changed: [localhost]

PLAY RECAP *************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=0</strong></pre>
<p>Once the playbook execution has completed, <kbd>PLAY RECAP</kbd> will show that one task has executed a change on your localhost. You can execute the <kbd>docker ps -a</kbd> command to confirm the project has successfully been deployed:</p>
<pre class="western"><strong>ubuntu@node01:$ docker ps -a
</strong><br/><strong>CONTAINER ID  IMAGE   COMMAND  CREATED   STATUS  PORTS  NAMES
4b4b3b032c61        aric49/nginx_demo-webserver:deploy   "/usr/bin/dumb-ini..."   5 minutes ago       Up 5 minutes        0.0.0.0:80-&gt;8000/tcp   nginxdemo_webserver_1</strong></pre>
<p>In a similar way, we can run this playbook again, passing in the <kbd>--restart</kbd> tag to restart the container on the Docker host. After executing the playbook for a second time, you should see that a single task has once more changed, indicating the container has been restarted. This mimics the functionality that the <kbd>ansible-container restart</kbd> command provides. The <kbd>status</kbd> column in <kbd>docker ps -a</kbd> will show that the container has only been up for a handful of seconds after executing the restart:</p>
<pre class="western"><strong>ubuntu@node01:$ docker ps -a
</strong><br/><strong>CONTAINER ID  IMAGE  COMMAND  CREATED  STATUS  PORTS  NAMES
4b4b3b032c61        aric49/nginx_demo-webserver:deploy "/usr/bin/dumb-ini..."   7 minutes ago       Up 2 seconds 0.0.0.0:80-&gt;8000/tcp   nginxdemo_webserver_1</strong></pre>
<p>The <kbd>stop</kbd> tag can be passed into the <kbd>ansible-playbook</kbd> command to temporarily stop the running container. Similar to <kbd>restart</kbd>, the <kbd>docker ps -a</kbd> output will show that the container is in an <kbd>exit</kbd> status:</p>
<pre class="western"><strong>ubuntu@node01:$ ansible-playbook nginx_demo.yml --tags stop
 </strong><br/><strong>[WARNING]: Host file not found: /etc/ansible/hosts

 [WARNING]: provided hosts list is empty, only localhost is available


PLAY [Deploy nginx_demo] *************************************

TASK [docker_service] **************************************************************
changed: [localhost]

TRUNCATED

PLAY RECAP **************************************************
localhost: ok=4    changed=2    unreachable=0    failed=0<br/><br/></strong></pre>
<p class="western">We can now check the status using <kbd>docker ps -a</kbd> command: <strong><br/></strong></p>
<pre class="western"><strong>ubuntu@node01:$ docker ps -a
</strong><br/><strong>CONTAINER ID  IMAGE  COMMAND  CREATED  STATUS  PORTS  NAMES
4b4b3b032c61        aric49/nginx_demo-webserver:deploy   "/usr/bin/dumb-ini..."   11 minutes ago      Exited (0) 2 minutes ago                       nginxdemo_webserver_1</strong></pre>
<p>Finally, the project can be removed entirely from our Docker host by passing in the <kbd>destroy</kbd> tag. Running this <kbd>playbook</kbd> tag will execute a few more steps in the playbook, but will ultimately remove all traces of your project from the host:</p>
<pre class="western"><strong>ubuntu@node01:$ ansible-playbook nginx_demo.yml --tags destroy
 </strong><br/><strong>[WARNING]: Host file not found: /etc/ansible/hosts

 [WARNING]: provided hosts list is empty, only localhost is available


PLAY [Deploy nginx_demo] *************************************

TASK [docker_service] **************************************************************
changed: [localhost]

TASK [docker_image] **************************************************************
ok: [localhost]

TASK [docker_image] **************************************************************

TASK [docker_image] **************************************************************
ok: [localhost]

TRUNCATED

PLAY RECAP **************************************************
localhost: ok=7    changed=2    unreachable=0    failed=0</strong></pre>
<p>Behind the scenes, when any of the core Ansible Container commands are executed, they are essentially wrappers around the same playbook that gets generated as a part of your project. The purpose of this portion of the chapter was to demonstrate to the reader the overall flow of deploying projects using Ansible Container locally, and to build upon these skills deeper in the lesson. Where deployment gets really interesting is when using Kubernetes and OpenShift as the target deployment engines. Using the Ansible Container workflow commands with the corresponding container platform engine, we can manage containerized deployments directly using the Ansible Container workflow commands instead of executing the playbooks directly.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying containers to Kubernetes</h1>
                
            
            
                
<p>One of the many aspects that makes the Ansible Container workflow so flexible and appealing to organizations and individuals looking to adopt Ansible as the native support for remote deployments using Kubernetes and OpenShift. In this section, we will look at using the <kbd>ansible-container deploy</kbd> command to deploy our containers to our Google Cloud Kubernetes cluster we created in <a href="ccc07e61-25e7-4984-953b-586b28b12aab.xhtml" target="_blank" rel="noopener noreferrer">Chapter 5</a>, C<em>ontainers at Scale with Kubernetes</em>.</p>
<p>As we discussed in the previous section, running <kbd>ansible-container deploy</kbd> by itself will, by default, push your container to any image registries you have configured in your <kbd>container.yml</kbd> file and generate a new directory in the <kbd>root</kbd> of your project called <kbd>ansible-deployment</kbd>. Inside of this directory, a single YAML playbook file named after the project will be present. This playbook is used to deploy your container to any Docker host, quite similar to the <kbd>ansible-container run</kbd> command. For the purposes of this example, we are going to run <kbd>ansible-container deploy</kbd> using the Kubernetes engine so we can leverage Ansible Container as a deployment tool for Kubernetes, creating service definitions and deployment abstractions automatically.</p>
<p>In order to enable the Kubernetes to deploy functionality in Ansible Container, we will add a couple of new parameters to the project <kbd>container.yml</kbd> file. Specifically, we need to point our project to our Kubernetes authentication configuration file, and define the namespace our container operates in within Kubernetes. For this example, I will use our previously used MariaDB project, but with a few modifications to the <kbd>container.yml</kbd> file to support Kubernetes.  For reference, this project can be found in the official Git repository for this book, in the  <kbd>Kubernetes/mariadb_demo_k8s</kbd> directory. Feel free to follow along, or modify the existing MariaDB project to support Kubernetes. In the settings section, we will add the <kbd>k8s_namespace</kbd> and <kbd>k8s_auth</kbd>.</p>
<p>The Kubernetes <kbd>namespace</kbd> will contain the name of the namespace in our cluster we want to deploy our project into, and the <kbd>auth</kbd> section will provide the path to our Kubernetes authentication configuration file. The default location of the Kubernetes authentication configuration is <kbd>/home/user/.kube/config</kbd>. If you are following along using the Vagrant lab, Google Cloud SDK places this configuration file at <kbd>/home/ubuntu/.kube/config</kbd>.</p>
<p>Before we can begin, though, we need to set up a default access token in order for Ansible Container to access the Google Cloud API. We can do this by executing the <kbd>gcloud auth application-default login</kbd> command. Executing this command will provide you with a hyperlink you can use to allow permissions to the Google Cloud API using your Google login credentials, similar to what we did in <a href="ccc07e61-25e7-4984-953b-586b28b12aab.xhtml" target="_blank" rel="noopener noreferrer">Chapter 5</a>, <em>Containers at Scale with Kubernetes</em>. The Google Cloud API will give you a token you can enter at the command line that will generate an application default credentials file, located at <kbd>/home/ubuntu/.config/gcloud/application_default_credentials.json</kbd>:</p>
<pre class="western"><strong>ubuntu@node01:$ gcloud auth application-default login
Go to the following link in your browser:

    https://accounts.google.com/o/oauth2/TRUNCATED


Enter verification code: XXXXXXXXXXXXXXX

Credentials saved to file: [/home/ubuntu/.config/gcloud/application_default_credentials.json]</strong></pre>
<p>These credentials will be used by any library that requests access to any Google Cloud resources, including Ansible Container.</p>
<p>The Google Container Engineer-specific steps are only required if you are using a Google Cloud Kubernetes cluster. You can skip these step if you are using a local Kubernetes environment such as Minikube.</p>
<p>Now that we have the proper permissions set in the Google Cloud API, we can modify the <kbd>container.yml</kbd> file of our MariaDB project to support the Kubernetes deployment engine:</p>
<pre class="western">version: "2"
settings:
  conductor_base: ubuntu:16.04
  project_name: mariadb-k8s
  roles_path:
    - ./roles/
  k8s_namespace:
    name: database
  k8s_auth:
    config_file: /home/ubuntu/.kube/config
services:
  mariadb-database:
    roles:
      - role: mariadb_role

registries:
  docker:
    url: https://index.docker.io/v1/
    namespace: username</pre>
<p>You will notice the following changes we have made to support the Kubernetes deployment:</p>
<ul>
<li><kbd>project_name</kbd>: For this example, we have added a field in the <kbd>settings</kbd> block called <kbd>project_name</kbd>. Throughout this book, we have allowed our projects to take the default name of the directory that it is built in. Kubernetes is limited as to the characters it can use to define services and pods, so we want to ensure we do not use illegal characters in our project name by overriding them in the <kbd>container.yml</kbd> file.</li>
<li><kbd>k8s_namespace</kbd>: This defines the Kubernetes namespace we will deploy our pods into. Leaving this stanza blank will cause Ansible Container to use the default namespace that we used in our NGINX deployment earlier in the chapter. In this example, we will use a different namespace called <kbd>database</kbd>.</li>
<li><kbd>k8s_auth</kbd>: This is where we specify the location of our Kubernetes authentication configuration file. Within this file, Ansible Container is able to extract the IP address of our API server, the access credentials to create resources in the cluster, as well as the SSL certificates required to connect to Kubernetes.</li>
</ul>
<p>Once these changes have been placed in your <kbd>container.yml</kbd> file, let's build the project:</p>
<pre class="western"><strong>ubuntu@node01:$ ansible-container build                                                                                                                                  
Building Docker Engine context...                                                                                                                                                                            
Starting Docker build of Ansible Container Conductor image (please be patient)...                                                                                                                            
Parsing conductor CLI args.                                                                                                                                                                                  
Docker™ daemon integration engine loaded. Build starting.       project=mariadb-k8s                                                                                                                          
Building service...     project=mariadb-k8s service=mariadb-database                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                             
PLAY [mariadb-database] ********************************************************                                                                                                                             
                                                                                                                                                                                                             
TASK [Gathering Facts] *********************************************************                                                                                                                             
ok: [mariadb-database]                                                                                                                                                                                       
                                                                                                                                                                                                             
TASK [mariadb_role : Install Base Packages] ************************************                                                                                                                             
changed: [mariadb-database] =&gt; (item=[u'ca-certificates', u'apt-utils'])                                                                                                                                     
                                                                                                                                                                                                             
TASK [mariadb_role : Install dumb-init for Container Init System] 

                                                                                                                            </strong></pre>
<p>Once the project has finished building, we can use the <kbd>ansible-container deploy</kbd> command, specifying the <kbd>--engine</kbd> flag to use <kbd>k8s</kbd>, and providing the details for the Docker image registry we want to push to as configured in our <kbd>container.yml</kbd> file. For the sake of separation, let's also tag the image version with <kbd>kubernetes</kbd> so we can keep this version separate in our repository. Ansible Container will then push our image to the Docker Hub repository and generate the deployment playbooks specific to Kubernetes:</p>
<p>K8s is shorthand for Kubernetes since Kubernetes comprises the letter K with 8 letters after it. This is commonly pronounced in the community as <strong>Kay-Eights</strong>.</p>
<pre class="western"><strong>ubuntu@node01:$ ansible-container --engine k8s deploy --push-to docker --tag kubernetes
Parsing conductor CLI args.
Engine integration loaded. Preparing push.      engine=K8s
Tagging aric49/mariadb-k8s-mariadb-database
Pushing aric49/mariadb-k8s-mariadb-database:kubernetes...
The push refers to a repository [docker.io/aric49/mariadb-k8s-mariadb-database]
Preparing
Waiting
Layer already exists
Pushing
Pushed
Pushing
Pushed
kubernetes: digest: sha256:563ec4593945b13b481c03ab7813bb64c0dc1b7a1d1ae8c4b61b744574df2926 size: 1569
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=27fd42d3920deb12f8c81802f151e95931315f516e04307a3a682bd9638fdf49 save_container=False
Parsing conductor CLI args.
Engine integration loaded. Preparing deploy.    engine=K8s
Verifying image for mariadb-database
ansible-galaxy 2.5.0
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
  ansible python module location = /usr/local/lib/py</strong>thon2.7/dist-packages/ansible
 <strong> executable location = /usr/local/bin/ansible-galaxy
  python version = 2.7.12 (default, Nov 19 2016, 06:48:10) [GCC 5.4.0 20160609]
Using /etc/ansible/ansible.cfg as config file
Opened /root/.ansible_galaxy
Processing role ansible.kubernetes-modules
Opened /root/.ansible_galaxy
- downloading role 'kubernetes-modules', owned by ansible
https://galaxy.ansible.com/api/v1/roles/?owner__username=ansible&amp;name=kubernetes-modules
https://galaxy.ansible.com/api/v1/roles/16501/versions/?page_size=50
- downloading role from https://github.com/ansible/ansible-kubernetes-modules/archive/master.tar.gz
- extracting ansible.kubernetes-modules to /vagrant/Kubernetes/mariadb_demo_k8s/ansible-deployment/roles/ansible.kubernetes-modules
- ansible.kubernetes-modules (master) was installed successfully
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=d66eb0a142190022fee50de4e2560c11260bd69ec3cc634be2a389425e6bf477 save_container=False</strong></pre>
<p>Once this process has completed, you will notice that, similar to the last example, a new directory has appeared in your project called <kbd>ansible-deployment</kbd>. Inside of this directory, you will find a single playbook and a <kbd>roles</kbd> directory that is responsible for performing the actual deployment of our service to Kubernetes. As with our previous localhost example, this playbook is likewise divided up based on tags that control starting, stopping, and restarting the service in our cluster. Since we have not yet deployed our service, we can start the deployment using the <kbd>ansible-container run</kbd> command with the <kbd>--engine k8s</kbd> flag to indicate a Kubernetes deployment. Assuming we have configured everything correctly in the <kbd>container.yml</kbd> file, you should see a successful playbook run, indicating the container has been deployed to the Kubernetes cluster:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ ansible-container --engine k8s run
Parsing conductor CLI args.
Engine integration loaded. Preparing run.       engine=k8s
Verifying service image service=mariadb-database
PLAY [Manage the lifecycle of mariadb-k8s on K8s] **************************************************

TASK [Create namespace database] ***************************************************
ok: [localhost]

TASK [Create service] ******************************************************
ok: [localhost]

TASK [Create deployment, and scale replicas up] *******************************************************
changed: [localhost]

PLAY RECAP ********************************************
localhost: ok=3    changed=1    unreachable=0    failed=0</strong></pre>
<p>Using the <kbd>kubectl get pods</kbd> command from earlier, we can validate that our Kubernetes pod has been deployed and is successfully running. Since we deployed this particular pod in its own namespace, we need to use the <kbd>--namespace</kbd> flag to see pods that are running in other namespaces:</p>
<pre class="western"><strong>ubuntu@node01:$ kubectl get pods --namespace database
NAME                                READY     STATUS    RESTARTS   AGE
mariadb-database-1880651791-979zd   1/1       Running   0          3m</strong></pre>
<p>Using the <kbd>ansible-container run</kbd> command with the Kubernetes engine is a powerful tool for creating cloud-native services that run on a Kubernetes cluster. Using Ansible Container, you have the flexibility to choose how you want to deploy applications, by executing the playbooks directly or automatically using the Ansible Container workflow. If you wish to delete the current deployment from your Kubernetes cluster, you can simply run the <kbd>ansible-container --engine k8s destroy</kbd> command to completely remove the pods and deployment artifacts from the cluster. It is important to note that the other Ansible Container workflow commands (start, stop, and restart) are perfectly applicable suffixes to use with the Kubernetes deployment engine. For the sake of reducing redundancy, let's take a look at how <kbd>ansible-container deploy</kbd> and the workflow commands work with the OpenShift deployment engine. Functionally, the Ansible Container workflow commands are identical for Kubernetes and OpenShift.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying containers to OpenShift</h1>
                
            
            
                
<p>The <kbd>ansible-container deploy</kbd> command also supports deployments directly to OpenShift using the native OpenShift APIs. Since OpenShift is built on top of Kubernetes, you will discover that deploying containers to OpenShift is quite a similar process to Kubernetes deployments, since OpenShift authentication works very similarly to Kubernetes on the backend. Before beginning, these examples are going to use the Vagrant lab VM running at the same time as the Minishift VM we created in <a href="d3c6ddae-003d-4f20-a3a5-efd018ac61ee.xhtml" target="_blank" rel="noopener noreferrer">Chapter 6</a>, <em>Managing Containers with OpenShift</em>. This can get quite CPU and RAM intensive. If you're attempting to run these examples with 8 GB of RAM or higher, you should get good performance.</p>
<p>However, if you are constrained on resources, these examples can run reasonably well using the OpenShift free tier cloud account, although you may run into issues with the limited quotas provided.</p>
<p>Before beginning, we need to first ensure that the Vagrant lab environment, as well as our Minishift VM, are running and reachable from the VirtualBox network. Since the hypervisors used to create our Vagrant lab environment and our Minishift cluster are both using VirtualBox, we should by default have network connectivity between the two VMs. We can validate this by attempting to ping the Minishift VM from our Vagrant lab VM. First, we need to start the Minishift VM using reasonable specifications for your local workstation. In this example, I am going to start the Minishift VM allocating 8 GB of RAM and 50 GB virtual hard disk storage for it. If you are running both VMs simultaneously, you may only be able to allocate the minimum 2 GB of RAM for Minishift:</p>
<pre class="western"><strong>aric@lemur:~/Development/minishift$ minishift start --vm-driver=virtualbox --disk-size=50GB --memory=8GB                                                         
</strong><br/><strong>-- Starting local OpenShift cluster using 'virtualbox' hypervisor ...                                                                                           
-- Starting Minishift VM .................... OK                                                                                                                       
-- Checking for IP address ... OK                                                                                                                                          
-- Checking if external host is reachable from the Minishift VM ...                                 
   Pinging 8.8.8.8 ... OK                                                                                      
-- Checking HTTP connectivity from the VM ...                                 
   Retrieving http://minishift.io/index.html ... OK
-- Checking if persistent storage volume is mounted ... OK
-- Checking available disk space ... 7% used OK                                                             
-- OpenShift cluster will be configured with ...                                                            
   Version: v3.6.0                               
-- Checking `oc` support for startup flags ...             
   host-config-dir ... OK                       
   host-data-dir ... OK                                                                                   
   host-pv-dir ... OK                                                                               
   host-volumes-dir ... OK                                                                                  
   routing-suffix ... OK                                                                               
Starting OpenShift using openshift/origin:v3.6.0 ...     
OpenShift server started.                            
                                              
The server is accessible via web console at:                                                                                                                                                            
    https://192.168.99.100:8443  </strong>                        </pre>
<p>At the end of the startup process, you should receive an IP from which the OpenShift Web UI is accessible. We need to ensure this IP address is reachable from our Vagrant lab node:</p>
<pre class="western"><strong>ubuntu@node01:~$ ping 192.168.99.100
PING 192.168.99.100 (192.168.99.100) 56(84) bytes of data.
64 bytes from 192.168.99.100: icmp_seq=1 ttl=63 time=0.642 ms
64 bytes from 192.168.99.100: icmp_seq=2 ttl=63 time=0.602 ms
64 bytes from 192.168.99.100: icmp_seq=3 ttl=63 time=0.929 ms
^C
--- 192.168.99.100 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2003ms
rtt min/avg/max/mdev = 0.602/0.724/0.929/0.147 ms</strong></pre>
<p>If this IP address is not pingable, you may have to configure your VirtualBox networking so that network connectivity is available. A great resource to learn more about configuring and debugging VirtualBox networks is the official VirtualBox documentation: <a href="https://www.virtualbox.org/manual/ch06.html">https://www.virtualbox.org/manual/ch06.html</a>.</p>
<p>Once networking has been established and verified between the Minishift VM and the Vagrant lab VM, next we will need to install the OC on our Vagrant lab VM to allow us to authenticate to OpenShift. This will be the exact same process we completed in <a href="d3c6ddae-003d-4f20-a3a5-efd018ac61ee.xhtml" target="_blank" rel="noopener noreferrer">Chapter 6</a>, <em>Managing Containers with OpenShift</em>:</p>
<ol>
<li>Download the OC binary packages using <kbd>wget</kbd>:</li>
</ol>
<pre style="padding-left: 60px;" class="western"><strong>ubuntu@node01:~$ wget https://mirror.openshift.com/pub/openshift-v3/clients/3.6.173.0.5/linux/oc.tar.gz                                                                                                                                                                                 

Resolving mirror.openshift.com (mirror.openshift.com)... 54.173.18.88, 54.172.163.83, 54.172.173.155                                                                                                         
Connecting to mirror.openshift.com (mirror.openshift.com)|54.173.18.88|:443... connected.                                                                                                                    
HTTP request sent, awaiting response... 200 OK                                                                                                                                                               
Length: 36147137 (34M) [application/x-gzip]                                                                                                                                                                  
Saving to: ‘oc.tar.gz' 
2017-10-28 15:21:24 (6.79 MB/s) - ‘oc.tar.gz' saved [36147137/36147137]</strong></pre>
<ol start="2">
<li>Extract the TAR archive using the <kbd>tar -xf</kbd> command:</li>
</ol>
<pre style="padding-left: 60px;" class="western"><strong>ubuntu@node01:~$ tar -xf oc.tar.gz
oc    oc.tar.gz</strong></pre>
<ol start="3">
<li>Copy the binary to a <kbd>$PATH</kbd> location:</li>
</ol>
<pre style="padding-left: 60px;"><strong>ubuntu@node01:~$ sudo cp oc /usr/local/bin</strong></pre>
<p>If you have existing Kubernetes credentials in <kbd>/home/ubuntu/.kube/config</kbd>, you will need to back them up to another location so the OC does not overwrite them (or simply delete the config file if you have no further use for it any longer: <kbd>rm -rf ~/.kube/config</kbd>).</p>
<p>Next, we need to authenticate to the local OpenShift cluster using the <kbd>oc login</kbd> command in order to generate our Kubernetes credential file that Ansible Container will leverage. The <kbd>oc login</kbd> command takes the URL endpoint of the OpenShift cluster as a parameter. By default, the OC will write a Kubernetes configuration file to <kbd>/home/ubuntu/.kube/config</kbd>. This file will serve as our means of authenticating to OpenShift Kubernetes to perform automated deployments. Remember to log in as the <kbd>developer</kbd> user, which uses any user-provided password to log in:</p>
<pre class="western"><strong>ubuntu@node01:$ oc login https://192.168.99.100:8443                                                                                                                     
</strong><br/><strong>The server uses a certificate signed by an unknown authority.                                                                                                                                                
You can bypass the certificate check, but any data you send to the server could be intercepted by others.                                                                                                    
Use insecure connections? (y/n): y                                                                                                                                                                           
                                                                                                                                                                                                             
Authentication required for https://192.168.99.100:8443 (openshift)                                                                                                                                          
Username: developer                                                                                                                                                                                          
Password:                                                                                                                                                                                                    
Login successful.                                                                                                                                                                                            
                                                                                                                                                                                                             
You have one project on this server: "myproject"                                                                                                                                                             
                                                                                                                                                                                                             
Using project "myproject".                                                                                                                                                                                   
Welcome! See 'oc help' to get started.</strong></pre>
<p>Once you have successfully authenticated, you should notice there is now a new Kubernetes configuration file written to the path <kbd>/home/ubuntu/.kube/config</kbd>. This is the configuration file that Ansible Container will use for access to OpenShift:</p>
<pre class="western"><strong>ubuntu@node01:~$ cat .kube/config
apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://192.168.99.100:8443
  name: 192-168-99-100:8443
contexts:
- context:
    cluster: 192-168-99-100:8443
    namespace: myproject
    user: developer/192-168-99-100:8443
  name: myproject/192-168-99-100:8443/developer
current-context: myproject/192-168-99-100:8443/developer
kind: Config
preferences: {}
users:
- name: developer/192-168-99-100:8443
  user:
    token: TOKEN-CENSORED</strong></pre>
<p class="mce-root">Let's test authentication to the local OpenShift instance by using the <kbd>oc get all</kbd> command. If authentication has been successful, you should see a list of pods, deployments, services, and routes currently running in your local OpenShift environment:</p>
<pre class="western"><strong>ubuntu@node01:~$ oc get all
NAME   DOCKER REPO   TAGS    UPDATED
is/oc-test-deployment   172.30.1.1:5000/myproject/oc-test-deployment   latest    13 days ago
is/test-deployment      172.30.1.1:5000/myproject/test-deployment      latest    2 weeks ago

NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/webserver   1         1         1            1           12d

NAME               HOST/PORT                            PATH      SERVICES    PORT      TERMINATION   WILDCARD
routes/webserver   awesomwebapp.192.168.99.100.nip.io             webserver   80-80                   None

NAME            CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
svc/webserver   172.30.136.131   &lt;none&gt;        80/TCP    12d

NAME                      DESIRED   CURRENT   READY     AGE
rs/webserver-1266346274   1         1         1         12d

NAME                            READY     STATUS    RESTARTS   AGE
po/webserver-1266346274-m2jvd   1/1       Running   3          12d</strong></pre>
<p>OpenShift, by default, leverages the same authentication mechanism that Kubernetes uses in our <kbd>container.yml</kbd> file. The only thing we need to provide is the path to our Kubernetes configuration file, as well as the Kubernetes namespace the project will be deployed into. Since we have previously configured this in our MariaDB project in the last section, let's reuse this same configuration to deploy our project to OpenShift. As a review, let's look at the content of our MariaDB project in the Vagrant Lab VM (<kbd>/vagrant/Kubernetes/mariadb_demo_k8s</kbd>), and look at the contents of the <kbd>container.yml</kbd> file:</p>
<pre class="western">version: "2"
settings:
  conductor_base: ubuntu:16.04
  project_name: mariadb-k8s
  roles_path:
    - ./roles/
  k8s_namespace:
    name: database
  k8s_auth:
    config_file: /home/ubuntu/.kube/config
services:
  mariadb-database:
    roles:
      - role: mariadb_role

registries:
  docker:
    url: https://index.docker.io/v1/
    namespace: aric49</pre>
<p>The only difference here is that the <kbd>k8s_namespace</kbd> parameter will define which OpenShift project you want to deploy your container into. In OpenShift terminology, <kbd>project</kbd> and <kbd>namespace</kbd> are essentially identical. For now, let's leave the configuration as is and look at how to deploy our project using the OpenShift engine. Deploying projects using OpenShift is very similar to how we deployed using Kubernetes, with the exception that we will prefix our Ansible Container commands with the <kbd>--engine openshift</kbd> flag so that our project will know to talk to the OpenShift API directly.  The same syntax rules apply here as well. We will give our <kbd>deploy</kbd> command the name of the repository defined in the <kbd>container.yml</kbd> file to push our container image to, and give it a unique tag to reference later:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ ansible-container --engine openshift deploy --push-to docker --tag openshift<br/>                                                                           
Parsing conductor CLI args.                                                                                                                                                                                  
Engine integration loaded. Preparing push.      engine=OpenShift™
Tagging aric49/mariadb-k8s-mariadb-database
Pushing aric49/mariadb-k8s-mariadb-database:openshift...
The push refers to a repository [docker.io/aric49/mariadb-k8s-mariadb-database]
Preparing
Waiting
Layer already exists
openshift: digest: sha256:99139cdd73b80ed29cedf8df4399b368f22e747f18e66e2529daf2e7fcf82c41 size: 1569
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=9b1bdcabd9399757c54a7218b8db2233503dacc4ecdde95d8a358a965bac954e save_container=False
Parsing conductor CLI args.
Engine integration loaded. Preparing deploy.    engine=OpenShift™
Verifying image for mariadb-database
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=76e86c54e5545328a6806bb3315d604c8e7cdb59d4484a2c32f78ba35787716b save_container=False</strong></pre>
<p>Once our container image has been pushed, we can validate the deployment playbooks have been generated in the <kbd>ansible-deployment</kbd> directory (<kbd>Kubernetes/mariadb_demo_k8s/ansible-deployment/mariadb-k8s.yml</kbd>):</p>
<pre class="western">  - name: Manage the lifecycle of mariadb-k8s on OpenShift™
    hosts: localhost
    gather_facts: no
    connection: local
    # Include Ansible Kubernetes and OpenShift modules
    roles:
      - role: ansible.kubernetes-modules
    vars_files: []
    # Tasks for setting the application state. Valid tags include: start, stop, restart, destroy
    tasks:
      - name: Create project myproject
        openshift_v1_project:
            name: myproject
            state: present
        tags:
          - start
      - name: Destroy the application by removing project myproject
        openshift_v1_project:
            name: myproject
            state: absent
        tags:
          - destroy
TRUNCATED</pre>
<p>Similar to the Docker and Kubernetes deployment engines, these playbooks can be executed independently using the <kbd>ansible-playbook</kbd> command, or by using the <kbd>ansible-container run</kbd> command. Let's run our project and deploy it into OpenShift using the <kbd>ansible-container run</kbd> command:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ ansible-container --engine openshift run
</strong><br/><strong>Parsing conductor CLI args.
Engine integration loaded. Preparing run.       engine=OpenShift™
Verifying service image service=mariadb-database

PLAY [Manage the lifecycle of mariadb-k8s on OpenShift?] ***********************

TASK [Create project database] *************************************************
changed: [localhost]

TASK [Create service] **********************************************************
changed: [localhost]

TASK [Create deployment, and scale replicas up] ********************************
changed: [localhost]

TASK [Create route] ************************************************************
changed: [localhost]

PLAY RECAP ******************************************</strong>***************************
<strong>localhost                  : ok=4    changed=4    unreachable=0    failed=0

All services running.   playbook_rc=0
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=ac5ead07a0bb145cc507b5cf815290597d1c1a8ee77a894c60916d2c0d543f18 save_container=False</strong></pre>
<p>Upon successful completion of the playbook run, we can log in to the OpenShift web user interface to look at the deployment we just executed. In a web browser, navigate to the URL provided in the output of the <kbd>minishift start</kbd> command (in my case, it is <kbd>192.168.99.100</kbd>), accept the self-signed certificate, and log in as the <kbd>developer</kbd> user:</p>
<div><img height="130" width="572" src="img/e2e97279-fcd3-47a3-9978-d3688f7efa3c.png"/></div>
<p>Figure 2: Logging into the OpenShift console</p>
<p>Upon logging in, you should see a new project has been created, called database. In this project you can see everything that the Ansible Container deployment has generated by default:</p>
<div><img height="133" width="616" src="img/38260fa4-260a-460b-a698-c2336770d800.png"/></div>
<p>Figure 3: The database project has been created under My Projects in the OpenShift console</p>
<p>Clicking on the project, <kbd>database</kbd> will bring you to a dashboard showing the relevant details for the deployment:</p>
<div><img height="364" width="623" src="img/0209240b-cc70-4315-84d7-fd71d2297ed8.png"/></div>
<p>Figure 4: MariaDB database deployment</p>
<p>As you can see, by default the Ansible Container playbooks used to deploy OpenShift run with a very useable set of default configuration options. Right away, we can see that Ansible Container has created a new project for our project, called <kbd>database</kbd>. Within this project, a default deployment exists that has our MariaDB pod created and running. It has even taken the steps for us to create a default service with a pre-configured set of labels, and created a route to access the service using the <kbd>nip.io</kbd> DNS service. Essentially, our new service is deployed and ready to go right out-of-the-box. In order to use the OpenShift deployment engine, we didn't actually have to change any of the <kbd>container.yml</kbd> configuration; we used exactly the same configuration we used to deploy to Kubernetes, with the exception of using a different Kubernetes config file, and specifying the OpenShift engine in our <kbd>run</kbd> command. As I'm sure you can see, having the ability to deploy to OpenShift or Kubernetes transparently is immensely powerful. This allows Ansible Container to function seamlessly no matter what target architecture your service is configured to use.</p>
<p>We can also validate the deployment by using the OC command-line interface client. From the Vagrant lab VM, you can use the <kbd>oc project</kbd> command to switch to the <kbd>database</kbd> project:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ oc project database
Now using project "database" on server "https://192.168.99.100:8443".</strong></pre>
<p>Once we have switched to a new project context, we can use the <kbd>oc get all</kbd> command to show everything configured to run in this project, including the pods, services, and route configuration generated by Ansible Container:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ oc get all
NAME                  REVISION   DESIRED   CURRENT   TRIGGERED BY
dc/mariadb-database   1          1         1         config

NAME                    DESIRED   CURRENT   READY     AGE
rc/mariadb-database-1   1         1         1         27s

NAME                           HOST/PORT                                              PATH      SERVICES           PORT            TERMINATION   WILDCARD
routes/mariadb-database-3306   mariadb-database-3306-database.192.168.99.100.nip.io             mariadb-database   port-3306-tcp                 None

NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
svc/mariadb-database   172.30.154.77   &lt;none&gt;        3306/TCP   28s

NAME                          READY     STATUS    RESTARTS   AGE
po/mariadb-database-1-bj19h   1/1       Running   0          26s</strong></pre>
<p>Along with <kbd>ansible-container run</kbd>, we can also use the standard Ansible Container workflow commands to manage our deployment, such as <kbd>stop</kbd>, <kbd>restart</kbd>, and <kbd>destroy</kbd>. As we discussed earlier, these workflow commands function identically with the Kubernetes engine. Let's first start the <kbd>ansible-container stop</kbd> command. <kbd>stop</kbd> will gracefully stop all running pods in the deployment, while keeping the other resources deployed and active. Let's try stopping the deployment and re-running the <kbd>get all</kbd> command to learn what happens:</p>
<pre class="western"><strong>ubuntu@node01:$ ansible-container --engine openshift stop
Parsing conductor CLI args.
Engine integration loaded. Preparing to stop all containers.    engine=OpenShift™

PLAY [Manage the lifecycle of mariadb-k8s on OpenShift?] ***********************

TASK [Stop running containers by scaling replicas down to 0] *******************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=0

All services stopped.   playbook_rc=0
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=8374839c00e2d92c02351905bd6dd463ba5592783430bc08c5b18ba068fece77 save_container=False</strong></pre>
<p>Once <kbd>stop</kbd> has completed successfully, re-run the <kbd>oc get all</kbd> command:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ oc get all
NAME                  REVISION   DESIRED   CURRENT   TRIGGERED BY
dc/mariadb-database   2          0         0         config

NAME                    DESIRED   CURRENT   READY     AGE
rc/mariadb-database-1   0         0         0         7m
rc/mariadb-database-2   0         0         0         1m

NAME                           HOST/PORT                                              PATH      SERVICES           PORT            TERMINATION   WILDCARD
routes/mariadb-database-3306   mariadb-database-3306-database.192.168.99.100.nip.io             mariadb-database   port-3306-tcp                 None

NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
svc/mariadb-database   172.30.154.77   &lt;none&gt;        3306/TCP   7m</strong></pre>
<p>From the preceding output, we can see that OpenShift has created a new revision for the configuration change we deployed (<kbd>REVISION 2</kbd>), which describes the deployment as having zero running pod replicas, indicative of the deployment existing in the stopped state (<kbd>Current 0</kbd>, <kbd>Desired 0</kbd>, <kbd>Ready 0</kbd>). However, the route and service artifacts still exist and are running in the cluster. One of the major benefits of OpenShift is the nature of OpenShift to readily track the changes made to the project under various revision definitions. This makes it very easy to roll back to a previous deployment should a change fail or need to be rolled back. Complementary to the <kbd>stop</kbd> command is the <kbd>restart</kbd> command, which ensures the current revision is in a running state, after first stopping the service.</p>
<p>Unlike <kbd>stop</kbd>, <kbd>restart</kbd> does not create a new revision, since our current revision is already scaled down to zero replicas, but instead will scale up the current revision to ensure that the desired number of pods is running in the project. Let's execute the <kbd>ansible-container restart</kbd> command for the OpenShift engine and see how this affects our deployment:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ ansible-container --engine openshift restart
Parsing conductor CLI args.
Engine integration loaded. Preparing to restart containers.     engine=OpenShift™

PLAY [Manage the lifecycle of mariadb-k8s on OpenShift?] ***********************

TASK [Stop running containers by scaling replicas down to 0] *******************
ok: [localhost]

TASK [Create deployment, and scale replicas up] ********************************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=2    changed=1    unreachable=0    failed=0

All services restarted. playbook_rc=0
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=d54f160a64f5f57cfe88e6b824dfe609a9000355bc2e29584509ec3d97eee82f save_container=False</strong></pre>
<p>Executing the <kbd>oc get all</kbd> command once more, we will see that our current revision (#2) is now running with the desired number of pods:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ oc get all
NAME                  REVISION   DESIRED   CURRENT   TRIGGERED BY
dc/mariadb-database   2          1         1         config

NAME                    DESIRED   CURRENT   READY     AGE
rc/mariadb-database-1   0         0         0         31m
rc/mariadb-database-2   1         1         1         26m

NAME                           HOST/PORT                                              PATH      SERVICES           PORT            TERMINATION   WILDCARD
routes/mariadb-database-3306   mariadb-database-3306-database.192.168.99.101.nip.io             mariadb-database   port-3306-tcp                 None

NAME                   CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
svc/mariadb-database   172.30.154.77   &lt;none&gt;        3306/TCP   31m

NAME                          READY     STATUS    RESTARTS   AGE
po/mariadb-database-2-g7r7f   1/1       Running   0          6m</strong></pre>
<p>Finally, we can use the <kbd>ansible-container destroy</kbd> command to completely remove all traces of our service from the OpenShift (or Kubernetes) cluster. Keep in mind that this will also remove the project as well as any other containers that are also running within the project that may have been deployed manually or by other means outside of Ansible Container. This is why it is important to separate application deployments by OpenShift project and Kubernetes namespace, especially when running commands such as <kbd>ansible-container destroy</kbd>:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ ansible-container --engine openshift destroy
Parsing conductor CLI args.
Engine integration loaded. Preparing to stop+delete all containers and built images.    engine=OpenShift™

PLAY [Manage the lifecycle of mariadb-k8s on OpenShift?] ***********************

TASK [Destroy the application by removing project database] ********************
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=1    unreachable=0    failed=0

All services destroyed. playbook_rc=0
Conductor terminated. Cleaning up.      command_rc=0 conductor_id=dde59aed5fb90b1e46df4fa9e4558a46ff043f89189cc101f25a01004878f472 save_container=False</strong></pre>
<p>According to the task execution, it appears that a single task that was run deleted the entire OpenShift project. This is reflected if we execute the <kbd>oc get all</kbd> command one final time:</p>
<pre class="western"><strong>ubuntu@node01:/vagrant/Kubernetes/mariadb_demo_k8s$ oc get all
</strong><br/><strong>Error from server (Forbidden): User "developer" cannot list buildconfigs in project "database"
Error from server (Forbidden): User "developer" cannot list builds in project "database"
Error from server (Forbidden): User "developer" cannot list imagestreams in project "database"
Error from server (Forbidden): User "developer" cannot list deploymentconfigs in project "database"
Error from server (Forbidden): User "developer" cannot list deployments.extensions in project "database"
Error from server (Forbidden): User "developer" cannot list horizontalpodautoscalers.autoscaling in project "database"
Error from server (Forbidden): User "developer" cannot list replicationcontrollers in project "database"
Error from server (Forbidden): User "developer" cannot list routes in project "database"
Error from server (Forbidden): User "developer" cannot list services in project "database"
Error from server (Forbidden): User "developer" cannot list statefulsets.apps in project "database"
Error from server (Forbidden): User "developer" cannot list jobs.batch in project "database"
Error from server (Forbidden): User "developer" cannot list replicasets.extensions in project "database"
Error from server (Forbidden): User "developer" cannot list pods in project "database"</strong></pre>
<p>These errors indicate that our user can no longer list anything that exists inside of the <kbd>database</kbd> project due to the fact that it no longer exists. All traces of the project, deployments, services, pods, and routes, have been deleted from the cluster. This is also apparent from the web interface because refreshing the web page will indicate that the projects no longer exist.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">References</h1>
                
            
            
                
<ul>
<li><strong>Ansible Container Deployment Guide</strong>: <a href="https://docs.ansible.com/ansible-container/reference/deploy.html">https://docs.ansible.com/ansible-container/reference/deploy.html</a></li>
<li><strong>VirtualBox Networking Guide</strong>: <a href="https://www.virtualbox.org/manual/ch06.html">https://www.virtualbox.org/manual/ch06.html</a></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Over the course of this chapter, we looked at the final Ansible Container workflow command: <kbd>ansible-container deploy</kbd>. <kbd>deploy</kbd> is one of the most versatile commands available in the Ansible Container arsenal since it allows us to run and manage containers in production-grade Kubernetes and OpenShift environments. <kbd>deploy</kbd> opens a new path in our journey to enable the flexibility and agility that containers give our infrastructure. We can now truly use a single tool to not only build and debug containerized applications locally, but also to deploy and manage these same applications in production. Having the ability to use the same expressive Ansible Playbook language to truly build reliable and scalable applications means that deployments can be built around DevOps and automation best practices from day one, instead of the painstaking task of re-engineering deployments so they are automated after the fact.</p>
<p>Just because we have finished learning about the major Ansible Container workflow components does not mean that our journey has ended. So far in this book, we have looked at using Ansible Container to deploy single-function microservices that require no dependencies on other services. Ansible Container being as powerful as it is also has the innate ability to build and deploy multiple containerized applications by expressing links and dependencies on other services. In the next chapter, we will look at how to build and deploy multi-container applications.</p>


            

            
        
    </body></html>