<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;The Impact of Containers on Networking"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. The Impact of Containers on Networking</h1></div></div></div><p>No modern IT book would be complete without a chapter on containers. In this chapter, we will look at the history of containers and the options currently available to deploy them. This chapter will look at the changes required to support running containers from a networking perspective. We will then focus on some of the technologies used to package containers, and how they can be incorporated into a Continuous Delivery process. Finally, we will focus on some of the orchestration tools that are being used to deploy containers.</p><p>In this chapter, the following topics will be covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Overview of containers</li><li class="listitem" style="list-style-type: disc">Packaging containers</li><li class="listitem" style="list-style-type: disc">Container orchestration tools</li><li class="listitem" style="list-style-type: disc">How containers fit into continuous integration and delivery</li></ul></div><div class="section" title="Overview of containers"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec52"/>Overview of containers</h1></div></div></div><p>There has been a lot of hype about containers in the IT industry of late; you could be forgiven for thinking that containers <a id="id860" class="indexterm"/>alone will solve every application deployment problem possible. There have been a lot of marketing campaigns from vendors stating that implementing containers will make a business more agile or that they mean a business is implementing <span class="emphasis"><em>DevOps</em></span> simply by deploying their applications in containers. This is undoubtedly the case if you listen to software vendors promoting their container technology or container orchestration software.</p><p>Containers are not a new concept, though. Far from it: Solaris 10 introduced the concept of Solaris <a id="id861" class="indexterm"/>Zones as <a id="id862" class="indexterm"/>far back as 2005, which allowed users to segregate the operating system into different components and run isolated processes. Modern technologies such as <span class="strong"><strong>Docker</strong></span> or <span class="strong"><strong>Rocket</strong></span> provide a container workflow that allows users to package and deploy containers.</p><p>However, like all infrastructure concepts, containers are simply facilitators of process, and implementing containers as a standalone initiative for the wrong reasons will likely bring no business value to a company. It seems it has become almost mandatory for large software vendors to have a container-based solution as part of their portfolio, given their recent popularity.</p><p>Containers, like all tools, can be very beneficial for certain use cases. It is important when considering containers to consider the benefits that they bring to microservice architectures. It is fair to say<a id="id863" class="indexterm"/> that containers have been seen by some <span class="strong"><strong>Platform as a Service</strong></span> (<span class="strong"><strong>PaaS</strong></span>) companies as being the bridge between development and operations.</p><p>Container technologies have allowed developers to package their applications in containers in a consistent way, while at the same time describing the way in which they wish to run their microservice <a id="id864" class="indexterm"/>application in production using PaaS technology. This construct can be understood by development and operations staff, as they are both familiar with the same container technology and constructs they use to deploy applications. This means that the container that is deployed on a development workstation will behave in the same way as it would on a production system.</p><p>This has allowed developers to define their application topology and load balancing requirements more consistently, so that they are deployed identically to test and production environments using a common suite of tooling.</p><p>Famous success stories such as Netflix have shown that containerizing their whole microservice architecture is possible and can be a success. With the rise in popularity of microservice applications, a common requirement is to package and deploy a microservice application across multiple hybrid clouds. This gives organizations real choice over which private or public cloud provider they use.</p><p>In microservice architectures, cloud-native microservice applications can be scaled up or down quickly to deal with busy or quiet periods for a business. When using a public cloud, it is desirable to only utilize what is required, which can often mean that microservices can be scaled up and scaled down throughout the day to save running costs.</p><p>Elastic scaling based on utilization is a common use case when deploying cloud-native microservices so that microservices can scale up and down based on reading data from their monitoring systems or from the cloud provider.</p><p>Microservices have followed the lead of service-oriented architectures and can be seen as the modern implementation of this concept of <span class="strong"><strong>service-oriented architectures</strong></span> (<span class="strong"><strong>SOA</strong></span>). Microservices such as SOA <a id="id865" class="indexterm"/>allow multiple different components to communicate via a network of services and common set of protocols. Microservices aim to decouple services from one another into specific functions, so they can be tested in isolation and joined together to create the overall system and, as illustrated, scaled up or down as required.</p><p>When using microservice architectures, instead of having to deploy the whole system each time, different component versions can be deployed independently of each other without causing system downtime.</p><p>Containers in some <a id="id866" class="indexterm"/>ways can be seen as the perfect solution for microservice applications as they can be used to carry out specific functions in isolation. Each microservice application can be deployed within the constructs of an individual container and networked together to provide an overall service to the end user.</p><p>Containers <a id="id867" class="indexterm"/>already <a id="id868" class="indexterm"/>natively run on any <a id="id869" class="indexterm"/>Linux <a id="id870" class="indexterm"/>operating system and are lightweight <a id="id871" class="indexterm"/>by nature, meaning they can be deployed, <a id="id872" class="indexterm"/>maintained, <a id="id873" class="indexterm"/>and updated easily when utilizing popular container technology such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Docker (<a class="ulink" href="https://www.docker.com/">https://www.docker.com/</a>)</li><li class="listitem" style="list-style-type: disc">Google Kubernetes (<a class="ulink" href="http://kubernetes.io/">http://kubernetes.io/</a>)</li><li class="listitem" style="list-style-type: disc">Apache Mesos (<a class="ulink" href="http://mesos.apache.org/">http://mesos.apache.org/</a>)</li><li class="listitem" style="list-style-type: disc">IBM Bluemix (<a class="ulink" href="http://www.ibm.com/cloud-computing/bluemix/containers/">http://www.ibm.com/cloud-computing/bluemix/containers/</a>)</li><li class="listitem" style="list-style-type: disc">Rackspace Catrina (<a class="ulink" href="http://thenewstack.io/rackspace-carina-bare-metal-caas-based-openstack/">http://thenewstack.io/rackspace-carina-bare-metal-caas-based-openstack/</a>)</li><li class="listitem" style="list-style-type: disc">CoreOS Rocket (<a class="ulink" href="https://coreos.com/blog/rocket/">https://coreos.com/blog/rocket/</a>)</li><li class="listitem" style="list-style-type: disc">Oracle Solaris Zones (<a class="ulink" href="https://docs.oracle.com/cd/E18440_01/doc.111/e18415/chapter_zones.htm#OPCUG426">https://docs.oracle.com/cd/E18440_01/doc.111/e18415/chapter_zones.htm#OPCUG426</a>)</li><li class="listitem" style="list-style-type: disc">Microsoft Azure Nano Server (<a class="ulink" href="https://technet.microsoft.com/en-us/windows-server-docs/get-started/getting-started-with-nano-server">https://technet.microsoft.com/en-us/windows-server-docs/get-started/getting-started-with-nano-server</a>)</li><li class="listitem" style="list-style-type: disc">VMware Photon (<a class="ulink" href="http://blogs.vmware.com/cloudnative/introducing-photon/">http://blogs.vmware.com/cloudnative/introducing-photon/</a>)</li></ul></div><p>
<span class="strong"><strong>Containerization</strong></span> in essence <a id="id874" class="indexterm"/>is virtualizing processes on the operating system and isolating them from one another into manageable components. Container orchestration technologies <a id="id875" class="indexterm"/>then create network interfaces to allow multiple <a id="id876" class="indexterm"/>containers to be connected to each other across the operating systems, or in more complex scenarios, create full overlay networks to connect containers running on multiple physical or virtual servers using programmatic APIs and key-value stores for service discovery.</p><div class="section" title="Solaris Zones"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec119"/>Solaris Zones</h2></div></div></div><p>In 2005, Solaris introduced the notion of <span class="strong"><strong>Solaris Zones</strong></span>, and from it came the concept of containment. After a user logged <a id="id877" class="indexterm"/>in to a fresh Solaris operating system, they would find <a id="id878" class="indexterm"/>themselves in a global Solaris Zone.</p><p>Solaris then gave users the option to create new zones, configure them, install the packages to run them, and finally, boot them so they could be used. This allowed each isolated zone to be used as a contained segment within the confines of a single Solaris operating system.</p><p>Solaris allowed zones to run as a completely isolated set of processes, all from the default global zone in terms of permissions, disk, and network configuration. Different persistent storage or raw devices could be exported to the zones and mounted to make external file systems accessible to a zone. This meant that multiple different applications could run within their own unique zone and communicate with external shared storage.</p><p>In terms of networking, the global Solaris Zone would have an IP address and be connected to the default router. All new zones would have their own unique IP address on the same subnet using the same default router. Each zone could even have their own unique DNS entry if required. The networking setup for Solaris Zones is shown in the following figure, with two zones connected to the router by accessing the network configuration on the <code class="literal">/zones</code> file system:</p><div class="mediaobject"><img src="graphics/B05559_10_01.jpg" alt="Solaris Zones"/></div><p>.</p></div><div class="section" title="Linux namespaces"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec120"/>Linux namespaces</h2></div></div></div><p>A <span class="strong"><strong>Linux namespace</strong></span> creates an <a id="id879" class="indexterm"/>abstraction layer for a system process and changes to <a id="id880" class="indexterm"/>that system process only affect other processes in the same namespace. Linux namespaces can be used to isolate processes on the Linux operating system; by default, when a Linux operating system is booted, all resources run under the default namespace so have the ability to view all the processes that are running.</p><p>The namespace API has the following system calls:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">clone</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">setns</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">unshare</code></li></ul></div><p>The <code class="literal">clone</code> system call creates a new process and links all specified processes to it; the <code class="literal">setns</code> system call, on the other hand, is used to join namespaces together, and the <code class="literal">unshare</code> system call moves a process out of a namespace to a new namespace.</p><p>The following namespaces are available on Linux:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Mounts</li><li class="listitem" style="list-style-type: disc">Process ID</li><li class="listitem" style="list-style-type: disc">Interprocess communication</li><li class="listitem" style="list-style-type: disc">UTS</li><li class="listitem" style="list-style-type: disc">Network</li><li class="listitem" style="list-style-type: disc">User</li></ul></div><p>The mounts namespace is used to isolate the Linux operating system's file system so specific mount points are only seen by a certain group of processes belonging to the same namespace. This allows different processes to have access to different mount points, depending on what namespace they are part of, which can be used to secure specific files.</p><p>The <span class="strong"><strong>Process ID</strong></span> (<span class="strong"><strong>PID</strong></span>) namespace <a id="id881" class="indexterm"/>allows the reuse of PID processes on a Linux machine as each set of PIDs is unique to a namespace. This allows containers to be migrated between hosts while keeping the same PIDs, so the operation does not interrupt the container. This also allows each container to have its own unique <code class="literal">init</code> process and makes containers extremely portable.</p><p>The <span class="strong"><strong>interprocess communication</strong></span> (<span class="strong"><strong>IPC</strong></span>) namespace <a id="id882" class="indexterm"/>is used to isolate certain specific resources, such as system objects and message queues between processes.</p><p>The UTS namespace allows containers to have their own domain and host name; this is very useful when using containers, as orchestration scripts can target specific host names as opposed to IPs.</p><p>The network namespace creates a layer of isolation around network resources such as the IP space, IP tables, and routing <a id="id883" class="indexterm"/>tables. This means that each container can have its own <a id="id884" class="indexterm"/>unique networking rules.</p><p>The user namespace is used to manage user permissions to namespaces.</p><p>So, from a networking perspective, namespaces allow multiple different routing tables to coexist on the same Linux operating system, as they have complete process isolation. This means each container can have its own unique networking rules applied if desired.</p></div><div class="section" title="Linux control groups"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec121"/>Linux control groups</h2></div></div></div><p>The use of <span class="strong"><strong>control groups</strong></span> (<span class="strong"><strong>cgroups</strong></span>) allows <a id="id885" class="indexterm"/>users to control Linux <a id="id886" class="indexterm"/>operating system resources that are part of a namespace. The following<a id="id887" class="indexterm"/> cgroups can be used to control Linux resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CPU</li><li class="listitem" style="list-style-type: disc">Memory</li><li class="listitem" style="list-style-type: disc">Freezer</li><li class="listitem" style="list-style-type: disc">Block I/O</li><li class="listitem" style="list-style-type: disc">Devices</li></ul></div><p>The CPU cgroup can use two different types of scheduler: either the <span class="strong"><strong>Completely Fair Scheduler</strong></span> (<span class="strong"><strong>CFS</strong></span>), which is based on <a id="id888" class="indexterm"/>distributing CPU based on a weighting system. The <span class="strong"><strong>Real-Time Scheduler</strong></span> (<span class="strong"><strong>RTS</strong></span>) is the other alternative, and is a <a id="id889" class="indexterm"/>task scheduler that caps tasks based on their real-time utilization.</p><p>The memory cgroup is used to generate reports on memory utilization used by the tasks in a cgroup. It sets limits on the memory use of processes associated with the cgroup can use.</p><p>The freezer cgroup is used to control the process status of all processes associated with the freezer cgroup. The freezer cgroup can be used to control batches of jobs and issue the <code class="literal">FREEZE</code> command, which will stop all processes in the user space; the <code class="literal">THAW</code> command can be used to restart them again.</p><p>The <span class="strong"><strong>Block I/O</strong></span> (<span class="strong"><strong>blkio</strong></span>) cgroup monitors <a id="id890" class="indexterm"/>access to I/O on block devices and introduces limits on I/O bandwidth or access to resources. Blkio uses an I/O scheduler and can assign weights to distribute I/O or provide I/O throttling by setting maximum limits to throttle <a id="id891" class="indexterm"/>the amount of read or writes that a process can do <a id="id892" class="indexterm"/>on a device.</p><p>The devices' cgroup allows or denies access to devices by defining tasks under <code class="literal">devices.allow</code> and <code class="literal">devices.deny</code>, and can list device access using <code class="literal">devices.list</code>.</p></div><div class="section" title="Benefits of containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec122"/>Benefits of containers</h2></div></div></div><p>Containers have many <a id="id893" class="indexterm"/>benefits, with a focus on portability, agility, security, and as touched upon earlier in this chapter, have helped many organizations such as Netflix deploy their microservice architectures.</p><p>Containers also allow users to allocate different resources on an operating system using namespaces and limit CPU, memory, network block I/O, and network bandwidth using cgroups.</p><p>Containers are very quick to provision so can be scaled up and scaled down rapidly to allow elastic scaling in cloud environments. They can be scaled up rapidly to meet demand and containers can be migrated from one server to another using numerous techniques.</p><p>Cgroups can be configured quickly based on system changes, which gives users complete control over the low-level scheduling features of an operating system, which are normally delegated to the base operating system when using virtual machines or bare-metal servers. Containers can be tweaked to give greater fine-grained control over performance.</p><p>In some scenarios, not all resources on a bare-metal server will be utilized, which can be wasteful, so containers can be utilized to use all of the CPU and RAM available on a guest operating system by running multiple instances of the same application isolated by namespaces at a kernel level. This means that to each process, they appear to be functioning on their own unique operating system.</p><p>One of the main drawbacks with containers up until now has been that they have been notoriously low-level and hard to manage at scale. So, tooling such as for large implementation, and orchestration engines such as Docker Swarm, Google Kubernetes, and Apache Mesos alleviate that pain by creating abstraction layers to manage containers at scale.</p><p>Another benefit of containers is that they are very secure as they limit the attack surface area with additional layers of security added to the operating system through the use of different namespaces. If an operating system was compromised, an attacker would still need to compromise the system at the namespace level as opposed to having access to all processes.</p><p>Containers can be very useful when running multiple flavors of the same process; an example is a business that wants to run multiple versions of the same application for different customers. They want to prevent a spike in logins and transactions from one customer affecting another at the <a id="id894" class="indexterm"/>application level. Containers in this scenario would be a feasible solution.</p></div><div class="section" title="Deploying containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec123"/>Deploying containers</h2></div></div></div><p>With the growing popularity of <a id="id895" class="indexterm"/>containers, traditional Linux distributions have been found to be sub-optimal and clunky when running a pure container platform.</p><p>As a result, very minimal operating systems have been created to host containers, such as CoreOS and Red Hat Atomic, which have been developed specifically to run containers.</p><p>Sharing information across operating systems is also a challenge for containers, as by design they are <a id="id896" class="indexterm"/>isolated <a id="id897" class="indexterm"/>by namespaces and cgroups to a particular host operating system. Key-value stores such as <span class="strong"><strong>etcd</strong></span>, <span class="strong"><strong>Consul</strong></span>, and <span class="strong"><strong>Zookeeper</strong></span> can be used to cluster <a id="id898" class="indexterm"/>and cluster and share information across hosts.</p><div class="section" title="CoreOS"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec16"/>CoreOS</h3></div></div></div><p>
<span class="strong"><strong>CoreOS</strong></span> is a Linux-based operating<a id="id899" class="indexterm"/> system specifically created to provide a minimal operating <a id="id900" class="indexterm"/>system to run clusters of containers. It is the widest-used container operating system today and designed to run at massive scale without the need to frequently patch and update the software on the operating system manually.</p><p>Any application that runs on CoreOS will run in container format; CoreOS can run on bare-metal or virtual machines, on public and private clouds such as AWS and OpenStack.</p><p>CoreOS works by automatically pulling frequent security updates without affecting the containers running on the operating system. This means CoreOS doesn't need Linux admins to intervene and patch servers, as CoreOS automatically takes care of this by patching using its zero downtime security updates.</p><p>CoreOS focuses on moving application dependencies out of the application and into the container layer, so containers are dependent on other containers for their dependency management.</p></div><div class="section" title="etcd"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec17"/>etcd</h3></div></div></div><p>CoreOS uses etcd, which is a <a id="id901" class="indexterm"/>distributed key-value store that allows multiple containers across <a id="id902" class="indexterm"/>multiple machines to connect to it for data and state.</p><p>Etcd uses the <span class="strong"><strong>Raft algorithm</strong></span> to <a id="id903" class="indexterm"/>elect a leader and uses followers to maintain consistency. When multiple etcd hosts are running, the state is pulled from the instance with the majority and propagated to the followers, so it is used to keep clusters consistent and up to date.</p><p>Applications can read and write data into etcd and it is designed to deal with fault and failure <a id="id904" class="indexterm"/>conditions. Etcd can be used to store connection strings to endpoints or other environment-specific data stores.</p></div></div><div class="section" title="Docker"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec124"/>Docker</h2></div></div></div><p>It would be impossible to talk about <a id="id905" class="indexterm"/>containers without mentioning Docker. In 2013, Docker <a id="id906" class="indexterm"/>was released as an open-source initiative that could be used to package and distribute containers. Docker was originally based on Linux LXC containers, but the Docker project has since drifted away from that standard as it has become more opinionated and mature.</p><p>Docker works on the principle of isolating a single process per container in the Linux kernel. Docker uses a union-capable file system, cgroups, and kernel namespaces to run containers and isolate processes. It has a command-line interface and a well thought out workflow.</p></div><div class="section" title="Docker registry"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec125"/>Docker registry</h2></div></div></div><p>When container images are <a id="id907" class="indexterm"/>packaged, they need to be pushed to Docker's container registry server, which is an image repository for containers.</p><p>The <span class="strong"><strong>Docker registry</strong></span> is used <a id="id908" class="indexterm"/>to store containers, which can be tagged and versioned much like a package repository. This allows different container versions to be stored for roll-forward and roll-back purposes.</p><p>By default, the Docker registry is a file-system volume and persists data on a local file system. Artifact repositories such as Artifactory and Nexus now support Docker registry as a repository type. The Docker registry can be set up with authentication and SSL certificates to secure container images.</p></div><div class="section" title="Docker daemon"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec126"/>Docker daemon</h2></div></div></div><p>During installation, Docker deploys <a id="id909" class="indexterm"/>a daemon on the target operating system that has been chosen to run containers. The <span class="strong"><strong>Docker daemon</strong></span> is used to communicate <a id="id910" class="indexterm"/>with the Docker image registry and issue pull commands to pull down the latest container images or a specific tagged version. The Docker command line can then be used to schedule the start-up of the containers using the container image that has been pulled from the registry. Docker daemons, by default, run as a constant process on target operating systems, but can be started or stopped using a process manager such as <code class="literal">systemd</code>.</p></div><div class="section" title="Packaging containers"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec127"/>Packaging containers</h2></div></div></div><p>Containers can be <a id="id911" class="indexterm"/>packaged in various different ways; two of the most popular ways of packaging containers is using Dockerfiles, and one of the lesser known ways is using a tool from <span class="strong"><strong>HashiCorp</strong></span> called <a id="id912" class="indexterm"/>Packer. Both have slightly different approaches to packaging container images.</p><div class="section" title="Dockerfile"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec18"/>Dockerfile</h3></div></div></div><p>Docker allows users to package containers using its very own configuration-management tool called <span class="strong"><strong>Dockerfile</strong></span>. Dockerfile will <a id="id913" class="indexterm"/>state the intent of the container by outlining the <a id="id914" class="indexterm"/>packages that should be installed on it using package managers at build time.</p><p>The following Dockerfile shows NGINX being installed on CentOS by issuing <code class="literal">yum</code> <code class="literal">install</code> commands and exposing port <code class="literal">80</code> to the guest operating system from the packaged container. Port <code class="literal">80</code> is exposed so NGINX can be accessed externally:</p><div class="mediaobject"><img src="graphics/B05559_10_02.jpg" alt="Dockerfile"/></div><p>Once the Dockerfile has been created, Docker's command-line interface allows users to issue the following command to build a container:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker build nginx</strong></span>
</pre></div><p>The one downside is that applications are typically installed using configuration management tools such as Puppet, Chef, Ansible, and Salt. The Dockerfile is very brittle, which means that packaging scripts need to be completely re-written.</p></div><div class="section" title="Packer-Docker integration"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec19"/>Packer-Docker integration</h3></div></div></div><p>
<span class="strong"><strong>Packer</strong></span> from HashiCorp is a <a id="id915" class="indexterm"/>command-line tool which <a id="id916" class="indexterm"/>uses multiple drivers to package virtual machine images <a id="id917" class="indexterm"/>and also supports creating Docker image files. Packer can be used to package <span class="strong"><strong>Amazon Machine Image</strong></span> (<span class="strong"><strong>AMI</strong></span>) images for <a id="id918" class="indexterm"/>AWS or <span class="strong"><strong>QEMU Copy On Write</strong></span> (<span class="strong"><strong>QCOW</strong></span>) images, which can be uploaded to OpenStack Glance.</p><p>When utilizing Packer, it skips the need for using Dockerfiles to create Docker images; instead, existing configuration-management tools such as Puppet, Chef, Ansible, and Salt can be used to provision and package Docker container images.</p><p>Packer has the following <a id="id919" class="indexterm"/>high-level architecture and uses a JSON file to describe the Packer workflow, with three main parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Builders</li><li class="listitem" style="list-style-type: disc">Provisioners</li><li class="listitem" style="list-style-type: disc">Post-processors</li></ul></div><p>
<span class="strong"><strong>Builders</strong></span> are used to boot <a id="id920" class="indexterm"/>an ISO, virtual machine on a Cloud platform, or in this case, start a Docker container from an image file on a build server.</p><p>Once booted, the configuration management <span class="strong"><strong>provisioner</strong></span> will run a set of installation steps. This will create the desired <a id="id921" class="indexterm"/>state for the image, emulating what the Dockerfile would carry out. Once complete, the image will be stopped and packaged.</p><p>A set of <span class="strong"><strong>post-processors</strong></span> will then <a id="id922" class="indexterm"/>be executed to push the image to an artifact repository or Docker registry, where it is tagged and versioned.</p><p>Using Packer means existing <a id="id923" class="indexterm"/>configuration management tools can be used to package virtual machines and containers in the same way rather than using a completely different configuration-management mechanism for containers. The Docker daemon will need to be installed as a prerequisite on the build server that is being used to package the container.</p><p>In the following example, an <code class="literal">nginx.json</code> Packer file is created; the <code class="literal">builders</code> section has the type <code class="literal">docker</code> defined, which lets Packer know to use the Docker builder.</p><p>The <code class="literal">export_path</code> is where the final Docker image will be exported to and <code class="literal">image</code> is the name of the Docker image file that will be pulled from the Docker registry and started.</p><p>One provisioner of the <code class="literal">ansible-local</code> type will then execute the <code class="literal">install_nginx.yml</code> playbook to install NGINX on the Docker image, using an Ansible playbook as opposed to the Dockerfile.</p><p>Finally, the post-processors will then import the packed image, complete with NGINX installed, into the Docker registry with the tag <code class="literal">1.1</code>:</p><div class="mediaobject"><img src="graphics/B05559_10_03.jpg" alt="Packer-Docker integration"/></div><p>To execute the Packer build, <a id="id924" class="indexterm"/>simply execute the following command passing the <code class="literal">nginx.json</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>packer build nginx.json</strong></span>
</pre></div></div></div><div class="section" title="Docker workflow"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec128"/>Docker workflow</h2></div></div></div><p>The <span class="strong"><strong>Docker workflow</strong></span> fits nicely <a id="id925" class="indexterm"/>into the continuous integration process that we covered as part of <a class="link" href="ch07.html" title="Chapter 7. Using Continuous Integration Builds for Network Configuration">Chapter 7</a>, 
<span class="emphasis"><em>Using Continuous Integration Builds for Network Configuration</em></span>
and the Continuous Delivery workflow we covered in 
<a class="link" href="ch09.html" title="Chapter 9. Using Continuous Delivery Pipelines to Deploy Network Changes">Chapter 9</a>, 
<span class="emphasis"><em>Using Continuous Delivery Pipelines to Deploy Network Changes</em></span>. 
After a developer pushes a <a id="id926" class="indexterm"/>new code commit, compiling and potentially packaging new code, the continuous integration process can be extended to execute a Dockerfile to package a new Docker image as a post-deployment step.</p><p>A Docker daemon is configured on each downstream test environment and production as part of the base operating system. At deployment time, the Docker daemon is scheduled to pull down the newly packaged Docker image and create a new set of containers doing a rolling update. </p><p>This process flow can be seen as follows:</p><div class="mediaobject"><img src="graphics/B05559_10_04.jpg" alt="Docker workflow"/></div></div><div class="section" title="Default Docker networking"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec129"/>Default Docker networking</h2></div></div></div><p>In terms of networking, when Docker is <a id="id927" class="indexterm"/>installed, it creates three default networks; the networks created are the <code class="literal">bridge</code>, <code class="literal">none</code>, and <code class="literal">host</code> networks, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B05559_10_05.jpg" alt="Default Docker networking"/></div><p>The Docker daemon creates containers against the bridge (<code class="literal">docker0</code>) network by default; this occurs when a <code class="literal">docker create</code> and <code class="literal">docker start</code> are issued on the target operating system, or <a id="id928" class="indexterm"/>alternatively, just a <code class="literal">docker run</code> command can be issued. These commands will create and start new containers on the host operating system from the defined Docker image.</p><p>The <code class="literal">none</code> network is used to create a container-specific network, which allows containers to be launched and left to run; it doesn't have a network interface, though. The <code class="literal">host</code> network adds containers to the same network as the guest operating system.</p><p>When containers are launched on it, Docker's bridge network assigns each container a unique IP address on the bridge network's subnet range. The containers can be viewed by issuing the following <code class="literal">docker network inspect</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker network inspect bridge</strong></span>
</pre></div><p>Docker allows users to inspect container configuration by using the <code class="literal">docker attach</code> command; in this instance, the <code class="literal">nginx</code> container can be inspected:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker attach nginx</strong></span>
</pre></div><p>Once attached, the <code class="literal">/etc/hosts</code> file can be inspected to show the network configuration. Docker bridge uses a NAT <a id="id929" class="indexterm"/>network and can use port forwarding using the following <code class="literal">–p</code> command-line argument. For <a id="id930" class="indexterm"/>example, <code class="literal">-p 8080:8080</code> forwards port <code class="literal">8080</code> from the host to the container. This allows all containers that are running on an operating system to be accessed directly by the localhost by their IPs, using port forwarding.</p><p>In its default networking mode, Docker allows containers to be interconnected using a <code class="literal">--links</code> command-line argument, which is used to connect containers, which writes entries into the <code class="literal">/etc/hosts </code>file of containers.</p><p>The default network setup is now not recommended for use, and more sophisticated networking is present, but the concepts it covers are still important.</p><p>Docker allows user-defined networks to be defined to host containers, using network drivers to create custom networks such as custom <code class="literal">bridge</code>, <code class="literal">overlay</code>, or layer 2 <code class="literal">MACVlLAN</code> network.</p></div><div class="section" title="Docker user-defined bridge network"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec130"/>Docker user-defined bridge network</h2></div></div></div><p>A user-defined bridge network is much like the default Docker network, but it means that each container can talk to <a id="id931" class="indexterm"/>each of the other containers on the <a id="id932" class="indexterm"/>same bridge network; there is no need for linking as with the default Docker networking.</p><p>To place containers on a user-defined network, containers can be launched on the <code class="literal">devops_for_networking_bridge</code> user-defined bridge network using the following command, with the <code class="literal">–net</code> option set:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run –d –name load_balancer –net devops_for_networking_bridge nginx</strong></span>
</pre></div><p>Each container that is launched will reside on the same operating system guest. Publish is used to expose specific using of the <code class="literal">-p 8080-8081:8080/tcp</code> command. Therefore, ranges can be published so that portions of the network can be exposed.</p></div><div class="section" title="Docker Swarm"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec131"/>Docker Swarm</h2></div></div></div><p>Overlay networks, can also be used with Docker and have already been covered at length in this book, are a virtualized abstraction layer for the network. Docker can create an overlay network for containers, which is used to create a network of containers that belong to multiple different operating system hosts.</p><p>Instead of isolating each container to a unique network existing on one host, Docker instead allows its overlay network to join multiple different clusters of containers that are deployed on separate hosts together.</p><p>This means that each <a id="id933" class="indexterm"/>container that shares an overlay network will have a <a id="id934" class="indexterm"/>unique IP address and name. To create an overlay network, Docker uses its own orchestration engine, called <span class="strong"><strong>Docker Swarm</strong></span>.</p><p>To run Docker in swarm mode, an external key-value store such as etcd, Consul, or Zookeeper needs to be used with Docker. This key-value store allows Docker to share information between different hosts, including the shared overlay network.</p><div class="section" title="Docker machine"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec20"/>Docker machine</h3></div></div></div><p>It is worth mentioning <a id="id935" class="indexterm"/>that <code class="literal">docker-machine</code> is a useful command-line utility that allows virtual machines to be provisioned in VirtualBox, OpenStack, AWS, and many more platforms that have drivers.</p><p>In the following example, we can see how a machine could be booted using <code class="literal">docker-machine</code> in OpenStack:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-machine create –driver openstack (boot arguments and credentials) docker-dev</strong></span>
</pre></div><p>One of the more useful functions of <code class="literal">docker-machine</code> is its ability to boot virtual machines in cloud environments while issuing Docker Swarm commands. This allows machines to be set up on boot to the specific profile that is required.</p></div><div class="section" title="Docker Compose"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec21"/>Docker Compose</h3></div></div></div><p>Another helpful tool for orchestrating containers is <span class="strong"><strong>Docker Compose</strong></span>, as running a command line for every container that needs to be deployed is not a feasible solution at scale. Therefore, Docker <a id="id936" class="indexterm"/>Compose allows users to specify their microservice-architecture topology in YAML format, so container dependencies are chained together to form a fully-fledged application.</p><p>Microservices will be comprised of different container types, which together make up a full application. Docker Compose allows each of those microservices to be defined as YAML in the <code class="literal">docker-compose</code> file so they can be deployed together in a manageable way.</p><p>In the following <code class="literal">docker-compose.yml</code> file, <code class="literal">web</code>, <code class="literal">nginx</code>, and <code class="literal">db</code> applications are configured and linked together, with the load balancer being exposed on port <code class="literal">8080 </code>for public access, and load<a id="id937" class="indexterm"/> balancing <code class="literal">app1</code>, which is connected to the <code class="literal">redis</code> database backend:</p><div class="mediaobject"><img src="graphics/B05559_10_06.jpg" alt="Docker Compose"/></div><p>Docker Compose can be executed in the same directory as the Docker Compose YAML file to invoke a new deployment the following command should be issued:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose up</strong></span>
</pre></div></div><div class="section" title="Swarm architecture"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec22"/>Swarm architecture</h3></div></div></div><p>The Swarm architecture works on the principle that each host runs a Swarm agent and one host runs a Swarm master. The master is responsible for the orchestration of containers on each of the <a id="id938" class="indexterm"/>hosts where agents are running and that are a member of the same discovery (key-value store).</p><p>An important principle for swarm is discovery, which is catered for using a key-value store such as etcd, Consul, or Zookeeper.</p><p>To set up a Docker swarm, a set up Docker machine can be used to provision the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Discovery server (key-value store such as etcd, Consul, or Zookeeper)</li><li class="listitem" style="list-style-type: disc">Swarm master with swarm agent installed, pointing at a key-value store</li><li class="listitem" style="list-style-type: disc">Two Swarm nodes with Swarm agent installed, pointing at a key-value store</li></ul></div><p>The Docker Swarm architecture shows a master node scheduling containers on two Docker agents while they are all advertising to the key-value store, which is used for service discovery:</p><div class="mediaobject"><img src="graphics/B05559_10_07.jpg" alt="Swarm architecture"/></div><p>When setting up a Swarm agent, in this case the Swarm master, they will be booted with the following options: <code class="literal">--swarm-discovery</code> defines the address of the discovery service, while <code class="literal">--cluster-advertise</code> advertises the host machine on the network and <code class="literal">--cluster-store</code> points at a key-value store of choice:</p><div class="mediaobject"><img src="graphics/B05559_10_08.jpg" alt="Swarm architecture"/></div><p>Once the architecture has been set up, an overlay network needs to be created to run containers across the two different hosts (in this instance the overlay network is called <code class="literal">devops_for_networking_overlay</code>) by issuing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker network create –d overlay devops_for_networking_overlay</strong></span>
</pre></div><p>Containers can then be created <a id="id939" class="indexterm"/>on the network from an image using the Docker Swarm master to schedule the commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d –name loadbalancer –net devops_for_netwotking_overlay nginx</strong></span>
</pre></div><p>As each host is running in Swarm mode and attached to the key-value store, upon creation, the network information meta-data will be shared by the key-value store. This means that the network is visible to all hosts that use the same key-value store.</p><p>Containers can then be launched from any of the Swarm masters onto the same overlay network, which will join the two hosts together. This will allow each host to communicate with other containers, via the overlay network, across hosts.</p><p>Multiple overlay networks can be created; though containers can only communicate across the same overlay network they cannot communicate between different overlay networks. To mitigate this, containers can be attached to multiple different networks.</p><p>Docker Swarm allows many specific containers to be assigned and exposed using port forwarding to load balance containers. Rolling updates can also be carried out to allow upgrades of the containers' application version.</p><p>Due to its completely decentralized design, Docker Swarm is very flexible in the number of networking use cases it can solve.</p></div></div><div class="section" title="Kubernetes"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec132"/>Kubernetes</h2></div></div></div><p>Kubernetes is a popular <a id="id940" class="indexterm"/>container orchestration tool from Google which was created <a id="id941" class="indexterm"/>in 2014 and is an open-source tool. Rather than Google coming up with their own container packaging tool and packaging repository, Kubernetes instead can plug seamlessly to use Docker registry as its container image repository.</p><p>Kubernetes can orchestrate containers <a id="id942" class="indexterm"/>that are created using Docker via a <span class="strong"><strong>Dockerfile</strong></span>, or alternatively, using Packer aided by configuration management tools such as Puppet, Chef, Ansible, and Salt.</p><p>Kubernetes can be seen as an alternative to Docker Swarm, but takes a slightly different approach in terms of its architectural design and has a lot of rich scheduling features to help with container management.</p><div class="section" title="Kubernetes architecture"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl3sec23"/>Kubernetes architecture</h3></div></div></div><p>A Kubernetes cluster needs to be set up before a user can use Kubernetes to schedule containers. There is a <a id="id943" class="indexterm"/>wide variety of configuration management tools that can be used to create a production-grade Kubernetes cluster with notable solutions available from Ansible, Chef, and Puppet.</p><p>Kubernetes clustering consists of the following high-level components, which in turn have their own subset <a id="id944" class="indexterm"/>of services. At a high level, a Kubernetes cluster consists of the following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Kubectl</li><li class="listitem" style="list-style-type: disc">Master node</li><li class="listitem" style="list-style-type: disc">Worker node<div class="mediaobject"><img src="graphics/B05559_10_09.jpg" alt="Kubernetes architecture"/></div></li></ul></div><div class="section" title="Kubernetes master node"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec01"/>Kubernetes master node</h4></div></div></div><p>The master node is responsible for <a id="id945" class="indexterm"/>managing the whole Kubernetes cluster and is used to take care of orchestrating worker nodes, <a id="id946" class="indexterm"/>which is where containers are scheduled.</p><p>The master node, when deployed, consists of the following high-level components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">API server</li><li class="listitem" style="list-style-type: disc">Etcd key-value store</li><li class="listitem" style="list-style-type: disc">Scheduler</li><li class="listitem" style="list-style-type: disc">Controller manager</li></ul></div><p>The API server has a RESTful API, which allows administrators to issue commands to Kubernetes.</p><p>Etcd, as covered earlier in this chapter, is a key-value store that allows Kubernetes to store state and push changes to the rest of the cluster after changes have been made. Etcd is used by Kubernetes to hold scheduling information about pods, services, state, or even namespace information.</p><p>The Kubernetes scheduler, as the name suggests, is used to schedule containers on Services or Pods. The Scheduler will check the availability of the Kubernetes cluster and make scheduling decisions based on availability of resources so it can schedule containers appropriately.</p><p>The controller-manager is a daemon that allows a Kubernetes master to run different controller types. Controllers are used by Kubernetes to analyze the state of a cluster and make sure it is in the desired state, so if a pod fails it will be recreated or re-started. It adheres to the thresholds that are specified and is controlled by the Kubernetes' administrator.</p></div><div class="section" title="Kubernetes worker node"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec02"/>Kubernetes worker node</h4></div></div></div><p>Worker nodes are where <a id="id947" class="indexterm"/>pods run; each pod has an IP address and <a id="id948" class="indexterm"/>runs containers. It is the pod that determines all the networking for the containers and governs how they communicate across different pods.</p><p>The worker node will contain all the necessary services to manage the networking between the containers, communicate with the master node, and are also used to assign resources to the scheduled containers.</p><p>Docker also runs on each of the worker nodes and is used to pull down containers from the Docker registry and schedule containers.</p><p>
<span class="strong"><strong>Kubelet</strong></span> is the worker<a id="id949" class="indexterm"/> service and is installed on worker nodes. It communicates with the API server on the Kubernetes <a id="id950" class="indexterm"/>master and retrieves information on the desired state of pods. Kubelet also reads <a id="id951" class="indexterm"/>information updates from etcd and writes updates about cluster events.</p><p>The <code class="literal">kube-proxy</code> takes care of load balancing and networking functions such as routing packets.</p></div><div class="section" title="Kubernetes kubectl"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec03"/>Kubernetes kubectl</h4></div></div></div><p>
<span class="strong"><strong>Kubectl</strong></span> is the Kubernetes command <a id="id952" class="indexterm"/>line, which issues commands to the master node to <a id="id953" class="indexterm"/>administer Kubernetes clusters. It can also be used to call YAML or JSON, as it is talking to the RESTful API server on the master node.</p><p>A Kubernetes service is created as an abstraction layer above pods, which can be targeted using a label selector.</p><p>In the following example, kubectl can be used to create a <code class="literal">loadbalancing_service</code> service deployment with a selector, <code class="literal">app: nginx</code>, which is defined by the <code class="literal">loadbalancing_service.yml</code> file:</p><div class="mediaobject"><img src="graphics/B05559_10_10.jpg" alt="Kubernetes kubectl"/></div><p>Kubectl executes the YAML file by specifying:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Kubectl create –f loadbalancing_service.yml</strong></span>
</pre></div><p>Kubectl can then create four replica pods using the <code class="literal">ReplicationController</code>, these four pods will be managed <a id="id954" class="indexterm"/>by the service, as the labels <code class="literal">app: nginx</code> match <a id="id955" class="indexterm"/>the service's selector and launch an NGINX container in each pod using the <code class="literal">nginx_pod.yml</code> file:</p><div class="mediaobject"><img src="graphics/B05559_10_11.jpg" alt="Kubernetes kubectl"/></div><p>Kubectl creates the service using the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>kubectl create –f nginx_pod.yml</strong></span>
</pre></div></div><div class="section" title="Kubernetes SDN integration"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl4sec04"/>Kubernetes SDN integration</h4></div></div></div><p>Kubernetes supports multiple <a id="id956" class="indexterm"/>networking techniques that could fill a whole book's worth of material on its own. With the Kubernetes, the pod is the <a id="id957" class="indexterm"/>major insertion point for networking.</p><p>Kubernetes supports the following networking options:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Google Compute Engine</li><li class="listitem" style="list-style-type: disc">Open vSwitch</li><li class="listitem" style="list-style-type: disc">Layer 2 Linux Bridge</li><li class="listitem" style="list-style-type: disc">Project Calico</li><li class="listitem" style="list-style-type: disc">Romana</li><li class="listitem" style="list-style-type: disc">Contiv</li></ul></div><p>Kubernetes looks to provide a pluggable framework to control a pod's networking configuration and aims to give users a choice; if a flat layer 2 is required, Kubernetes caters for it, if a more complex layer-3 overlay network is required, then it can cater for this, too.</p><p>With Open vSwitch being widely used with enterprise SDN controllers such as Nuage Networks VSP platform, which was covered in <a class="link" href="ch02.html" title="Chapter 2. The Emergence of Software-defined Networking">Chapter 2</a>, <span class="emphasis"><em>The Emergence of Software-defined Networking</em></span> and <a class="link" href="ch06.html" title="Chapter 6. Orchestrating SDN Controllers Using Ansible">Chapter 6</a>, <span class="emphasis"><em>Orchestrating SDN Controllers Using Ansible</em></span>. This focused upon how flow information could be pushed down to Open vSwitch on each hypervisor to create a stateful  <a id="id958" class="indexterm"/>firewall and govern the ACL policies.</p><p>A similar implementation is <a id="id959" class="indexterm"/>carried out when integrating Kubernetes, with Open vSwitch, being deployed onto each worker node and pod traffic being deferred to Open vSwitch.</p><p>In Nuage's case, a version of their customized version of Open vSwitch, known as the VRS, is deployed on each Kubernetes worker to govern policy controlled by the VSD Nuage VSPs policy engine..</p><p>The workflow for the Nuage SDN integration with Kubernetes is shown in the following figure, which shows that enterprise SDN controllers can integrate with orchestration engines such as Kubernetes and Docker to provide enterprise-grade networking:</p><div class="mediaobject"><img src="graphics/B05559_10_12.jpg" alt="Kubernetes SDN integration"/></div></div></div></div></div></div>
<div class="section" title="Impact of containers on networking"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec53"/>Impact of containers on networking</h1></div></div></div><p>Containers have undoubtedly meant that a lot of networking has shifted into the application tier, so really, <a id="id960" class="indexterm"/>containers can be seen as a PaaS offering in its truest form.</p><p>Infrastructure is, of course, <a id="id961" class="indexterm"/>still required to run containers, be it on bare-metal servers or virtual machines. The merits of virtual machines being used to run containers long term are debatable, as in a way it means a double set of virtualization, and anyone using nested virtualization will know it isn't always optimal for performance. So with more organizations using containers to deploy their microservice architectures, it will undoubtedly mean that users having a choice to run containers on either virtual or physical machines will be in demand.</p><p>Cloud has notoriously meant virtual machines, so running containers on virtual machines is probably born out of necessity rather than choice. Being able to orchestrate containers on bare-metal servers with an overlay network on top of them is definitely more appealing as it pushes the container closer to the physical machine resources without the visualization overhead.</p><p>This allows containers to maximize the physical machine resources, and users then only care about anti-infinity in terms of whether the service can run across multiple clouds and data centers, giving true disaster recovery.</p><p>With hybrid cloud solutions, the industry is moving beyond thinking about rack redundancy. Instead it is moving toward a model which will focus on splitting applications across multiple cloud providers. So having the ability to orchestrate the networking and applications in an identical way using orchestration engines such as Docker Swarm or Kubernetes can be used to make that goal a reality.</p><p>What does this mean for the network operator? It means that the role is evolving, it means that the network engineer's role becomes advisory, helping the developers architect the network in the best possible way to run their applications. Rather than building a network as a side project in a private cloud, network operators can instead focus on providing an overlay network as a service to developers while making the underlay network fabric fast and performant so that it can scale out to meet the developer's needs.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec54"/>Summary</h1></div></div></div><p>Containers have been said to be a major disruptor of the virtualization market. Gartner have predicted the following:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"By 2018, more than 50-60% of new workloads will be deployed into containers in at least one stage of the application life cycle".</em></span></p></blockquote></div><p>This is based on Gartner's analysis of the IT market, so this is a bold statement, but if it comes to fruition, it will prove to be a huge cultural shift in the way applications are deployed, in the same way virtualization was before it.</p><p>In this chapter, we showed that containers can help organizations deploy their microservice architectures and analyzed the internal mechanics and benefits that containers bring. The key benefits are portability, speed of deployment, elastic scalability, isolation and maximization of different resources, performance control, limited attack vector, and support for multiple networking types.</p><p>Aside from the benefits containers bring, this chapter looked at the Docker tool and illustrated how the Docker workflow can be fitted into a Continuous Delivery model, which is at the heart of most DevOps initiatives.</p><p>The focus of the chapter then shifted to Docker networking and the layer-2 networking options available to network containers. We illustrated how to use overlay networks to join multiple hosts together to form a cluster and we showed how container technology can integrate with SDN controllers such as Nuage VSP Platform using Open vSwitch.</p><p>The chapter also covered container orchestration solutions such as Docker Swarm and Kubernetes, their unique architectures, and ways in which they can be used to network containers over multiple hosts and act as a Platform as a Service layer.</p><p>The importance of containerization and its impact on Platform as a Service (PaaS) solutions cannot be underestimated, with Forrester stating the following:</p><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>"Containers as a Service (CaaS) is becoming the new Platform as a Service (PaaS). With the interest in containers and micro-services skyrocketing among developers, cloud providers are capitalizing on the opportunity through hosted container management services."
</em></span>
</p></blockquote></div><p>In summary, it is fair to conclude that containerization can have many benefits and help aid developers in the implementation of Continuous Delivery workflows and PaaS solutions. Containerization also gives the added flexibility of deploying workloads across multiple cloud providers, be they private or public, using a common orchestration layer such as Kubernetes, Apache Mesos, or Docker Swarm.</p><p>In the following chapter, the focus will shift from containers toward securing the network when using software-defined overlay networks and a Continuous Delivery model. It will explore techniques that can be used to help secure a modern private cloud in an API-driven environment, so that software-defined networking solutions can be implemented without compromising security requirements.</p></div></body></html>