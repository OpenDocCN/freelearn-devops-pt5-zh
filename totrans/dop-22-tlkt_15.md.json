["```\n`1` resp_time{method=\"GET\",path=\"/demo/hello\",service=\"go-demo\"} 0.611446292 \n```", "```````````````````````````````````````````````` Through those labels, we would know which service the metric belongs to, the path of the request, and the method.    In the sea of possible additional labels we could add, there is one more that could be considered critical. We should know the status code. If we adopt standard HTTP response codes, the same ones our backend provides with the rest of the response, we can easily filter metrics and, for example, retrieve only those that are related to server errors.    Our updated metric could be as follows.    ``` `1` resp_time{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\"} 0.611446\\ `2` 292  ```   ``````````````````````````````````````````````` That is surely it. Right? Well, that’s not quite what we truly need. A few other critical things are missing but we will not comment on them just yet. Since the libraries used to instrument code add a few additional features, we’ll comment on them once we reach the hands-on part. For now, it is enough to know that we can instrument services to generate metrics to count (e.g. errors) or observe (e.g. response times). Any additional information can be provided through labels.    ### Differentiating Services Based On Their Types    Before we start instrumenting our services, we should discuss services we’re deploying. They can be divided into three categories: online services, offline services, and batch processes. While there is overlap between each of those types and it is often not that easy to place a service into only one of them, such a division will provide us with a good understanding of the types of metrics we should implement.    We can define online services as those that accept requests from another service, a human, or a client (e.g. browser). Those who send requests to online services often expect an immediate response. Front-end, APIs, and databases are only a few of the examples of such services. Due to the expectations we have from them, the key metrics we are interested in are the number of requests they served, the number of errors they produced, and latency.    Offline services are those that do not have a client that is waiting for a response. Something, or someone, instructs those services to do some tasks without waiting until they are finished. A good example of such a service would be Jenkins. Even though it does have it’s UI and API and can fall in the category of online services, most of the work it does is offline. An example would be builds triggered by a webhook from a code repository. Those webhooks do not wait until Jenkins finishes building the jobs initiated by them. Instead, they announce that there is a new commit and trust that Jenkins will know what to do and will do it well. With offline services, we usually want to track the number of tasks being executed, the last time something was processed, and the length of queues.    Finally, the last group of services is batch processes. The significant difference when compared with offline services is that batch jobs do not run continuously. They start execution of a task or a group of tasks, terminate, and disappear. That makes them very difficult to scrape. Prometheus would not know when a batch job started nor when it should end. We cannot expect a system (Prometheus or any other) to pull metrics from a batch job. Our best bet is to push them instead. With such services, we usually track how long it takes to complete them, how long each stage of a job lasted, the time a job finished executing, and whether it produced an error or it was successful.    I prefer avoiding batch jobs since they are very hard to track and measure. Instead, when possible, we should consider converting them into offline services. A good example is, again, Jenkins. It allows us to schedule execution of a job thus providing a similar functionality as a batch process while still providing easy to scrape metrics and health checks.    Now that we divided services into groups, we can discuss different types of metrics we can define when instrumenting our services.    ### Choosing Instrumentation Type    Prometheus supports four major metric types. We can make a choice between counters, gauges, summaries, and histograms. We will see them in action soon. For now, we’ll limit the discussion to a brief overview of each.    A *counter* can only go up. We cannot decrease its value. It is useful for accumulating values. An example would be errors. Each error in the system should increase the counter by one.    Unlike counters, *gauge* values can go both up and down. A good example of a gauge is memory usage. It can increase, only to decrease a few moments later.    *Histograms* and *summaries* are more complex types. They are often used to measure durations of requests and sizes of responses. They track both summaries and counts. When those two are combined, we can measure averages over time. Their data is usually placed in buckets that form quantiles.    We’ll go deeper into each of the metric types through practical examples starting with a *counter* as the simplest of all. But, before we do that, we need to create a cluster that will serve as our playground.    ### Creating The Cluster And Deploying Services    All hands-on parts of the past chapters started with the execution of a script that creates a cluster and deploys the services we’ll need. This chapter is no exception. You know the drill so let’s get to it.    ``` `1` chmod +x scripts/dm-swarm-11.sh `2`  `3` ./scripts/dm-swarm-11.sh `4`  `5` `eval` `$(`docker-machine env swarm-1`)` `6`  `7` docker stack ls  ```   `````````````````````````````````````````````` We executed the `dm-swarm-11.sh` script which, in turn, created a Swarm cluster composed of Docker Machines, created the networks, and deployed only one stack. The last command listed all the stacks in the cluster and showed that we are running only the `proxy` stack.    Let’s move into the *counter* metric.    ### Instrumenting Services Using Counters    There are many usages of the *counter* metric. We can measure the number of requests entering the system, the number of bytes sent through the network, the number of errors, and so on. Whenever we want to record an incremental value, a counter is a good choice.    We’ll use counter to track errors produced by a service. With such a goal, a counter is usually put around code that handles errors.    The examples that follow are taken from [Docker Flow Swarm Listener](http://swarmlistener.dockerflow.com/). The code is written in Go. Do not be afraid if that is not your language of choice. As you will see, examples are straightforward and can be easily extrapolated to any programming language.    Prometheus provides [client libraries](https://prometheus.io/docs/instrumenting/clientlibs/) for a myriad of languages. Even if your favorite language is not one of them, it should be relatively easy to roll-out your own solution based on [exposition formats](https://prometheus.io/docs/instrumenting/exposition_formats/).    The reason for choosing *Docker Flow Swarm Listener* lies in its type. It is (mostly) an offline service. Most of its objectives are to listen to Docker events through its socket and propagate Swarm events to other services like *Docker Flow Proxy* and *Docker Flow Monitor*. It does have an API but, since it is not its primary function, we’ll ignore it. As such, it is not a good candidate for more complex types of metrics thus making it suitable for a counter. That does not mean that the counter is the only metric it implements. However, we need to start from something simple, so we’ll ignore the others.    There are a few essential things code needs to do to start producing metrics.    We must define a variable that determines the type of the metric. Since we want to count errors, the code can be as follows.    ``` `1` `var` `errorCounter` `=` `prometheus``.``NewCounterVec``(` `2 `  `prometheus``.``CounterOpts``{` `3 `    `Subsystem``:` `\"docker_flow\"``,` `4 `    `Name``:` `\"error\"``,` `5 `    `Help``:` `\"Error counter\"``,` `6 `  `},` `7 `  `[]``string``{``\"service\"``,` `\"operation\"``},` `8` `)`  ```    ``` errorCounter based on CounterVec structure provided through the NewCounterVec function. The function requires two arguments. The first one (CounterOpts) defines options of the counter. In our case, we set the subsytem to docker_flow and the name to error. The fully qualified metric name consists of the namespace (we’re not using it today), subsystem, and name. When combined, the metric we are creating will be called docker_flow_error. Help is only for informative purposes and should help users of our metrics understand better its purpose. As you can see, I was not very descriptive with the help. Hopefully, it is clear what it does without a more detailed explanation. ```    The second argument is the list of labels. They are critical since they allow us to filter metrics. In our case, we want to know which service generated metrics. That way, we can have the same instrumentation across many services and choose whether to explore them all at once or filter by the service name.    Knowing which service produced errors is often not enough. We should be able to pinpoint a particular operation that caused a problem. The second label called `operation` provides that additional info.    It is important to specify all the labels we might need when filtering metrics, but not more. Each label requires extra resources. While that is in most cases negligible overhead, it could still have a negative impact when dealing with big systems. Just follow the rule of “everything you need, but not more” and you should be on the right track.    Please read the [Use labels](https://prometheus.io/docs/practices/instrumentation/#use-labels) section of the instrumentation page for a discussion about dos and don’ts.    The `errorCounter` variable is, in Prometheus terms, called collector. Each collector needs to be registered. We’ll do that inside `init` function that is executed automatically, thus saving us from worrying about it.    ``` `1` `func` `init``()` `{` `2 `  `prometheus``.``MustRegister``(``errorCounter``)` `3` `}`  ```   ````````````````````````````````````````````` Now we are ready to start incrementing the `errorCounter`. Since I do not like repeated code, the code that increments the metric is wrapped into another function. It is as follows.    ``` `1` `func` `recordError``(``operation` `string``,` `err` `error``)` `{` `2 `  `metrics``.``errorCounter``.``With``(``prometheus``.``Labels``{` `3 `    `\"service\"``:`   `metrics``.``serviceName``,` `4 `    `\"operation\"``:` `operation``,` `5 `  `}).``Inc``()` `6` `}`  ```   ```````````````````````````````````````````` Whenever this function is called, `errorCounter` will be incremented by one (`Inc()`). Each time that happens, the name of the service and the operation that produced the error will be recorded as labels.    An example invocation of the `recordError` function is as follows.    ```  `1` `...`  `2` `err` `=` `n``.``ServicesCreate``(`  `3`  `newServices``,`  `4`  `args``.``Retry``,`  `5`  `args``.``RetryInterval``,`  `6` `)`  `7` `if` `err` `!=` `nil` `{`  `8`  `metrics``.``RecordError``(``\"ServicesCreate\"``)`  `9` `}` `10` `...`  ```   ``````````````````````````````````````````` The function `ServicesCreate` returns an `err` (short for `error`). If the `err` is not `nil`, the `recordError` is called passing `GetServices` as operation and thus incrementing the counter.    The last piece missing is to enable `/metrics` as the endpoint Prometheus can use to scrape metrics from out service.    ``` `1` `func` `(``m` `*``Serve``)` `Run``()` `error` `{` `2 `  `mux` `:=` `http``.``NewServeMux``()` `3 `  `...` `4 `  `mux``.``Handle``(``\"/metrics\"``,` `prometheus``.``Handler``())` `5 `  `...` `6` `}`  ```    ``` /metrics as the address that is handled by Prometheus handler provided with the GoLang client library we’re using. ```    I hope that those few snippets of Go code were not scary. Even if you never worked with Go, you probably managed to understand the gist of it and will be able to create something similar in your favorite language. Remember to visit [Client Libraries](https://prometheus.io/docs/instrumenting/clientlibs/) page, choose the preferred language, and follow the instructions.    If you’re interested in the full source code behind the snippets, please visit [vfarcic/docker-flow-swarm-listener](https://github.com/vfarcic/docker-flow-swarm-listener) GitHub repository.    Let’s see those metrics in action.    Since `swarm-listener` deployed through the `proxy` stack does not publish port `8080`, we’ll create a new service attached to the `proxy` network. It will be global so that it is guaranteed to run on each node. That way it’ll be easier to find the container, enter into it, and send requests to `swarm-listener`.    ``` `1` docker service create --name util `\\` `2 `    --mode global `\\` `3 `    --network proxy `\\` `4 `    alpine sleep `1000000`  ```   `````````````````````````````````````````` We created the `util` service based on the `alpine` image and made it sleep for a very long time. Please confirm that it is up-and-running by executing `docker service ps util`.    Let’s find the ID of the container running on the node our Docker client points to and enter inside it.    ``` `1` `ID``=``$(`docker container ls -q `\\` `2 `    -f `\"label=com.docker.swarm.service.name=util\"``)` `3`  `4` docker container `exec` -it `$ID` sh  ```   ````````````````````````````````````````` The only thing missing is to install `curl`.    ``` `1` apk add --update curl  ```   ```````````````````````````````````````` Now we can send a request to `swarm-listener` and retrieve metrics.    ``` `1` curl `\"http://swarm-listener:8080/metrics\"`  ```   ``````````````````````````````````````` You’ll see a lot of metrics that come out of the box when using Prometheus clients. In this case, most of the metrics are very particular to Go, so we’ll skip them. What you won’t be able to see is `docker_flow_error`. Since the service did not produce any errors, that metric does not show.    Let’s get out of the container we’re in.    ``` `1` `exit`  ```   `````````````````````````````````````` My guess is that you would not be delighted reaching this far without seeing the metric we discussed so let us generate a situation in which `swarm-listener` will produce errors.    *Docker Flow Swarm Listener* discovers services by communicating with Docker Engine through its socket. Typically, the service mounts the socket to the host and, in that way, Docker client inside the container communicates with Docker Engine running on the node. If we remove that mount, the communication will be broken, and *Docker Flow Swarm Listener* will start reporting errors.    Let’s test it out.    ``` `1` docker service update `\\` `2 `    --mount-rm /var/run/docker.sock `\\` `3 `    proxy_swarm-listener  ```   ````````````````````````````````````` We removed the `/var/run/docker.sock` mount and the communication between Docker client inside the container and Docker engine on the host was cut. We should wait a few moments until Docker reschedules a new replica. If you want to confirm that the update was finished, please execute `docker stack ps proxy`.    Let’s check the logs and confirm that the service is indeed generating errors.    ``` `1` docker service logs proxy_swarm-listener  ```   ```````````````````````````````````` One of the output entries should be similar to the one that follows.    ``` `1` ... `2` Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docke\\ `3` r daemon running?  ```   ``````````````````````````````````` Now that the service is generating errors, we can take another look at the metrics and confirm that `docker_flow_error` is indeed added to the list of metrics.    ``` `1` docker container `exec` -it `$ID` sh `2`  `3` curl `\"http://swarm-listener:8080/metrics\"`  ```   `````````````````````````````````` We entered the `util` replica and sent a request to `swarm-listener` endpoint `/metrics`. The output, limited to the relevant parts, should be as follows.    ``` `1` ... `2` # HELP docker_flow_error Error counter `3` # TYPE docker_flow_error counter `4` docker_flow_error{operation=\"GetServices\",service=\"swarm_listener\"} 10 `5` ...  ```   ````````````````````````````````` Please note that metrics are ordered alphabetically, so `docker_flow_error` should be somewhere around the top.    As you can see, `docker_flow_error` generated `10` errors. By inspecting labels, we can see that the operation that causes errors is `GetServices` and that the service is `swarm_listener`. If this would be a production system, we’d know not only that there is a problem with the service but also which part of it caused the issue. That is very important since the actions the system should take are rarely the same for the whole service. Knowing that the problem is related to a particular operation or a function, lets us fine tune the actions the system should take when certain thresholds are reached.    Before we continue, let us exit the container we’re in and restore the `swarm-listener` to its original state.    ``` `1` `exit` `2`  `3` docker stack deploy `\\` `4 `    -c stacks/docker-flow-proxy-mem.yml `\\` `5 `    proxy  ```   ```````````````````````````````` We redeployed the stack and thus restored the mount we removed.    Let’s try to generate the same metric with a different value. We can, for example, remove `proxy` service and deploy `go-demo` stack. *Docker Flow Swarm Listener* will detect a new service and try to send the information to the `proxy`. If it fails to do so, Prometheus client will increase `docker_flow_error` by one.    ``` `1` docker service rm proxy_proxy `2`  `3` docker stack deploy `\\` `4 `    -c stacks/go-demo-scale.yml `\\` `5 `    go-demo  ```   ``````````````````````````````` We removed the proxy and deployed `go-demo` stack. *Docker Flow Swarm Listener* will try to send service information to the proxy and, since we removed it, fail to do so. By default, if `swarm-listener` fails to deliver information, it retries for fifty times with five seconds delay in between. That means that we need to wait a bit over 4 minutes for `swarm-listener` to give up and throw an error.    After a while, we can check the logs.    ``` `1` docker service logs proxy_swarm-listener  ```   `````````````````````````````` After fifty retries, you should see log entries similar to the ones that follow.    ``` `1` ... `2` Retrying service created notification to ... `3` ERROR: Get ...: dial tcp: lookup proxy on 127.0.0.11:53: no such host  ```   ````````````````````````````` Now we can go back to the `util` container and take another look at the metrics.    ``` `1` docker container `exec` -it `$ID` sh `2`  `3` curl `\"http://swarm-listener:8080/metrics\"`  ```   ```````````````````````````` This time, `docker_flow_error` metric is slightly different.    ``` `1` # HELP docker_flow_error Error counter `2` # TYPE docker_flow_error counter `3` docker_flow_error{operation=\"notificationSendCreateServiceRequest\",service=\"swar\\ `4` m_listener\"} 1 `5` ...  ```   ``````````````````````````` The `operation` label has the value `notificationSendCreateServiceRequest` clearly indicating that it comes from a different place than the previous error.    The two errors we explored are of quite a different nature and should be treated differently. The one associated with the label `GetServices` means that there is no communication with the Docker socket. That could be caused by a faulty manager and the action that should remedy that could be to reschedule the service to a different node or maybe even to remove that node altogether. The code of the service will retry establishing socket connection so we should probably not react on the first occupancy of the metric but wait until, for example, it reaches twenty failed attempts over the timespan of five minutes or less.    The error related to the `notificationSendCreateServiceRequest` means that there is no communication with the services that should receive notifications. In this case, that destination is the proxy. The problem might be related to networking, or the proxy is not running. Our action might be to check whether the proxy is running and, if it isn’t, deploy it again. Or maybe there should be no action at all. The proxy itself should have its own alerts that will remedy the situation. Moreover, the service does not throw an error when the connection with the proxy fails. Instead, it retries it for a while and errors only if all attempts failed. That means that we should react on the first occurrence of the error.    As you can see, even though those two errors come from the same service, the causes and the actions associated with them are entirely different. For that reason, we are using the `operation` label to distinguish them. Later on, it should be relatively easy to filter them in Prometheus and define different alerts.    Instrumenting our service with counters was easy. Let’s see whether gauge is any different.    Since we removed the proxy service, we should exit the `util` container and restore the stack to its original state before we proceed further.    ``` `1` `exit` `2`  `3` docker stack deploy `\\` `4 `    -c stacks/docker-flow-proxy-mem.yml `\\` `5 `    proxy  ```   `````````````````````````` ### Instrumenting Services Using Gauges    Gauges are very similar to counters. The only significant difference is that we can not only increment, but also decrease their values. We’ll continue exploring [vfarcic/docker-flow-swarm-listener](https://github.com/vfarcic/docker-flow-swarm-listener) GitHub repository for an example of a gauge.    Since `gauge` is almost identical to `counter`, we won’t go into many details but only briefly explore a few snippets.    Just as with a counter, we need to declare a variable that defines the type of the metric. A simple example is as follows.    ``` `1` `var` `serviceGauge` `=` `prometheus``.``NewGaugeVec``(` `2` \t`prometheus``.``GaugeOpts``{` `3` \t\t`Subsystem``:` `\"docker_flow\"``,` `4` \t\t`Name``:` `\"service_count\"``,` `5` \t\t`Help``:` `\"Service gauge\"``,` `6` \t`},` `7` \t`[]``string``{``\"service\"``},` `8` `)`  ```   ````````````````````````` Next, we need to register it with Prometheus. We’ll reuse the code from the `init` function where we defined the `errorCounter` and add `serviceGauge`.    ``` `1` `func` `init``()` `{` `2` \t`prometheus``.``MustRegister``(``errorCounter``,` `serviceGauge``)` `3` `}`  ```   ```````````````````````` There’s also a function that simplifies the usage of the metric.    ``` `1` func RecordService(count int) { `2` \tserviceGauge.With(prometheus.Labels{ `3` \t\t\"service\":   serviceName, `4` \t}).Set(float64(count)) `5` }  ```   ``````````````````````` We’re setting the value of the gauge using the `Set` function. Alternatively, we could have used `Add` or `Sub` functions to add or subtract the value. `Inc` or `Dec` can be used to increase of decrease the value by one.    Finally, on every iteration of the `swarm-listener`, we are setting the gauge to the number of services retrieved by `swarm-listener`.    ``` `1` `metrics``.``RecordService``(``len``(``service``.``Services``))`  ```   `````````````````````` Let’s take another look at the `/metrics` endpoint.    ``` `1` docker container `exec` -it `$ID` sh `2`  `3` curl `\"http://swarm-listener:8080/metrics\"`  ```   ````````````````````` One of the metric entries is as follows.    ``` `1` # HELP docker_flow_service_count Service gauge `2` # TYPE docker_flow_service_count gauge `3` docker_flow_service_count{service=\"swarm_listener\"} 1  ```   ```````````````````` It might look confusing that the value of the metric is one since we are running a few other services. *Docker Flow Swarm Listener* fetches only services with the `com.df.notify` label. Among the services we’re currently running, only `go-demo_main` has that label, hence being the only one included in the metric.    Let’s see what happens if we remove `go-demo_main` service.    ``` `1` `exit` `2`  `3` docker service rm go-demo_main `4`  `5` docker container `exec` -it `$ID` sh `6`  `7` curl `\"http://swarm-listener:8080/metrics\"`  ```   ``````````````````` The output of the `/metrics` API is as follows (limited to the relevant parts).    ``` `1` # HELP docker_flow_service_count Service gauge `2` # TYPE docker_flow_service_count gauge `3` docker_flow_service_count{service=\"swarm_listener\"} 0 `4` ...  ```   `````````````````` As you can see, the `docker_flow_service_count` metric is now set to zero thus accurately representing the number of services discovered by `swarm-listener`. If, in your case, the number is still one, please wait a few moments and try again. *Docker Swarm Listener* has five seconds iterations, and you might have requested metrics too soon.    Let us exit the `util` container and restore the `go-demo` stack before we proceed into histograms.    ``` `1` `exit` `2`  `3` docker stack deploy `\\` `4 `    -c stacks/go-demo-scale.yml `\\` `5 `    go-demo  ```   ````````````````` ### Instrumenting Services Using Histograms And Summaries    When compared with counters and gauges, histograms are much more complex. That does not mean that they are harder to implement but that the data they provide is less simple when compared with the other metric types we explored. We’ll comment on them by studying a sample code and the output it provides.    We’ll switch from the [vfarcic/docker-flow-swarm-listener](https://github.com/vfarcic/docker-flow-swarm-listener) repository to [vfarcic/go-demo](https://github.com/vfarcic/go-demo) since it provides a simple example of a histogram.    Just as with the other types of metrics, histogram also needs to be declared as a variable of the particular type.    ```  `1` `var` `(`  `2`  `histogram` `=` `prometheus``.``NewHistogramVec``(``prometheus``.``HistogramOpts``{`  `3`    `Subsystem``:` `\"http_server\"``,`  `4`    `Name``:` `\"resp_time\"``,`  `5`    `Help``:` `\"Request response time\"``,`  `6`  `},` `[]``string``{`  `7`    `\"service\"``,`  `8`    `\"code\"``,`  `9`    `\"method\"``,` `10 `    `\"path\"``,` `11 `  `})` `12` `)`  ```   ```````````````` The objective of the metric is to record information about response times. Its labels provide additional information like the name of the service (`service`), the response code (`code`), the method of the request (`method`), and the path (`path`). All those labels together should give us a fairly accurate picture of the response times of the service, and we’ll be able to filter the results using any combination of the labels.    Next is a helper function that will allow us to record metrics easily.    ```  `1` `func` `recordMetrics``(``start` `time``.``Time``,` `req` `*``http``.``Request``,` `code` `int``)` `{`  `2`  `duration` `:=` `time``.``Since``(``start``)`  `3`  `histogram``.``With``(`  `4`    `prometheus``.``Labels``{`  `5`      `\"service\"``:` `serviceName``,`  `6`      `\"code\"``:` `fmt``.``Sprintf``(``\"%d\"``,` `code``),`  `7`      `\"method\"``:` `req``.``Method``,`  `8`      `\"path\"``:` `req``.``URL``.``Path``,`  `9`    `},` `10 `  `).``Observe``(``duration``.``Seconds``())` `11` `}`  ```   ``````````````` The `recordMetrics` function accepts argument that defines the time when a request started (`start`), the request itself (`req`), and the response code (`code`). We’re calling histogram’s’ `Observe` function with the duration of the request expressed in seconds. The duration is obtained by calculating the time passed since the value of the `start` variable.    Let’s take a look at one of the functions that invokes `recordMetrics`.    ``` `1` `func` `HelloServer``(``w` `http``.``ResponseWriter``,` `req` `*``http``.``Request``)` `{` `2 `  `start` `:=` `time``.``Now``()` `3 `  `defer` `func``()` `{` `recordMetrics``(``start``,` `req``,` `http``.``StatusOK``)` `}()` `4`  `5 `  `// The rest of the code that processes the request.` `6` `}`  ```   `````````````` Whenever a request is made to a particular path, the web server invokes the `HelloServer` function. That function starts by recording the current time and storing it in the `start` variable. Go has a special statement that defers execution of a function. In this case, we defined that the invocation of the `recordMetrics` should be deferred. As a result, it will be executed before the `HelloServer` function exists, thus giving us an (almost) exact duration of the requests.    A similar logic is applied to all endpoints of the service thus providing us with the response times of the whole service.    If you’re interested in the full source code behind the snippets, please visit [vfarcic/go-demo](https://github.com/vfarcic/go-demo) GitHub repository.    Let us send some traffic to the `go-demo` service before we explore the histogram metrics.    ``` `1` `for` i in `{``1`..100`}``;` `do` `2 `    curl `\"http://``$(`docker-machine ip swarm-1`)``/demo/hello\"` `3` `done`  ```   ````````````` We’ll repeat the already familiar process of entering the `util` container and retrieving the metrics. The only difference is that this time we’ll explore `go-demo_main` metrics instead of those from the `swarm-listener`.    ``` `1` docker container `exec` -it `$ID` sh `2`  `3` curl `\"http://go-demo_main:8080/metrics\"`  ```   ```````````` The output, limited to relevant parts, is as follows.    ```  `1` ...  `2` # HELP resp_time Request response time  `3` # TYPE resp_time histogram  `4` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\  `5` =\"0.005\"} 69  `6` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\  `7` =\"0.01\"} 69  `8` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\  `9` =\"0.025\"} 69 `10` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `11` =\"0.05\"} 69 `12` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `13` =\"0.1\"} 69 `14` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `15` =\"0.25\"} 69 `16` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `17` =\"0.5\"} 69 `18` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `19` =\"1\"} 69 `20` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `21` =\"2.5\"} 69 `22` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `23` =\"5\"} 69 `24` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `25` =\"10\"} 69 `26` resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\",le\\ `27` =\"+Inf\"} 69 `28` resp_time_sum{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\"} 0.00\\ `29` 3403602 `30` resp_time_count{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go-demo\"} 69 `31` ...  ```   ``````````` Unlike counters and gauges, each histogram produces quite a few metrics. The major one is `resp_time_sum` that provides a summary of all the recorded responses. Below it is `resp_time_counter` with the number of responses. Based on those two, we can see that `69` responses took `0.0034` seconds. If we’d like to get the average time of the responses, we’d need to divide `sum` with `count`.    In addition to `sum` and `count`, we can observe the number of responses grouped into different buckets called quantiles. At the moment, all sixty-nine requests fall into all of the quantiles, so we’ll postpone discussion about them until we reach the examples with more differencing response times.    One thing worth noting is that the metrics come from only one of the three replicas, so our current examples do not paint the full picture. Later on, when we start scraping the metrics with Prometheus, we’ll see that they are aggregated from all the replicas.    Finally, you might have expected around thirty-three responses since we sent a hundred requests that were distributed across three replicas. However, the service continuously pings itself, so the final number was quite higher.    Let’s get out of the `util` container and try to generate some requests that will end with errored responses.    ``` `1` `exit` `2`  `3` `for` i in `{``1`..100`}``;` `do` `4 `    curl `\"http://``$(`docker-machine ip swarm-1`)``/demo/random-error\"` `5` `done`  ```   `````````` The `/demo/random-error` endpoint produces response code `500` in approximately ten percent of cases. The rest should be “normal” responses with status code `200`.    The output should be similar to the one that follows.    ``` `1` ... `2` Everything is still OK `3` Everything is still OK `4` Everything is still OK `5` Everything is still OK `6` ERROR: Something, somewhere, went wrong! `7` ...  ```   ````````` Let’s see how do metrics look like now.    ``` `1` docker container `exec` -it `$ID` sh `2`  `3` curl `\"http://go-demo_main:8080/metrics\"`  ```   ```````` The output limited to the relevant parts is as follows.    ```  `1` ...  `2` # HELP http_server_resp_time Request response time  `3` # TYPE http_server_resp_time histogram  `4` ...  `5` http_server_resp_time_sum{code=\"200\",method=\"GET\",path=\"/demo/random-error\",serv\\  `6` ice=\"go-demo\"} 0.001033751  `7` http_server_resp_time_count{code=\"200\",method=\"GET\",path=\"/demo/random-error\",se\\  `8` rvice=\"go-demo\"} 32  `9` ... `10` http_server_resp_time_sum{code=\"500\",method=\"GET\",path=\"/demo/random-error\",serv\\ `11` ice=\"go-demo\"} 7.033700000000001e-05 `12` http_server_resp_time_count{code=\"500\",method=\"GET\",path=\"/demo/random-error\",se\\ `13` rvice=\"go-demo\"} 2 `14` ...  ```   ``````` Since the response code is one of the labels, we got two metrics; one for the code `200`, and the other for `500`. Since those hundred requests were load balanced across three replicas, the one that produced this output got approximately one-third of them (32+2). We can see that the requests that produce errors take considerably longer time with the total of seven seconds for only two requests.    You might have been “unlucky” and did not get a single response with the code `500`. If that was the case, feel free to send another hundred requests.    Now that we confirmed that our response metrics are separated by different labels, we should explore quantiles. For that, we need to simulate queries with varying response times. Fortunately, `go-demo` has such an endpoint.    ``` `1` `exit` `2`  `3` `for` i in `{``1`..30`}``;` `do` `4 `    `DELAY``=`$`[` `$RANDOM` % `6000` `]` `5 `    curl `\"http://``$(`docker-machine ip swarm-1`)``/demo/hello?delay=``$DELAY``\"` `6` `done`  ```   `````` When `delay` query parameter is set, `go-demo` goes to sleep for the specified number of milliseconds. We made thirty iterations. Each generated a random number between 0 and 6000 and sent that number as the `delay` parameter. As a result, the service should have received requests with a wide range of response times.    Let’s take another look at the metrics.    ``` `1` docker container `exec` -it `$ID` sh `2`  `3` curl `\"http://go-demo_main:8080/metrics\"`  ```   ````` The output, limited to relevant parts, is as follows.    ```  `1` ...  `2` # HELP http_server_resp_time Request response time  `3` # TYPE http_server_resp_time histogram  `4` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\  `5` \"go-demo\",le=\"0.005\"} 78  `6` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\  `7` \"go-demo\",le=\"0.01\"} 78  `8` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\  `9` \"go-demo\",le=\"0.025\"} 78 `10` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `11` \"go-demo\",le=\"0.05\"} 78 `12` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `13` \"go-demo\",le=\"0.1\"} 78 `14` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `15` \"go-demo\",le=\"0.25\"} 78 `16` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `17` \"go-demo\",le=\"0.5\"} 79 `18` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `19` \"go-demo\",le=\"1\"} 80 `20` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `21` \"go-demo\",le=\"2.5\"} 83 `22` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `23` \"go-demo\",le=\"5\"} 87 `24` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `25` \"go-demo\",le=\"10\"} 88 `26` http_server_resp_time_bucket{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\\ `27` \"go-demo\",le=\"+Inf\"} 88 `28` http_server_resp_time_sum{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"go\\ `29` -demo\"} 29.430902277 `30` http_server_resp_time_count{code=\"200\",method=\"GET\",path=\"/demo/hello\",service=\"\\ `31` go-demo\"} 88 `32` ...  ```   ```` Now we have the combination of the fast responses from before combined with those with a delay of up to six seconds. If we focus only on the last two lines, we can see that there are `88` responses in total with the summed time of `29.43` seconds. The average time of responses is around `0.33` seconds. That, in itself, does not give us enough information. Maybe two requests lasted for `10` seconds each, and all of the rest were lightning fast. Or, perhaps, all of the requests were below `0.5` seconds. We cannot know that by just looking at the sum of all response times and dividing them with the count. We need quantiles.    The histogram used in `go-demo` did not specify buckets, so the quantiles are those defined by default. They range from as low as `0.005` to as high as `10` seconds. If you pay closer attention to the numbers beside each of those buckets, you’ll see that `78` requests were below `0.25` seconds, `79` below `0.5`, and so on all the way until all of the `88` requests being below `10` seconds. All the requests from a smaller bucket belong to the larger one. That might be confusing the first time we look at the metrics, but it makes perfect sense. A request that lasted less than, for example, `0.5` seconds, definitely lasted less than, `1` seconds, and so on.    Using quantiles (or buckets) will be essential when we start defining Prometheus alerts based on those metrics, so we’ll postpone further discussion until we reach that part.    As you can see, unlike counters and gauges, histograms go beyond simple additions and subtractions. They provide observations over a period. They track the number of observations and their summaries thus allowing us to calculate average values. The number of observations behaves like a counter. It can only be increased. The sum, on the other hand, is similar to a gauge. It can be both increased and decreased depending on the values we observe. If it is negative, the sum will decrease. We did not explore such an example since response times are always positive.    The most common usage of histrograms is to record request durations and response times. We explored one of those two through our examples.    How about summaries? They are the only metric type we did not explore.    *Summary* is similar to *histogram* metric type. Both sample observations. The major difference is that summary calculates quantiles based on a sliding time frame. We won’t go deeper into summaries. Instead, please read the [Histograms And Summaries](https://prometheus.io/docs/practices/histograms/) page that explains both in more detail and provides a comparison of the two.    ### What Now?    We explored, through a few examples, how to instrument our services and provide more detailed metrics than what we would be able to do through exporters. Early in the book, I said that we should use exporters instead instrumentation unless they do not provide enough information. It turned out that they do not. If, for example, we used an exporter, we would get metrics based on requests coming through the proxy. We would not be aware of internal communication between services nor would we be able to obtain response times of certain parts of the services we’re deploying. Actually, [HAProxy Exporter](https://github.com/prometheus/haproxy_exporter) does not even provide response times since the internal metrics it exposes is not entirely compatible with Prometheus and cannot be exported without sacrificing accuracy. That does not mean that HAProxy metrics are not accurate but that they use a different logic. Instead of having a counter, HAProxy exposes response as exponentially decaying value. It cannot be transformed into a histogram.    If you’re interested in the discussion about *HAProxy Exporter* response time, please visit [issue 37](https://github.com/prometheus/haproxy_exporter/issues/37).    Without accurate response times, we cannot instruct our system to scale and de-scale them effectively. We need to obtain more information if we want to get closer to building a truly *self-adapting* system.    While instrumentation we explored through examples is by no means all the instrumentation we should add, it does provide a step forward. Even though response times are not the only metric we’re missing, it is probably the most important one. Counting errors is useful as well but does not provide clear guidance. Some errors will need a different set of actions, and many cannot even be hooked into the system that auto-corrects itself. Generally speaking, errors often (but not always) require human intervention. Response times, on the other hand, are easy to grasp. They do provide clear guidance for the system. If it goes over a certain threshold within a predefined period, scale up. If it goes down, scale down.    The next chapter will continue exploring response times. We’ll see what we can do with them in Prometheus and how we can improve our current alerts by incorporating this new data.    And now we need a break. Take a rest, go to sleep, recharge your batteries. Before you do any of that, remember that your computer needs a rest too. Get out of the `util` container and remove the machines we created.    ``` `1` `exit` `2`  `3` docker-machine rm -f swarm-1 swarm-2 swarm-3  ``` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```````````````` ````````````````` `````````````````` ``````````````````` ```````````````````` ````````````````````` `````````````````````` ``````````````````````` ```````````````````````` ````````````````````````` `````````````````````````` ``````````````````````````` ```````````````````````````` ````````````````````````````` `````````````````````````````` ``````````````````````````````` ```````````````````````````````` ````````````````````````````````` `````````````````````````````````` ``````````````````````````````````` ```````````````````````````````````` ````````````````````````````````````` `````````````````````````````````````` ``````````````````````````````````````` ```````````````````````````````````````` ````````````````````````````````````````` `````````````````````````````````````````` ``````````````````````````````````````````` ```````````````````````````````````````````` ````````````````````````````````````````````` `````````````````````````````````````````````` ``````````````````````````````````````````````` ````````````````````````````````````````````````"]