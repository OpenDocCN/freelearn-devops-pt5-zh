<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Logging, Monitoring, and Recovery Techniques</h1></div></div></div><p>In this chapter, we are going to look into one of our schedulers and look at wrapping some more operational tasks around it. So far, in this book, we have covered more glamorous subjects; however, monitoring, logging, and automated recovery are just as important. We want to take this knowledge and make it work in the real world. From there, we can start to see the benefits to both the development and operations teams. We are going to use Docker Swarm as our scheduler in this chapter. For logging, we will use the ELK stack, and for monitoring, we will use Consul. Since Docker Swarm version 1.1.3, there are some cool features that will help us use recovery, so we will look at them. We will cover the following topics in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Logging</li><li class="listitem" style="list-style-type: disc">Monitoring</li><li class="listitem" style="list-style-type: disc">Recovery techniques</li></ul></div><div><div><div><div><h1 class="title"><a id="ch08lvl1sec34"/>Logging</h1></div></div></div><p>The importance of logging is second to <a id="id258" class="indexterm"/>none in a solution. If we need to debug any issues with any code/infrastructure, the logs are the first place to go. In the container world, this is no different. In one of the previous chapters, we built the ELK stack. We are going to use that again to process all the logs from our containers. In this solution, we will use a fair chunk of the knowledge that we got so far. We will use a scheduler, a Docker network, and lastly, service discovery with Consul. So, let's look at the solution, and like we did in the past chapters, we will get to coding.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec47"/>The solution</h2></div></div></div><p>As I mentioned in the introduction <a id="id259" class="indexterm"/>of this chapter, we will be using Docker Swarm for this solution. The reason for this choice is that I want to highlight some of the features of Swarm as it has come on leaps and bounds in the last few releases. For the logging portion of this chapter, we are going to deploy our three containers and let<a id="id260" class="indexterm"/> Swarm schedule them. We will use a combination of the Docker networking DNS and our service discovery with Consul to tie everything together. In Swarm, we will use the same servers as we did in the last chapter: three member nodes and two replicated masters. Each node will be a member of our Consul cluster. We will again use Puppet to install Consul on the host system natively.</p></div><div><div><div><div><h2 class="title"><a id="ch08lvl2sec48"/>The code</h2></div></div></div><p>In this chapter, we will build<a id="id261" class="indexterm"/> on the code that we used in the last chapter for Docker Swarm. So, we will go through the plumbing of the Vagrant repo fairly swiftly, only calling out the differences from the last chapter. We are going to create a new Vagrant repo again for this chapter. You should be a master at this by now. Once the new repo is set up, open the <code class="literal">servers.yaml</code> file. We will add the following code to it:</p><div><img src="img/B05201_08_01.jpg" alt="The code"/><div><p>Code for the severs.yaml file</p></div></div><p>As you can see, it's not that different<a id="id262" class="indexterm"/> from the last chapter. There is one call out. We have added a new line to each server <code class="literal">- { shell: 'echo -e "PEERDNS=no\nDNS1=127.0.0.1\nDNS2=8.8.8.8"&gt;&gt;/etc/sysconfig/network-scripts/ifcfg-enp0s8 &amp;&amp; systemctl restart network'}</code>. We will add this as the server is multihomed. We will want to make sure that we are resolving DNS correctly on each interface.</p><p>The next thing we will look at is the puppetfile, which is as follows:</p><div><img src="img/B05201_08_03.jpg" alt="The code"/></div><p>As you can see, there<a id="id263" class="indexterm"/> are no new changes compared with the last chapter. So, let's move to our Hiera file located at <code class="literal">heiradata/global.yml</code> in the root of our module:</p><div><img src="img/B05201_08_04.jpg" alt="The code"/></div><p>Again, we are setting the Swarm version to <code class="literal">v1.1.3</code>, and the backend is set to <code class="literal">consul</code>. We set the IP address of the first node in the Consul cluster, and we set the Consul port to <code class="literal">8500</code>. We will set the <code class="literal">swarm</code> interface that we will advertise from, and last but not least, we will set our Consul version to <code class="literal">0.6.3</code>.</p><p>Now, we will create our module. We will again call the <code class="literal">config</code> module. Once you have created your <code class="literal">&lt;AUTHOR&gt;-config</code> module, move it to the <code class="literal">modules</code> folder in the root of your Vagrant repo.</p><p>Now that we have our module, let's add our code to it. We will need to create the following files in the <code class="literal">manifests</code> directory: <code class="literal">compose.pp</code>, <code class="literal">consul_config</code>, <code class="literal">dns.pp</code>, <code class="literal">run_containers.pp</code>, and <code class="literal">swarm.pp</code>. We have no <code class="literal">params.pp</code> as we are using Hiera in this example.</p><p>So, let's go through the<a id="id264" class="indexterm"/> files in an alphabetical order. In our <code class="literal">compose.pp</code> file, we will add the following code:</p><div><img src="img/B05201_08_05.jpg" alt="The code"/></div><p>As you can see from the code, we are adding our <code class="literal">docker-compose.yml</code> file to any node that is not a swarm master. We will come back to the <code class="literal">docker compose.yml</code> file when we look at the <code class="literal">templates</code> directory. The next file is <code class="literal">consul_config.pp</code>, which is as follows:</p><div><img src="img/B05201_08_06.jpg" alt="The code"/></div><p>In this file, we are<a id="id265" class="indexterm"/> declaring our Consul cluster and defining the bootstrap server. This should look familiar as it is the same code that we used in the last chapter. The next file is <code class="literal">dns.pp</code>, which is given as follows:</p><div><img src="img/B05201_08_07.jpg" alt="The code"/></div><p>Again, this code <a id="id266" class="indexterm"/>should look familiar to you, as we have used it before in the last chapter. Just to recap, this is setting and configuring our bind package to use Consul as the DNS server. The next file we will look at is <code class="literal">init.pp</code>:</p><div><img src="img/B05201_08_08.jpg" alt="The code"/></div><p>In the <code class="literal">init.pp</code> file, we <a id="id267" class="indexterm"/>are just ordering our classes within our module. We will now move on to <code class="literal">run_containers.pp</code>. This is where we will schedule our ELK containers across the swarm cluster:</p><div><img src="img/B05201_08_09.jpg" alt="The code"/></div><p>Let's have a look at this in <a id="id268" class="indexterm"/>detail, as there is a lot of new code here. The first declaration that we will use is to schedule the containers from the second Swarm master.</p><p>The next block of code will configure our <code class="literal">logstash</code> container. We will need to first have these containers here in this example, as we are using them as the syslog server. If at the time of spawning the containers they can't connect to logstash on TCP port <code class="literal">5000</code>, the build of the container will fail. So, let's move on to the configuration of <code class="literal">logstash</code>. We will use the container that I put in, as it is the official container with the <code class="literal">logstash.conf</code> file that we already added. We will then add <code class="literal">logstash</code> to our internal <code class="literal">swarm-private</code> Docker network and expose all the ports for <code class="literal">logstash</code> on all networks. So, we can pipe logs from <a id="id269" class="indexterm"/>anywhere to it. After this, we will set the location of <code class="literal">elasticsearch</code> and then we will give the command to start.</p><div><div><div><div><h3 class="title"><a id="ch08lvl3sec03"/>Logstash</h3></div></div></div><p>In the second block of code, we will install and configure <code class="literal">elasticsearch</code>. We will use the official <code class="literal">elasticsearch</code> container (version 2.1.0). We will only add <code class="literal">elasticsearch</code> to our private Docker network, <code class="literal">swarm-private</code>. We will make the data persistent by declaring a volume mapping. We<a id="id270" class="indexterm"/> will set the command with arguments to start <code class="literal">elasticsearch</code> with the command value. Next, we will set the log drive to syslog and the syslog server to <code class="literal">tcp://logstash-5000.service.consul:5000</code>. Note that we are using our Consul service discovery address as we are exposing <code class="literal">logstash</code> on the external network. Lastly, we set the dependency on <code class="literal">logstash</code>. As I mentioned earlier, the syslog server needs to be available at the time this container spawns, so we need <code class="literal">logstash</code> to be there prior to either this container or <code class="literal">kibana</code>. Talking of Kibana, let's move on to our last block of code.</p><p>In our <code class="literal">kibana</code> container, we will add the following configuration. First, we will use the official <code class="literal">kibana</code> image (Version 4.3.0). We will add <code class="literal">kibana</code> to our <code class="literal">swarm-private</code> network so it can access our <code class="literal">elasticsearch</code> container. We will map and expose ports <code class="literal">5601</code> to <code class="literal">80</code> on the host network. In the last few lines, we will set the syslog configuration in the same way as we did with <code class="literal">elasticsearch</code>.</p><p>Now, it's time for our last file, <code class="literal">swarm.pp</code>, which is as follows:</p><div><img src="img/B05201_08_10.jpg" alt="Logstash"/></div><p>In this code, we are configuring our Swarm cluster and Docker network.</p><p>We will now move to our <code class="literal">templates</code> folder in the root of our module. We need to create three files. The two files <code class="literal">Consul.conf.erb</code> and <code class="literal">named.conf.erb</code> are for our bind config. The last file<a id="id271" class="indexterm"/> is our <code class="literal">registrator.yml.erb</code> Docker compose file. We will add the code to the following files.</p><p>Let's first see the code for <code class="literal">consul.conf.erb</code>, which is as follows:</p><div><img src="img/B05201_08_16.jpg" alt="Logstash"/></div><p>Now, let's see the code for <code class="literal">named.conf.erb</code>, which is as follows:</p><div><img src="img/B05201_08_17.jpg" alt="Logstash"/></div><p>Finally, let's see the code for <code class="literal">registrator.yml.erb</code>, which is as follows:</p><div><img src="img/B05201_08_18.jpg" alt="Logstash"/></div><p>All the code in these files should look fairly familiar, as we have used it in previous chapters.</p><p>Now, we have just<a id="id272" class="indexterm"/> one more configuration before we can run our cluster. So, let's go to our <code class="literal">default.pp</code> manifest file in the <code class="literal">manifests</code> folder located in the root of our Vagrant repo.</p><p>Now, we will add the relevant node definitions to our manifest file:</p><div><img src="img/B05201_08_11.jpg" alt="Logstash"/></div><p>We are ready to go to our terminal, where we will change the directory to the root of our Vagrant repo. As so many times that we did before, we will issue the <code class="literal">vagrant up</code> command. If you have the boxes still configured from the last chapter, issue the <code class="literal">vagrant destroy -f &amp;&amp; vagrant up</code> command.</p><p>Once Vagrant is run and Puppet has built our five nodes, we can now open a browser and enter the <code class="literal">http://127.0.0.1:9501/</code>. We should get the following page after this:</p><div><img src="img/B05201_08_12.jpg" alt="Logstash"/></div><p>As you can see, all our <a id="id273" class="indexterm"/>services are shown with green color that displays the state of health. We will now need to find what node our <code class="literal">kibana</code> container is running on. We will do that by clicking on the <strong>kibana</strong> service.</p><div><img src="img/B05201_08_13.jpg" alt="Logstash"/></div><p>In my example, <strong>kibana</strong> has come up on <strong>swarm-101</strong>. If this is not the same for you, don't worry as the Swarm cluster could have scheduled the container on any of the three nodes. Now, I will open a <a id="id274" class="indexterm"/>browser tab and enter the <code class="literal">127.0.0.1:8001/</code>, as shown in the following screenshot:</p><div><img src="img/B05201_08_14.jpg" alt="Logstash"/></div><p>If your host is different, consult the <code class="literal">servers.yaml</code> file to get the right port.</p><p>We will then create our index and click on the <strong>Discovery</strong> tab, and as you can see in this screenshot, we have our logs coming in:</p><div><img src="img/B05201_08_15.jpg" alt="Logstash"/><div><p>Logs after creating index</p></div></div></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec35"/>Monitoring</h1></div></div></div><p>In the world of containers, there are a<a id="id275" class="indexterm"/> few levels of monitoring that you can deploy. For example, you have your traditional ops monitoring. So your Nagios and Zabbix of the world or even perhaps a cloud solution like Datadog. All these solutions have good hooks into Docker and can be deployed with Puppet. We are going to assume in this book that the ops team has this covered and your traditional monitoring is in place. We are going to look at the next level of monitoring. We will concentrate on the container connectivity and Swarm cluster health. We will do this all in Consul and deploy our code with Puppet.</p><p>The reason we are looking at this level of monitoring is because we can make decisions about what Consul is reporting. Do we need to scale containers? Is a Swarm node sick? Should we take it out of the cluster? Now to cover these topics, we will need to write a separate book. I won't be covering these solutions. What we will look at is the first step to get there. Now that the seed has been planted, you will want to explore your options further. The challenge is to change the way we think about monitoring and how it needs reactive interaction from a human, so we can trust our code to make choices for us and to make our solutions fully automated.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec49"/>Monitoring with Consul</h2></div></div></div><p>One really good thing about using Consul is that Hashicorp did an awesome job at documenting their <a id="id276" class="indexterm"/>applications. Consul is no exception. If you would<a id="id277" class="indexterm"/> like to read more about the options you have with Consul<a id="id278" class="indexterm"/> monitoring, refer to the documentation at <a class="ulink" href="https://www.consul.io/docs/agent/services.html">https://www.consul.io/docs/agent/services.html</a> and <a class="ulink" href="https://www.consul.io/docs/agent/checks.html">https://www.consul.io/docs/agent/checks.html</a>. We are going to set up both checks and a service. In the last chapter, we wrote a service with Consul to monitor our Docker service on every node:</p><div><img src="img/B05201_08_19.jpg" alt="Monitoring with Consul"/></div><p>On the Consul web UI, we get the following reading of the Docker service on the node:</p><div><img src="img/B05201_08_20.jpg" alt="Monitoring with Consul"/></div><p>We are going to roll out <a id="id279" class="indexterm"/>all our new checks on both the Swarm masters. The reason for this is that both the nodes are external from the nodes cluster. The abstraction gives us the benefit of not having to worry about the nodes that the containers are<a id="id280" class="indexterm"/> running on. You also have the monitoring polling from multiple locations. For example, in AWS, your Swarm masters could be split across multiple AZs. So, if you lose an AZ, your monitoring will still be available.</p><p>As we are going to use the logging solution from the example that we covered in the previous section, we will check and make sure that both Logstash and Kibana are available; Logstash on port 5000 and Kibana on port 80.</p><p>We are going to add two new service checks to our config module in the <code class="literal">consul_config.pp</code> file, as follows:</p><div><img src="img/B05201_08_21.jpg" alt="Monitoring with Consul"/></div><p>As you can see, we have set a TCP check for both <code class="literal">kibana</code> and <code class="literal">logstash</code>, and we will use the service discovery address to test the connections. We will open our terminal and change the directory to the <a id="id281" class="indexterm"/>root of our Vagrant repo.</p><p>Now, we are going to <a id="id282" class="indexterm"/>assume that your five boxes are running. We will issue the command to Vagrant to provision only the two master nodes. This command is <code class="literal">vagrant provision swarm-master-01 &amp;&amp; vagrant provision swarm-master-02</code>. We will then open our web browser and enter <code class="literal">127.0.0.1:9501</code>. We can then click on <strong>swarm-master-01</strong> or <strong>swarm-master-02</strong>, the choice is up to you. After this, we will get the following result:</p><div><img src="img/B05201_08_22.jpg" alt="Monitoring with Consul"/></div><p>As you can see, our<a id="id283" class="indexterm"/> monitoring was successful. We will move back to <a id="id284" class="indexterm"/>our code and add a check for our swarm master to determine its health. We will do that with the following code:</p><div><img src="img/B05201_08_23.jpg" alt="Monitoring with Consul"/></div><p>We will then issue the <a id="id285" class="indexterm"/>Vagrant provision command, <code class="literal">vagrant provision swarm-master-01 &amp;&amp; vagrant provision swarm-master-02</code>. Again, we will open our <a id="id286" class="indexterm"/>web browser and click on <strong>swarm-master-01</strong> or <strong>swarm-master-02</strong>. You should get the following result after this:</p><div><img src="img/B05201_08_24.jpg" alt="Monitoring with Consul"/></div><p>As you can see from the<a id="id287" class="indexterm"/> information in the check, we can easily see which Swarm master is primary, the strategy for scheduling. This will come in really handy <a id="id288" class="indexterm"/>when you have any issues.</p><p>So as you can see, Consul is a really handy tool, and if you want to take away the things we covered in this chapter, you can really do some cool stuff.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec36"/>Recovery techniques</h1></div></div></div><p>It is important in every<a id="id289" class="indexterm"/> solution to have some recovery techniques. In the container world, this is no different. There are many ways to skin this cat, such as load balancing with HA proxy or even using a container-based application that was built for this <a id="id290" class="indexterm"/>purpose such as interlock (<a class="ulink" href="https://github.com/ehazlett/interlock">https://github.com/ehazlett/interlock</a>). If you have not checked out interlock, it's awesome!!! There are so many combinations of solutions we could use depending on the underlying application. So here, we are going to look at the built-in HA in Docker Swarm. From there, you could use something like an interlock to make sure that there is no downtime in accessing your containers.</p><div><div><div><div><h2 class="title"><a id="ch08lvl2sec50"/>Built-in HA</h2></div></div></div><p>Docker Swarm has two kind of nodes: master nodes and member nodes. Each one of these has different built-in protection for failure. The first node type we will look at is master nodes.</p><p>In the last topic, we set up <a id="id291" class="indexterm"/>a health check to get the information regarding our Swarm cluster. There, we saw that we had a master or primary Swarm master and a replica. Swarm replicates all its cluster information over TCP port 4000. So, to simulate failure, we are going to turn off the master. My master is <code class="literal">swarm-master-01</code>, but yours could be different. We will use the health check that we already created to test out the failure and watch how Swarm handles itself. We will issue the <code class="literal">vagrant halt swarm-master-01</code> command. We will open up our browser again to our Consul web UI, <code class="literal">127.0.0.1:9501</code>. As we can see in the following screenshot, <code class="literal">swarm-master-02</code> is now a master:</p><div><img src="img/B05201_08_26.jpg" alt="Built-in HA"/></div><p>Now, we will move on to container resecluding with our Swarm node HA. As of version 1.1.3, Swarm is shipped with a feature where a container will respawn on a healthy node if the original node fails. There<a id="id292" class="indexterm"/> are some rules to this such as when you have filtering rules or linked containers. To know more on this topic, you can read the Docker docs about Swarm located at <a class="ulink" href="https://github.com/docker/swarm/tree/master/experimental">https://github.com/docker/swarm/tree/master/experimental</a>.</p><p>To test this out, I will halt the node that hosts Kibana. We will need to add some code to our <code class="literal">kibana</code> container so that it will restart on failure. This is added to the <code class="literal">env</code> resource, as shown in the following screenshot:</p><div><img src="img/B05201_08_27.jpg" alt="Built-in HA"/></div><p>We will first need to kill our <a id="id293" class="indexterm"/>old container to add the restart policy. We can do that by setting the <strong>ensure</strong> resource to <strong>absent</strong> and running the Vagrant provision <code class="literal">swarm-master-02</code>.</p><p>Once the Vagrant run is complete, we will change it back to <strong>present</strong> and run <code class="literal">vagrant provision swarm-master-02</code>.</p><p>For me, my <code class="literal">kibana</code> container is on <strong>swarm-102</strong> (this could be different for you). Once that node fails, <code class="literal">kibana</code> will restart on a healthy node. So, let's issue <code class="literal">vagrant halt swarm-102</code>. If we go to our Consul URL, <code class="literal">127.0.0.1:9501</code>, we should see some failures on our nodes and checks, as shown in the following screenshot:</p><div><img src="img/B05201_08_28.jpg" alt="Built-in HA"/></div><p>If you wait a minute <a id="id294" class="indexterm"/>or so, you will see that <code class="literal">kibana</code> alerts came back and the container spawned on another server. For me, <code class="literal">kibana</code> came back on <strong>swarm-101</strong>, as you can see in the following screenshot:</p><div><img src="img/B05201_08_31.jpg" alt="Built-in HA"/></div><p>We can then go to our<a id="id295" class="indexterm"/> browser and look at <code class="literal">kibana</code>. For me, it will be at <code class="literal">127.0.0.1:8001</code>:</p><div><img src="img/B05201_08_32.jpg" alt="Built-in HA"/><div><p>Kibana after connecting to Elasticsearch</p></div></div><p>As you can see, all<a id="id296" class="indexterm"/> our logs are there; our service discovery worked perfectly as once the container changed nodes, our health checks turned green.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch08lvl1sec37"/>Summary</h1></div></div></div><p>In this chapter, we looked at how to operationalize our container environment using Puppet. We covered a logging solution using ELK. We took Consul to the next level with more in-depth health checks and creating services to monitor our cluster. We then tested the built-in HA functionality that ships with Swarm. We have covered a lot of ground now since our humble first module in <a class="link" href="ch02.html" title="Chapter 2. Working with Docker Hub">Chapter 2</a>, <em>Working with Docker Hub</em>. You are fully equipped to take the knowledge that you have got here and apply it in the real world.</p></div></body></html>