- en: Persisting State
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having fault-tolerance and high-availability is of no use if we lose application
    state during rescheduling. Having state is unavoidable, and we need to preserve
    it no matter what happens to our applications, servers, or even a whole datacenter.
  prefs: []
  type: TYPE_NORMAL
- en: The way to preserve the state of our applications depends on their architecture.
    Some are storing data in-memory and rely on periodic backups. Others are capable
    of synchronizing data between multiple replicas, so that loss instance of one
    does not result in loss of data. Most, however, are relying on disk to store their
    state. We'll focus on that group of stateful applications.
  prefs: []
  type: TYPE_NORMAL
- en: If we are to build fault-tolerant systems, we need to make sure that failure
    of any part of the system is recoverable. Since speed is of the essence, we cannot
    rely on manual operations to recuperate from failures. Even if we could, no one
    wants to be the person sitting in front of a screen, waiting for something to
    fail, only to bring it back to its previous state.
  prefs: []
  type: TYPE_NORMAL
- en: We already saw that Kubernetes would, in most cases, recuperate from a failure
    of an application, of a server, or even of a whole datacenter. It'll reschedule
    Pods to healthy nodes. We also experienced how AWS and kops accomplish more or
    less the same effect on the infrastructure level. Auto-scaling groups will recreate
    failed nodes and, since they are provisioned with kops startup processes, new
    instances will have everything they need, and they will join the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing that prevents us from saying that our system is (mostly) highly
    available and fault tolerant is the fact that we did not solve the problem of
    persisting state across failures. That's the subject we'll explore next.
  prefs: []
  type: TYPE_NORMAL
- en: We'll try to preserve our data no matter what happens to our stateful applications
    or the servers where they run.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kubernetes cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll start by recreating a similar cluster as the one we used in the previous
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the [`15-pv.sh`](https://gist.github.com/41c86eb385dfc5c881d910c5e98596f2)
    ([https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2](https://gist.github.com/vfarcic/41c86eb385dfc5c881d910c5e98596f2))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We entered the local copy of the `k8s-specs` repository, pulled the latest code,
    and went into the `cluster` directory.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we stored the environment variables we used in the
    `kops` file. Let's take a quick look at them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, without the keys, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By storing the environment variables in a file, we can fast-track the process
    by loading them using the `source` command.
  prefs: []
  type: TYPE_NORMAL
- en: In the older editions of the book, there was an error in the command we used
    to store the environment variables in the `kops` file. The `export` commands were
    missing. Please ensure that your copy of the file has all the lines starting with
    `export`. If that's not the case, please update it accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the environment variables are set, we can proceed to create an `S3`
    bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The command that creates the `kops` alias is as follows. Execute it only if
    you are a **Windows user**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we can, finally, create a new Kubernetes cluster in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we compare that command with the one we executed in the previous chapter,
    we'll notice only a few minor changes. We increased `node-count` to `2` and `node-size`
    to `t2.medium`. That will give us more than enough capacity for the exercises
    we'll run in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s validate the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Assuming that enough time passed since we executed `kops create cluster`, the
    output should indicate that the `cluster devops23.k8s.local is ready`.
  prefs: []
  type: TYPE_NORMAL
- en: A note to Windows users
  prefs: []
  type: TYPE_NORMAL
- en: Kops was executed inside a container. It changed the context inside the container
    that is now gone. As a result, your local `kubectl` context was left intact. We'll
    fix that by executing `kops export kubecfg --name ${NAME}` and `export KUBECONFIG=$PWD/config/kubecfg.yaml`.
    The first command exported the config to `/config/kubecfg.yaml`. That path was
    specified through the environment variable `KUBECONFIG` and is mounted as `config/kubecfg.yaml`
    on local hard disk. The latter command exports `KUBECONFIG` locally. Through that
    variable, `kubectl` is now instructed to use the configuration in `config/kubecfg.yaml`
    instead of the default one. Before you run those commands, please give AWS a few
    minutes to create all the EC2 instances and for them to join the cluster. After
    waiting and executing those commands, you'll be all set.
  prefs: []
  type: TYPE_NORMAL
- en: We'll need Ingress if we'd like to access the applications we'll deploy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Ingress will not help us much without the ELB DNS, so we''ll get that as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The output of the latter command should end with `us-east-2.elb.amazonaws.com`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, now that we are finished with the cluster setup, we can go back to
    the repository root directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Deploying stateful applications without persisting state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start the exploration by deploying a stateful application without any
    mechanism to persist its state. That will give us a better insight into benefits
    behind of some of the Kubernetes concepts and resources we'll use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We already deployed Jenkins a few times. Since it is a stateful application,
    it is an excellent candidate to serve as a playground.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at a definition stored in the `pv/jenkins-no-pv.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The YAML defines the `jenkins` namespace, an Ingress controller, and a service.
    We're already familiar with those types of resources so we'll skip explaining
    them and jump straight to the Deployment definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the `cat` command, limited to the `jenkins` Deployment, is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There's nothing special about this Deployment. We already used a very similar
    one. Besides, by now, you're an expert at Deployment controllers.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing worth mentioning is that there is only one volume mount and it
    references a secret we're using to provide Jenkins with the initial administrative
    user. Jenkins is persisting its state in `/var/jenkins_home`, and we are not mounting
    that directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the resources defined in `pv/jenkins-no-pv.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll take a quick look at the events as a way to check that everything was
    deployed successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the setup of the only volume failed since it could not find
    the secret referenced as `jenkins-creds`. Let''s create it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, with the secret `jenkins-creds` created in the `jenkins` namespace, we
    can confirm that the rollout of the Deployment was successful.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see, from the output, that the `deployment "jenkins" was successfully
    rolled out`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything is up and running, we can open Jenkins UI in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: A note to Windows users
  prefs: []
  type: TYPE_NORMAL
- en: Git Bash might not be able to use the `open` command. If that's the case, please
    replace the `open` command with `echo`. As a result, you'll get the full address
    that should be opened directly in your browser of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Please click the Log in link, type `jdoe` as the User, and `incognito` as the
    Password. When finished, click the log in button.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are authenticated as jdoe administrator, we can proceed and create
    a job. That will generate a state that we can use to explore what happens when
    a stateful application fails.
  prefs: []
  type: TYPE_NORMAL
- en: Please click the create new jobs link, type `my-job` as the item name, select
    Pipeline as the job type, and press the OK button.
  prefs: []
  type: TYPE_NORMAL
- en: You'll be presented with the job configuration screen. There's no need to do
    anything here since we are not, at the moment, interested in any specific Pipeline
    definition. It's enough to click the Save button.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll simulate a failure by killing `java` process running inside the
    Pod created by the `jenkins` Deployment. To do that, we need to find out the name
    of the Pod.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We retrieved the Pods from the `jenkins` namespace, filtered them with the selector
    `api=jenkins`, and formatted the output as `json`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output, limited to the relevant parts, as is follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the name is inside `metadata` entry of one of the `items`.
    We can use that to formulate `jsonpath` that will retrieve only the name of the
    Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The name of the Pod is now stored in the environment variable `POD_NAME`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the latter command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know the name of the Pod hosting Jenkins, we can proceed and kill
    the `java` process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The container failed once we killed Jenkins process. We already know from experience
    that a failed container inside a Pod will be recreated. As a result, we had a
    short downtime, but Jenkins is running once again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what happened to the job we created earlier. I''m sure you know
    the answer, but we''ll check it anyway:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As expected, `my-job` is nowhere to be found. The container that was hosting
    `/var/jenkins_home` directory failed, and it was replaced with a new one. The
    state we created is lost.
  prefs: []
  type: TYPE_NORMAL
- en: Truth be told, we already saw in the [Chapter 8](a8a54c79-1dda-41ec-8ad5-4986c17b7041.xhtml), *Using
    Volumes to Access Host's File System* that we can mount a volume in an attempt
    to preserve state across failures. However, in the past, we used `emptyDir` which
    mounts a local volume. Even though that's better than nothing, such a volume exists
    only as long as the server it is stored in is up and running. If the server would
    fail, the state stored in `emptyDir` would be gone. Such a solution would be only
    slightly better than not using any volume. By using local disk we would only postpone
    inevitable, and, sooner or later, we'd get to the same situation. We'd be left
    wondering why we lost everything we created in Jenkins. We can do better than
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Creating AWS volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we want to persist state that will survive even server failures, we have
    two options we can choose. We could, for example, store data locally and replicate
    it to multiple servers. That way, a container could use local storage knowing
    that the files are available on all the servers. Such a setup would be too complicated
    if we'd like to implement the process ourselves. Truth be told, we could use one
    of the volume drivers for that. However, we'll opt for a more commonly used method
    to persist the state across failures. We'll use external storage.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are running our cluster in AWS, we can choose between [`S3`](https://aws.amazon.com/s3/)
    ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)), [**Elastic File System**
    (**EFS**)](https://aws.amazon.com/efs/) ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/)),
    and [**Elastic Block Store**](https://aws.amazon.com/ebs/) (**EBS**) ([https://aws.amazon.com/ebs/](https://aws.amazon.com/ebs/)).
  prefs: []
  type: TYPE_NORMAL
- en: S3 is meant to be accessed through its API and is not suitable as a local disk
    replacement. That leaves us with EFS and EBS.
  prefs: []
  type: TYPE_NORMAL
- en: EFS, has a distinct advantage that it can be mounted to multiple EC2 instances
    spread across multiple availability zones. It is closest we can get to fault-tolerant
    storage. Even if a whole zone (datacenter) fails, we'll still be able to use EFS
    in the rest of the zones used by our cluster. However, that comes at a cost. EFS
    introduces a performance penalty. It is, after all, a **network file system**
    (**NFS**), and that entails higher latency.
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic Block Store** (**EBS**) is the fastest storage we can use in AWS.
    Its data access latency is very low thus making it the best choice when performance
    is the primary concern. The downside is availability. It doesn''t work in multiple
    availability zones. Failure of one will mean downtime, at least until the zone
    is restored to its operational state.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll choose EBS for our storage needs. Jenkins depends heavily on IO, and we
    need data access to be as fast as possible. However, there is another reason for
    such a choice. EBS is fully supported by Kubernetes. EFS will come but, at the
    time of this writing, it is still in the experimental stage. As a bonus advantage,
    EBS is much cheaper than EFS.
  prefs: []
  type: TYPE_NORMAL
- en: Given the requirements and what Kubernetes offers, the choice is obvious. We'll
    use EBS, even though we might run into trouble if the availability zone where
    our Jenkins will run goes down. In such a case, we'd need to migrate EBS volume
    to a healthy zone. There's no such thing as a perfect solution.
  prefs: []
  type: TYPE_NORMAL
- en: We are jumping ahead of ourselves. We'll leave Kubernetes aside for a while
    and concentrate on creating an EBS volume.
  prefs: []
  type: TYPE_NORMAL
- en: Each EBS volume is tied to an availability zone. Unlike EFS, EBS cannot span
    multiple zones. So, the first thing we need to do is to find out which are the
    zones worker nodes are running in. We can get that information by describing the
    EC2 instances belonging to the security group `nodes.devops23.k8s.local`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the information is inside the `Reservations.Instances` array.
    To get the zone, we need to filter the output by the `SecurityGroups.GroupName`
    field. Zone name is located in the `Placement.AvailabilityZone` field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that does the filtering and retrieves the availability zones of
    the worker nodes is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the two worker nodes are located in the zones `us-east-2a` and
    `us-east-2c`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The commands that retrieve the zones of the two worker nodes and store them
    in environment variables is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We retrieved the zones and stored the output into the `zones` file. Further
    on, we retrieved the first row with the `head` command and stored it in the environment
    variable `AZ_1`. Similarly, we stored the last (the second) row in the variable
    `AZ_2`.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have all the information we need to create a few volumes.
  prefs: []
  type: TYPE_NORMAL
- en: The command that follows requires a relatively newer version of `aws`. If it
    fails, please update your AWS CLI binary to the latest version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We executed `aws ec2 create-volume` command three times. As a result, we created
    three EBS volumes. Two of them are in one zone, while the third is in another.
    They all have `10` GB of space. We chose `gp2` as the type of the volumes. The
    other types either require bigger sizes or are more expensive. When in doubt,
    `gp2` is usually the best choice for EBS volumes.
  prefs: []
  type: TYPE_NORMAL
- en: We also defined a tag that will help us distinguish the volumes dedicated to
    this cluster from those we might have in our AWS account for other purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `jq` filtered the output so that only the volume ID is retrieved. The
    results are stored in the environment variables `VOLUME_ID_1`, `VOLUME_ID_2`,
    and `VOLUME_ID_3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at one of the IDs we stored as an environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to be on the safe side, we'll list the volume that matches the ID and
    thus confirm, without doubt, that the EBS was indeed created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now that the EBS volumes are indeed `available` and in the same zones as the
    worker nodes, we can proceed and create Kubernetes persistent volumes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c2542d1-2653-4569-9932-9be7c575aa3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-1: EBS volumes created in the same zones as the worker nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Creating Kubernetes persistent volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fact that we have a few EBS volumes available does not mean that Kubernetes
    knows about their existence. We need to add PersistentVolumes that will act as
    a bridge between our Kubernetes cluster and AWS EBS volumes.
  prefs: []
  type: TYPE_NORMAL
- en: PersistentVolumes allow us to abstract details of how storage is provided (for
    example, EBS) from how it is consumed. Just like Volumes, PersistentVolumes are
    resources in a Kubernetes cluster. The main difference is that their lifecycle
    is independent of individual Pods that are using them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a definition that will create a few PersistentVolumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the first of the three volumes, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `spec` section features a few interesting details. We set `manual-ebs` as
    the storage class name. We'll see later what is its function. For now, just remember
    the name.
  prefs: []
  type: TYPE_NORMAL
- en: We defined that the storage capacity is `5Gi`. It does not need to be the same
    as the capacity of the EBS we created earlier, as long as it is not bigger. Kubernetes
    will try to match `PersistentVolume` with, in this case, EBS that has a similar,
    if not the same capacity. Since we have only one EBS volume with 10 GB, it is
    the closest (and the only) match to the `PersistentVolume` request of `5Gi`. Ideally,
    persistent volumes capacity should match EBS size, but I wanted to demonstrate
    that any value equal to or less then the actual size should do.
  prefs: []
  type: TYPE_NORMAL
- en: We specified that the access mode should be `ReadWriteOnce`. That means that
    we'll be able to mount the volume as read-write only once. Only one Pod will be
    able to use it at any given moment. Such a strategy fits us well since EBS cannot
    be mounted to multiple instances. Our choice of the access mode is not truly a
    choice, but more an acknowledgment of the way how EBS works. The alternative modes
    are `ReadOnlyMany` and `ReadWriteMany`. Both modes would result in volumes that
    could be mounted to multiple Pods, either as read-only or read-write. Those modes
    would be more suitable for NFS like, for example, EFS, which can be mounted by
    multiple instances.
  prefs: []
  type: TYPE_NORMAL
- en: The `spec` fields we explored so far are common to all persistent volume types.
    Besides those, there are entries specific to the actual volume we are associating
    with a Kubernetes `PersistentVolume`. Since we're going to use EBS, we specified
    `awsElasticBlockStore` with the volume ID and file system type. Since I could
    not know in advance what will be the ID of your EBS volume, the definition has
    the value set to `REPLACE_ME`. Later on, we'll replace it with the ID of the EBS
    we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: There are many other types we could have specified instead. If this cluster
    would run on Azure, we could use `azureDisk` or `azureFile`. In **Google Compute
    Engine** (**GCE**) it would be `GCEPersistentDisk`. We could have setup `Glusterfs`.
    Or, if we would have this cluster running in an on-prem data center, it would
    probably be `nfs`. There are quite a few others we could use but, since we're
    running the cluster in AWS, many would not work, while others could be too difficult
    to set up. Since EBS is already available, we'll just roll with it. All in all,
    this cluster is in AWS, and `awsElasticBlockStore` is the easiest, if not the
    best choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an understanding of the YAML definition, we can proceed and
    create the `PersistentVolume`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We used `cat` to output the contents of the `pv/pv.yml` file and pipe it into
    `sed` commands which, in turn, replaced the `REPLACE_ME_*` strings with the IDs
    of the EBS volumes we created earlier. The result was sent to the `kubectl create`
    command that created persistent volumes. As a result, we can see from the output
    that all three PersistentVolumes were created.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the persistent volumes currently available in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It should come as no surprise that we have three volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: The interesting part of the information we're seeing are the statuses. The persistent
    volumes are `available`. We created them, but no one is using them. They just
    sit there waiting for someone to claim them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44bc1af7-6d59-4e75-96e4-ef26a0e9ff49.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-2: Kubernetes persistent volumes tied to EBS volumes'
  prefs: []
  type: TYPE_NORMAL
- en: Claiming persistent volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes persistent volumes are useless if no one uses them. They exist only
    as objects with relation to, in our case, specific EBS volumes. They are waiting
    for someone to claim them through the `PersistentVolumeClaim` resource.
  prefs: []
  type: TYPE_NORMAL
- en: Just like Pods which can request specific resources like memory and CPU, `PersistentVolumeClaims`
    can request particular sizes and access modes. Both are, in a way, consuming resources,
    even though of different types. Just as Pods should not specify on which node
    they should run, `PersistentVolumeClaims` cannot define which volume they should
    mount. Instead, Kubernetes scheduler will assign them a volume depending on the
    claimed resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use `pv/pvc.yml` to explore how we could claim a persistent volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The YAML file defines a `PersistentVolumeClaim` with the storage class name
    `manual-ebs`. That is the same class as the persistent volumes `manual-ebs-*`
    we created earlier. The access mode and the storage request are also matching
    what we defined for the persistent volume.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we are not specifying which volume we'd like to use. Instead,
    this claim specifies a set of attributes (`storageClassName`, `accessModes`, and
    `storage`). Any of the volumes in the system that match those specifications might
    be claimed by the `PersistentVolumeClaim` named `jenkins`. Bear in mind that `resources`
    do not have to be the exact match. Any volume that has the same or bigger amount
    of storage is considered a match. A claim for `1Gi` can be translated to *at least
    1Gi*. In our case, a claim for `1Gi` matches all three persistent volumes since
    they are set to `5Gi`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we explored the definition of the claim, we can proceed, and create
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The output indicates that the `persistentvolumeclaim "jenkins" was created`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s list the claims and see what we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We see from the output that the status of the claim is `Bound`. That means
    that the claim found a matching persistent volume and bounded it. We can confirm
    that by listing the volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We can see that one of the volumes (`manual-ebs-02`) changed the status from
    `Available` to `Bound`. That is the volume bound to the claim we created a moment
    ago. We can see that the claim comes from `jenkins` namespace and `jenkins``PersistentVolumeClaim`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba1b318f-887f-4c15-a39a-4affbbabffe2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-3: Creation of a Persistent Volume Claim'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that if a PersistentVolumeClaim cannot find a matching volume, it
    will remain unbound indefinitely, unless we add a new PersistentVolume with the
    matching specifications.
  prefs: []
  type: TYPE_NORMAL
- en: We still haven't accomplished our goal. The fact that we claimed a volume does
    not mean that anyone uses it. On the other hand, our Jenkins needs to persist
    its state. We'll join our `PersistentVolumeClaim` with a Jenkins container.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching claimed volumes to Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant parts of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: You'll notice that, this time, we added a new volume `jenkins-home`, which references
    the `PersistentVolumeClaim` called `jenkins`. From the container's perspective,
    the claim is a volume.
  prefs: []
  type: TYPE_NORMAL
- en: Let's deploy Jenkins resources and confirm that everything works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We'll wait until the Deployment rolls out before proceeding with a test that
    will confirm whether Jenkins state is now persisted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Once the rollout is finished, we'll see a message stating that the `deployment
    "jenkins" was successfully rolled out`.
  prefs: []
  type: TYPE_NORMAL
- en: We sent a request to the Kubernetes API to create a Deployment. As a result,
    we got a `ReplicaSet` that, in turn, created the `jenkins` Pod. It mounted the
    `PersistentVolumeClaim`, which is bound to the `PersistenceVolume`, that is tied
    to the EBS volume. As a result, the EBS volume was mounted to the `jenkins` container
    running in a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: A simplified version of the sequence of events is depicted in the *Figure 15-4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b4b1b21-dd3f-4e03-9ba1-7d22b85a55ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-4: The sequence of events initiated with a request to create a Jenkins
    Pod with the PersistentVolumeClaim'
  prefs: []
  type: TYPE_NORMAL
- en: We executed `kubectl` command
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl` sent a request to `kube-apiserver` to create the resources defined
    in `pv/jenkins-pv.yml`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Among others, the `jenkins` Pod was created in one of the worker nodes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since `jenkins` container in the Pod has a `PersistentVolumeClaim`, it mounted
    it as a logical volume
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `PersistentVolumeClaim` was already bound to one of the PersistentVolumes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PersistentVolume is associated with one of the EBS volumes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The EBS volume was mounted as a physical volume to the `jenkins` Pod
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that Jenkins is up-and-running, we'll execute a similar set of steps as
    before, and validate that the state is persisted across failures.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We opened Jenkins home screen. If you are not authenticated, please click the
    Log in link and type `jdoe` as the User and `**incognito*` as the Password. Click
    the log in button.
  prefs: []
  type: TYPE_NORMAL
- en: You'll see the create new jobs link. Click it. Type `my-job` as the item name,
    select `Pipeline` as the job type, and click the OK button. Once inside the job
    configuration screen, all we have to do is click the Save button. An empty job
    will be enough to test persistence.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to find out the name of the Pod created through the `jenkins` Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: With the name of the Pod stored in the environment variable `POD_NAME`, we can
    proceed and kill `java` process that's running Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We killed the Jenkins process and thus simulated failure of the container. As
    a result, Kubernetes detected the failure and recreated the container.
  prefs: []
  type: TYPE_NORMAL
- en: A minute later, we can open Jenkins home screen again, and check whether the
    state (the job we created) was preserved.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the job is still available thus proving that we successfully
    mounted the EBS volume as the directory where Jenkins preserves its state.
  prefs: []
  type: TYPE_NORMAL
- en: If instead of destroying the container, we terminated the server where the Pod
    is running, the result, from the functional perspective, would be the same. The
    Pod would be rescheduled to a healthy node. Jenkins would start again and restore
    its state from the EBS volume. Or, at least, that's what we'd hope. However, such
    behavior is not guaranteed to happen in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We have only two worker nodes, distributed in two (out of three) availability
    zones. If the node that hosted Jenkins failed, we'd be left with only one node.
    To be more precise, we'd have only one worker node running in the cluster until
    the auto-scaling group detects that an EC2 instance is missing and recreates it.
    During those few minutes, the single node we're left with is not in the same zone.
    As we already mentioned, each EBS instance is tied to a zone, and the one we mounted
    to the Jenkins Pod would not be associated with the zone where the other EC2 instance
    is running. As a result, the PersistentVolume could not re-bound the EBS volume
    and, therefore, the failed container could not be recreated, until the failed
    EC2 instance is recreated.
  prefs: []
  type: TYPE_NORMAL
- en: The chances are that the new EC2 instance would not be in the same zone as the
    one where the failed server was running. Since we're using three availability
    zones, and one of them already has an EC2 instance, AWS would recreate the failed
    server in one of the other two zones. We'd have fifty percent chances that the
    new EC2 would be in the same zone as the one where the failed server was running.
    Those are not good odds.
  prefs: []
  type: TYPE_NORMAL
- en: In the real-world scenario, we'd probably have more than two worker nodes. Even
    a slight increase to three nodes would give us a very good chance that the failed
    server would be recreated in the same zone. Auto-scaling groups are trying to
    distribute EC2 instances more or less equally across all the zones. However, that
    is not guaranteed to happen. A good minimum number of worker nodes would be six.
  prefs: []
  type: TYPE_NORMAL
- en: The more servers we have, the higher are the chances that the cluster is fault
    tolerant. That is especially true if we are hosting stateful applications. As
    it goes, we almost certainly have those. There's hardly any system that does not
    have a state in some form or another.
  prefs: []
  type: TYPE_NORMAL
- en: If it's better to have more servers than less, we might be in a complicated
    position if our system is small and needs, let's say, less than six servers. In
    such cases, I'd recommend running smaller VMs. If, for example, you planned to
    use three `t2.xlarge` EC2 instances for worker nodes, you might reconsider that
    and switch to six `t2.large` servers. Sure, more nodes mean more resource overhead
    spent on operating systems, Kubernetes system Pods, and few other things. However,
    I believe that is compensated with bigger stability of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There is still one more situation we might encounter. A whole availability zone
    (data center) might fail. Kubernetes will continue operating correctly. It'll
    have two instead of three master nodes, and the failed worker nodes will be recreated
    in healthy zones. However, we'd run into trouble with our stateful services. Kubernetes
    would not be able to reschedule those that were mounted to EBS volumes from the
    failed zone. We'd need to wait for the availability zone to come back online,
    or we'd need to move the EBS volume to a healthy zone manually. The chances are
    that, in such a case, the EBS would not be available and, therefore, could not
    be moved.
  prefs: []
  type: TYPE_NORMAL
- en: We could create a process that would be replicating data in (near) real-time
    between EBS volumes spread across multiple availability zones, but that also comes
    with a downside. Such an operation would be expensive and would likely slow down
    state retrieval while everything is fully operational. Should we choose lower
    performance over high-availability? Is the increased operational overhead worth
    the trouble? The answer to those questions will differ from one use-case to another.
  prefs: []
  type: TYPE_NORMAL
- en: There is yet another option. We could use [EFS](https://aws.amazon.com/efs/)
    ([https://aws.amazon.com/efs/](https://aws.amazon.com/efs/)) instead of EBS. But,
    that would also impact performance since EFS tends to be slower than EBS. On top
    of that, there is no production-ready EFS support in Kubernetes. At the time of
    this writing, the EFS provisioner ([https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs](https://github.com/kubernetes-incubator/external-storage/tree/master/aws/efs))
    is still in beta phase. By the time you read this, things might have changed.
    Or maybe they didn't. Even when the *efs provisioner* becomes stable, it will
    still be slower and more expensive solution than EBS.
  prefs: []
  type: TYPE_NORMAL
- en: Maybe you'll decide to ditch EBS (and EFS) in favor of some other type of persistent
    storage. There are many different options you can choose. We won't explore them
    since an in-depth comparison of all the popular solutions would require much more
    space than what we have left. Consider them an advanced topic that will be covered
    in the next book. Or maybe it won't. I do not yet know the scope of *The DevOps
    2.4 Toolkit* book.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, every solution has pros and cons and none would fit all use-cases.
    For good or bad, we'll stick with EBS for the remainder of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to PersistentVolumes tied to EBS...
  prefs: []
  type: TYPE_NORMAL
- en: Now that we explored how to manage static persistent volumes, we'll try to accomplish
    the same results using dynamic approach. But, before we do that, we'll see what
    happens when some of the resources we created are removed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's delete the `jenkins` Deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The output shows us that the `deployment "jenkins" was deleted`.
  prefs: []
  type: TYPE_NORMAL
- en: Did anything happen with the PersistentVolumeClaim and the PersistentVolume?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The combined output of both commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Even though we removed Jenkins Deployment and, with it, the Pod that used the
    claim, both the PersistentVolumeClaim and PersistentVolumes are intact. The `manual-ebs-01`
    volume is still bound to the `jenkins` claim.
  prefs: []
  type: TYPE_NORMAL
- en: What would happen if we remove the PersistentVolumeClaim `jenkins`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the `persistentvolumeclaim "jenkins" was deleted`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see what happened with the PersistentVolumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: This time, the `manual-ebs-2` volume is `Released`.
  prefs: []
  type: TYPE_NORMAL
- en: This might be a good moment to explain the `Retain` policy applied to the PersistentVolumes
    we created.
  prefs: []
  type: TYPE_NORMAL
- en: '`ReclaimPolicy` defines what should be done with a volume after it''s released
    from its claim. The policy was applied the moment we deleted the PersistentVolumeClaim
    that was bound to `manual-ebs-02`. When we created the PersistentVolumes, we did
    not specify `ReclaimPolicy`, so the volumes were assigned the default policy which
    is `Retain`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Retain` reclaim policy enforces manual reclamation of the resource. When
    the PersistentVolumeClaim is deleted, the PersistentVolume still exists, and the
    volume is considered `released`. But it is not yet available for other claims
    because the previous claimant's data remains on the volume. In our case, that
    data is Jenkins state. If we'd like this PersistentVolume to become available,
    we'd need to delete all the data on the EBS volume.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are running the cluster in AWS, it is easier to delete than to recycle
    resources, so we'll remove the released PersistentVolume instead of trying to
    clean everything we generated inside the EBS. Actually, we'll remove all the volumes
    since we are about to explore how we can accomplish the same effects dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: The other two reclaim policies are `Recycle` and `Delete`. `Recycle` is considered
    deprecated so we won't waste time explaining it. The `Delete` policy requires
    dynamic provisioning, but we'll postpone the explanation until we explore that
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s delete some stuff:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: We can see that all three PersistentVolumes were deleted. However, only Kubernetes
    resources were removed. We still need to manually delete the EBS volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you go to your AWS console, you''ll see that all three EBS volumes are now
    in the `available` state and waiting to be mounted. We''ll delete them all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We are finished with our tour around manual creation of persistent volumes.
    If we'd use this approach to volume management, cluster administrator would need
    to ensure that there is always an extra number of available volumes that can be
    used by new claims. It is tedious work that often results in having more volumes
    than we need. On the other hand, if we don't have a sufficient number of available
    (unused) volumes, we're risking that someone will create a claim that will not
    find a suitable volume to mount.
  prefs: []
  type: TYPE_NORMAL
- en: Manual volume management is sometimes unavoidable, especially if chose to use
    on-prem infrastructure combined with NFS. However, this is not our case. AWS is
    all about dynamic resource provisioning, and we'll exploit that to its fullest.
  prefs: []
  type: TYPE_NORMAL
- en: Using storage classes to dynamically provision persistent volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we used static PersistentVolumes. We had to create both EBS volumes
    and Kubernetes PersistentVolumes manually. Only after both became available were
    we able to deploy Pods that are mounting those volumes through PersistentVolumeClaims.
    We'll call this process static volume provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, static volume provisioning is a necessity. Our infrastructure
    might not be capable of creating dynamic volumes. That is often the case with
    on-premise infrastructure with volumes based on NFS. Even then, with a few tools,
    a change in processes, and right choices for supported volume types, we can often
    reach the point where volume provisioning is dynamic. Still, that might prove
    to be a challenge with legacy processes and infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Since our cluster is in AWS, we cannot blame legacy infrastructure for provisioning
    volumes manually. Indeed, we could have jumped straight into this section. After
    all, AWS is all about dynamic infrastructure management. However, I felt that
    it will be easier to understand the processes by exploring manual provisioning
    first. The knowledge we obtained thus far will help us understand better what's
    coming next. The second reason for starting with manual provisioning lies in my
    inability to predict your plans. Maybe you will run a Kubernetes cluster on infrastructure
    that has to be static. Even though we're using AWS for the examples, everything
    you learned this far can be implemented on static infrastructure. You'll only
    have to change EBS with NFS and go through NFSVolumeSource ([https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core](https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#nfsvolumesource-v1-core))
    documentation. There are only three NFS-specific fields so you should be up-and-running
    in no time.
  prefs: []
  type: TYPE_NORMAL
- en: Before we discuss how to enable dynamic persistent volume provisioning, we should
    understand that it will be used only if none of the static PersistentVolumes match
    our claims. In other words, Kubernetes will always select statically created PersistentVolumes
    over dynamic ones.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic volume provisioning allows us to create storage on-demand. Instead of
    manually pre-provisioning storage, we can provision it automatically when a resource
    requests it.
  prefs: []
  type: TYPE_NORMAL
- en: We can enable dynamic provisioning through the usage of StorageClasses from
    the `storage.k8s.io` API group. They allow us to describe the types of storage
    that can be claimed. On the one hand, cluster administrator can create as many
    StorageClasses as there are storage flavours. On the other hand, the users of
    the cluster do not have to worry about the details of each available external
    storage. It's a win-win situation where the administrators do not have to create
    PersistentVolumes in advance, and the users can simply claim the storage type
    they need.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable dynamic provisioning, we need to create at least one StorageClass
    object. Luckily for us, kops already set up a few, so we might just as well take
    a look at the StorageClasses currently available in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there are two StorageClasses in our cluster. Both are using
    the same `aws-ebs` provisioner. Besides the names, the only difference, at least
    in this output, is that one of them is marked as `default`. We'll explore what
    that means a bit later. For now, we'll trust that kops configured those classes
    correctly and try to claim a PersistentVolume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at yet another `jenkins` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This Jenkins definition is almost the same as the one we used before. The only
    difference is in the PersistentVolumeClaim that, this time, specified `gp2` as
    the `StorageClassName`. There is one more difference though. This time we do not
    have any PersistentVolume pre-provisioned. If everything works as expected, a
    new PersistentVolume will be created dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We can see that some of the resources were re-configured, while others were
    created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll wait until the `jenkins` Deployment is rolled out successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Now we should be able to see what happened through the `jenkins` namespace events.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the last few lines, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We can see that a new PersistentVolume was `successfully provisioned`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at the status of the PersistentVolumeClaim.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The part of the output that matters is the status. We can see that it `Bound`
    with the PersistentVolume thus confirming, again, that the volume was indeed created
    dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be on the safe side, we''ll list the PersistentVolumes as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the PersistentVolume was created, it is bound to the PersistentVolumeClaim,
    and its reclaim policy is `Delete`. We'll see the policy in action soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the last verification we''ll perform is to confirm that the EBS volume
    was created as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: We can see that a new EBS volume was created in the availability zone `us-east-2c`,
    that the type is `gp2`, and that its state is `in-use`.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic provisioning works! Given that we're using AWS, it is a much better
    solution than using static resources.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move into a next subject, we'll explore the effect of the reclaim
    policy `Delete`. To do so, we'll delete the Deployment and the PersistentVolumeClaim.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Now that the claim to the volume was removed, we can check what happened with
    the dynamically provisioned PersistentVolumes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that `no resources` were found, clearly indicating that the
    PersistentVolume that was created through the claim is now gone.
  prefs: []
  type: TYPE_NORMAL
- en: How about the AWS EBS volume? Was it removed as well?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: We got an empty array proving that the EBS volume was removed as well.
  prefs: []
  type: TYPE_NORMAL
- en: Through dynamic volume provisioning, not only that volumes are created when
    resources claim them, but they are also removed when the claims are released.
    Dynamic removal is accomplished through the reclaim policy `Delete`.
  prefs: []
  type: TYPE_NORMAL
- en: Using default storage classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with dynamic provisioning simplifies a few things. Still, a user needs
    to know which volume type to use. While in many cases that is an important choice,
    there are often situations when a user might not want to worry about that. It
    might be easier to use the cluster administrator's choice for volume types and
    let all claims that do not specify `storageClassName` get a default volume. We'll
    try to accomplish that through one of the admission controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Admission controllers are intercepting requests to the Kubernetes API server.
    We won't go into details of admission controllers since the list of those supported
    by Kubernetes is relatively big. We are interested only in the `DefaultStorageClass`
    which happens to be already enabled in the cluster we created with kops.
  prefs: []
  type: TYPE_NORMAL
- en: '`DefaultStorageClass` admission controller observes creation of PersistentVolumeClaims.
    Through it, those that do not request any specific storage class are automatically
    added a default storage class to them. As a result, PersistentVolumeClaims that
    do not request any special storage class are bound to PersistentVolumes created
    from the default `StorageClass`. From user''s perspective, there''s no need to
    care about volume types since they will be provisioned based on the default type
    unless they choose a specific class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the storage classes currently available in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: This is not the first time we're listing the storage classes in our cluster.
    However, we did not discuss that one of the two (`gp2`) is marked as the default
    `StorageClass`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's describe the `gp2` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: The important part lies in the annotations. One of them is `".../is-default-class":"true"`.
    It sets that `StorageClass` as default. As a result, it will be used to create
    PersistentVolumes by any PersistentVolumeClaim that does not specify StorageClass
    name.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to adapt Jenkins stack to use the ability to dynamically provision
    a volume associated with the `DefaultStorageClass`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new Jenkins definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: The output, limited to the `PersistentVolumeClaim`, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s hard to spot the difference between that YAML file and the one we used
    before. It is very small and hard to notice change so we''ll execute `diff` to
    compare the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the only difference is that `pv/jenkins-dynamic.yml` doesn''t
    have `storageClassName: gp2`. That field is omitted from the new definition. Our
    new `PersistentVolumeClaim` does not have an associated StorageClass.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s `apply` the new definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: What we're interested in are PersistentVolumes, so let's retrieve them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, even though we did not specify any StorageClass, a volume was
    created based on the `gp2` class, which happens to be the default one.
  prefs: []
  type: TYPE_NORMAL
- en: We'll delete the `jenkins` Deployment and PersistentVolumeClaim before we explore
    how we can create our own StorageClasses.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Creating storage classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though kops created two StorageClasses, both are based on `gp2`. While
    that is the most commonly used EBS type, we might want to create volumes based
    on one of the other three options offered by AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we want the fastest EBS volume type for our Jenkins. That would
    be `io1`. Since kops did not create a StorageClass of that type, we might want
    to create our own.
  prefs: []
  type: TYPE_NORMAL
- en: YAML file that creates StorageClass based on EBS `io1` is defined in `pv/sc.yml`.
    Let's take a quick look.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: We used `kubernetes.io/aws-ebs` as the `provisioner`. It is a mandatory field
    that determines the plugin that will be used for provisioning PersistentVolumes.
    Since we are running the cluster in AWS, `aws-ebs` is the logical choice. There
    are quite a few other provisioners we could choose. Some of them are specific
    to a hosting provider (for example, `GCEPersistentDisk` and `AzureDisk`) while
    others can be used anywhere (for example, `GlusterFS`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of supported provisioners is growing. At the time of this writing,
    the following types are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Volume Plugin | Internal Provisioner |'
  prefs: []
  type: TYPE_TB
- en: '| AWSElasticBlockStore | yes |'
  prefs: []
  type: TYPE_TB
- en: '| AzureFile | yes |'
  prefs: []
  type: TYPE_TB
- en: '| AzureDisk | yes |'
  prefs: []
  type: TYPE_TB
- en: '| CephFS | no |'
  prefs: []
  type: TYPE_TB
- en: '| Cinder | yes |'
  prefs: []
  type: TYPE_TB
- en: '| FC | no |'
  prefs: []
  type: TYPE_TB
- en: '| FlexVolume | no |'
  prefs: []
  type: TYPE_TB
- en: '| Flocker | yes |'
  prefs: []
  type: TYPE_TB
- en: '| GCEPersistentDisk | yes |'
  prefs: []
  type: TYPE_TB
- en: '| Glusterfs | yes |'
  prefs: []
  type: TYPE_TB
- en: '| iSCSI | no |'
  prefs: []
  type: TYPE_TB
- en: '| PhotonPersistentDisk | yes |'
  prefs: []
  type: TYPE_TB
- en: '| Quobyte | yes |'
  prefs: []
  type: TYPE_TB
- en: '| NFS | no |'
  prefs: []
  type: TYPE_TB
- en: '| RBD | yes |'
  prefs: []
  type: TYPE_TB
- en: '| VsphereVolume | yes |'
  prefs: []
  type: TYPE_TB
- en: '| PortworxVolume | yes |'
  prefs: []
  type: TYPE_TB
- en: '| ScaleIO | yes |'
  prefs: []
  type: TYPE_TB
- en: '| StorageOS | yes |'
  prefs: []
  type: TYPE_TB
- en: '| Local | no |'
  prefs: []
  type: TYPE_TB
- en: The internal provisioners are those with names prefixed with `kubernetes.io`
    (for example, `kubernetes.io/aws-ebs`). They are shipped with Kubernetes. External
    provisioners, on the other hand, are independent programs shipped separately from
    Kubernetes. An example of a commonly used external provisioner is `NFS`. The parameters
    depend on the StorageClass. We used the `aws-ebs` provisioner which allows us
    to specify the `type` parameter that defines one of the supported Amazon EBS volume
    types. It can be EBS Provisioned IOPS SSD (`io1`), EBS **General Purpose SSD**
    (**gp2**), Throughput Optimized HDD (`st1`), and Cold HDD (`sc1`). We set it to
    `io1` which is the highest performance SSD volume. Please consult [*Parameters*](https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters)
    ([https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters](https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters))
    section of the *Storage Classes* documentation for more info. Finally, we set
    the `reclaimPolicy` to `Delete`. Unlike `Retain` that forces us to delete the
    contents of the released volume before it becomes available to new PersistentVolumeClaims,
    `Delete` removes both the PersistentVolume as well as the associated volume in
    the external architecture. The `Delete` reclaim policy works only with some of
    the external volumes like AWS EBS, Azure Disk, or Cinder volume. Now that we dipped
    our toes into the StorageClass definition, we can proceed and create it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the `storageclass "fast" was created`, so we'll list,
    one more time, the StorageClassses in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, this time, we have a new StorageClass.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at yet another Jenkins definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: The only difference, when compared with the previous definition, is that we
    are now using the newly created StorageClass named `fast`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll confirm that the new StorageClass works by deploying the new
    `jenkins` definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: As the final verification, we'll list the EBS volumes and confirm that a new
    one was created based on the new class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the type of the newly created EBS volume is `io1` and that it
    is `in-use`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3596ac4-ca51-4225-aaee-457ec369a86e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-5: The sequence of events initiated with a request to create a Jenkins
    Pod with the PersistentVolumeClaim using a custom StorageClass'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simplified version of the flow of events initiated with the creation of the
    `jenkins` Deployment is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We created the `jenkins` Deployment, which created a ReplicaSet, which, in turn,
    created a Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Pod requested persistent storage through the PersistentVolumeClaim.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PersistentVolumeClaim requested PersistentStorage with the StorageClass
    name `fast`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: StorageClass `fast` is defined to create a new EBS volume, so it requested one
    from the AWS API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS API created a new EBS volume.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: EBS volume was mounted to the `jenkins` Pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We're finished exploring persistent volumes. You should be equipped with the
    knowledge how to persist your stateful applications, and the only pending action
    is to remove the volumes and the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There's nothing left to do but to destroy what we did so far.
  prefs: []
  type: TYPE_NORMAL
- en: This time, we cannot just delete the cluster. Such an action would leave the
    EBS volumes running. So, we need to remove them first.
  prefs: []
  type: TYPE_NORMAL
- en: We could remove EBS volumes through AWS CLI. However, there is an easier way.
    If we delete all the claims to EBS volumes, they will be deleted as well since
    our PersistentVolumes are created with the reclaim policy set to `Delete`. EBS
    volumes are created when needed and destroyed when not.
  prefs: []
  type: TYPE_NORMAL
- en: Since all our claims are in the `jenkins` namespace, removing it is the easiest
    way to delete them all.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the `namespace "jenkins" was deleted` and we can proceed
    to delete the cluster as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the output that the cluster `devops23.k8s.local` was deleted
    and we are left only with the S3 bucket used for kops state. We'll delete it as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Before you leave, please consult the following API references for more information
    about volume-related resources.
  prefs: []
  type: TYPE_NORMAL
- en: '[PersistentVolume v1 core](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolume-v1-core)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PersistentVolumeClaim v1 core](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#persistentvolumeclaim-v1-core))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[StorageClass v1 storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage)
    ([https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage](https://v1-8.docs.kubernetes.io/docs/api-reference/v1.8/#storageclass-v1-storage))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That's it. There's nothing left.
  prefs: []
  type: TYPE_NORMAL
