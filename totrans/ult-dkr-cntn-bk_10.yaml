- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Single-Host Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the most important architectural patterns
    and best practices that are used when dealing with distributed application architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the Docker container networking model and
    its single-host implementation in the form of the bridge network. This chapter
    also introduces the concept of **Software Defined Networks** (**SDNs**) and how
    they are used to secure containerized applications. Furthermore, we will demonstrate
    how container ports can be opened to the public and thus make containerized components
    accessible to the outside world. Finally, we will introduce Traefik, a reverse
    proxy, which can be used to enable sophisticated HTTP application-level routing
    between containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Dissecting the container network model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network firewalling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the bridge network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The host and null network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running in an existing network namespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing container ports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP-level routing using a reverse proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After completing this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create, inspect, and delete a custom bridge network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a container attached to a custom bridge network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isolate containers from each other by running them on different bridge networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publish a container port to a host port of your choice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add Traefik as a reverse proxy to enable application-level routing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, the only thing you will need is a Docker host that is able
    to run Linux containers. You can use your laptop with Docker Desktop for this
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, let’s first create a folder for this chapter where we are going
    to store the code for our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the folder where you have cloned the repository accompanying this
    book. Usually, this is the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a subfolder for this chapter and navigate to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Dissecting the container network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have mostly worked with single containers, but in reality, a containerized
    business application consists of several containers that need to collaborate to
    achieve a goal. Therefore, we need a way for individual containers to communicate
    with each other. This is achieved by establishing pathways, which we can use to
    send data packets back and forth between containers. These pathways are called
    networks. Docker has defined a very simple networking model, the so-called **container
    network model** (**CNM**), to specify the requirements that any software that
    implements a container network has to fulfill. The following is a graphical representation
    of the CNM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – The Docker CNM](img/B19199_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – The Docker CNM
  prefs: []
  type: TYPE_NORMAL
- en: 'The CNM has three elements – sandboxes, endpoints, and networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network Sandboxes**: The sandbox perfectly isolates a container from the
    outside world. No inbound network connection is allowed into the sandboxed container,
    but it is very unlikely that a container will be of any value in a system if absolutely
    no communication with it is possible. To work around this, we have element number
    two, which is the endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoint**: An endpoint is a controlled gateway from the outside world into
    the network’s sandbox, which shields the container. The endpoint connects the
    network sandbox (but not the container) to the third element of the model, which
    is the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network**: The network is the pathway that transports the data packets of
    an instance of communication from endpoint to endpoint or, ultimately, from container
    to container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that a network sandbox can have zero to many endpoints,
    or, said differently, each container living in a network sandbox can either be
    attached to no network at all or it can be attached to multiple different networks
    at the same time. In the preceding diagram, the middle one of the three **Network
    Sandboxes** is attached to both **Network 1** and **Network 2** using an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: This networking model is very generic and does not specify where the individual
    containers that communicate with each other over a network run. All containers
    could, for example, run on the same host (local) or they could be distributed
    across a cluster of hosts (global).
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the CNM is just a model describing how networking works among containers.
    To be able to use networking with our containers, we need real implementations
    of the CNM. For both local and global scopes, we have multiple implementations
    of the CNM. In the following table, we’ve given a short overview of the existing
    implementations and their main characteristics. The list is in no particular order:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Network** | **Company** | **Scope** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Bridge | Docker | Local | Simple network based on Linux bridges to allow
    networking on a single host |'
  prefs: []
  type: TYPE_TB
- en: '| Macvlan | Docker | Local | Configures multiple layer-2 (that is, MAC) addresses
    on a single physical host interface |'
  prefs: []
  type: TYPE_TB
- en: '| Overlay | Docker | Global | Multi-node capable container network based on
    **Virtual Extensible** **LAN** (**VXLan**) |'
  prefs: []
  type: TYPE_TB
- en: '| Weave Net | Weaveworks | Global | Simple, resilient, multi-host Docker networking
    |'
  prefs: []
  type: TYPE_TB
- en: '| Contiv Network Plugin | Cisco | Global | Open source container networking
    |'
  prefs: []
  type: TYPE_TB
- en: Table 10.1 – Network types
  prefs: []
  type: TYPE_NORMAL
- en: All network types not directly provided by Docker can be added to a Docker host
    as a plugin.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will describe how network firewalling works.
  prefs: []
  type: TYPE_NORMAL
- en: Network firewalling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker has always had the mantra of security first. This philosophy had a direct
    influence on how networking in a single- and multi-host Docker environment was
    designed and implemented. SDNs are easy and cheap to create, yet they perfectly
    firewall containers that are attached to this network from other non-attached
    containers, and from the outside world. All containers that belong to the same
    network can freely communicate with each other, while others have no means to
    do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we have two networks called **front** and **back**.
    Attached to the **front** network, we have containers **c1** and **c2**, and attached
    to the **back** network, we have containers **c3** and **c4**. **c1** and **c2**
    can freely communicate with each other, as can **c3** and **c4**, but **c1** and
    **c2** have no way to communicate with either **c3** or **c4**, and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Docker networks](img/B19199_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Docker networks
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what about a situation in which we have an application consisting of three
    services: `webAPI`, `productCatalog`, and `database`? We want `webAPI` to be able
    to communicate with `productCatalog`, but not with the database, and we want `productCatalog`
    to be able to communicate with the database service. We can solve this situation
    by placing `webAPI` and the database on different networks and attaching `productCatalog`
    to both of these networks, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Container attached to multiple networks](img/B19199_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Container attached to multiple networks
  prefs: []
  type: TYPE_NORMAL
- en: Since creating SDNs is cheap, and each network provides added security by isolating
    resources from unauthorized access, it is highly recommended that you design and
    run applications so that they use multiple networks and only run services on the
    same network that absolutely need to communicate with each other. In the preceding
    example, there is absolutely no need for the `webAPI` component to ever communicate
    directly with the `database` service, so we have put them on different networks.
    If the worst-case scenario happens and a hacker compromises `webAPI`, they won't
    be able to access the database from there without also hacking the `productCatalog`
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to discuss the first implementation of the CNM, the bridge
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the bridge network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Docker bridge network is the first implementation of the CNM that we’re
    going to look at in detail. This network implementation is based on the Linux
    bridge.
  prefs: []
  type: TYPE_NORMAL
- en: When the Docker daemon runs for the first time, it creates a Linux bridge and
    calls it `docker0`. This is the default behavior and can be changed by changing
    the configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Docker then creates a network with this Linux bridge and calls it the network
    bridge. All the containers that we create on a Docker host and that we do not
    explicitly bind to another network lead to Docker automatically attaching the
    containers to this bridge network.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that we indeed have a network called `bridge` of the `bridge` type
    defined on our host, we can list all the networks on the host with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This should provide an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Listing all the Docker networks available by default](img/B19199_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Listing all the Docker networks available by default
  prefs: []
  type: TYPE_NORMAL
- en: In your case, the IDs will be different, but the rest of the output should look
    the same. We do indeed have a first network called `bridge` using the `bridge`
    driver. The scope being `local` just means that this type of network is restricted
    to a single host and cannot span across multiple hosts. In [*Chapter 14*](B19199_14.xhtml#_idTextAnchor303),
    *Introducing* *Docker Swarm*, we will also discuss other types of networks that
    have a global scope, meaning they can span whole clusters of hosts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look a little bit deeper into what this bridge network is all about.
    For this, we are going to use the Docker `inspect` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When executed, this outputs a big chunk of detailed information about the network
    in question. This information should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Output generated when inspecting the Docker bridge network](img/B19199_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Output generated when inspecting the Docker bridge network
  prefs: []
  type: TYPE_NORMAL
- en: We saw the `ID`, `Name`, `Driver`, and `Scope` values when we listed all the
    networks, so that is nothing new, but let’s have a look at the **IP address management**
    (**IPAM**) block.
  prefs: []
  type: TYPE_NORMAL
- en: 'IPAM is a piece of software that is used to track the IP addresses that are
    used on a computer. The important part of the IPAM block is the *config* node
    with its values for the subnet and gateway. The subnet for the bridge network
    is defined by default as `172.17.0.0/16`. This means that all containers attached
    to this network will get an IP address assigned by Docker that is taken from the
    given range, which is `172.17.0.2` to `172.17.255.255`. The `172.17.0.1` address
    is reserved for the router of this network whose role in this type of network
    is taken by the Linux bridge. We can expect that the very first container that
    will be attached to this network by Docker will get the `172.17.0.2` address.
    All subsequent containers will get a higher number; the following diagram illustrates
    this fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The bridge network](img/B19199_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – The bridge network
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see the network namespace of the host, which
    includes the host’s `eth0` endpoint, which is typically an NIC if the Docker host
    runs on bare metal or a virtual NIC if the Docker host is a VM. All traffic to
    the host comes through `eth0`. The Linux bridge is responsible for routing the
    network traffic between the host’s network and the subnet of the bridge network.
  prefs: []
  type: TYPE_NORMAL
- en: What is a NIC?
  prefs: []
  type: TYPE_NORMAL
- en: A **Network Interface Card** (**NIC**), sometimes referred to as a network interface
    connector, is a hardware component that enables a computer or device to connect
    to a network. It serves as an interface between the computer and the network,
    allowing data to be transmitted and received. NICs are typically built-in components
    on motherboards or installed as expansion cards and support various types of network
    connections, such as Ethernet, Wi-Fi, or fiber-optic connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, only egress traffic is allowed, and all ingress is blocked. What
    this means is that while containerized applications can reach the internet, they
    cannot be reached by any outside traffic. Each container attached to the network
    gets its own **virtual ethernet** (**veth**) connection to the bridge. This is
    illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Details of the bridge network](img/B19199_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Details of the bridge network
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram shows us the world from the perspective of the host.
    We will explore what this situation looks like from within a container later on
    in this section. We are not limited to just the bridge network, as Docker allows
    us to define our own custom bridge networks. This is not just a feature that is
    nice to have; it is a recommended best practice not to run all containers on the
    same network. Instead, we should use additional bridge networks to further isolate
    containers that have no need to communicate with each other. To create a custom
    bridge network called `sample-net`, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we do this, we can then inspect what subnet Docker has created for this
    new custom network, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Evidently, Docker has just assigned the next free block of IP addresses to
    our new custom bridge network. If, for some reason, we want to specify our own
    subnet range when creating a network, we can do so by using the `--``subnet` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To avoid conflicts due to duplicate IP addresses, make sure you avoid creating
    networks with overlapping subnets.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed what a bridge network is and how we can create a
    custom bridge network, we want to understand how we can attach containers to these
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s interactively run an Alpine container without specifying the network
    to be attached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In another Terminal window, let’s inspect the `c1` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the vast output, let’s concentrate for a moment on the part that provides
    network-related information. This can be found under the `NetworkSettings` node.
    I have it listed in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – The NetworkSettings section of the container metadata](img/B19199_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – The NetworkSettings section of the container metadata
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, we can see that the container is indeed attached to
    the bridge network since `NetworkID` is equal to `d172692...`, which we can see
    from the preceding code being the ID of the bridge network. We can also see that
    the container was assigned the IP address of `172.17.0.2` as expected and that
    the gateway is at `172.17.0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the container also had a `MacAddress` associated with it. This
    is important as the Linux bridge uses the `MacAddress` for routing.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have approached this from the outside of the container’s network
    namespace. Now, let’s see what the situation looks like when we’re not only inside
    the container but inside the container’s network namespace. Inside the `c1` container,
    let’s use the `ip` tool to inspect what’s going on. Run the `ip addr` command
    and observe the output that is generated, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Container namespace, as seen by the IP tool](img/B19199_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Container namespace, as seen by the IP tool
  prefs: []
  type: TYPE_NORMAL
- en: The interesting part of the preceding output is `54:`, that is, the `eth0` endpoint.
    The `veth0` endpoint that the Linux bridge created outside of the container namespace
    is mapped to `eth0` inside the container. Docker always maps the first endpoint
    of a container network namespace to `eth0`, as seen from inside the namespace.
    If the network namespace is attached to an additional network, then that endpoint
    will be mapped to `eth1`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since at this point, we’re not really interested in any endpoint other than
    `eth0`, we could have used a more specific variant of the command, which would
    have given us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – eth0 endpoint as seen from inside of the container](img/B19199_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – eth0 endpoint as seen from inside of the container
  prefs: []
  type: TYPE_NORMAL
- en: In the output, we can also see what MAC address (`02:42:ac:11:00:02`) and what
    IP (`172.17.0.2`) have been associated with this container network namespace by
    Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also get some information about how requests are routed by using the
    `ip` `route` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This output tells us that all the traffic to the gateway at `172.17.0.1` is
    routed through the `eth0` device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run another container called `c2` on the same network and in `detach`
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `c2` container will also be attached to the bridge network since we have
    not specified any other network. Its IP address will be the next free one within
    the subnet, which is `172.17.0.3`, as we can readily test with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have two containers attached to the bridge network. We can try to inspect
    this network once again to find a list of all containers attached to it in the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This information can be found under the `Containers` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – The Containers section of the output of the Docker network
    inspect bridge](img/B19199_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – The Containers section of the output of the Docker network inspect
    bridge
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we have shortened the output to the relevant part for readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create two additional containers, `c3` and `c4`, and attach them
    to `sample-net`, which we created earlier. For this, we’ll use the `--``network`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s inspect the `sample-net` network and confirm that `c3` and `c4` are indeed
    attached to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output for the `Containers` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – The Containers section of the Docker network inspect test-net
    command](img/B19199_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – The Containers section of the Docker network inspect test-net
    command
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question we’re going to ask ourselves is whether the `c3` and `c4`
    containers can freely communicate with each other. To demonstrate that this is
    indeed the case, we can `exec` into the `c3` container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once inside the container, we can try to `ping` container `c4` by name and
    by IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of the container name, here, we use `c4`’s IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The answer in both cases confirms to us that the communication between containers
    attached to the same network is working as expected. The fact that we can even
    use the name of the container we want to connect to shows us that the name resolution
    provided by the Docker DNS service works inside this network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we want to make sure that the `bridge` and `sample-net` networks are firewalled
    from each other. To demonstrate this, we can try to `ping` the `c2` container
    from the `c3` container, either by its name or by its IP address. Let’s start
    with pinging by name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result of the ping using the IP address of the `c2` container
    instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'It gives us this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command remained hanging and I had to terminate the command with
    *Ctrl* + *C*. From the output of pinging `c2`, we can also see that the name resolution
    does not work across networks. This is the expected behavior. Networks provide
    an extra layer of isolation, and thus security, to containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, we learned that a container can be attached to multiple networks.
    Let’s first create a network called `test-net`. Note that the following command
    does not define the driver of the network; thus, the default driver is used, which
    happens to be the bridge driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we attach a container, `c5`, to our `sample-net` network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we attach the `c6` container to the `sample-net` and `test-net` networks
    at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can test that `c6` is reachable from the `c5` container attached to
    the `test-net` network, as well as from the `c3` container attached to the `sample-net`
    network. The result will show that the connection indeed works.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to remove an existing network, we can use the `docker network rm`
    command, but note that we cannot accidentally delete a network that has containers
    attached to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It results in this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we continue, let’s clean up and remove all the containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can remove the two custom networks that we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we could remove all the networks that no container is attached
    to with the `prune` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: I used the `--force` (or `-f`) argument here to prevent Docker from reconfirming
    that I really want to remove all unused networks.
  prefs: []
  type: TYPE_NORMAL
- en: Double-check with the `docker network ls` command that you are only left with
    the three default networks provided by Docker.
  prefs: []
  type: TYPE_NORMAL
- en: The next network types we are going to inspect a bit are the `host` and `null`
    network types.
  prefs: []
  type: TYPE_NORMAL
- en: The host and null networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to look at two predefined and somewhat unique
    types of networks, the host and the null networks. Let’s start with the former.
  prefs: []
  type: TYPE_NORMAL
- en: The host network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are occasions when we want to run a container in the network namespace
    of the host. This may be necessary when we need to run some software in a container
    that is used to analyze or debug the host network's traffic, but keep in mind
    that these are very specific scenarios. When running business software in containers,
    there is no good reason to ever run the respective containers attached to the
    host’s network. For security reasons, it is strongly recommended that you do not
    run any such container attached to the host network in a production or production-like
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, how can we run a container inside the network namespace of the host?
    Simply by attaching the container to the `host` network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run an Alpine container and attach it to the `host` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `ip` tool to analyze the network namespace from within the container.
    You will see that we get exactly the same picture as we would if we were running
    the `ip` tool directly on the host. For example, I inspect the `eth0` device on
    my laptop with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a result, I get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Showing the eth0 device from inside a container](img/B19199_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – Showing the eth0 device from inside a container
  prefs: []
  type: TYPE_NORMAL
- en: Here, I can see that `192.168.65.3` is the IP address that the host has been
    assigned and that the MAC address shown here also corresponds to that of the host.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also inspect the routes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On my MacBook Air M1, this is what I get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Routes from within a container](img/B19199_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Routes from within a container
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the next section of this chapter, I want to once again
    point out that running a container on the host network can be dangerous due to
    potential security vulnerabilities and conflicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security risks**: By using the host network, the container has the same network
    access as the host machine. This means that if an application running within the
    container has a vulnerability that is exploited, the attacker could gain access
    to the host network and potentially compromise other services or data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Port conflicts**: When a container uses the host network, it shares the same
    network namespace as the host. This means that if your containerized application
    and a host application listen on the same port, there can be conflicts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolation**: One of the major benefits of using Docker is the isolation it
    provides at various levels (process, filesystem, or network). By using the host
    network, you lose this level of isolation, which could lead to unforeseen issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, it’s generally recommended to use a user-defined network instead
    of the host network when running Docker containers, as it provides better isolation
    and reduces the risk of conflicts and security vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The null network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sometimes, we need to run a few application services or jobs that do not need
    any network connection at all to execute the task at hand. It is strongly advised
    that you run those applications in a container that is attached to the `none`
    network. This container will be completely isolated and is thus safe from any
    outside access. Let’s run such a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Once inside the container, we can verify that there is no `eth0` network endpoint
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'There is also no routing information available, as we can demonstrate by using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This returns nothing.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we are going to learn how we can run a container inside
    the existing network namespace of another container.
  prefs: []
  type: TYPE_NORMAL
- en: Running in an existing network namespace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normally, Docker creates a new network namespace for each container we run.
    The network namespace of the container corresponds to the sandbox of the container
    network model we described earlier on. As we attach the container to a network,
    we define an endpoint that connects the container network namespace to the actual
    network. This way, we have one container per network namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker provides an additional way for us to define the network namespace that
    a container runs in. When creating a new container, we can specify that it should
    be attached to (or maybe we should say included in) the network namespace of an
    existing container. With this technique, we can run multiple containers in a single
    network namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Multiple containers running in a single network namespace](img/B19199_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – Multiple containers running in a single network namespace
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see that in the leftmost network namespace,
    we have two containers. The two containers, since they share the same namespace,
    can communicate on `localhost` with each other. The network namespace (and not
    the individual containers) is then attached to the **front** network.
  prefs: []
  type: TYPE_NORMAL
- en: This is useful when we want to debug the network of an existing container without
    running additional processes inside that container. We can just attach a special
    utility container to the network namespace of the container to inspect. This feature
    is also used by Kubernetes when it creates a Pod. We will learn more about Kubernetes
    and Pods in subsequent chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s demonstrate how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a new `bridge` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we run a container attached to this network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we run another container and attach it to the network of our web container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specifically, note how we define the network: `--network container:web`. This
    tells Docker that our new container will use the same network namespace as the
    container called `web`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the new container is in the same network namespace as the web container
    running nginx, we’re now able to access nginx on `localhost`! We can prove this
    by using the `wget` tool, which is part of the Alpine container, to connect to
    nginx. We should see the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we have shortened the output for readability. Please also note that
    there is an important difference between running two containers attached to the
    same network and two containers running in the same network namespace. In both
    cases, the containers can freely communicate with each other, but in the latter
    case, the communication happens over `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clean up the container and network, we can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next section, we are going to learn how to expose container ports on
    the container host.
  prefs: []
  type: TYPE_NORMAL
- en: Managing container ports
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how we can isolate firewall containers from each other by placing
    them on different networks, and that we can have a container attached to more
    than one network, we have one problem that remains unsolved. How can we expose
    an application service to the outside world? Imagine a container running a web
    server hosting our `webAPI` from before. We want customers from the internet to
    be able to access this API. We have designed it to be a publicly accessible API.
    To achieve this, we have to, figuratively speaking, open a gate in our firewall
    through which we can funnel external traffic to our API. For security reasons,
    we don’t just want to open the doors wide; we want to have a single controlled
    gate that traffic flows through.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create this kind of gate by mapping a container port to an available
    port on the host. We’re also calling this container port to publish a port. Remember
    that the container has its own virtual network stack, as does the host. Therefore,
    container ports and host ports exist completely independently and by default have
    nothing in common at all, but we can now wire a container port with a free host
    port and funnel external traffic through this link, as illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Mapping container ports to host ports](img/B19199_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – Mapping container ports to host ports
  prefs: []
  type: TYPE_NORMAL
- en: 'But now, it is time to demonstrate how we can actually map a container port
    to a host port. This is done when creating a container. We have different ways
    of doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can let Docker decide which host port our container port should be
    mapped to. Docker will then select one of the free host ports in the range of
    `32xxx`. This automatic mapping is done by using the `-``P` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding command runs an nginx server in a container. nginx is listening
    at port `80` inside the container. With the `-P` parameter, we’re telling Docker
    to map all the exposed container ports to a free port in the `32xxx` range. We
    can find out which host port Docker is using by using the `docker container` `port`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The nginx container only exposes port `80`, and we can see that it has been
    mapped to the host port `32768`. If we open a new browser window and navigate
    to `localhost:32768`, we should see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – The welcome page of nginx](img/B19199_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – The welcome page of nginx
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative way to find out which host port Docker is using for our container
    is to inspect it. The host port is part of the `NetworkSettings` node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the third way of getting this information is to list the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note that in the preceding output, the `/tcp` part tells us that the
    port has been opened for communication with the TCP protocol, but not for the
    UDP protocol. TCP is the default, and if we want to specify that we want to open
    the port for UDP, then we have to specify this explicitly. The special (IP) address,
    `0.0.0.0`, in the mapping tells us that traffic from any host IP address can now
    reach container port `80` of the web container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, we want to map a container port to a very specific host port. We
    can do this by using the `-p` parameter (or `--publish`). Let’s look at how this
    is done with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The value of the `-p` parameter is in the form of `<host port>:<container port>`.
    Therefore, in the preceding case, we map container port `80` to host port `8080`.
    Once the `web2` container runs, we can test it in the browser by navigating to
    `localhost:8080`, and we should be greeted by the same nginx welcome page that
    we saw in the previous example that dealt with automatic port mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the UDP protocol for communication over a certain port, the publish
    parameter will look like so: `-p 3000:4321/udp`. Note that if we want to allow
    communication with both TCP and UDP protocols over the same port, then we have
    to map each protocol separately.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk about HTTP routing using a reverse proxy.
  prefs: []
  type: TYPE_NORMAL
- en: HTTP-level routing using a reverse proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you have been tasked with containerizing a monolithic application. The
    application has organically evolved over the years into an unmaintainable behemoth.
    Changing even a minor feature in the source code may break other features due
    to the tight coupling that exists in the code base. Releases are rare due to their
    complexity and require the whole team to be on board. The application has to be
    taken down during the release window, which costs the company a lot of money due
    to lost opportunities, not to mention their loss of reputation.
  prefs: []
  type: TYPE_NORMAL
- en: Management has decided to put an end to that vicious cycle and improve the situation
    by containerizing the monolith. This alone will lead to a massively decreased
    time between releases, as witnessed within the industry. As a later step, the
    company wants to break out every piece of functionality from the monolith and
    implement it as a microservice. This process will continue until the monolith
    has been completely starved.
  prefs: []
  type: TYPE_NORMAL
- en: But it is this second point that leads to some head-scratching for the team
    involved. How will we break down the monolith into loosely coupled microservices
    without affecting all the many clients of the monolith out there? The public API
    of the monolith, though very complex, has a well-structured design. Public URIs
    were carefully crafted and should not be changed at any cost. For example, there
    is a product catalog function implemented in the app that can be accessed via
    [https://acme.com/catalog?category=bicycles](https://acme.com/catalog?category=bicycles)
    so that we can access a list of bicycles offered by the company.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there is a URL called `https://acme.com/checkout` that we
    can use to initiate the checkout of a customer’s shopping cart, and so on. I hope
    it is clear where we are going with this.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing the monolith
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with the monolith. I have prepared a simple code base that has
    been implemented in Python 3.7 and uses Flask to implement the public REST API.
    The sample app is not really a full-blown application but just complex enough
    to allow for some redesign. The sample code can be found in the `ch10/e-shop`
    folder. Inside this folder is a subfolder called `monolith` containing the Python
    application. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a new Terminal window, navigate to that folder, install the required dependencies,
    and run the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The application will start and listen on localhost on port `5000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18 – Running the Python monolith](img/B19199_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – Running the Python monolith
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `curl` to test the app. Open another Terminal window and use the
    following command to retrieve a list of all the bicycles the company offers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have a JSON-formatted list of three types of bicycles. OK – so far,
    so good.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s change the `hosts` file, add an entry for `acme.com`, and map it
    to `127.0.0.1`, the loop-back address. This way, we can simulate a real client
    accessing the app at `http://acme.com/catalog?type=bicycle` instead of using `localhost`.
    You need to use `sudo` to edit the `/etc/hosts` file on a macOS or on Linux. You
    should add a line to the `hosts` file that looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Windows host file
  prefs: []
  type: TYPE_NORMAL
- en: On Windows, you can edit the file by, for example, running Notepad as an administrator,
    opening the `c:\Windows\System32\Drivers\etc\hosts` file, and modifying it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save your changes and assert that it works by pinging `acme.com`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After all this, it is time to containerize the application. The only change
    we need to make to the application is ensuring that we have the application web
    server listening on `0.0.0.0` instead of `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this easily by modifying the application and adding the following
    start logic at the end of `main.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can start the application as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, add a Dockerfile to the monolith folder with the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.19 – The Dockerfile for the monolith](img/B19199_10_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 – The Dockerfile for the monolith
  prefs: []
  type: TYPE_NORMAL
- en: 'In your Terminal window, from within the monolith folder, execute the following
    command to build a Docker image for the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the image has been built, try to run the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that the output from the app now running inside a container is indistinguishable
    from what we got when running the application directly on the host. We can now
    test whether the application still works as before by using the two `curl` commands
    to access the catalog and the checkout logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.20 – Testing the monolith while running in a container](img/B19199_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 – Testing the monolith while running in a container
  prefs: []
  type: TYPE_NORMAL
- en: Evidently, the monolith still works exactly the same way as before, even when
    using the correct URL, that is, `http://acme.com`. Great! Now, let’s break out
    part of the monolith’s functionality into a Node.js microservice, which will be
    deployed separately.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the first microservice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The team, after some brainstorming, has decided that the catalog product is
    a good candidate for the first piece of functionality that is cohesive yet self-contained
    enough to be extracted from the monolith. They decide to implement the product
    catalog as a microservice implemented in Node.js.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code they came up with and the Dockerfile in the `catalog`
    subfolder of the project folder, that is, `e-shop`. It is a simple Express.js
    application that replicates the functionality that was previously available in
    the monolith. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your Terminal window, from within the `catalog` folder, build the Docker
    image for this new microservice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, run a container from the new image you just built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From a different Terminal window, try to access the microservice and validate
    that it returns the same data as the monolith:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please notice the differences in the URL compared to when accessing the same
    functionality in the monolith. Here, we are accessing the microservice on port
    `3000` (instead of `5000`).
  prefs: []
  type: TYPE_NORMAL
- en: But we said that we didn’t want to have to change the clients that access our
    e-shop application. What can we do? Luckily, there are solutions to problems like
    this. We need to reroute incoming requests. We’ll show you how to do this in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Traefik to reroute traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we realized that we would have to reroute incoming
    traffic with a target URL starting with `http://acme.com:5000/catalog` to an alternative
    URL such as `product-catalog:3000/catalog`. We will be using Traefik to do exactly
    that.
  prefs: []
  type: TYPE_NORMAL
- en: Traefik is a cloud-native edge router and it is open source, which is great
    for our specific case. It even has a nice web UI that you can use to manage and
    monitor your routes. Traefik can be combined with Docker in a very straightforward
    way, as we will see in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate well with Docker, Traefik relies on the metadata found for each
    container or service. This metadata can be applied in the form of labels that
    contain the routing information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s look at how to run the `catalog` service. Here is the Docker `run`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s quickly look at the four labels we define:'
  prefs: []
  type: TYPE_NORMAL
- en: '`traefik.enable=true`: This tells Traefik that this particular container should
    be included in the routing (the default is `false`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traefik.port=3000`: The router should forward the call to port `3000` (which
    is the port that the Express.js app is listening on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traefik.priority=10`: This gives this route high priority. We will see why
    in a second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`traefik.http.routers.catalog.rule="Host(\"acme.com\") && PathPrefix(\"/catalog\")"`:
    The route must include the hostname, `acme.com`, and the path must start with
    `/catalog` in order to be rerouted to this service. As an example, `acme.com/catalog?type=bicycles`
    would qualify for this rule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note the special form of the fourth label. Its general form is `traefik.http.routers.<service
    name>.rule`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at how we can run the `eshop` container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we forward any matching calls to port `5000`, which corresponds to the
    port where the `eshop` application is listening. Pay attention to the priority,
    which is set to `1` (low). This, in combination with the high priority of the
    catalog service, allows us to filter out all URLs starting with `/catalog` and
    redirect them to the `catalog` service, while all other URLs will go to the `eshop`
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can finally run Traefik as the edge router that will serve as a reverse
    proxy in front of our application. This is how we start it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note how we mount the Docker socket into the container using the `-v` (or `--volume`)
    parameter so that Traefik can interact with the Docker engine. We will be able
    to send web traffic to port `80` of Traefik, from where it will be rerouted according
    to our rules in the routing definitions found in the metadata of the participating
    container. Furthermore, we can access the web UI of Traefik via port `8080`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that everything is running, that is, the monolith, the first microservice
    called `catalog`, and Traefik, we can test whether everything works as expected.
    Use `curl` once again to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we mentioned earlier, we are now sending all traffic to port `80`, which
    is the port Traefik is listening on. This proxy will then reroute the traffic
    to the correct destination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding, stop and remove all containers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That’s it for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about how containers running on a single host can
    communicate with each other. First, we looked at the CNM, which defines the requirements
    of a container network, and then we looked at several implementations of the CNM,
    such as the bridge network. We then looked at how the bridge network functions
    in detail and also what kind of information Docker provides us with about the
    networks and the containers attached to those networks. We also learned about
    adopting two different perspectives, from both outside and inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to introduce Docker Compose. We will learn
    about creating an application that consists of multiple services, each running
    in a container, and how Docker Compose allows us to easily build, run, and scale
    such an application using a declarative approach.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some articles that describe the topics that were presented in this
    chapter in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Docker networking* *overview*: [http://dockr.ly/2sXGzQ](http://dockr.ly/2sXGzQn)n'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is a* *bridge?*: [https://bit.ly/2HyC3Od](https://bit.ly/2HyC3Od)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using bridge* *networks*: [http://dockr.ly/2BNxjRr](http://dockr.ly/2BNxjRr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using Macvlan* *networks*: [http://dockr.ly/2ETjy2x](http://dockr.ly/2ETjy2x)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Networking using the host* *network*: [http://dockr.ly/2F4aI59](http://dockr.ly/2F4aI59)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess the skills that you have gained from this chapter, please try to
    answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Name the three core elements of the **container network** **model** (**CNM**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you create a custom `bridge` network called, for example, `frontend`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you run two `nginx:alpine` containers attached to the frontend network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the frontend network, get the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The IPs of all the attached containers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The subnet associated with the network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the purpose of the `host` network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name one or two scenarios where the use of the `host` network is appropriate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of the `none` network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In what scenarios should the `none` network be used?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why would we use a reverse proxy such as Traefik together with our containerized
    application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are example answers for the questions of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The three core elements of the Docker CNM are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sandbox**: A network namespace for a container where its network stack resides'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoint**: An interface that connects a container to a network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network**: A collection of endpoints that can communicate with each other
    directly'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To create a custom Docker `bridge` network called `frontend`, you can use the
    `docker network create` command with the `--driver` flag set to `bridge` (which
    is the default driver) and the `--subnet` flag to specify the subnet for the network.
    Here’s an example command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create a bridge network named `frontend` with a subnet of `172.25.0.0/16`.
    You can then use this network when starting containers with the `--``network`
    option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'To run two `nginx:alpine` containers attached to the `frontend` network that
    we created earlier, you can use the following `docker` `run` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These commands will start two containers named `nginx1` and `nginx2` with the
    `nginx:alpine` image and attach them to the `frontend` network. The `-d` flag
    runs the containers in the background as daemons. You can then access the containers
    by their container names (`nginx1` and `nginx2`) or their IP addresses within
    the `frontend` network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the solution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get the IPs of all the containers attached to the `frontend` Docker network,
    you can use the `docker network inspect` command, followed by the network name.
    Here’s an example command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will output the IPv4 addresses of all the containers attached to the frontend
    network, separated by spaces.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get the subnet associated with the `frontend` network, you can again use
    the `docker network inspect` command followed by the network name. Here’s an example
    command:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will output the subnet associated with the `frontend` network in CIDR notation
    (e.g., `172.25.0.0/16`). The `jq` command is used here to parse the output of
    the `docker network inspect` command and extract the subnet.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The Docker `host` network is a networking mode that allows a Docker container
    to use the host’s networking stack instead of creating a separate network namespace.
    In other words, containers running in `host` network mode can directly access
    the network interfaces and ports of the Docker host machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The purpose of using the `host` network mode is to improve network performance
    since it avoids the overhead of containerization and network virtualization. This
    mode is often used for applications that require low-latency network communication
    or need to listen on a large number of ports.
  prefs: []
  type: TYPE_NORMAL
- en: However, using the `host` network mode can also present security risks since
    it exposes the container’s services directly on the Docker host’s network interfaces,
    potentially making them accessible to other containers or hosts on the same network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker `host` network mode is appropriate for scenarios where network performance
    is critical and where network isolation is not a requirement. For example, see
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In cases where the containerized application needs to communicate with other
    services running on the host machine, such as a database or cache service, the
    use of the `host` network mode can improve performance by eliminating the need
    for `host` network mode can simplify network configuration and management by allowing
    the container to use the same network interfaces and IP addresses as the host
    machine, without the need to manage port mapping between the container and host
    network namespaces.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of the Docker `none` network is to completely disable networking
    for a container. When a container is started with the `none` network mode, it
    does not have any network interfaces or access to the network stack of the host
    machine. This means that the container cannot communicate with the outside world
    or any other container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `none` network mode is useful in scenarios where the container does not
    require network connectivity, such as when running a batch process or a single-use
    container that performs a specific task and then exits. It can also be used for
    security purposes, to isolate the container from the network and prevent any potential
    network-based attacks.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that when a container is started with the `none` network
    mode, it can still access its own filesystem and any volumes that are mounted
    to it. However, if the container requires network access later on, it must be
    stopped and restarted with a different network mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Docker `none` network mode is useful in scenarios where the container does
    not require network connectivity, such as the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running a batch process or a single-use container that performs a specific task
    and then exits
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a container that does not need to communicate with other containers
    or the host machine
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a container that has no need for external network access, such as a
    container that is used only for testing or debugging
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a container that requires high security and isolation from the network
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several reasons why we might use a reverse proxy such as Traefik
    together with our containerized application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Load balancing**: A reverse proxy can distribute incoming traffic across
    multiple instances of our application running on different containers, ensuring
    that no single instance becomes overwhelmed with requests.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Routing**: With a reverse proxy, we can route incoming requests to the appropriate
    container based on the URL or domain name. This allows us to run multiple applications
    on the same host, each with its own unique domain or URL.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSL/TLS termination**: A reverse proxy can terminate SSL/TLS connections
    and handle certificate management, eliminating the need for our application to
    do this itself. This can simplify our application code and reduce the risk of
    security vulnerabilities.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: A reverse proxy can act as a buffer between our application and
    the public internet, providing an additional layer of security. For example, it
    can block certain types of traffic or filter out malicious requests.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: By using a reverse proxy such as Traefik, we can quickly and
    easily scale our application up or down by adding or removing containers. The
    reverse proxy can automatically route traffic to the appropriate containers, making
    it easy to manage our application’s infrastructure.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
