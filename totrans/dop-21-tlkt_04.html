<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Service Discovery inside a Swarm Cluster</h1>
            </header>

            <article>
                
<div class="packt_quote">It does not take much strength to do things, but it requires a great deal of strength to decide what to do.<br/>
                                                                                                             -Elbert Hubbard</div>
<p>If you used the old Swarm, the one shipped as a standalone product before <em>Docker 1.12</em>, you were forced to set up a service registry alongside it. You might have chosen Consul, etcd, or Zookeper. The standalone Swarm could not work without one of them. Why is that? What was the reason for such a strong dependency?</p>
<p>Before we discuss reasons behind using an external service registry with the old Swarm, let's discuss how would Swarm behave without it.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What would Docker Swarm look like without?</h1>
            </header>

            <article>
                
<p>Let's say we have a cluster with three nodes. Two of them run <strong>Swarm managers</strong>, and one is a worker. Managers accept our requests, decide what should be done, and send tasks to <strong>Swarm workers</strong>. In turn, workers translate those tasks into commands that are sent to the local <strong>Docker Engine</strong>. Managers act as workers as well.</p>
<p>If we describe the flow we did earlier with the <kbd>go-demo</kbd> service, and imagine there is no service discovery associated with Swarm, it would be as follows.<br/>
A user sends a request to one of the managers. The request is not a declarative instruction but an expression of the desired state. For example, I want to have two instances of the <kbd>go-demo</kbd> service and one instance of the <kbd>DB</kbd> running inside the cluster:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img class="image-border" src="assets/swarm-without-service-registry-user.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-1: User sends a request to one of the managers</div>
<p>Once <strong>Swarm manager</strong> receives our request for the desired state, it compares it with the current state of the cluster, generates tasks, and sends them to <strong>Swarm workers</strong>. The tasks might be to run an instance of the <kbd>go-demo</kbd> service on <strong>node-1</strong> and <strong>node-2</strong>, and an instance of the <kbd>go-demo-db</kbd> service on <strong>node-3</strong>:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img class="image-border" src="assets/swarm-without-service-registry-manager.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-2: Swarm manager compares the current state of the cluster with the desired state, generates tasks, and sends them to Swarm workers.</div>
<p><strong>Swarm workers</strong> receive tasks from the managers, translate them into Docker Engine commands, and send them to their local <strong>Docker Engine</strong> instances:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img class="image-border" src="assets/swarm-without-service-registry-node.png"/></div>
<div class="CDPAlignCenter packt_figref CDPAlign">Figure 4-3: Swarm nodes translate received tasks to Docker Engine commands<span><br/></span></div>
<p>Docker Engine receives a command from the Swarm worker and executes it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/swarm-without-service-registry-engine.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-4: Docker Engine manages local containers.</div>
<p>Next, let's say that we send a new desired state to the manager. For example, we might want to scale the number of the <strong>go-demo</strong> instances to <strong>node-3</strong>. We would send a request to the <strong>Swarm manager</strong> on <strong>node-1</strong>, it would consult the <span>cluster state it stored internally and make a decision to, for example, run a new instance on</span> <strong>node-2</strong><span>. Once the decision is made, the manager would create a new task and </span>send <span>it to the</span> <strong>Swarm worker</strong> <span>on</span> <strong>node-2</strong><span>. In turn, the worker would translate the task into a Docker command, and send it to the local engine. Once the command is executed, we would have the third instance of the</span> <strong>go-demo</strong> <span>service running on</span> <strong>node-2</strong><span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/swarm-without-service-registry-scale.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-5: A scale request is sent to the Swarm manager</div>
<p>If the flow were as described, we would have quite a lot of problems that would make such a solution almost useless.</p>
<p>Let's try to list some of the issues we would face.</p>
<p>A Docker manager uses the information we sent to it. That would work as long as we always use the same manager and the state of the cluster does not change due to factors outside the control of the manager. The important thing to understand is that the information about the cluster is not stored in one place, nor it is complete. Each manager knows only about the things it did. Why is that such a problem?</p>
<p>Let's explore a few alternative (but not uncommon) paths.</p>
<p>What would happen if we sent the request to scale to three instances to the manager on <strong>node-2</strong>? That manager would be oblivious of the tasks created by the manager in <strong>node-1</strong>. As a result, it would try to run three new instances of the <strong>go-demo</strong> service resulting in five instances in total. We’d have two instances created by the manager in <strong>node-1</strong> and three by the manager in <strong>node-2</strong>.<br/>
It would be tempting always to use the same manager, but, in that case, we would have a single point of failure. What would happen if the whole <strong>node-1</strong> fails? We would have no managers available or would be forced to use the manager on <strong>node-2</strong>.</p>
<p>Many other factors might produce such discrepancies. Maybe one of the containers stopped unexpectedly. In such a case, when we decide to scale to three instances, the manager on <strong>node-1</strong> would think that two instances are running and would create a task to run one more. However, that would not result in three but two instances running inside the cluster.</p>
<p>The list of things that might go wrong is infinite, and we won't go into more examples.</p>
<p>The important thing to note is that it is unacceptable for any single manager to be stateful in isolation. Every manager needs to have the same information as any other. On the other hand, every node needs to monitor events generated by Docker Engine and make sure that any change to its server is propagated to all managers. Finally, we need to oversee the state of each server in case one of them fails. In other words, each manager needs to have an up-to-date picture of the entire cluster. Only then it can translate our requests for the desired state into tasks that will be dispatched to the Swarm nodes.</p>
<p>How can all the managers have a complete view of the whole cluster no matter who made a change to it?</p>
<p>The answer to that question depends on the requirements we set. We need a place where all the information is stored. Such a place need to be distributed so that the failure of one server does not affect the correct functioning of the tool. Being distributed provides fault tolerance, but that, by itself, does not mean data is synchronized across the cluster. The tool needs to maintain data replicated across all the instances. Replication is not anything new except that, in this case, it needs to be very fast so that the services that would consult it can receive data in (near) real-time. Moreover, we need a system that will monitor each server inside the cluster and update the data if anything changes.</p>
<p>To summarize, we need a distributed service registry and a monitoring system in place. The first requirement is best accomplished with one of the service registries or key-value stores. The old Swarm (standalone version before Docker 1.12) supports <em>Consul</em> (<a href="https://www.consul.io/">https://www.consul.io/</a>), <em>etcd</em> (<a href="https://github.com/coreos/etcd">https://github.com/coreos/etcd</a>), and <em>Zookeeper</em> (<a href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a>). My preference is towards Consul, but any of the three should do.</p>
<p>For a more detailed discussion about service discovery and the comparison of the major service registries, please consult the service discovery: The Key to Distributed services chapter of <em>The DevOps 2.0 Toolkit</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What does standalone Docker Swarm look like with service discovery?</h1>
            </header>

            <article>
                
<p>Now that we have a better understanding of the requirements and the reasons behind the usage of service discovery, we can define the (real) flow of a request to a Docker Swarm manager.</p>
<p>Please note that we are still exploring how the old (standalone) Swarm is working:</p>
<ol>
<li>A user sends a request with the desired state to one of the Swarm managers.</li>
<li>The Swarm manager gets the cluster information from the service registry, creates a set of tasks, and dispatches them to Swarm workers.</li>
<li>Swarm workers translate the tasks into commands and send them to the local Docker Engine which, in turn, runs or stops containers<strong>.</strong></li>
<li>Swarm workers continuously monitor Docker events and update the <strong>service registry</strong>.</li>
</ol>
<p>That way, information about the whole cluster is always up-to-date. The exception is when one of the managers or workers fails. Since managers are monitoring each other, the failure of a manager or a worker is considered a failure of the whole node. After all, without a worker, containers cannot be scheduled on that node:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/swarm-standalone.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-6: Docker Swarm (standalone) flow</div>
<p>Now that we established that service discovery is an essential tool for managing a cluster, the natural question is what happened to it in Swarm Mode (<em>Docker 1.12</em>)?</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Service discovery in the Swarm cluster</h1>
            </header>

            <article>
                
<p>The old (standalone) Swarm required a service registry so that all its managers can have the same view of the cluster state. When instantiating the old Swarm nodes, we had to specify the address of a service registry. However, if you take a look at setup instructions of the new Swarm (Swarm Mode introduced in <em>Docker 1.12</em>), you'll notice that we did not set up anything beyond Docker Engines. You will not find any mention of an external service registry or a key-value store.</p>
<p>Does that mean that Swarm does not need service discovery? Quite the contrary. The need for service discovery is as strong as ever, and Docker decided to incorporate it inside Docker Engine. It is bundled inside just as Swarm is. The internal process is, essentially, still very similar to the one used by the standalone Swarm, only with less moving parts. Docker Engine now acts as a Swarm manager, Swarm worker, and service registry.</p>
<p>The decision to bundle everything inside the engine provoked a mixed response. Some thought that such a decision creates too much coupling and increases Docker Engine's level of instability. Others think that such a bundle makes the engine more robust and opens the door to some new possibilities. While both sides have valid arguments, I am more inclined towards the opinion of the latter group. Docker Swarm Mode is a huge step forward, and it is questionable whether the same result could be accomplished without bundling service registry inside the engine.</p>
<p>Knowing how Docker Swarm works, especially networking, the question that might be on your mind is whether we need service discovery (beyond Swarms internal usage). In <em>The DevOps 2.0 Toolkit</em>, I argued that service discovery is a must and urged everyone to set up <em>Consul </em>(<a href="https://www.consul.io/">https://www.consul.io/</a>) or <em>etcd (</em><a href="https://github.com/coreos/etcd">https://gith</a><a href="https://github.com/coreos/etcd">ub.com/coreos/etcd</a>) as service registries, Registrator as a mechanism to register changes inside the cluster, and Consul Template or confd (<a href="https://github.com/kelseyhightower/confd">https://github.com/kelseyhightower/confd</a>) as a templating solution. Do we still need those tools?<br/></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Do we need service discovery?</h1>
            </header>

            <article>
                
<p>It is hard to provide a general recommendation whether service discovery tools are needed when working inside a Swarm cluster. If we look at the need to find services as the main use case for those tools, the answer is usually no. We don't need external service discovery for that. As long as all services that should communicate with each other are inside the same network, all we need is the name of the destination service. For example, for the go-demo (<a href="https://github.com/vfarcic/go-demo">https://github.com/vfarcic/go-demo</a>) service to find the related database, it only needs to know its DNS <kbd>go-demo-db</kbd>. The <a href="fc49e3b5-55fc-4ebe-8f43-9b15cdf924ba.xhtml">Chapter 3</a>, <em>Docker Swarm Networking and Reverse Proxy</em>  proved that proper networking usage is enough for most use cases.</p>
<p>However, finding services and load balancing requests among them is not the only reason for service discovery. We might have other uses for service registries or key-value stores. We might need to store some information such that it is distributed and fault tolerant.</p>
<p>An example of the need for a key-value store can be seen inside the <em>Docker Flow Proxy</em> (<a href="https://github.com/vfarcic/docker-flow-proxy">https://github.com/vfarcic/docker-flow-p</a><a href="https://github.com/vfarcic/docker-flow-proxy">roxy</a>) project. It is based on HAProxy which is a stateful service. It loads the information from a configuration file into memory. Having stateful services inside a dynamic cluster represents a challenge that needs to be solved. Otherwise, we might lose state when a service is scaled, rescheduled after a failure, and so on.</p>
<p>Before we go into more details and problems related with stateful services, let's see how we could set up Consul as our key-value store of choice and go through its basic features.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up Consul as service registry inside a Swarm cluster</h1>
            </header>

            <article>
                
<p>As before, we'll start by setting up a Swarm cluster. From there on, we'll proceed with the Consul setup and a quick overview of the basic operations we can do with it. That will give us the knowledge necessary for the rest of this chapter.</p>
<div class="packt_infobox"><strong>A note to The DevOps 2.0 Toolkit readers</strong><br/>
You might be tempted to skip this sub-chapter since you already learned how to set up Consul. I recommend you read on. We'll use the official Consul image that was not available at the time I wrote the previous book. At the same time, I promise to keep this sub-chapter as brief as possible without confusing the new readers too much.</div>
<p>Practice makes perfect, but there is a limit after which there is no reason to repeat the same commands over and over. I'm sure that, by now, you got tired of writing the commands that create <span>a Swarm cluster. So, I prepared the</span> <kbd>scripts/dm-swarm.sh</kbd><span> (</span><a href="https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh">https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh</a><span>) script that will create Docker Machine </span>nodes <span>and join them into a Swarm cluster.</span></p>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>04-service-discovery.sh</kbd> (<a href="https://gist.github.com/vfarcic/fa57e88faf09651c9a7e9e46c8950ef5">https://gist.github.com/vfarcic/fa57e88faf09651c9a7e9e46c8950ef5</a>) Gist.</div>
<p>Let's clone the code and run the script:</p>
<div class="packt_tip">Some of the files will be shared between the host file system and Docker Machines we'll create soon. Docker Machine makes the whole directory that belongs to the current user available inside the VM. Therefore, please make sure that the code is cloned inside one of the user's sub-folders.</div>
<pre>
<strong>git clone https://github.com/vfarcic/cloud-provisioning.git<br/><br/><span class="hljs-built_in">cd</span> cloud-provisioning<br/><br/>scripts/dm-swarm.sh<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker node ls</strong>
</pre>
<p>The output of the <kbd>node ls</kbd> command is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME STATUS AVAILABILITY MANAGER STATUS<br/>swarm-<span class="hljs-number">2</span> Ready <span class="hljs-keyword">Active</span> Reachable<br/>swarm-<span class="hljs-number">3</span> Ready <span class="hljs-keyword">Active</span> Reachable<br/>swarm-<span class="hljs-number">1</span> Ready <span class="hljs-keyword">Active</span> Leader</strong>
</pre>
<p>Please note that this time there was a slight change in the commands. We used the <kbd>manager</kbd> token so that all three nodes are set up as managers.</p>
<p>As a general rule, we should have a least three Swarm managers. That way, if one of them fails, the others will reschedule the failed containers and can be used as our access points to the system. As is often the case with solutions that require a quorum, an odd number is usually the best. Hence, we have three.<br/>
You might be tempted to run all nodes as managers. I advise you against that. Managers synchronize data between themselves. The more manager instances are running, the more time the synchronization might last. While that is not even noticeable when there are only a few, if, for example, you'd run a hundred managers there would be some lag. After all, that's why we have workers. Managers are our entry points to the system and coordinators of the tasks, while workers do the actual work.</p>
<p>With that out of the way, we can proceed and set up Consul.</p>
<p>We'll start by downloading the <kbd>docker-compose.yml</kbd> (<a href="https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml">https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml</a>) file from the <em>Docker Flow Proxy</em> (<a href="https://github.com/vfarcic/docker-flow-proxy">https://github.com/vfarcic/docker-flow-proxy</a>) project. It already contains Consul defined as Compose services.</p>
<pre style="padding-left: 30px">
<strong>curl -o docker-compose-proxy.yml \<br/>    https://raw.githubusercontent.com/\ <br/>vfarcic/docker-flow-proxy/master/docker-compose.yml<br/><br/>cat docker-compose-proxy.yml</strong>
</pre>
<p>Just as Docker Swarm node can act as a manager or a worker, Consul can be run as a server or an agent. We'll start with the server.</p>
<p>The Compose definition of the Consul service that acts as a server is as follows:</p>
<pre>
<strong>consul<span class="hljs-attribute">-server</span>:<br/>  container_name: consul<br/>  image: consul<br/>  network_mode: host<br/>  environment:<br/><span class="hljs-subst">    -</span> <span class="hljs-string">'CONSUL_LOCAL_CONFIG={"skip_leave_on_interrupt": true}'</span><br/>  command: agent <span class="hljs-attribute">-server</span> <span class="hljs-attribute">-bind</span><span class="hljs-subst">=</span><span class="hljs-variable">$DOCKER_IP</span> <span class="hljs-attribute">-bootstrap</span><span class="hljs-attribute">-expect</span><span class="hljs-subst">=</span><span class="hljs-number">1</span> <span class="hljs-attribute">-client</span><span class="hljs-subst">=</span><span class="hljs-variable">$DOCKER_IP</span></strong>
</pre>
<p>The important thing to note is that we set up the network mode as <kbd>host</kbd>. That means that the container will share the same network as the host it is running on. This is followed by an environment variable and the command.</p>
<p>The command will run the agent in server mode and, initially, it expects to be the only one in the cluster <kbd>-bootstrap-expect=1</kbd>.</p>
<p>You'll notice the usage of the <kbd>DOCKER_IP</kbd> environment variable. Consul expects the information about the binding and the client address. Since we don't know the IP of the servers in advance, it had to be a variable.<br/>
At this moment you might be wondering why are we talking about Docker Compose services inside a Swarm cluster. Shouldn't we run <kbd>docker service create</kbd> command? The truth is, at the time of this writing, the official consul image is still not adapted to the "Swarm way" of running things. Most images do not require any changes before launching them inside a Swarm cluster. Consul is one of the very few exceptions. I will do my best to update the instructions as soon as the situation changes. Until then, the good old Compose should do:</p>
<pre>
<strong><span class="hljs-keyword">export</span> DOCKER_IP=$(docker-machine ip swarm-<span class="hljs-number">1</span>)<br/><br/>docker-compose <span class="hljs-operator">-f</span> docker-compose-proxy.yml \<br/>    up <span class="hljs-operator">-d</span> consul-server</strong>
</pre>
<p>You'll notice <kbd>WARNING: The Docker Engine you're using is running in swarm mode</kbd> message in the output. It is only a friendly reminder that we are not running this as Docker service. Feel free to ignore it.</p>
<p>Now that we have a Consul instance running, we can go through the basic operations.</p>
<p>We can, for example, put some information into the key-value store:</p>
<pre>
<strong>curl -X PUT <span class="hljs-operator">-d</span> <span class="hljs-string">'this is a test'</span> \<br/><span class="hljs-string">    "http://<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8500/v1/kv/msg1"</span></strong>
</pre>
<p>The <kbd>curl</kbd> command put this is a test value as the <kbd>msg1</kbd> key inside Consul.</p>
<p>We can confirm that the key-value combination is indeed stored by sending a <kbd>GET</kbd> request:</p>
<pre>
<strong>curl <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8500/v1/kv/msg1"</span></strong>
</pre>
<p>The output is as follows (formatted for readability):</p>
<pre>
<strong>[<br/> {<br/>   "<span class="hljs-attribute">LockIndex</span>": <span class="hljs-value"><span class="hljs-number">0</span></span>,<br/>   "<span class="hljs-attribute">Key</span>": <span class="hljs-value"><span class="hljs-string">"msg1"</span></span>,<br/>   "<span class="hljs-attribute">Flags</span>": <span class="hljs-value"><span class="hljs-number">0</span></span>,<br/>   "<span class="hljs-attribute">Value</span>": <span class="hljs-value"><span class="hljs-string">"dGhpcyBpcyBhIHRlc3Q="</span></span>,<br/>   "<span class="hljs-attribute">CreateIndex</span>": <span class="hljs-value"><span class="hljs-number">17</span></span>,<br/>   "<span class="hljs-attribute">ModifyIndex</span>": <span class="hljs-value"><span class="hljs-number">17</span><br/></span> }<br/>]</strong>
</pre>
<p>You'll notice that the value is encoded. If we add the <kbd>raw</kbd> parameter to the request, Consul will return only the value in its raw format:</p>
<pre>
<strong>curl <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8500/v1/kv/msg1?raw"</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-keyword">this</span> <span class="hljs-keyword">is</span> a test</strong>
</pre>
<p>Right now, we have only one Consul instance. If the node it is running in fails <kbd>swarm-1</kbd>, all the data will be lost and service registry will be unavailable. That's not a good situation to be in.</p>
<p>We can create fault tolerance by running a few more Consul instances. This time, we'll run agents.</p>
<p>Just as the Consul server instance, the agent is also defined in the <kbd>docker-compose.yml</kbd> (<a href="https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml">https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose.yml</a>) file in the <em>Docker Flow Proxy</em> (<a href="https://github.com/vfarcic/docker-flow-proxy">https://github.com/vfarc</a><a href="https://github.com/vfarcic/docker-flow-proxy">ic/docker-flow-proxy</a>) project. Remember, we downloaded it with the name <kbd>docker-compose-proxy.yml</kbd>. Let's take a look at the service definition:</p>
<pre>
<strong>cat docker-compose-proxy.yml</strong>
</pre>
<p>The part of the output that defines the <kbd>Consul-agent</kbd> service is as follows:</p>
<pre>
<strong>consul<span class="hljs-attribute">-agent</span>:<br/>  container_name: consul<br/>  image: consul<br/>  network_mode: host<br/>  environment:<br/><span class="hljs-subst">    -</span> <span class="hljs-string">'CONSUL_LOCAL_CONFIG={"leave_on_terminate": true}'</span><br/>  command: agent <span class="hljs-attribute">-bind</span><span class="hljs-subst">=</span><span class="hljs-variable">$DOCKER_IP</span> <span class="hljs-attribute">-retry</span><span class="hljs-attribute">-join</span><span class="hljs-subst">=</span><span class="hljs-variable">$CONSUL_SERVER_IP \<br/></span><span class="hljs-attribute">-client</span><span class="hljs-subst">=</span><span class="hljs-variable">$DOCKER_IP</span></strong>
</pre>
<p>It is almost the same as the definition we used to run the Consul server instance. The only important difference is that the <kbd>-server</kbd> is missing and that we have the <kbd>-retry-join</kbd> argument. We're using the latter to specify the address of another instance. Consul uses the gossip protocol. As long as every instance is aware of at least one other instance, the protocol will propagate the information across all of them.<br/>
Let's run agents on the other two nodes <kbd>swarm-2</kbd> and <kbd>swarm-3</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">export</span> CONSUL_SERVER_IP=$(docker-machine ip swarm-<span class="hljs-number">1</span>)<br/><br/><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>; <span class="hljs-keyword">do</span><br/><span class="hljs-built_in">    eval</span> $(docker-machine env swarm-<span class="hljs-variable">$i</span>)<br/><br/><span class="hljs-keyword">    export</span> DOCKER_IP=$(docker-machine ip swarm-<span class="hljs-variable">$i</span>)<br/><br/>    docker-compose <span class="hljs-operator">-f</span> docker-compose-proxy.yml \<br/>        up <span class="hljs-operator">-d</span> consul-agent<br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>Now that we have three Consul instances running inside the cluster (one on each node), we can confirm that gossip indeed works.</p>
<p>Let's request the value of the <kbd>msg1</kbd> key. This time, we'll request it from the Consul instance running on <kbd>swarm-2</kbd>:</p>
<pre>
<strong>curl <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-2)</span>:8500/v1/kv/msg1"</span></strong>
</pre>
<p>As you can see from the output, even though we put the information to the instance running on <kbd>swarm-1</kbd>, it is available from the instance in <kbd>swarm-2</kbd>. The information is propagated through all the instances.</p>
<p>We can give the gossip protocol one more round of testing:</p>
<pre>
<strong>curl -X PUT <span class="hljs-operator">-d</span> <span class="hljs-string">'this is another test'</span> \</strong><br/><strong><span class="hljs-string"> "http://<span class="hljs-variable">$(docker-machine ip swarm-2)</span>:8500/v1/kv/messages/msg2"</span></strong><br/><br/><strong>curl -X PUT <span class="hljs-operator">-d</span> <span class="hljs-string">'this is a test with flags'</span> \</strong><br/><strong><span class="hljs-string"> "http://<span class="hljs-variable">$(docker-machine ip swarm-3)</span>:8500/v1/kv/messages/msg3?\ flags=1234"</span></strong><br/><br/><strong>curl <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8500/v1/kv/?recurse"</span></strong>
</pre>
<p>We sent one <kbd>PUT</kbd> request to the instance running in <kbd>swarm-2</kbd> and another to the instance in <kbd>swarm-3</kbd>. When we requested all the keys from the instance running in <kbd>swarm-1</kbd>, all three were returned. In other words, no matter what we do with data, it is always in sync in all of the instances.</p>
<p>Similarly, we can delete information:</p>
<pre>
<strong>curl -X DELETE <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-2)</span>:\<br/>8500/v1/kv/?recurse"</span></strong><br/><br/><strong>curl <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-3)</span>:8500/v1/kv/?recurse"</span></strong>
</pre>
<p>We sent the request to the <kbd>swarm-2</kbd> to delete all keys. When we queried the instance running in <kbd>swarm-3</kbd>, we got an empty response meaning that everything is, indeed, gone.</p>
<p>With a setup similar to the one we explored, we can have a reliable, distributed, and fault-tolerant way for storing and retrieving any information our services might need.</p>
<p>We'll use this knowledge to explore a possible solution for some of the problems that might arise when running stateful services inside a Swarm cluster. But before we start discussing the solution, let's see what the problem is with stateful services.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Problems when scaling stateful instances</h1>
            </header>

            <article>
                
<p>Scaling services inside a Swarm cluster is easy, isn't it? Just execute <kbd>docker service scale &lt;SERVICE_NAME&gt;=&lt;NUMBER_OF_INSTANCES&gt;</kbd> and, all of a sudden, the service is running multiple copies.</p>
<p>The previous statement is only partly true. The more precise wording would be that "scaling stateless services inside a Swarm cluster is easy".</p>
<p>The reason that scaling stateless services is easy lies in the fact that there is no state to think about. An instance is the same no matter how long it runs. There is no difference between a new instance and one that run for a week. Since the state does not change over time, we can create new copies at any given moment, and they will all be exactly the same.</p>
<p>However, the world is not stateless. State is an unavoidable part of our industry. As soon as the first piece of information is created, it needs to be stored somewhere. The place we store data must be stateful. It has a state that changes over time. If we want to scale such a stateful service, there are at least two things we need to consider:</p>
<ol>
<li>How do we propagate a change of state of one instance to the rest of the instances?</li>
<li>How do we create a copy (a new instance) of a stateful service, and make sure that the state is copied as well?</li>
</ol>
<p>We usually combine stateless and stateful services into one logical entity. A back-end service could be stateless and rely on a database service as an external data storage. That way, there is a clear separation of concerns and a different lifecycle of each of those services.<br/>
Before we proceed, I must state that there is no silver bullet that makes stateful services scalable and fault-tolerant. Throughout the book, I will go through a couple of examples that might, or might not, apply to your use case. An obvious, and very typical example of a stateful service is a database. While there are some common patterns, almost every database provides a different mechanism for data replication. That, in itself, is enough to prevent us from having a definitive answer that would apply to all. We'll explore scalability of a MongoDB later on in the book. We'll also see an example with Jenkins that uses a file system for its state.</p>
<p>The first case we'll tackle will be of a different type. We'll discuss scalability of a service that has its state stored in a configuration file. To make things more complicated, the configuration is dynamic. It changes over time, throughout the lifetime of the service. We'll explore ways to make HAProxy scalable.</p>
<p>If we use the official <em>HAProxy</em> (<a href="https://hub.docker.com/_/haproxy/">https://hub.doc</a><a href="https://hub.docker.com/_/haproxy/">ker.com/_/haproxy/</a>) image, one of the challenges we would face is deciding how to update the state of all the instances. We'd have to change the configuration and reload each copy of the <kbd>proxy</kbd>.</p>
<p>We can, for example, mount an NFS volume on each node in the cluster and make sure that the same host volume is mounted inside all HAProxy containers. At first, it might seem that that would solve the problem with the state since all instances would share the same configuration file. Any change to the config on the host would be available inside all the instances we would have. However, that, in itself, would not change the state of the service.</p>
<p>HAProxy loads the configuration file during initialization, and it is oblivious to any changes we might make to the configuration afterward. For the change of the state of the file to be reflected in the state of the service, we'd need to reload it. The problem is that instances can run on any of the nodes inside the cluster. On top of that, if we adopt dynamic scaling (more on that later on), we might not even know how many instances are running. So we'd need to discover how many instances we have, find out on which nodes they are running, get IDs of each of the containers, and, only then, send a signal to reload the <kbd>proxy</kbd>. While all this can be scripted, it is far from an optimum solution. Moreover, mounting an NFS volume is a single point of failure. If the server that hosts the volume fails, data is lost. Sure, we can create backups, but they would only provide a way to restore lost data partially. That is, we can restore a backup, but the data generated between the moment the last backup was created, and the node failure would be lost.<br/>
An alternative would be to embed the configuration into HAProxy images. We could create a new Dockerfile that would be based on <kbd>haproxy</kbd> and add the <kbd>COPY</kbd> instruction that would add the configuration. That would mean that every time we want to reconfigure the proxy, we'd need to change the config, build a new set of images (a new release), and update the <kbd>proxy</kbd> service currently running inside the cluster. As you can imagine, this is also not practical. It's too big of a process for a simple proxy reconfiguration.</p>
<p><em>Docker Flow Proxy</em> uses a different, less conventional, approach to the problem. It stores a replica of its state in Consul. It also uses an undocumented Swarm networking feature (at least at the time of this writing).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using service registry to store the state</h1>
            </header>

            <article>
                
<p>Now that we have Consul instances set up let us explore how to exploit them to our own benefit. We'll study the design of the <em>Docker Flow Proxy</em> as a way to demonstrate some of the challenges and solutions you might want to apply to your own services.</p>
<p>Let us create the <kbd>proxy</kbd> network and the service:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker network create --driver overlay proxy<br/><br/>docker service create --name proxy \<br/>    -p <span class="hljs-number">80</span>:<span class="hljs-number">80</span> \<br/>    -p <span class="hljs-number">443</span>:<span class="hljs-number">443</span> \<br/>    -p <span class="hljs-number">8080</span>:<span class="hljs-number">8080</span> \<br/>    --network proxy \<br/><span class="hljs-operator">    -e</span> MODE=swarm \<br/>    --replicas <span class="hljs-number">3</span> \<br/><span class="hljs-operator">    -e</span> CONSUL_ADDRESS=<span class="hljs-string">"<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8500 \<br/>,<span class="hljs-variable">$(docker-machine ip \<br/>swarm-2)</span>:8500,<span class="hljs-variable">$(docker-machine ip swarm-3)</span>:8500"</span> \<br/>    vfarcic/docker-flow-proxy</strong>
</pre>
<p>The command we used to create the <strong>proxy</strong> service is slightly different than before. Namely, now we have the <kbd>CONSUL_ADDRESS</kbd> variable with the comma separated addresses of all three <strong>Consul</strong> instances. The <strong>proxy</strong> is made in a way that it will try the first address. If it does not respond, it will try the next one, and so on. That way, as long as at least one <strong>Consul</strong> instance is running, the <strong>proxy</strong> will be able to fetch and put data. We would not need to do this loop if <strong>Consul</strong> would run as a Swarm service. In that case, all we'd need to do is put both inside the same network and use the service name as the address.</p>
<p>Unfortunately, <strong>Consul</strong> cannot, yet, run as a Swarm service, so we are forced to specify all addresses, refer to the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="245" src="assets/proxy-scaled-service-view.png" width="464"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-7: The proxy scaled to three instances</div>
<p>Before we proceed, we should make sure that all instances of the <kbd>proxy</kbd> are running:</p>
<pre>
<strong>docker service ps proxy</strong>
</pre>
<p>Please wait until the current state of all the instances is set to <kbd>Running</kbd>.</p>
<p>Let's create the <kbd>go-demo</kbd> service. It will act as a catalyst for a discussion around challenges we might face with a scaled reverse <kbd>proxy</kbd>:</p>
<pre>
<strong>docker network create --driver overlay go-demo<br/><br/>docker service create --name go-demo-db \<br/>    --network go-demo \<br/>    mongo:<span class="hljs-number">3.2</span>.<span class="hljs-number">10</span><br/><br/>docker service create --name go-demo \<br/><span class="hljs-operator">    -e</span> DB=go-demo-db \<br/>    --network go-demo \<br/>    --network proxy \<br/>    vfarcic/go-demo:<span class="hljs-number">1.0</span></strong>
</pre>
<p>There's no reason to explain the commands in detail. They are the same as those we've run in the previous chapters.</p>
<p>Please wait until the current state of the <kbd>go-demo</kbd> service is Running. Feel free to use <kbd>docker service ps go-demo</kbd> command to check the status.</p>
<p>If we would repeat the same process we used in the <a href="fc49e3b5-55fc-4ebe-8f43-9b15cdf924ba.xhtml">Chapter 3</a>, <em>Docker Swarm Networking and Reverse Proxy</em> the request to reconfigure the proxy would be as follows (please do not run it).</p>
<pre>
<strong>curl <span class="hljs-string">"<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8080/v1/\<br/>proxy/reconfigure?serviceName=go-demo&amp;servicePath=/demo&amp;port=8080"</span></strong>
</pre>
<p>We would send a reconfigure request to the <kbd>proxy</kbd> service. Can you guess what would be the result?</p>
<p>A user sends a request to reconfigure the <strong>proxy</strong>. The request is picked by the routing mesh and load balanced across all the instances of the <strong>proxy</strong>. The request is forwarded to one of the instances. Since the <strong>proxy</strong> is using <strong>Consul</strong> to store its configuration, it sends the info to one of the <strong>Consul</strong> instances which, in turn, synchronizes the data across all others.<br/>
As a result, we have <strong>proxy</strong> instances with different states. The one that received the request is reconfigured to use the <kbd>go-demo</kbd> service. The other two are, still, oblivious to it. If we try to ping the <kbd>go-demo</kbd> service through the <strong>proxy</strong>, we will get mixed responses. One out of three times, the response would be status <kbd>200</kbd>. The rest of the time, we would get <kbd>404</kbd>, not found:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="226" src="assets/proxy-scaled-service-view-without-distribute.png" width="412"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-8: A request to reconfigure the proxy</div>
<p>We would experience a similar result if we scale MongoDB. The <strong>routing mesh</strong> would load balance across all instances, and their states would start to diverge. We could solve the problem with MongoDB by using replica sets. That's the mechanism that allows us to replicate data across all <kbd>DB</kbd> instances. However, HAProxy does not have such a feature. So, I had to add it myself.</p>
<p>The correct request to reconfigure the proxy running multiple instances is as follows:</p>
<pre>
<strong>curl <span class="hljs-string">"<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8080/v1/\<br/>docker-flow-proxy/reconfigure \<br/>serviceName=go-</span><span class="hljs-string">demo&amp;servicePath=/demo&amp;port=8080&amp;distribute=true"</span></strong>
</pre>
<p>Please note the new parameter <kbd>distribute=true</kbd>. When specified, the <strong>proxy</strong> will accept the request, reconfigure itself, and resend the request to all other instances:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="228" src="assets/proxy-scaled-service-view-distribute-1.png" width="486"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4-9: The proxy instance that received the request and resent it to all others</div>
<p>That way, the <strong>proxy</strong> implements a mechanism similar to replica sets in MongoDB. A change to one of the instances is propagated to all others.</p>
<p>Let us confirm that it indeed works as expected:</p>
<pre>
<strong>curl -i <span class="hljs-string">"<span class="hljs-variable">$(docker-machine ip swarm-1)</span>/demo/hello"</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-status">HTTP/1.1 <span class="hljs-number">200</span> OK</span><br/><span class="hljs-attribute">Date</span>: <span class="hljs-string">Fri, 09 Sep 2016 16:04:05 GMT</span><br/><span class="hljs-attribute">Content-Length</span>: <span class="hljs-string">14</span><br/><span class="hljs-attribute">Content-Type</span>: <span class="hljs-string">text/plain; charset=utf-8</span><br/><br/><span class="erlang-repl"><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></span></strong>
</pre>
<p>The response is <kbd>200</kbd> meaning that the <kbd>go-demo</kbd> service received the request forwarded by the <kbd>proxy</kbd> service. Since the routing mesh is in play, the request entered the system, was load balanced and resent to one of the proxy instances. The proxy instance that received the request evaluated the path and decided that it should go to the <kbd>go-demo</kbd> service. As a result, the request is resent to the <kbd>go-demo</kbd> network, load balanced again and forwarded to one of the <kbd>go-demo</kbd> instances. In other words, any of the <kbd>proxy</kbd> and <kbd>go-demo</kbd> instances could have received the request. If the proxy state was not synchronized across all the instances, two out of three requests would fail.</p>
<p>Feel free to repeat the <kbd>curl -i $(docker-machine ip swarm-1)/demo/hello</kbd> command. The result should always be the same.</p>
<p>We can double check that the configuration is indeed synchronized by taking a peek into one of the containers.</p>
<p>Let's take a look at, let's say, proxy instance number three.</p>
<p>The first thing we should do is find out the node the instance is running in:</p>
<pre>
<strong>NODE=$(docker service ps proxy | grep <span class="hljs-string">"proxy.3"</span> | awk <span class="hljs-string">'{print $4}'</span>)</strong>
</pre>
<p>We listed all <kbd>proxy</kbd> service processes <kbd>docker service ps proxy</kbd>, filtered the result with the third instance <kbd>grep "proxy.3"</kbd>, and returned the name of the node stored in the fourth column of the output <kbd>awk '{print $4}'</kbd>. The result was stored in the environment variable <kbd>NODE</kbd>.</p>
<p>Now that we know the server this instance is running in, we can enter the container and display the contents of the configuration file:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env <span class="hljs-variable">$NODE</span>)<br/><br/>ID=$(docker ps | grep <span class="hljs-string">"proxy.3"</span> | awk <span class="hljs-string">'{print $1}'</span>)</strong>
</pre>
<p>We changed the Docker client to point to the node. That was followed with the command that lists all running processes <kbd>docker ps</kbd>, filters out the third instance <kbd>grep "proxy.3"</kbd>, and outputs the container ID stored in the first column <kbd>awk '{print $1}'</kbd>. The result was stored in the environment variable ID.</p>
<p>With the client pointing to the correct node and the ID stored as the environment variable ID, we can, finally, enter the container and display the configuration:</p>
<pre>
<strong>docker <span class="hljs-keyword">exec</span> -it <span class="hljs-variable">$ID</span> cat /cfg/haproxy.cfg</strong>
</pre>
<p>The relevant part of the output is as follows:</p>
<pre>
<strong>frontend services<br/>    bind <span class="hljs-subst">*</span>:<span class="hljs-number">80</span><br/>    bind <span class="hljs-subst">*</span>:<span class="hljs-number">443</span><br/>    mode http<br/><br/>    acl url_go<span class="hljs-attribute">-demo8080</span> path_beg /demo<br/>    use_backend go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-be8080</span> <span class="hljs-keyword">if</span> url_go<span class="hljs-attribute">-demo8080</span><br/><br/>backend go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-be8080</span><br/>    mode http<br/>    server go<span class="hljs-attribute">-demo</span> go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">8080</span></strong>
</pre>
<p>As you can see, the third instance of the <kbd>proxy</kbd> is indeed configured correctly with the <kbd>go-demo</kbd> service. Feel free to repeat the process with the other two instances. The result should be exactly the same proving that synchronization works.</p>
<p>How was it done? How did the <kbd>proxy</kbd> instance that received the request discover the IPs of all the other instances? After all, there is no Registrator that would provide the IPs to Consul, and we cannot access Swarms internal service discovery API.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Discovering addresses of all instances that form a service</h1>
            </header>

            <article>
                
<p>If you browse through the official Docker documentation, you will not find any reference to addresses of individual instances that form a service.</p>
<p>The previous sentence might not be true at the time you're reading this. Someone might have updated the documentation. However, at the time I'm writing this chapter, there is not a trace of such information.</p>
<p>The fact that something is not documented does not mean that it does not exist. Indeed, there is a special DNS that will return all IPs.</p>
<p>To see it in action, we'll create the global service called util and attach it to the <kbd>proxy</kbd> network:</p>
<pre>
<strong>docker service create --name util \<br/>    --network proxy --mode global \<br/>    alpine sleep <span class="hljs-number">1000000000</span><br/><br/>docker service ps util</strong>
</pre>
<p>Before proceeding, please wait until the current state is set to running.</p>
<p>Next, we'll find the ID of one of the util instances and install drill that will show us the information related to DNS entries:</p>
<pre>
<strong>ID=$(docker ps -q --filter label=com.docker.swarm.service.name=util)<br/><br/>docker <span class="hljs-keyword">exec</span> -it <span class="hljs-variable">$ID</span> apk add --update drill</strong>
</pre>
<p>Let's start by drilling the DNS proxy:</p>
<pre>
<strong>docker <span class="hljs-keyword">exec</span> -it <span class="hljs-variable">$ID</span> drill proxy</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>;; -&gt;&gt;<span class="hljs-tag">HEADER</span>&lt;&lt;- opcode<span class="hljs-value">: QUERY, rcode: NOERROR, id: <span class="hljs-number">31878</span><br/>;</span>; flags<span class="hljs-value">: qr rd ra ;</span> QUERY<span class="hljs-value">: <span class="hljs-number">1</span>, ANSWER: <span class="hljs-number">1</span>, AUTHORITY: <span class="hljs-number">0</span>, ADDITIONAL: <span class="hljs-number">0</span><br/>;</span>; QUESTION <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/>;</span>; proxy.              IN             <span class="hljs-tag">A</span><br/><br/>;; ANSWER <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/>proxy.        <span class="hljs-number">600</span>             IN             A           <span class="hljs-number">10.0</span>.<span class="hljs-number">0.2</span><br/><br/>;</span>; AUTHORITY <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/><br/>;</span>; ADDITIONAL <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/><br/>;</span>; Query <span class="hljs-tag">time</span><span class="hljs-value">: <span class="hljs-number">0</span> msec<br/>;</span>; SERVER<span class="hljs-value">: <span class="hljs-number">127.0</span>.<span class="hljs-number">0.11</span><br/>;</span>; WHEN<span class="hljs-value">: Fri Sep <span class="hljs-number">9</span> <span class="hljs-number">16</span>:<span class="hljs-number">43</span>:<span class="hljs-number">23</span> <span class="hljs-number">2016</span><br/>;</span>; MSG SIZE rcvd<span class="hljs-value">: <span class="hljs-number">44</span></span></strong>
</pre>
<p>As you can see, even though we are running three instances of the service, only one IP is returned <kbd>10.0.0.2</kbd>. That is the IP of the service, not an individual instance. To be more concrete, it is the IP of the <kbd>proxy</kbd> service network end-point. When a request reaches that end-point, Docker network performs load balancing across all the instances.</p>
<p>In most cases, we do not need anything else. All we have to know is the name of the service and Docker will do the rest of the work for us. However, in a few cases, we might need more. We might need to know the IPs of every single instance of a service. That is the problem <em>Docker Flow Proxy</em> faced.</p>
<p>To find the IPs of all the instances of a service we can use the "undocumented" feature. We need to add the tasks prefix to the service name.</p>
<p>Let's drill again:</p>
<pre>
<strong>docker <span class="hljs-keyword">exec</span> -it <span class="hljs-variable">$ID</span> drill tasks.proxy</strong>
</pre>
<p>This time, the output is different:</p>
<pre>
<strong>;; -&gt;&gt;<span class="hljs-tag">HEADER</span>&lt;&lt;- opcode<span class="hljs-value">: QUERY, rcode: NOERROR, id: <span class="hljs-number">54408</span><br/>;</span>; flags<span class="hljs-value">: qr rd ra ;</span> QUERY<span class="hljs-value">: <span class="hljs-number">1</span>, ANSWER: <span class="hljs-number">3</span>, AUTHORITY: <span class="hljs-number">0</span>, ADDITIONAL: <span class="hljs-number">0</span><br/>;</span>; QUESTION <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/>;</span>; tasks<span class="hljs-class">.proxy</span>. IN <span class="hljs-tag">A</span><br/><br/>;; ANSWER <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/>tasks.proxy. <span class="hljs-number">600</span> IN A <span class="hljs-number">10.0</span>.<span class="hljs-number">0.4</span><br/>tasks.proxy. <span class="hljs-number">600</span> IN A <span class="hljs-number">10.0</span>.<span class="hljs-number">0.3</span><br/>tasks.proxy. <span class="hljs-number">600</span> IN A <span class="hljs-number">10.0</span>.<span class="hljs-number">0.5</span><br/><br/>;</span>; AUTHORITY <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/><br/>;</span>; ADDITIONAL <span class="hljs-tag">SECTION</span><span class="hljs-value">:<br/><br/>;</span>; Query <span class="hljs-tag">time</span><span class="hljs-value">: <span class="hljs-number">0</span> msec<br/>;</span>; SERVER<span class="hljs-value">: <span class="hljs-number">127.0</span>.<span class="hljs-number">0.11</span><br/>;</span>; WHEN<span class="hljs-value">: Fri Sep <span class="hljs-number">9</span> <span class="hljs-number">16</span>:<span class="hljs-number">48</span>:<span class="hljs-number">46</span> <span class="hljs-number">2016</span><br/>;</span>; MSG SIZE rcvd<span class="hljs-value">: <span class="hljs-number">110</span></span></strong>
</pre>
<p>We got three answers, each with a different IP <kbd>10.0.0.4, 10.0.0.3, 10.0.0.5</kbd>.</p>
<p>Knowing the IPs of all the instances solved the problem of having to synchronize data. With tasks.<kbd>&lt;SERVICE_NAME&gt;</kbd> we have all the info we need. The rest is only a bit of coding that will utilize those IPs. It is a similar mechanism used when synchronizing databases (more on that later).</p>
<p>We are not done yet. The fact that we can synchronize data on demand (or events) does not mean that the service is fault tolerant. What should we do if we need to create a new instance? What happens if an instance fails and Swarm reschedules it somewhere else?</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using service registry or key value store to store service state</h1>
            </header>

            <article>
                
<p>We'll continue using <em>Docker Flow Proxy</em> as a playground to explore some of the mechanisms and decisions we might make when dealing with stateful services. Please note that, in this chapter, we are concentrating on services with a relatively small state. We'll explore other use cases in the chapters that follow.</p>
<p>Imagine that the proxy does not use Consul to store data and that we do not use volumes. What would happen if we were to scale it up? The new instances would be out of sync. Their state would be the same as the initial state of the first instance we created. In other words, there would be no state, even though the instances that are already running changed over time and generated data.</p>
<p>That is where Consul comes into play. Every time an instance of the proxy receives a request that results in the change of its state, it propagates that change to other instances, as well as to Consul. On the other hand, the first action the proxy performs when initialized is to consult Consul, and create the configuration from its data.</p>
<p>We can observe the state stored in Consul by sending a request for all the data with keys starting with <kbd>docker-flow</kbd>:</p>
<pre>
<strong>curl <span class="hljs-string">"http://<span class="hljs-variable">$(docker-machine ip swarm-1)</span>:8500/v1/kv/\<br/>docker-flow?recurse"</span></strong>
</pre>
<p>A part of the output is as follows:</p>
<pre>
<strong>[<br/><span class="hljs-keyword">...</span><br/>  {<br/><span class="hljs-string">   "LockIndex"</span>: <span class="hljs-number">0</span>,<br/><span class="hljs-string">   "Key"</span>: <span class="hljs-string">"docker-flow/go-demo/path"</span>,<br/><span class="hljs-string">   "Flags"</span>: <span class="hljs-number">0</span>,<br/><span class="hljs-string">   "Value"</span>: <span class="hljs-string">"L2RlbW8="</span>,<br/><span class="hljs-string">   "CreateIndex"</span>: <span class="hljs-number">233</span>,<br/><span class="hljs-string">   "ModifyIndex"</span>: <span class="hljs-number">245</span><br/>  },<br/><span class="hljs-keyword">...</span><br/>  {<br/><span class="hljs-string">   "LockIndex"</span>: <span class="hljs-number">0</span>,<br/><span class="hljs-string">   "Key"</span>: <span class="hljs-string">"docker-flow/go-demo/port"</span>,<br/><span class="hljs-string">   "Flags"</span>: <span class="hljs-number">0</span>,<br/><span class="hljs-string">   "Value"</span>: <span class="hljs-string">"ODA4MA=="</span>,<br/><span class="hljs-string">   "CreateIndex"</span>: <span class="hljs-number">231</span>,<br/><span class="hljs-string">   "ModifyIndex"</span>: <span class="hljs-number">243</span><br/>  },<br/><span class="hljs-keyword">...</span><br/>]</strong>
</pre>
<p>The preceding example shows that the path and the port we specified when we reconfigured the proxy for the <kbd>go-demo</kbd> service, is stored in Consul. If we instruct Swarm manager to scale the <kbd>proxy</kbd> service, new instances will be created. Those instances will query Consul and use the information to generate their configurations.</p>
<p>Let's give it a try:</p>
<pre>
<strong>docker service scale proxy=<span class="hljs-number">6</span></strong>
</pre>
<p>We increased the number of instances from three to six.</p>
<p>Let's take a sneak peek into the instance number six:</p>
<pre>
<strong>NODE=$(docker service ps proxy | grep <span class="hljs-string">"proxy.6"</span> | awk <span class="hljs-string">'{print $4}'</span>)<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env <span class="hljs-variable">$NODE</span>)<br/><br/>ID=$(docker ps | grep <span class="hljs-string">"proxy.6"</span> | awk <span class="hljs-string">'{print $1}'</span>)<br/><br/>docker <span class="hljs-keyword">exec</span> -it <span class="hljs-variable">$ID</span> cat /cfg/haproxy.cfg</strong>
</pre>
<p>A part of the output of the <kbd>exec</kbd> command is as follows:</p>
<pre>
<strong><span class="hljs-tag">frontend</span> <span class="hljs-tag">services</span><br/><span class="hljs-tag">    bind</span> *<span class="hljs-pseudo">:80</span><br/><span class="hljs-tag">    bind</span> *<span class="hljs-pseudo">:443</span><br/><span class="hljs-tag">    mode</span> <span class="hljs-tag">http</span><br/><br/><span class="hljs-tag">backend</span> <span class="hljs-tag">go-demo-be8080</span><br/><span class="hljs-tag">   mode</span> <span class="hljs-tag">http</span><br/><span class="hljs-tag">   server</span> <span class="hljs-tag">go-demo</span> <span class="hljs-pseudo">:8080</span></strong>
</pre>
<p>As you can see, the new instance recuperated all the information from Consul. As a result, its state became the same as the state of any other <kbd>proxy</kbd> instance running inside the cluster.</p>
<p>If we destroy an instance, the result will, again, be the same. Swarm will detect that an instance crashed and schedule a new one. The new instance will repeat the same process of querying Consul and create the same state as the other instances:</p>
<pre>
<strong>docker rm <span class="hljs-operator">-f</span> $(docker ps \<br/>    | grep proxy.<span class="hljs-number">6</span> \<br/>    | awk <span class="hljs-string">'{print $1}'</span>)</strong>
</pre>
<p>We should wait for a few moments until Swarm detects the failure and creates a new instance. </p>
<p>Once it's running, we can take a look at the configuration of the new instance. It will be the same as before:</p>
<pre>
<strong>NODE=$(docker service ps \<br/><span class="hljs-operator">    -f</span> desired-state=running proxy \<br/>    | grep <span class="hljs-string">"proxy.6"</span> \<br/>    | awk <span class="hljs-string">'{print $4}'</span>)<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env <span class="hljs-variable">$NODE</span>)<br/><br/>ID=$(docker ps | grep <span class="hljs-string">"proxy.6"</span> | awk <span class="hljs-string">'{print $1}'</span>)<br/><br/>docker <span class="hljs-keyword">exec</span> -it <span class="hljs-variable">$ID</span> cat /cfg/haproxy.cfg</strong>
</pre>
<p>The explanation of <em>Docker Flow Proxy</em> inner workings is mostly for educational purposes. I wanted to show you one of the possible solutions when dealing with stateful services. The methods we discussed are applicable only when the state is relatively small. When it is bigger, as is the case with databases, we should employ different mechanisms to accomplish the same goals.</p>
<p>If we go one level higher, the primary requirements, or prerequisites, when running stateful services inside a cluster are as follows:</p>
<ol>
<li>Ability to synchronize the state across all instances of the service.</li>
<li>Ability to recuperate the state during initialization.</li>
</ol>
<p>If we manage to fulfill those two requirements, we are on the right path towards solving one of the major bottlenecks when operating stateful services inside the cluster.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What now?</h1>
            </header>

            <article>
                
<p>That concludes the exploration of basic concepts behind the usage of service discovery inside a Swarm cluster.</p>
<p>Are we done learning about Swarm features? We are far from knowing everything there is to know about Docker Swarm. However, at this point, we have enough knowledge to go back to the end of <a href="44df5a4c-1e47-4de0-9442-660034287e66.xhtml">Chapter 1</a>, <em>Continuous Integration with Docker Containers</em> and make the next logical step. We can design a Continuous Delivery flow.</p>
<p>Now is the time to take a break before diving into the next chapter. As before, we'll destroy the machines we created and start fresh:</p>
<pre>
<strong>docker-machine rm <span class="hljs-operator">-f</span> swarm-<span class="hljs-number">1</span> swarm-<span class="hljs-number">2</span> swarm-<span class="hljs-number">3</span></strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>