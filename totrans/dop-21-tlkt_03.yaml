- en: Docker Swarm Networking and Reverse Proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most compelling reason for most people to buy a computer for the home will
    be to link it to a nationwide communications network. We’re just in the beginning
    stages of what will be a truly remarkable breakthrough for most people - as remarkable
    as the telephone.
  prefs: []
  type: TYPE_NORMAL
- en: –Steve Jobs
  prefs: []
  type: TYPE_NORMAL
- en: '**Software-Defined Network** (**SDN**) is a cornerstone of efficient cluster
    management. Without it, services distributed across the cluster would not be able
    to find each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Having proxies based on static configuration does not fit the world of highly
    dynamic scheduling. Services are created, updated, moved around the cluster, scaled
    and de-scaled, and so on. In such a setting, information changes all the time.
  prefs: []
  type: TYPE_NORMAL
- en: One approach we can take is to use a proxy as a central communication point
    and make all the services speak with each other through it. Such a setting would
    require us to monitor changes in the cluster continuously and update the proxy
    accordingly. To make our lives easier, a monitoring process would probably use
    one of the service registries to store the information and a templating solution
    that would update proxy configuration whenever a change in the registry is detected.
    As you can imagine, building such a system is anything but trivial.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Swarm comes with a brand new networking capability. In a nutshell,
    we can create networks and attach them to services. All services that belong to
    the same network can speak with each other using only the name of the service.
    It goes even further. If we scale a service, Swarm networking will perform round-robin
    load balancing and distribute the requests across all the instances. When even
    that is not enough, we have a new network called `ingress` with `routing mesh`
    that has all those and a few additional features.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient usage of Swarm networking is not sufficient by itself. We still need
    a reverse proxy that will be a bridge between the external world and our services.
    Unless there are special requirements, the proxy does not need to perform load
    balancing (Swarm networking does that for us). However, it does need to evaluate
    request paths and forward requests to a destination service. Even in that case,
    Swarm networking helps a lot. Configuring reverse proxy becomes a relatively easy
    thing to do as long as we understand how networking works and can harness its
    full potential.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the networking in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll create a similar environment as we did in the previous chapter. We'll
    have three nodes which will form a Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `03-networking.sh` ([https://gist.github.com/vfarcic/fd7d7e04e1133fc3c90084c4c1a919fe](https://gist.github.com/vfarcic/fd7d7e04e1133fc3c90084c4c1a919fe))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: 'By this time, you already know how to set up a cluster so we''ll skip the explanation
    and just do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the last command `node ls` is as follows (IDs were removed for
    brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have a cluster of three nodes with `node-1` being the only
    manager (and hence the leader).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a fully operating cluster, we can explore the benefits Docker
    networking provides in conjunction with Swarm. We already worked with Swarm networking
    in the previous chapter. Now its time to go deeper, gain a better understanding
    of what we already saw, and unlock some new features and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements of secured and fault tolerant services running with high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us quickly go over the internals of the *go-demo* application. It consists
    of two services. Data is stored in a MongoDB. The database is consumed by a backend
    service called `go-demo`. No other service should access the database directly.
    If another service needs the data, it should send a request to the `go-demo` service.
    That way we have clear boundaries. Data is owned and managed by the `go-demo`
    service. It exposes an API that is the only access point to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The system should be able to host multiple applications. Each will have a unique
    base URL. For example, the `go-demo` path starts with `/demo`. The other applications
    will have different paths (example:  `/users`, `/products`, and so on). The system
    will be accessible only through ports `80`  for HTTP and `443` HTTPS. Please note
    that there can be no two processes that can listen to the same port. In other
    words, only a single service can be configured to listen to port `80`.'
  prefs: []
  type: TYPE_NORMAL
- en: To meet load fluctuations and use the resources effectively, we must be able
    to scale (or de-scale) each service individually and independently from the others.
    Any request to any of the services should pass through a load balancer that will
    distribute the load across all instances. As a minimum, at least two instances
    of any service should be running at any given moment. That way, we can accomplish
    high availability even in case one of the instances stops working. We should aim
    even higher than that and make sure that even a failure of a whole node does not
    interrupt the system as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: To meet performance and fail-over needs services should be distributed across
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll make a temporary exception to the rule that each service should run
    multiple instances. Mongo volumes do not work with Docker Machine on OS X and
    Windows. Later on, when we reach the chapters that provide guidance towards production
    setup inside major hosting providers (example: AWS), we''ll remove this exception
    and make sure that the database is also configured to run with multiple instances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking all this into account, we can make the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: A **load balancer** will distribute requests evenly (*round-robin*) across all
    instances of any given service (**proxy** included). It should be fault tolerant
    and not depend on any single node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A reverse proxy will be in charge of routing requests based on their base URLs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **go-demo** service will be able to communicate freely with the **go-demo-db**
    service and will be accessible only through the reverse proxy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The database will be isolated from any but the service it belongs to **go-demo**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A logical architecture of what we''re trying to accomplish can be presented
    with the diagram that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/go-demo-logical.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1: A logical architecture of the go-demo service'
  prefs: []
  type: TYPE_NORMAL
- en: How can we accomplish those requirements?
  prefs: []
  type: TYPE_NORMAL
- en: Let us solve each of the four requirements one by one. We'll start from the
    bottom and move towards the top.
  prefs: []
  type: TYPE_NORMAL
- en: The first problem to tackle is how to run a database isolated from any but the
    service it belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: Running a database in isolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can isolate a database service by not exposing its ports. That can be accomplished
    easily with the `service create` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can confirm that the ports are indeed not exposed by inspecting the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there is no mention of any port. Our `go-demo-db` service is
    fully isolated and inaccessible to anyone. However, that is too much isolation.
    We want the service to be isolated from anything but the service it belongs to
    `go-demo`. We can accomplish that through the usage of Docker Swarm networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us remove the service we created and start over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we should create a network and make sure that the `go-demo-db` service
    is attached to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We created an overlay network called `go-demo` followed with the `go-demo-db
    service`. This time, we used the `--network` argument to attach the service to
    the network. From this moment on, all services attached to the `go-demo` network
    will be accessible to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s inspect the service and confirm that it is indeed attached to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `service inspect` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this time, there is a `Networks` entry with the value set to
    the ID of the `go-demo` network we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us confirm that networking truly works. To prove it, we''ll create a global
    service called `util`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Just as `go-demo-db`, the `util` service also has the `go-demo` network attached.
  prefs: []
  type: TYPE_NORMAL
- en: A new argument is `--mode`. When set to global, the service will run on every
    node of the cluster. That is a very useful feature when we want to set up infrastructure
    services that should span the whole cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm that it is running everywhere by executing the `service ps`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs and ERROR PORTS columns are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `util` service is running on all three nodes.
  prefs: []
  type: TYPE_NORMAL
- en: We are running the `alpine` image (a minuscule Linux distribution). We put it
    to sleep for a very long time. Otherwise, since no processes are running, the
    service would stop, Swarm would restart it, it would stop again, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the `util` service will be to demonstrate some of the concepts
    we're exploring. We'll exec into it and confirm that the networking truly works.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enter the `util` container, we need to find out the ID of the instance running
    on the `node-1` (the node our local Docker is pointing to):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We listed all the processes `ps` in quiet mode so that only IDs are returned
    **`-q`**, and limited the result to the service name util:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The result is stored as the environment variable ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll install a tool called *drill*. It is a tool designed to get all sorts
    of information out of a DNS and it will come in handy very soon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*Alpine* Linux uses the package management called `apk`, so we told it to add
    drill.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we can see whether networking truly works. Since both `go-demo-db` and util
    services belong to the same network, they should be able to communicate with each
    other using DNS names. Whenever we attach a service to the network, a new virtual
    IP is created together with a DNS that matches the name of the services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try it out as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We entered into one of the instances of the `util` service and "drilled" the
    DNS `go-demo-db`. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The response code is `NOERROR` and the `ANSWER` is `1` meaning that the DNS
    `go-demo-db` responded correctly. It is reachable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also observe that the `go-demo-db` DNS is associated with the IP `10.0.0.2`.
    Every service attached to a network gets its IP. Please note that I said service,
    not an instance. That’s a huge difference that we''ll explore later. For now,
    it is important to understand that all services that belong to the same network
    are accessible through service names:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/swarm-nodes-go-demo-db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-2: go-demo-db service attached to the go-demo network'
  prefs: []
  type: TYPE_NORMAL
- en: Let's move up through the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Running a service through a reverse proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want the `go-demo` service to be able to communicate freely with the `go-demo-db`
    service and to be accessible only through the reverse proxy. We already know how
    to accomplish the first part. All we have to do is make sure that both services
    belong to the same network `go-demo`.
  prefs: []
  type: TYPE_NORMAL
- en: How can we accomplish the integration with a reverse proxy?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by creating a new network and attach it to all services that should
    be accessible through a reverse proxy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s list the currently running `overlay` networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We have the `go-demo` and `proxy` networks we created earlier. The third one
    is called ingress. It is set up by default and has a special purpose that we'll
    explore later.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to run the `go-demo` service. We want it to be able to communicate
    with the `go-demo-db` service so it must be attached to the `go-demo` network.
    We also want it to be accessible to a `proxy` (we'll create it soon) so we'll
    attach it to the `proxy` network as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that creates the `go-demo` service is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It is very similar to the command we executed in the previous chapter with
    the addition of the `--network proxy` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/swarm-nodes-proxy-sdn.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-3: Docker Swarm cluster with three nodes, two networks and a few containers'
  prefs: []
  type: TYPE_NORMAL
- en: Now both services are running somewhere inside the cluster and can communicate
    with each other through the `go-demo` network. Let's bring the proxy into the
    mix. We'll use the *Docker Flow Proxy* ([https:/](https://github.com/vfarcic/docker-flow-proxy)[/github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    project that is a combination of HAProxy ([http://www.haproxy.org/](http://www.haproxy.org/))
    and a few additional features that make it more dynamic. The principles we'll
    explore are the same no matter which one will be your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, at this moment, none of the services are accessible to anyone
    except those attached to the same network.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a reverse proxy service in charge of routing requests depending on
    their base URLs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can implement a reverse proxy in a couple of ways. One would be to create
    a new image based on HAProxy ([https://hub.docker.com/_/haproxy/](https://hub.docker.com/_/haproxy/))
    and include configuration files inside it. That approach would be a good one if
    the number of different services is relatively static. Otherwise, we'd need to
    create a new image with a new configuration every time there is a new service
    (not a new release).
  prefs: []
  type: TYPE_NORMAL
- en: The second approach would be to expose a volume. That way, when needed, we could
    modify the configuration file instead building a whole new image. However, that
    has downsides as well. When Deploying to a cluster, we should avoid using volumes
    whenever they're not necessary. As you'll see soon, a proxy is one of those that
    do not require a volume. As a side note, `--volume` has been replaced with the
    `docker service` argument `--mount`.
  prefs: []
  type: TYPE_NORMAL
- en: The third option is to use one of the proxies designed to work with Docker Swarm.
    In this case, we'll use the container `vfarcic/docker-flow-proxy` ([https://hub.docker.com/r/vfarcic/docker-flow-proxy/](https://hub.docker.com/r/vfarcic/docker-flow-proxy/))
    It is based on HAProxy with additional features that allow us to reconfigure it
    by sending HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: Let's give it a spin.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that creates the `proxy` service is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We opened ports `80` and `443` that will serve Internet traffic (HTTP and HTTPS).
    The third port is 8080\. We'll use it to send configuration requests to the proxy.
    Further on, we specified that it should belong to the proxy network. That way,
    since go-demo is also attached to the same network, the proxy can access it through
    the proxy-SDN.
  prefs: []
  type: TYPE_NORMAL
- en: Through the **proxy** we just ran, we can observe one of the cool features of
    the network routing mesh. It does not matter which server the **proxy** is running
    in. We can send a request to any of the nodes and Docker networking will make
    sure that it is redirected to one of the proxies. We'll see that in action very
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: The last argument is the environment variable `MODE` that tells the proxy that
    containers will be deployed to a Swarm cluster. Please consult the project README
    ([https://github.com/vfarcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    for other combinations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/swarm-nodes-proxy.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4: Docker Swarm cluster with the proxy service'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the **proxy**, even though it is running inside one of the
    nodes, is placed outside to illustrate the logical separation better.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, let's confirm that the `proxy` is running.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We can proceed if the `CURRENT STATE` is `Running`. Otherwise, please wait until
    the service is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the proxy is Deployed, we should let it know about the existence of
    the `go-demo` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The request was sent to reconfigure the `proxy` specifying the service name
    `go-demo`, base URL path of the API `/demo`, and the internal port of the service
    `8080`. From now on, all the requests to the `proxy` with the path that starts
    with `/demo` will be redirected to the `go-demo` service. This request is one
    of the additional features Docker Flow Proxy provides on top of HAProxy.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we sent the request to `node-1`. The proxy could be running
    inside any of the nodes and, yet, the request was successful. That is where Docker's
    Routing Mesh plays a critical role. We'll explore it in more detail later. For
    now, the important thing to note is that we can send a request to any of the nodes,
    and it will be redirected to the service that listens to the same port (in this
    case `8080`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the request is as follows (formatted for readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: I won't go into details but note that the `Status` is `OK` indicating that the
    `proxy` was reconfigured correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test that the `proxy` indeed works as expected by sending an HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The output of the `curl` command is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `proxy` works! It responded with the HTTP status `200` and returned the
    API response `hello, world!`. As before, the request was not, necessarily, sent
    to the node that hosts the service but to the routing mesh that forwarded it to
    the `proxy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s send the same request but this time, to `node-3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The result is still the same.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore the configuration generated by the `proxy`. It will give us more
    insights into the Docker Swarm Networking inner workings. As another benefit,
    if you choose to roll your own `proxy` solution, it might be useful to understand
    how to configure the `proxy` and leverage new Docker networking features.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by examining the configuration *Docker Flow Proxy* ([https://github.com/vfa](https://github.com/vfarcic/docker-flow-proxy)[rcic/docker-flow-proxy](https://github.com/vfarcic/docker-flow-proxy))
    created for us. We can do that by entering the running container to take a sneak
    peek at the file `/cfg/haproxy.cfg`. The problem is that finding a container run
    by Docker Swarm is a bit tricky. If we deployed it with Docker Compose, the container
    name would be predictable. It would use the format `<PROJECT>_<SERVICE>_<INDEX>`.
  prefs: []
  type: TYPE_NORMAL
- en: The `docker service command` runs containers with hashed names. The `docker-flow-proxy`
    created on my laptop has the name `proxy.1.e07jvhdb9e6s76mr9ol41u4sn`. Therefore,
    to get inside a running container deployed with Docker Swarm, we need to use a
    filter with, for example, an image name.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to find out on which node the `proxy` is running execute the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We listed the `proxy` service processes `docker service ps proxy`, removed the
    header `tail -n +2`, and output the node that resides inside the fourth column
    `awk '{print $4}'`. The output is stored as the environment variable `NODE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can point our local Docker Engine to the node where the `proxy` resides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the only thing left is to find the ID of the `proxy` container. We
    can do that with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the container ID stored inside the variable, we can execute
    the command that will retrieve the HAProxy configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The important part of the configuration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The first part `frontend` should be familiar to those who have used HAProxy.
    It accepts requests on ports `80` HTTP and `443` HTTPS. If the path starts with
    `/demo`, it will be redirected to the `backend go-demo-be`. Inside it, requests
    are sent to the address `go-demo` on the port `8080`. The address is the same
    as the name of the service we deployed. Since `go-demo` belongs to the same network
    as the `proxy`, Docker will make sure that the request is redirected to the destination
    container. Neat, isn't it? There is no need, anymore, to specify IPs and external
    ports.
  prefs: []
  type: TYPE_NORMAL
- en: The next question is how to do load balancing. How should we specify that the
    `proxy` should, for example, perform round-robin across all instances? Should
    we use a `proxy` for such a task?
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing requests across all instances of a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we explore load balancing, we need to have something to balance. We
    need multiple instances of a service. Since we already explored scaling in the
    previous chapter, the command should not come as a surprise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Within a few moments, five instances of the `go-demo` service will be running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/swarm-nodes-proxy-scaled.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-5: Docker Swarm cluster with the go-demo service scaled'
  prefs: []
  type: TYPE_NORMAL
- en: What should we do to make the **proxy** load balance requests across all instances?
    The answer is nothing. No action is necessary on our part. Actually, the question
    is wrong. The **proxy** will not load balance requests at all. Docker Swarm networking
    will. So, let us reformulate the question. What should we do to make the *Docker
    Swarm network* load balance requests across all instances? Again, the answer is
    nothing. No action is necessary on our part.
  prefs: []
  type: TYPE_NORMAL
- en: To understand load balancing, we might want to go back in time and discuss load
    balancing before Docker networking came into being.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, if we didn''t leverage Docker Swarm features, we would have something
    similar to the following **proxy** configuration mock-up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Every time a new instance is added, we would need to add it to the configuration
    as well. If an instance is removed, we would need to remove it from the configuration.
    If an instance failed… Well, you get the point. We would need to monitor the state
    of the cluster and update the `proxy` configuration whenever a change occurs.
  prefs: []
  type: TYPE_NORMAL
- en: If you read *The DevOps 2.0 Toolkit*, you probably remember that I advised a
    combination of *Registrator* ([https://github.com/gliderlabs/registrator](https://github.com/gliderlabs/registrator)),
    *Consul* ([https://www](https://www.consul.io/)[.consul.io/](https://www.consul.io/)),
    and *Consul Template* ([https://github.com/hashicorp/consul-template](https://github.com/hashicorp/consul-template)).
    Registrator would monitor Docker events and update Consul whenever a container
    is created or destroyed. With the information stored in Consul, we would use Consul
    Template to update nginx or HAProxy configuration. There is no need for such a
    combination anymore. While those tools still provide value, for this particular
    purpose, there is no need for them.
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to update the `proxy` every time there is a change inside the
    cluster, for example, a scaling event. Instead, we are going to update the proxy
    every time a new service is created. Please note that service updates (Deployment
    of new releases) do not count as service creation. We create a service once and
    update it with each new release (among other reasons). So, only a new service
    requires a change in the `proxy` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason behind that reasoning is in the fact that load balancing is now
    part of Docker Swarm networking. Let''s do another round of drilling from the
    `util` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The IP `10.0.0.8` represents the `go-demo` service, not an individual instance.
    When we sent a drill request, Swarm networking performed **load balancing** (**LB**)
    across all of the instances of the service. To be more precise, it performed *round-robin*
    LB.
  prefs: []
  type: TYPE_NORMAL
- en: Besides creating a virtual IP for each service, each instance gets its own IP
    as well. In most cases, there is no need discovering those IPs (or any Docker
    network endpoint IP) since all we need is a service name, which gets translated
    to an IP and load balanced in the background.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That concludes the exploration of basic concepts of the Docker Swarm networking.
  prefs: []
  type: TYPE_NORMAL
- en: Is this everything there is to know to run a Swarm cluster successfully? In
    this chapter, we went deeper into Swarm features, but we are not yet done. There
    are quite a few questions waiting to be answered. In the next chapter, we'll explore
    *service discovery* and the role it has in the Swarm Mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is the time to take a break before diving into the next chapter. As before,
    we''ll destroy the machines we created and start fresh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
