- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Shipping Logs and Monitoring Containers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发送日志和监控容器
- en: In the previous chapter, we introduced the Docker Compose tool. We learned that
    this tool is mostly used to run and scale multi-service applications on a single
    Docker host. Typically, developers and CI servers work with single hosts and they
    are the main users of Docker Compose. We saw that the tool uses YAML files as
    input, which contain the description of the application in a declarative way.
    We investigated many useful tasks the tool can be used for, such as building and
    pushing images, to just name the most important ones.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了Docker Compose工具。我们了解到，该工具主要用于在单一Docker主机上运行和扩展多服务应用。通常，开发人员和CI服务器使用单主机，它们是Docker
    Compose的主要用户。我们看到，该工具使用YAML文件作为输入，文件以声明式方式描述应用。我们探讨了该工具可用于的许多有用任务，例如构建和推送镜像，只是列举其中最重要的一些。
- en: This chapter discusses why logging and monitoring are so important and shows
    how container logs can be collected and shipped to a central location where the
    aggregated log can then be parsed for useful information.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了日志记录和监控为何如此重要，并展示了如何收集容器日志并将其发送到中央位置，在那里聚合的日志可以被解析以提取有用信息。
- en: You will also learn how to instrument an application so that it exposes metrics
    and how those metrics can be scraped and shipped again to a central location.
    Finally, you will learn how to convert those collected metrics into graphical
    dashboards that can be used to monitor a containerized application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将学习如何为应用添加监控，使其暴露指标，以及如何抓取并再次将这些指标发送到中央位置。最后，你将学习如何将这些收集到的指标转换为图形仪表盘，用于监控容器化应用。
- en: We will be using Filebeat as an example to collect logs from a default location
    where Docker directs the logs at `/var/lib/docker/containers`. This is straightforward
    on Linux. Luckily, on a production or production-like system, we mostly find Linux
    as the OS of choice.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Filebeat作为示例，从Docker将日志默认指向的`/var/lib/docker/containers`位置收集日志。在Linux上这非常简单。幸运的是，在生产环境或类似生产的系统中，我们通常会选择Linux作为操作系统。
- en: Collecting metrics on a Windows or Mac machine, on the other hand, is a bit
    more involved than on a Linux machine. Thus, we will generate a special Docker
    Compose stack, including Filebeat, that can run on a Mac or Windows computer by
    using the workaround of redirecting the standard log output to a file whose parent
    folder is mapped to a Docker volume. This volume is then mounted to Filebeat,
    which, in turn, forwards the logs to Elasticsearch.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows或Mac机器上收集指标，较之Linux机器，稍微复杂一些。因此，我们将生成一个特殊的Docker Compose堆栈，包括Filebeat，可以通过将标准日志输出重定向到一个文件，并将该文件的父文件夹映射到Docker卷来在Mac或Windows计算机上运行。这个卷随后会挂载到Filebeat上，Filebeat再将日志转发到Elasticsearch。
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Why is logging and monitoring important?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么日志记录和监控如此重要？
- en: Shipping container and Docker daemon logs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送容器和Docker守护进程日志
- en: Querying a centralized log
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询集中日志
- en: Collecting and scraping metrics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集和抓取指标
- en: Monitoring a containerized application
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控容器化应用
- en: 'After reading this chapter, you should be able to do the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你应该能够完成以下操作：
- en: Define a log driver for your containers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的容器定义日志驱动
- en: Install an agent to collect and ship your container and Docker daemon logs
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装代理以收集并发送容器和Docker守护进程日志
- en: Execute simple queries in the aggregate log to pinpoint interesting information
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在聚合日志中执行简单的查询，找出有趣的信息
- en: Instrument your application services so that they expose infrastructure and
    business metrics
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的应用服务添加监控，使其暴露基础设施和业务指标
- en: Convert the collected metrics into dashboards to monitor your containers
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将收集到的指标转换为仪表盘以监控你的容器
- en: Technical requirements
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code accompanying this chapter can be found at [https://github.com/PacktPublishing/The-Ultimate-Docker-Container-Book/tree/main/sample-solutions/ch12](https://github.com/PacktPublishing/The-Ultimate-Docker-Container-Book/tree/main/sample-solutions/ch12).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章相关的代码可以在[https://github.com/PacktPublishing/The-Ultimate-Docker-Container-Book/tree/main/sample-solutions/ch12](https://github.com/PacktPublishing/The-Ultimate-Docker-Container-Book/tree/main/sample-solutions/ch12)找到。
- en: Before we start, let’s make sure you have a folder ready for the code you are
    going to implement in this chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，确保你已经准备好一个文件夹，用于存放你将在本章中实现的代码。
- en: 'Navigate to the folder where you cloned the code repository that accompanies
    this book. Normally, this is the `The-Ultimate-Docker-Container-Book` folder in
    your `home` folder:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 进入你克隆的代码库所在的文件夹，这个文件夹通常是位于你`home`文件夹中的`The-Ultimate-Docker-Container-Book`文件夹：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a subfolder called `ch12` and navigate to it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`ch12`的子文件夹并进入该文件夹：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Without further ado, let’s dive into the first topic of shipping containers
    and daemon logs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不再多说，让我们深入探讨第一个话题：集装箱和守护进程日志。
- en: Why is logging and monitoring important?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么日志记录和监控很重要？
- en: When working with a distributed mission-critical application in production or
    any production-like environment, it is of utmost importance to gain as much insight
    as possible into the inner workings of those applications. Have you ever had a
    chance to investigate the cockpit of an airplane or the command center of a nuclear
    power plant? Both, an airplane and a power plant are examples of highly complex
    systems that deliver mission-critical services. If a plane crashes or a power
    plant shuts down unexpectedly, a lot of people are negatively affected, to say
    the least. Thus, the cockpit and the command center are full of instruments showing
    the current or past state of some parts of the system. What you see there is the
    visual representation of some sensors that are placed in strategic parts of the
    system and constantly collect data such as the temperature or the flow rate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理生产环境或任何类似生产环境的分布式关键任务应用程序时，获取尽可能多的应用内部运行状况的洞察是至关重要的。你是否有机会调查过飞机的驾驶舱或核电站的指挥中心？飞机和电厂都是高度复杂的系统，提供关键任务服务。如果飞机坠毁或电厂意外停运，至少可以说会有很多人受到负面影响。因此，驾驶舱和指挥中心充满了仪器，显示着系统某些部分的当前状态或过去的状态。你在这里看到的，是一些放置在系统关键部分的传感器的视觉表现，这些传感器不断地收集诸如温度或流量等数据。
- en: Similar to an airplane or a power plant, our application needs to be instrumented
    with “sensors” that can feel the “temperature” of our application services or
    the infrastructure they run on. I put the word temperature in double quotes since
    it is only a placeholder for things that matter in an application, such as the
    number of requests per second on a given RESTful endpoint, or the average latency
    of requests to the same endpoint.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于飞机或电厂，我们的应用程序需要配备“传感器”，这些传感器能够感知我们应用服务或其运行基础设施的“温度”。我将“温度”一词加上了引号，因为它只是一个占位符，代表应用中真正重要的事物，比如某个RESTful接口每秒的请求数，或是对同一接口的请求的平均延迟。
- en: The resulting values or readings that we collect, such as the average latency
    of requests, are often called **metrics**. It should be our goal to expose as
    many meaningful metrics as possible of the application services we build. Metrics
    can be both functional and non-functional. Functional metrics are values that
    say something business-relevant about the application service, such as how many
    checkouts are performed per minute if the service is part of an e-commerce application,
    or what are the 5 most popular songs over the last 24 hours if we are talking
    about a streaming application.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集到的结果值或读数，比如请求的平均延迟，通常被称为**指标**。我们的目标应该是暴露尽可能多的应用服务的有意义的指标。指标可以是功能性指标或非功能性指标。功能性指标是与应用服务的业务相关的值，例如，如果服务是电子商务应用的一部分，则每分钟的结账次数，或者如果我们谈论的是流媒体应用，则过去24小时内最受欢迎的5首歌曲。
- en: Non-functional metrics are important values that are not specific to the kind
    of business the application is used for, such as the average latency of a particular
    web request, how many 4xx status codes are returned per minute by another endpoint,
    or how much RAM or how many CPU cycles a given service is consuming.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 非功能性指标是一些重要的值，这些值与应用程序用于的业务类型无关，例如某个特定Web请求的平均延迟、某个接口每分钟返回的4xx状态码数量，或者某个服务消耗的RAM或CPU周期数。
- en: In a distributed system where each part is exposing metrics, some overarching
    service should be collecting and aggregating the values periodically from each
    component. Alternatively, each component should forward its metrics to a central
    metrics server. Only if the metrics for all components of our highly distributed
    system are available for inspection in a central location are they of any value.
    Otherwise, monitoring the system becomes impossible. That’s why pilots of an airplane
    never have to go and inspect individual and critical parts of the airplane in
    person during a flight; all necessary readings are collected and displayed in
    the cockpit.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个分布式系统中，每个部分都暴露着指标，应该有一个总服务定期收集并聚合来自各个组件的值。或者，每个组件应该将其指标转发到一个中央指标服务器。只有当我们高度分布式系统中所有组件的指标可以在一个中央位置进行检查时，它们才有价值。否则，监控系统就变得不可能。这就像飞机驾驶员在飞行过程中不需要亲自检查飞机的每个重要部件一样；所有必要的读数都会收集并显示在驾驶舱中。
- en: Today, one of the most popular services that is used to expose, collect, and
    store metrics is **Prometheus**. It is an open source project and has been donated
    to the **Cloud Native Computing Foundation** (**CNCF**). Prometheus has first-class
    integration with Docker containers, Kubernetes, and many other systems and programming
    platforms. In this chapter, we will use Prometheus to demonstrate how to instrument
    a simple service that exposes important metrics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，最流行的服务之一是**Prometheus**，它用于暴露、收集和存储指标。它是一个开源项目，并已捐赠给**云原生计算基金会**（**CNCF**）。Prometheus
    与 Docker 容器、Kubernetes 以及许多其他系统和编程平台具有一流的集成。在本章中，我们将使用 Prometheus 演示如何为一个简单的服务添加指标暴露功能。
- en: In the next section, we are going to show you how to ship containers and Docker
    daemon logs to a central location.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将向您展示如何将容器和 Docker 守护进程日志发送到一个中央位置。
- en: Shipping containers and Docker daemon logs
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发送容器和 Docker 守护进程日志
- en: 'In the world of containerization, understanding the logs generated by your
    Docker environment is crucial for maintaining a healthy and well-functioning system.
    This section will provide an overview of two key types of logs you will encounter:
    shipping **container logs** and **Docker** **daemon logs**.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器化的世界中，了解 Docker 环境生成的日志对于保持系统健康和正常运行至关重要。本节将概述您将遇到的两种关键日志类型：发送的**容器日志**和**Docker**
    **守护进程日志**。
- en: Shipping container logs
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发送容器日志
- en: As applications run within containers, they generate log messages that provide
    valuable insights into their performance and any potential problems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序在容器中运行时，它们会生成日志信息，这些信息提供了有关其性能和潜在问题的宝贵洞察。
- en: Container logs can be accessed using the `docker logs` command, followed by
    the container’s ID or name. These logs can help developers and system administrators
    diagnose issues, monitor container activities, and ensure the smooth operation
    of deployed applications. Centralizing and analyzing container logs is essential
    for optimizing resource usage, identifying performance bottlenecks, and troubleshooting
    application issues.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`docker logs`命令来访问容器日志，后面跟上容器的 ID 或名称。这些日志可以帮助开发人员和系统管理员诊断问题、监控容器活动，并确保已部署应用程序的顺利运行。集中管理和分析容器日志对于优化资源使用、识别性能瓶颈以及排除应用程序问题至关重要。
- en: 'Some best practices for managing shipping container logs include the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 管理运输容器日志的一些最佳实践包括以下内容：
- en: Configuring log rotation and retention policies to prevent excessive disk space
    usage
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置日志轮换和保留策略以防止过度使用磁盘空间
- en: Using a log management system to centralize logs from multiple containers
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用日志管理系统将多个容器的日志集中管理
- en: Setting up log filtering and alerting mechanisms to identify critical events
    and anomalies
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置日志过滤和警报机制，以识别关键事件和异常
- en: Let’s look at these recommendations in detail, starting with log rotation and
    retention policies.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解这些建议，从日志轮换和保留策略开始。
- en: Configuring log rotation and retention policies
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置日志轮换和保留策略
- en: Configuring log rotation and retention policies for container logs is essential
    for preventing excessive disk space usage and maintaining optimal performance.
    Here’s a step-by-step guide on how to set up these policies for Docker container
    logs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 配置容器日志的日志轮换和保留策略对于防止过度使用磁盘空间并保持最佳性能非常重要。以下是如何为 Docker 容器日志设置这些策略的逐步指南。
- en: Configuring the logging driver
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 配置日志驱动程序
- en: Docker supports various logging drivers, such as `json-file`, `syslog`, `journald`,
    and more. To configure the logging driver, you can either set it up globally for
    the entire Docker daemon or individually for each container. For this example,
    we’ll use the `json-file` logging driver, which is the default driver for Docker.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 支持多种日志驱动程序，如 `json-file`、`syslog`、`journald` 等。要配置日志驱动程序，你可以选择全局设置整个
    Docker 守护进程的日志驱动程序，或者为每个容器单独设置。在此示例中，我们将使用 `json-file` 日志驱动程序，这是 Docker 的默认驱动程序。
- en: Globally setting the log driver
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全局设置日志驱动程序
- en: 'To set the logging driver globally, edit the `/etc/docker/daemon.json` configuration
    file (create it if it doesn’t exist) and do the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要全局设置日志驱动程序，请编辑 `/etc/docker/daemon.json` 配置文件（如果文件不存在，则创建它），并执行以下操作：
- en: 'Open the dashboard of Docker Desktop and navigate to **Settings**, then **Docker
    Engine**. You should see something similar to this:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Docker Desktop 的仪表板并导航至 **设置**，然后选择 **Docker 引擎**。你应该看到类似于以下内容的界面：
- en: '![Figure 12.1 – Configuration of the Docker daemon](img/B19199_12_01.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – Docker 守护进程配置](img/B19199_12_01.jpg)'
- en: Figure 12.1 – Configuration of the Docker daemon
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – Docker 守护进程配置
- en: 'Analyze the existing configuration and add the following key-value pair to
    it, if not present already:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析现有的配置，并在其中添加以下键值对（如果尚未存在）：
- en: '[PRE2]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, the (shortened) result will look like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，（简化后的）结果将如下所示：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Restart the Docker daemon to apply the changes.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新启动 Docker 守护进程以应用更改。
- en: Locally setting the log driver
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本地设置日志驱动程序
- en: 'If you prefer to set the logging driver for an individual container instead
    of globally, then use the `--log-driver` option when starting the container:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更倾向于为单个容器设置日志驱动程序而不是全局设置，请在启动容器时使用 `--log-driver` 选项：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, let’s learn how to specify log rotation and retention policies.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何指定日志轮换和保留策略。
- en: Setting log rotation and retention policies
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置日志轮换和保留策略
- en: 'We can configure log rotation and retention policies by specifying the `max-size`
    and `max-file` options for the logging driver:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过为日志驱动程序指定 `max-size` 和 `max-file` 选项来配置日志轮换和保留策略：
- en: '`max-size`: This option limits the size of each log file. When a log file reaches
    the specified size, Docker creates a new file and starts logging into it. For
    example, to limit each log file to 10 MB, set `max-size=10m`.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max-size`：此选项限制每个日志文件的大小。当日志文件达到指定大小时，Docker 会创建一个新文件并开始记录。例如，要将每个日志文件限制为
    10 MB，设置 `max-size=10m`。'
- en: '`max-file`: This option limits the number of log files to keep. When the limit
    is reached, Docker removes the oldest log file. For example, to keep only the
    last five log files, set `max-file=5`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max-file`：此选项限制要保留的日志文件数量。当达到限制时，Docker 会删除最旧的日志文件。例如，要只保留最近的五个日志文件，设置 `max-file=5`。'
- en: 'To set these options globally, add them to the `/etc/docker/daemon.json` configuration
    file. We can add the `log-opts` section to the daemon configuration right after
    the `log-driver` node that we added earlier:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要全局设置这些选项，请将它们添加到 `/etc/docker/daemon.json` 配置文件中。我们可以在之前添加的 `log-driver` 节点后面添加
    `log-opts` 部分：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We suggest that you modify the daemon configuration once again via the dashboard
    of Docker Desktop. Once you have modified the configuration, restart the Docker
    daemon to apply the changes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议你通过 Docker Desktop 的仪表板再次修改守护进程配置。修改配置后，请重新启动 Docker 守护进程以应用更改。
- en: 'To set these options for an individual container, use the `--log-opt` option
    when starting the container:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要为单个容器设置这些选项，请在启动容器时使用 `--log-opt` 选项：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By configuring log rotation and retention policies, you can prevent excessive
    disk space usage and maintain a well-functioning Docker environment. Remember
    to choose appropriate values for `max-size` and `max-file` based on your specific
    use case and storage capacity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过配置日志轮换和保留策略，你可以防止磁盘空间的过度使用，并保持 Docker 环境的正常运行。记得根据你的具体使用情况和存储容量选择合适的 `max-size`
    和 `max-file` 值。
- en: Using a log management system
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用日志管理系统
- en: Using a log management system to centralize logs from multiple containers is
    essential for efficient monitoring and troubleshooting in a Docker environment.
    This allows you to aggregate logs from all containers, analyze them in one place,
    and identify patterns or issues. In this chapter, we’ll use the **Elasticsearch,
    Logstash, and Kibana** (**ELK**) Stack as an example log management system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用日志管理系统将多个容器的日志集中管理，对于在Docker环境中进行高效监控和故障排除至关重要。这使得你可以将所有容器的日志集中分析，找出模式或问题。在本章中，我们将使用**Elasticsearch,
    Logstash和Kibana**（**ELK**）Stack作为示例日志管理系统。
- en: The ELK Stack
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ELK Stack
- en: The ELK Stack, also known as the Elastic Stack, is a collection of open source
    software products that facilitate the ingestion, storage, processing, searching,
    and visualization of large volumes of data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ELK Stack，也称为Elastic Stack，是一组开源软件产品，旨在促进大规模数据的摄取、存储、处理、搜索和可视化。
- en: ELK is an acronym that stands for Elasticsearch, Logstash, and Kibana, which
    are the main components of the stack.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ELK是Elasticsearch、Logstash和Kibana的缩写，它们是该堆栈的主要组件。
- en: '**Elasticsearch**: Elasticsearch is a distributed, RESTful search and analytics
    engine built on top of Apache Lucene. It provides a scalable and near real-time
    search platform with powerful full-text search capabilities, as well as support
    for aggregations and analytics. Elasticsearch is commonly used for log and event
    data analysis, application search, and various other use cases that require high-performance
    searching and indexing capabilities.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**Elasticsearch**：Elasticsearch是一个分布式的、基于REST的搜索和分析引擎，建立在Apache Lucene之上。它提供了一个可扩展的、近实时的搜索平台，具备强大的全文搜索功能，同时支持聚合和分析。Elasticsearch通常用于日志和事件数据分析、应用程序搜索，以及各种需要高性能搜索和索引功能的用例。'
- en: '**Logstash**: Logstash is a flexible, server-side data processing pipeline
    that ingests, processes, and forwards data to various outputs, including Elasticsearch.
    Logstash supports multiple input sources, such as log files, databases, and message
    queues, and can transform and enrich data using filters before forwarding it.
    Logstash is often used to collect and normalize logs and events from various sources,
    making it easier to analyze and visualize the data in Elasticsearch.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**Logstash**：Logstash是一个灵活的服务器端数据处理管道，能够摄取、处理并将数据转发到多个输出，包括Elasticsearch。Logstash支持多种输入源，如日志文件、数据库和消息队列，并可以在转发数据前使用过滤器进行转换和增强。Logstash通常用于收集和规范化来自不同源的日志和事件，使得在Elasticsearch中分析和可视化数据更加容易。'
- en: '**Kibana**: Kibana is a web-based data visualization and exploration tool that
    provides a user interface for interacting with Elasticsearch data. Kibana offers
    various visualization types, such as bar charts, line charts, pie charts, and
    maps, as well as support for creating custom dashboards to display and analyze
    data. Kibana also includes features such as Dev Tools for Elasticsearch query
    testing, monitoring, and alerting capabilities, and machine learning integration.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kibana**：Kibana是一个基于Web的数据可视化和探索工具，提供了与Elasticsearch数据交互的用户界面。Kibana提供多种可视化类型，如柱状图、折线图、饼图和地图，并支持创建自定义仪表板来展示和分析数据。Kibana还包括Dev
    Tools用于Elasticsearch查询测试、监控和警报功能，并支持机器学习集成。'
- en: Note that the following description applies to a Linux system. If you happen
    to be one of the lucky people running Linux natively on your developer machine,
    then go for it and start right away with *Step 1 – setting up the ELK Stack* *on
    Linux*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以下描述适用于Linux系统。如果你恰好是那些在开发机器上原生运行Linux的幸运人之一，那就直接开始*第1步 – 在Linux上设置ELK Stack*吧。
- en: If, on the other hand, you are using a Mac or Windows machine for work, then
    we have created some step-by-step instructions on how to test the setup. Of special
    notice is *Step 2 – installing and configuring Filebeat*. See the part that matches
    your setup and give it a try.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Mac或Windows机器进行工作，我们已经创建了详细的步骤说明，教你如何测试设置。特别需要注意的是*第2步 – 安装和配置Filebeat*。请查看与你的设置相匹配的部分并尝试一下。
- en: Step 1 – setting up the ELK Stack on Linux
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步 – 在Linux上设置ELK Stack
- en: 'Deploy ELK using Docker containers or install them directly on your system.
    For detailed instructions, refer to the official ELK Stack documentation: [https://www.elastic.co/guide/index.xhtml](https://www.elastic.co/guide/index.xhtml).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker容器部署ELK，或将其直接安装在你的系统上。详细的安装说明，请参考官方的ELK Stack文档：[https://www.elastic.co/guide/index.xhtml](https://www.elastic.co/guide/index.xhtml)。
- en: Ensure that Elasticsearch and Kibana are properly configured and running. Verify
    this by accessing the Kibana dashboard through a web browser.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 确保 Elasticsearch 和 Kibana 配置正确并正在运行。通过使用 web 浏览器访问 Kibana 仪表板来验证这一点。
- en: Step 2 – installing and configuring Filebeat
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2 – 安装并配置 Filebeat
- en: 'Filebeat is a lightweight log shipper that forwards logs from your Docker containers
    to the ELK Stack. Install Filebeat on the Docker host machine and configure it
    to collect container logs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Filebeat 是一个轻量级的日志传输工具，可以将日志从 Docker 容器转发到 ELK Stack。你可以在 Docker 主机上安装 Filebeat，并配置它以收集容器日志：
- en: 'Install Filebeat using the official installation guide for your specific operating
    system. You can find the docs here: [https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.xhtml](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.xhtml).'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用官方安装指南安装 Filebeat，针对你的操作系统进行安装。你可以在这里找到相关文档：[https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.xhtml](https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-installation-configuration.xhtml)。
- en: 'Configure Filebeat by editing the `filebeat.yml` configuration file (usually
    located in `/etc/filebeat` on Linux systems). Add the following configuration
    to collect Docker container logs:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编辑 `filebeat.yml` 配置文件来配置 Filebeat（通常位于 Linux 系统的 `/etc/filebeat` 中）。添加以下配置以收集
    Docker 容器日志：
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set up the output to forward logs to Elasticsearch. Replace `<elasticsearch_host>`
    and `<elasticsearch_port>` with the appropriate values:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置输出将日志转发到 Elasticsearch。将 `<elasticsearch_host>` 和 `<elasticsearch_port>` 替换为适当的值：
- en: '[PRE8]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Save the configuration file and start Filebeat:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存配置文件并启动 Filebeat：
- en: '[PRE9]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that this setup is strictly for Linux systems. On Mac or Windows, the situation
    is slightly more complicated given the fact that Docker runs in a VM on both systems
    and, as such, accessing the Docker logs that live inside this VM is slightly more
    involved. Please consult the documentation if you want to install Filebeat natively
    on your Mac or Windows machine as this is outside the scope of this book.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种配置仅适用于 Linux 系统。在 Mac 或 Windows 上，由于 Docker 在两个系统上都运行在虚拟机中，因此访问这个虚拟机中的
    Docker 日志稍微复杂一些。如果你希望在 Mac 或 Windows 机器上本地安装 Filebeat，请查阅相关文档，因为这超出了本书的范围。
- en: Alternatively, we can run Filebeat in a container, side by side with the ELK
    Stack.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以将 Filebeat 运行在容器中，与 ELK Stack 并行使用。
- en: 'Here is a complete Docker Compose file that will run the ELK Stack and Filebeat
    on a Linux computer:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个完整的 Docker Compose 文件，将在 Linux 计算机上运行 ELK Stack 和 Filebeat：
- en: '![Figure 12.2 – Docker Compose file for the ELK Stack and Filebeat](img/B19199_12_02.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2 – ELK Stack 和 Filebeat 的 Docker Compose 文件](img/B19199_12_02.jpg)'
- en: Figure 12.2 – Docker Compose file for the ELK Stack and Filebeat
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – ELK Stack 和 Filebeat 的 Docker Compose 文件
- en: Now that we have learned how to run Filebeat on a Linux computer or server,
    we want to show how Filebeat can be used on a Mac or Windows computer, which is
    important during development.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何在 Linux 计算机或服务器上运行 Filebeat，接下来我们想展示如何在 Mac 或 Windows 计算机上使用 Filebeat，这在开发过程中非常重要。
- en: Running the sample on a Mac or Windows computer
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 Mac 或 Windows 计算机上运行示例
- en: The preceding example will not run on a Mac or Windows computer since Docker
    is transparently running inside a VM and thus the Docker log files will not be
    found at `/var/lib/docker/containers`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例无法在 Mac 或 Windows 计算机上运行，因为 Docker 是透明地运行在虚拟机中，因此 Docker 日志文件将无法在 `/var/lib/docker/containers`
    中找到。
- en: 'We can navigate around this problem by using a workaround: we can configure
    all our containers to write their respective logs into a file that is part of
    a Docker volume. Then, we can mount that volume into the Filebeat container instead
    of what we did on line 44 of the preceding Docker Compose file.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一种变通方法来解决这个问题：我们可以配置所有容器将各自的日志写入一个属于 Docker 卷的文件中。然后，我们可以将这个卷挂载到 Filebeat
    容器中，而不是在前面的 Docker Compose 文件的第 44 行做的那样。
- en: 'Here is a sample that uses a simple Node.js/Express.js application to demonstrate
    this. Please follow these steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，使用一个简单的 Node.js/Express.js 应用程序来演示这个过程。请按照以下步骤操作：
- en: Create a folder called `mac-or-windows` in your `ch12` chapter folder.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `ch12` 章节文件夹中创建一个名为 `mac-or-windows` 的文件夹。
- en: Inside this folder, create a subfolder called `app` and navigate to it.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个文件夹内，创建一个名为 `app` 的子文件夹，并进入该文件夹。
- en: 'Inside the `app` folder, initialize the Node.js application with the following
    command:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `app` 文件夹内，使用以下命令初始化 Node.js 应用程序：
- en: '[PRE10]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Accept all the defaults.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接受所有默认设置。
- en: 'Install Express.js with the following command:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装 Express.js：
- en: '[PRE11]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Modify the `package.json` file and add a script called `start` with its value
    set to `node index.js`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 `package.json` 文件，并添加一个名为 `start` 的脚本，值设置为 `node index.js`。
- en: 'Add a file called `index.js` to the folder with the following content:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向文件夹中添加一个名为 `index.js` 的文件，内容如下：
- en: '![Figure 12.3 – The index.js application file](img/B19199_12_03.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3 – index.js 应用文件](img/B19199_12_03.jpg)'
- en: Figure 12.3 – The index.js application file
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – index.js 应用文件
- en: This simple Express.js application has two routes, `/` and `/test`. It also
    has middleware to log incoming requests and logs when handling specific routes
    or a `404 Not` `Found` error.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的 Express.js 应用程序有两个路由，`/` 和 `/test`。它还包含中间件，用于记录传入的请求，并在处理特定路由或出现 `404
    Not` `Found` 错误时记录日志。
- en: 'Add a script file called `entrypoint.sh` to the folder with this content:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向文件夹中添加一个名为 `entrypoint.sh` 的脚本文件，内容如下：
- en: '![Figure 12.4 – The entrypoint.sh file for the sample application](img/B19199_12_04.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – 示例应用的 entrypoint.sh 文件](img/B19199_12_04.jpg)'
- en: Figure 12.4 – The entrypoint.sh file for the sample application
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 示例应用的 entrypoint.sh 文件
- en: This script will be used to run our sample application and redirect its logs
    to the specified `LOGGING_FILE`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本将用于运行我们的示例应用程序，并将其日志重定向到指定的 `LOGGING_FILE`。
- en: 'Make the preceding file executable with the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令将前面的文件设为可执行文件：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Add a Dockerfile to the folder with this content:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向文件夹中添加一个 Dockerfile，内容如下：
- en: '![Figure 12.5 – The Dockerfile for the sample application](img/B19199_12_05.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – 示例应用的 Dockerfile](img/B19199_12_05.jpg)'
- en: Figure 12.5 – The Dockerfile for the sample application
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – 示例应用的 Dockerfile
- en: 'Add a file called `docker-compose.yml` to the `mac-or-windows` folder with
    this content:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 `mac-or-windows` 文件夹中添加一个名为 `docker-compose.yml` 的文件，内容如下：
- en: '![Figure 12.6 – Docker Compose file for the Mac or Windows use case](img/B19199_12_06.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.6 – Mac 或 Windows 使用场景的 Docker Compose 文件](img/B19199_12_06.jpg)'
- en: Figure 12.6 – Docker Compose file for the Mac or Windows use case
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 – Mac 或 Windows 使用场景的 Docker Compose 文件
- en: Note the environment variable on line 9, which defines the name and location
    of the log file generated by the Node.js/Express.js application. Also, note the
    volume mapping on line 11, which will make sure the log file is funneled to the
    Docker `app_logs` volume. This volume is then mounted to the `filebeat` container
    on line 25\. This way, we make sure Filebeat can collect the logs and forward
    them to Kibana.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意第 9 行中的环境变量，它定义了由 Node.js/Express.js 应用生成的日志文件的名称和位置。还请注意第 11 行中的卷映射，这将确保日志文件被导入到
    Docker 的 `app_logs` 卷中。然后，这个卷会挂载到第 25 行的 `filebeat` 容器中。通过这种方式，我们确保 Filebeat 能够收集日志并将其转发到
    Kibana。
- en: 'Also, add a file called `filebeat.yml` to the `mac-or-windows` folder that
    contains the following configuration for Filebeat:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，向 `mac-or-windows` 文件夹中添加一个名为 `filebeat.yml` 的文件，包含以下 Filebeat 配置：
- en: '![Figure 12.7 – Configuration for Filebeat on Mac or Windows](img/B19199_12_07.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.7 – Mac 或 Windows 上 Filebeat 配置](img/B19199_12_07.jpg)'
- en: Figure 12.7 – Configuration for Filebeat on Mac or Windows
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7 – Mac 或 Windows 上的 Filebeat 配置
- en: 'From within the folder where the `docker-compose.yml` file is located, build
    the Node.js application image with the following command:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `docker-compose.yml` 文件所在的文件夹内，使用以下命令构建 Node.js 应用镜像：
- en: '[PRE13]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, you are ready to run the stack, like so:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你已经准备好运行整个栈了，像这样：
- en: '[PRE14]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Use a REST client to access the `http://localhost:3000` and `http://localhost:3000/test`
    endpoints a few times to have the application generate a few log outputs.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 REST 客户端访问 `http://localhost:3000` 和 `http://localhost:3000/test` 端点几次，以使应用生成一些日志输出。
- en: Now, we are ready to explore the collected logs centrally in Kibana.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好在 Kibana 中集中查看收集到的日志了。
- en: Step 3 – visualizing logs in Kibana
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 3 步 – 在 Kibana 中可视化日志
- en: Access the Kibana dashboard through a web browser at `http://localhost:5601`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Web 浏览器访问 Kibana 仪表板，网址为 `http://localhost:5601`。
- en: For more details, refer to the *Querying a centralized log* section later in
    this chapter. Here is a quick rundown.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多详细信息，请参阅本章后面关于 *查询集中日志* 部分的内容。这里是一个简要概述。
- en: Go to the `filebeat-*`) to start analyzing the collected logs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 `filebeat-*`）开始分析收集到的日志。
- en: Navigate to the **Discover** section to search, filter, and visualize the logs
    from your Docker containers.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 **Discover** 部分，搜索、筛选并可视化来自 Docker 容器的日志。
- en: 'Once you have configured your Kibana dashboard, you should see something like
    this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好 Kibana 仪表板后，你应该会看到如下内容：
- en: '![Figure 12.8 – Application logs in Kibana provided by Filebeat](img/B19199_12_08.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.8 – Kibana 中由 Filebeat 提供的应用日志](img/B19199_12_08.jpg)'
- en: Figure 12.8 – Application logs in Kibana provided by Filebeat
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8 – 由 Filebeat 提供的 Kibana 中的应用日志
- en: By following these steps, you’ll have a centralized log management system that
    aggregates logs from multiple Docker containers, allowing you to analyze and monitor
    your containerized applications efficiently. Note that there are other log management
    systems and log shippers available, such as Splunk, Graylog, and Fluentd. The
    process of setting up these systems will be similar but may require different
    configuration steps.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤，你将拥有一个集中式日志管理系统，能够汇总来自多个 Docker 容器的日志，帮助你高效地分析和监控容器化应用程序。需要注意的是，还有其他日志管理系统和日志传输工具，如
    Splunk、Graylog 和 Fluentd，设置这些系统的过程类似，但可能需要不同的配置步骤。
- en: Setting up log filtering and alerting mechanisms
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置日志过滤和警报机制
- en: Setting up log filtering and alerting mechanisms helps you focus on important
    log messages, minimize noise, and respond to potential issues proactively. Here,
    we will use the ELK Stack along with the ElastAlert plugin to demonstrate log
    filtering and alerting.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 设置日志过滤和警报机制有助于你集中精力处理重要的日志信息，减少噪音，并主动响应潜在问题。在这里，我们将使用 ELK Stack 配合 ElastAlert
    插件来演示日志过滤和警报。
- en: Step 1 – setting up the Elastic Stack
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 1 步 – 设置 Elastic Stack
- en: First, follow the instructions provided in the *Setting up the ELK Stack* section
    to set up the Elastic Stack for centralized logging. This includes running Elasticsearch,
    Logstash, and Kibana in Docker containers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，按照 *设置 ELK Stack* 部分提供的说明，设置 Elastic Stack 进行集中式日志记录。这包括在 Docker 容器中运行 Elasticsearch、Logstash
    和 Kibana。
- en: Step 2 – setting up log filtering with Logstash
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 2 步 – 使用 Logstash 设置日志过滤
- en: 'Configure Logstash to filter logs based on specific conditions, such as log
    levels, keywords, or patterns. Update your `logstash.conf` file with appropriate
    filters in the `filter` section. For example, to filter logs based on log level,
    you can use the following configuration:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Logstash 根据特定条件（如日志级别、关键词或模式）过滤日志。更新你的 `logstash.conf` 文件，在 `filter` 部分添加适当的过滤器。例如，要根据日志级别过滤日志，你可以使用以下配置：
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This configuration checks whether the log level is `ERROR` and adds a tag of
    `error` to the log event. Restart the Logstash container to apply the new configuration:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置检查日志级别是否为 `ERROR`，并将 `error` 标签添加到日志事件中。重启 Logstash 容器以应用新配置：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Step 3 – setting up ElastAlert for alerting
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 3 步 – 配置 ElastAlert 以进行警报
- en: 'ElastAlert is a simple framework for alerting anomalies, spikes, or other patterns
    of interest found in data stored in Elasticsearch. Let’s set it up:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ElastAlert 是一个简单的框架，用于警报在 Elasticsearch 存储的数据中发现的异常、峰值或其他感兴趣的模式。让我们来设置它：
- en: 'Clone the ElastAlert repository and navigate to the ElastAlert directory:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆 ElastAlert 仓库并导航到 ElastAlert 目录：
- en: '[PRE17]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Install ElastAlert:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 ElastAlert：
- en: '[PRE18]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a configuration file for ElastAlert, `config.yaml`, and update it with
    the following contents:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 ElastAlert 创建一个配置文件 `config.yaml`，并使用以下内容更新它：
- en: '[PRE19]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create a `rules` directory and define your alerting rules. For example, to
    create an alert for logs with the `error` tag, create a file called `error_logs.yaml`
    in the `rules` directory with the following contents:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `rules` 目录，并定义你的警报规则。例如，要为带有 `error` 标签的日志创建警报，可以在 `rules` 目录中创建一个名为 `error_logs.yaml`
    的文件，内容如下：
- en: '[PRE20]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This rule triggers an email alert if there is at least one log event with the
    `error` tag within a 1-minute timeframe.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则会在 1 分钟内，如果至少有一个带有 `error` 标签的日志事件，触发邮件警报。
- en: 'Start ElastAlert:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 ElastAlert：
- en: '[PRE21]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, ElastAlert will monitor the Elasticsearch data based on your defined rules
    and send alerts when the conditions are met.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，ElastAlert 将根据你定义的规则监控 Elasticsearch 数据，并在满足条件时发送警报。
- en: Step 4 – monitoring and responding to alerts
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 4 步 – 监控和响应警报
- en: With log filtering and alerting mechanisms in place, you can focus on critical
    log messages and respond to potential issues proactively. Monitor your email or
    other configured notification channels for alerts and investigate the root causes
    to improve your application’s reliability and performance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好日志过滤和警报机制后，你可以集中精力处理关键日志信息，并主动响应潜在问题。监控你的电子邮件或其他配置的通知渠道，接收警报并调查根本原因，以提高应用程序的可靠性和性能。
- en: Keep refining your Logstash filters and ElastAlert rules to minimize noise,
    detect important log patterns, and respond to potential issues more effectively.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不断完善你的 Logstash 过滤器和 ElastAlert 规则，以减少噪音，检测重要的日志模式，并更有效地响应潜在问题。
- en: In the next section, we will discuss how to ship Docker daemon logs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何传输 Docker 守护进程日志。
- en: Shipping Docker daemon logs
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传输 Docker 守护进程日志
- en: Docker daemon logs pertain to the overall functioning of the Docker platform.
    The Docker daemon is responsible for managing all Docker containers, and its logs
    record system-wide events and messages. These logs help in identifying issues
    related to the Docker daemon itself, such as networking problems, resource allocation
    errors, and container orchestration challenges.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 守护进程日志涉及 Docker 平台的整体功能。Docker 守护进程负责管理所有 Docker 容器，其日志记录了系统范围的事件和消息。这些日志有助于识别与
    Docker 守护进程本身相关的问题，如网络问题、资源分配错误和容器编排挑战。
- en: Depending on the operating system, the location and configuration of Docker
    daemon logs may differ. For instance, on a Linux system, daemon logs are usually
    found in `/var/log/docker.log`, while on Windows, they are located in `%programdata%\docker\logs\daemon.log`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 根据操作系统的不同，Docker 守护进程日志的位置和配置可能有所不同。例如，在 Linux 系统上，守护进程日志通常位于`/var/log/docker.log`，而在
    Windows 系统上，它们位于`%programdata%\docker\logs\daemon.log`。
- en: Note
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Daemon logs on Mac will be covered in the next section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Mac 上的守护进程日志将在下一节中介绍。
- en: 'To effectively manage Docker daemon logs, consider the following best practices:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效管理 Docker 守护进程日志，可以考虑以下最佳实践：
- en: Regularly review daemon logs to identify potential issues and anomalies
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期查看守护进程日志，以识别潜在问题和异常
- en: Set up log rotation and retention policies to manage disk space usage
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置日志轮换和保留策略以管理磁盘空间使用
- en: Use a log management system to centralize and analyze logs for better visibility
    into the overall Docker environment
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用日志管理系统集中管理并分析日志，以更好地查看整体 Docker 环境。
- en: In conclusion, both shipping containers and Docker daemon logs play vital roles
    in monitoring and maintaining a healthy Docker environment. By effectively managing
    these logs, system administrators and developers can ensure optimal performance,
    minimize downtime, and resolve issues promptly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，运输容器和 Docker 守护进程日志在监控和维护健康的 Docker 环境中起着至关重要的作用。通过有效地管理这些日志，系统管理员和开发人员可以确保最佳性能，最小化停机时间，并及时解决问题。
- en: Docker daemon logs on Mac
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Mac 上的 Docker 守护进程日志
- en: 'On a Mac with Docker Desktop installed, you can view the Docker daemon logs
    using the `log stream` command provided by the macOS log utility. Follow these
    steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了 Docker Desktop 的 Mac 上，你可以使用 macOS 日志工具提供的`log stream`命令查看 Docker 守护进程日志。按照以下步骤操作：
- en: Open the Terminal application.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端应用程序。
- en: 'Run the following command:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令：
- en: '[PRE22]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This command will display a real-time stream of logs related to Docker Desktop,
    including the Docker daemon logs. You can stop the stream by pressing *Ctrl* +
    *C*.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将显示与 Docker Desktop 相关的日志的实时流，包括 Docker 守护进程日志。你可以通过按*Ctrl* + *C*来停止日志流。
- en: 'Alternatively, you can use the following command to view the Docker daemon
    logs in a file format:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，你可以使用以下命令以文件格式查看 Docker 守护进程日志：
- en: '[PRE23]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This command will create a file named `docker_daemon_logs.log` in the current
    directory, containing the Docker daemon logs from the last 1 day. You can change
    the `--last 1d` option to specify a different time range (for example, `--last
    2h` for the last 2 hours). Open the `docker_daemon_logs.log` file with any text
    editor to view the logs.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将在当前目录创建一个名为`docker_daemon_logs.log`的文件，文件包含过去 1 天的 Docker 守护进程日志。你可以更改`--last
    1d`选项来指定不同的时间范围（例如，`--last 2h`表示过去 2 小时）。使用任何文本编辑器打开`docker_daemon_logs.log`文件以查看日志。
- en: Please note that you may need administrator privileges to execute these commands.
    If you encounter permission issues, prepend the commands with `sudo`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，执行这些命令可能需要管理员权限。如果遇到权限问题，请在命令前加上`sudo`。
- en: Docker daemon logs on a Windows computer
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Windows 计算机上的 Docker 守护进程日志
- en: 'On a Windows 11 machine with Docker Desktop installed, the Docker daemon logs
    are stored as text files. You can access these logs by following these steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了 Docker Desktop 的 Windows 11 机器上，Docker 守护进程日志以文本文件的形式存储。你可以通过以下步骤访问这些日志：
- en: Open File Explorer.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开文件资源管理器。
- en: 'Navigate to the following directory:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到以下目录：
- en: '[PRE24]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this directory, you’ll find the `DockerDesktopVM.log` file, which contains
    the Docker daemon logs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在该目录中，你将找到包含 Docker 守护进程日志的`DockerDesktopVM.log`文件。
- en: Open the `DockerDesktopVM.log` file with any text editor to view the logs.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用任何文本编辑器打开`DockerDesktopVM.log`文件以查看日志。
- en: Please note that the `C:\ProgramData` folder might be hidden by default. To
    display hidden folders in File Explorer, click on the **View** tab and check the
    **Hidden** **items** checkbox.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`C:\ProgramData`文件夹可能默认是隐藏的。要在文件资源管理器中显示隐藏的文件夹，请点击**查看**选项卡并勾选**隐藏的项目**复选框。
- en: 'Alternatively, you can use PowerShell to read the logs:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，您可以使用 PowerShell 阅读日志：
- en: Open PowerShell.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 PowerShell。
- en: 'Execute the following command:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令：
- en: '[PRE25]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This command will display the last 50 lines of the Docker daemon log file. You
    can change the number after `-Tail` to display a different number of lines.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将显示 Docker 守护进程日志文件的最后 50 行。您可以更改`-Tail`后的数字来显示不同数量的行。
- en: Next, we are going to learn how to query a centralized log.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何查询集中式日志。
- en: Querying a centralized log
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询集中式日志
- en: Once your containerized application logs have been collected and stored in the
    ELK Stack, you can query the centralized logs using Elasticsearch's Query **Domain
    Specific Language** (**DSL**) and visualize the results in Kibana.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的容器化应用程序日志被收集并存储在 ELK Stack 中，您就可以使用 Elasticsearch 的查询**领域特定语言**（**DSL**）查询集中式日志，并在
    Kibana 中可视化结果。
- en: Step 1 – accessing Kibana
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步 - 访问 Kibana
- en: Kibana provides a user-friendly interface for querying and visualizing Elasticsearch
    data. In the provided `docker-compose.yml` file, Kibana can be accessed on port
    `5601`. Open your browser and navigate to `http://localhost:5601`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 提供了一个用户友好的界面来查询和可视化 Elasticsearch 数据。在提供的 `docker-compose.yml` 文件中，Kibana
    可以通过端口 `5601` 进行访问。打开您的浏览器并导航到 `http://localhost:5601`。
- en: Step 2 – setting up an index pattern
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步 - 设置索引模式
- en: 'Before you can query the logs, you need to create an index pattern in Kibana
    to identify the Elasticsearch indices containing the log data. Follow these steps
    to create an index pattern:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询日志之前，您需要在 Kibana 中创建一个索引模式，以识别包含日志数据的 Elasticsearch 索引。按照以下步骤创建索引模式：
- en: The first time you access Kibana, you will be asked to add integrations. You
    can safely ignore this as we are using Filebeat to ship the logs.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一次访问 Kibana 时，系统会要求您添加集成。由于我们使用 Filebeat 来发送日志，因此可以安全地忽略此请求。
- en: Instead, locate the “hamburger menu” in the top left of the view and click it.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相反，请在视图的左上角找到“汉堡菜单”，并点击它。
- en: 'Locate the **Management** tab in the left-hand navigation menu and select **Stack
    Management**:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航菜单中找到**管理**选项卡并选择**堆栈管理**：
- en: '![Figure 12.9 – The Management tab in Kibana](img/B19199_12_09.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.9 - Kibana 中的管理选项卡](img/B19199_12_09.jpg)'
- en: Figure 12.9 – The Management tab in Kibana
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9 - Kibana 中的管理选项卡
- en: 'Under the **Kibana** section, click **Index Patterns**:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**Kibana**部分，点击**索引模式**：
- en: '![Figure 12.10 – The Index Patterns entry of Kibana](img/B19199_12_10.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.10 - Kibana 的索引模式条目](img/B19199_12_10.jpg)'
- en: Figure 12.10 – The Index Patterns entry of Kibana
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10 - Kibana 的索引模式条目
- en: Click the **Create index** **pattern** button.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建索引** **模式**按钮。
- en: Enter the index pattern that matches your Logstash indices. For example, if
    your Logstash configuration uses the `logstash-%{+YYYY.MM.dd}` index pattern,
    enter `logstash-*` in the **Name** field.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入与您的 Logstash 索引匹配的索引模式。例如，如果您的 Logstash 配置使用`logstash-%{+YYYY.MM.dd}`索引模式，请在**名称**字段中输入`logstash-*`。
- en: In the `@``timestamp` field.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`@``timestamp`字段中。
- en: Click **Create** **index pattern**.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建** **索引模式**。
- en: Now, we are ready to query our container logs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好查询我们的容器日志。
- en: Step 3 – querying the logs in Kibana
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步 - 在 Kibana 中查询日志
- en: 'Now, you’re ready to query the logs using Kibana’s **Discover** feature. Follow
    these steps:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经准备好使用 Kibana 的**发现**功能来查询日志。按照以下步骤操作：
- en: Once again, locate the “hamburger menu” in the top left of the view and click
    it.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次在视图的左上角找到“汉堡菜单”，并点击它。
- en: Locate the **Analytics** tab and select **Discover**.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到**分析**选项卡并选择**发现**。
- en: Select the index pattern you created earlier from the drop-down menu in the
    top-left corner.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左上角的下拉菜单中选择您之前创建的索引模式。
- en: Use the time filter in the top-right corner to choose a specific time range
    for your query.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用右上角的时间过滤器选择一个特定的时间范围进行查询。
- en: To search for specific log entries, enter your query in the search bar and press
    *Enter*. Kibana uses the Elasticsearch Query DSL to perform searches.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要搜索特定的日志条目，请在搜索框中输入查询并按*Enter*。Kibana 使用 Elasticsearch 查询 DSL 执行搜索。
- en: 'Here are some example queries:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例查询：
- en: 'To find logs containing the word “error”: `error`'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查找包含“error”一词的日志：`error`
- en: 'To find logs with a specific field value: `container.name: "my-container"`'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '要查找具有特定字段值的日志：`container.name: "my-container"`'
- en: 'To use a wildcard search (for example, logs with a `container.name` starting
    with “`app`”): `container.name: "app*"`'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '要使用通配符搜索（例如，查找以“`app`”开头的`container.name`日志）：`container.name: "app*"`'
- en: 'To use Boolean operators for more complex queries: `error` AND `container.name:
    "my-container"`'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '要使用布尔运算符进行更复杂的查询：`error` AND `container.name: "my-container"`'
- en: Step 4 – visualizing the logs
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 可视化日志
- en: 'You can create visualizations and dashboards in Kibana to analyze the logs
    more effectively. To create a visualization, follow these steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Kibana中创建可视化和仪表盘，以更有效地分析日志。要创建可视化，请按以下步骤操作：
- en: Click on the **Visualize** tab in the left-hand navigation menu.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧导航菜单中的**可视化**选项卡。
- en: Click the **Create** **visualization** button.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建** **可视化**按钮。
- en: Choose a visualization type (for example, pie chart, bar chart, line chart,
    and so on).
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个可视化类型（例如，饼图、条形图、折线图等）。
- en: Select the index pattern you created earlier.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您之前创建的索引模式。
- en: Configure the visualization by selecting the fields and aggregation types.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择字段和聚合类型来配置可视化。
- en: Click **Save** to save your visualization.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存**以保存您的可视化。
- en: 'You can create multiple visualizations and add them to a dashboard for a comprehensive
    view of your log data. To create a dashboard, do the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建多个可视化并将它们添加到仪表盘，以全面查看您的日志数据。要创建一个仪表盘，请执行以下操作：
- en: Click on the **Dashboard** tab in the left-hand navigation menu.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧导航菜单中的**仪表盘**选项卡。
- en: Click the **Create** **dashboard** button.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建** **仪表盘**按钮。
- en: Click **Add** to add visualizations to the dashboard.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加**以将可视化添加到仪表盘。
- en: Resize and rearrange the visualizations as needed.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要调整可视化的大小和重新排列位置。
- en: Click **Save** to save your dashboard.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存**以保存您的仪表盘。
- en: Now, you have a centralized view of your containerized application logs and
    you can query, analyze, and visualize the logs using Kibana.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以集中查看容器化应用程序的日志，并可以使用Kibana查询、分析和可视化这些日志。
- en: In the following section, we will learn how to collect and scrape metrics exposed
    by Docker and your application.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习如何收集和抓取Docker和您的应用程序暴露的指标。
- en: Collecting and scraping metrics
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集和抓取指标
- en: To collect and scrape metrics from containers running on a system with Docker
    Desktop installed, you can use Prometheus and **Container Advisor** (**cAdvisor**).
    Prometheus is a powerful open source monitoring and alerting toolkit, while cAdvisor
    provides container users with an understanding of the resource usage and performance
    characteristics of their running containers.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要从运行在安装了Docker Desktop的系统上的容器中收集和抓取指标，您可以使用Prometheus和**容器顾问**（**cAdvisor**）。Prometheus是一个强大的开源监控和告警工具集，而cAdvisor为容器用户提供有关其运行容器的资源使用情况和性能特征的理解。
- en: In this section, we’ll provide a step-by-step guide to setting up Prometheus
    and cAdvisor to collect and scrape metrics from containers running on Docker Desktop.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供逐步指南，帮助您设置Prometheus和cAdvisor，从容器中收集和抓取指标。
- en: Step 1 – running cAdvisor in a Docker container
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 在Docker容器中运行cAdvisor
- en: 'cAdvisor is a Google-developed tool that collects, processes, and exports container
    metrics. Let’s take a look:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: cAdvisor是一个由Google开发的工具，用于收集、处理和导出容器指标。让我们来看看：
- en: 'In the chapter folder, `ch12`, create a new subfolder called `metrics`:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在章节文件夹`ch12`中，创建一个名为`metrics`的新子文件夹：
- en: '[PRE26]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this folder, create a file called `docker-compose.yml` and add the following
    snippet to it:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此文件夹中，创建一个名为`docker-compose.yml`的文件，并将以下代码片段添加到其中：
- en: '[PRE27]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Run cAdvisor in a Docker container using the following command:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在Docker容器中运行cAdvisor：
- en: '[PRE28]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Replace `v0.45.0` with the latest cAdvisor version available on the cAdvisor
    repository.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 将`v0.45.0`替换为cAdvisor仓库中最新的版本。
- en: This command mounts the necessary directories from the host system and exposes
    cAdvisor’s web interface on port `8080`.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令挂载主机系统所需的目录，并在端口`8080`上暴露cAdvisor的Web界面。
- en: Attention
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A version lower than the one shown here will not run, for example, on a Mac
    with an M1 or M2 processor.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 版本低于此处显示的版本将无法运行，例如，在配备M1或M2处理器的Mac上。
- en: You can access the cAdvisor web interface by navigating to `http://localhost:8080`
    in your browser.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过在浏览器中导航到`http://localhost:8080`访问cAdvisor的Web界面。
- en: Step 2 – setting up and running Prometheus
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 设置并运行Prometheus
- en: 'Next, let’s set up Prometheus using the following step-by-step instructions:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们按照以下逐步说明设置Prometheus：
- en: Create a subfolder called `prometheus` in the `metrics` folder.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`metrics`文件夹中创建一个名为`prometheus`的子文件夹。
- en: 'In this new folder, create a `prometheus.yml` configuration file with the following
    contents:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个新文件夹中，创建一个名为`prometheus.yml`的配置文件，内容如下：
- en: '[PRE29]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This configuration specifies the global scrape interval and two scrape jobs:
    one for Prometheus itself and another for cAdvisor running on port `8080`.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置指定了全局抓取间隔和两个抓取作业：一个用于Prometheus本身，另一个用于运行在端口`8080`上的cAdvisor。
- en: 'Add the following snippet to the end of the `docker-compose.yml` file:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`docker-compose.yml`文件的末尾添加以下片段：
- en: '[PRE30]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This instruction mounts the `prometheus.yml` configuration file and exposes
    Prometheus on port `9090`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 此指令挂载了`prometheus.yml`配置文件，并在端口`9090`上公开了Prometheus。
- en: 'The preceding `prometheus` service uses a volume called `prometheus_data`.
    To define this, please add the following two lines to the end of the `docker-compose.yml`
    file:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述的`prometheus`服务使用了名为`prometheus_data`的卷。要定义这一点，请将以下两行添加到`docker-compose.yml`文件的末尾：
- en: '[PRE31]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You can access the Prometheus web interface by navigating to `http://localhost:9090`
    in your browser.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过浏览器访问`http://localhost:9090`来访问Prometheus Web界面。
- en: 'Once Prometheus is up and running, you can verify that it’s successfully scraping
    metrics from cAdvisor:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Prometheus启动并运行，您可以验证它是否成功从cAdvisor获取指标：
- en: Open the Prometheus web interface at `http://localhost:9090`.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`http://localhost:9090`打开Prometheus Web界面。
- en: Click on **Status** in the top navigation bar, then select **Targets**.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在顶部导航栏中点击**状态**，然后选择**目标**。
- en: Ensure that both the `prometheus` and `cadvisor` targets are listed with `UP`.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保`prometheus`和`cadvisor`目标都列为`UP`。
- en: 'Now, Prometheus can collect and store metrics from the containers running on
    your Docker Desktop system. You can use Prometheus’ built-in expression browser
    to query metrics or set up Grafana for advanced visualization and dashboarding:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Prometheus可以收集和存储运行在您的Docker Desktop系统上的容器的指标。您可以使用Prometheus内置的表达式浏览器查询指标或设置Grafana进行高级可视化和仪表板：
- en: In the `query text` field, enter something like `container_start_time_seconds`
    to get the value for the startup time of all containers.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`query text`字段中输入类似`container_start_time_seconds`的内容，以获取所有容器的启动时间值。
- en: To refine the query and only get the value for the cAdvisor container, enter
    `container_start_time_seconds{job="cadvisor"}`.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要细化查询并仅获取cAdvisor容器的值，请输入`container_start_time_seconds{job="cadvisor"}`。
- en: Note that in the `query text` field, you get IntelliSense, which is convenient
    when you do not remember all the details of a command and its parameters.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在`query text`字段中，您可以获得智能感知（IntelliSense），当您不记得命令及其参数的所有细节时，这非常方便。
- en: 'Before you continue, stop cAdvisor and Prometheus with the following command:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请使用以下命令停止cAdvisor和Prometheus：
- en: '[PRE32]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the last section of this chapter, you will learn how to monitor a containerized
    application using a tool such as Grafana.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节，您将学习如何使用Grafana等工具监控容器化应用程序。
- en: Monitoring a containerized application
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控容器化应用程序
- en: Monitoring a containerized application is crucial for understanding the application’s
    performance, resource usage, and potential bottlenecks. This section will detail
    a step-by-step process for monitoring a containerized application using Prometheus,
    Grafana, and cAdvisor.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 监控容器化应用程序对理解应用程序的性能、资源使用情况和潜在瓶颈至关重要。本节将详细介绍使用Prometheus、Grafana和cAdvisor监控容器化应用程序的逐步过程。
- en: Step 1 – setting up Prometheus
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 – 设置Prometheus
- en: Follow the instructions from the previous section to set up Prometheus and cAdvisor
    to collect and scrape metrics from containers running on Docker Desktop.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上一节的说明设置Prometheus和cAdvisor，以从运行在Docker Desktop上的容器中收集和抓取指标。
- en: Step 2 – instrumenting your application with Prometheus metrics
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步 – 使用Prometheus指标为您的应用程序进行仪表化
- en: To monitor a containerized application, you need to instrument the application
    with Prometheus metrics. This involves adding Prometheus client libraries to your
    application code and exposing metrics on an HTTP endpoint, usually `/metrics`.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控容器化应用程序，您需要使用Prometheus指标为应用程序进行仪表化。这涉及向应用程序代码添加Prometheus客户端库，并在HTTP端点（通常为`/metrics`）上公开指标。
- en: 'Choose the appropriate Prometheus client library for your application’s programming
    language from the official list: [https://prometheus.io/docs/instrumenting/clientlibs/](https://prometheus.io/docs/instrumenting/clientlibs/).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 从官方列表[https://prometheus.io/docs/instrumenting/clientlibs/](https://prometheus.io/docs/instrumenting/clientlibs/)中选择适合您应用程序编程语言的适当Prometheus客户端库。
- en: Add the library to your application while following the library’s documentation
    and examples.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循库的文档和示例时，将库添加到您的应用程序中。
- en: Expose the `/metrics` endpoint, which will be scraped by Prometheus.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 暴露`/metrics`端点，这将由Prometheus进行抓取。
- en: Example using Kotlin and Spring Boot
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Kotlin和Spring Boot的示例
- en: 'To expose Prometheus metrics from a Kotlin and Spring Boot API, you need to
    follow these steps:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Kotlin和Spring Boot API暴露Prometheus指标，您需要遵循以下步骤：
- en: Create a new Kotlin Spring Boot project.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的Kotlin Spring Boot项目。
- en: Add the necessary dependencies.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加必要的依赖项。
- en: Implement the API and expose Prometheus metrics.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现API并暴露Prometheus指标。
- en: Expose the actuator endpoints.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暴露actuator端点。
- en: Create a Dockerfile.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Dockerfile。
- en: Integrate with the Docker Compose file.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与Docker Compose文件集成。
- en: Step 1 – creating a new Kotlin Spring Boot project
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤1 – 创建一个新的Kotlin Spring Boot项目
- en: You can use Spring Initializr ([https://start.spring.io/](https://start.spring.io/))
    to create a new Kotlin Spring Boot project. Name the artifact `kotlin-api`. Then,
    select Kotlin as the language, choose the packaging type (JAR or WAR), and add
    the necessary dependencies. For this example, select **Web**, **Actuator**, and
    **Prometheus** under the **Dependencies** section.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Spring Initializr（[https://start.spring.io/](https://start.spring.io/)）来创建一个新的Kotlin
    Spring Boot项目。将构件命名为`kotlin-api`，然后选择Kotlin作为语言，选择打包类型（JAR或WAR），并添加必要的依赖项。对于此示例，在**依赖项**部分选择**Web**、**Actuator**和**Prometheus**。
- en: Download the generated project and extract it.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 下载生成的项目并解压缩。
- en: Step 2 – verifying the necessary dependencies
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤2 – 验证必要的依赖项
- en: 'In your `build.gradle.kts` file, assert that the following dependencies are
    included:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的`build.gradle.kts`文件中，确保包含以下依赖项：
- en: '[PRE33]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Step 3 – implementing the API and exposing Prometheus metrics
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤3 – 实现API并暴露Prometheus指标
- en: 'Locate the Kotlin `KotlinApiApplication.kt` file in the `src/main/kotlin/com/example/kotlinapi/`
    subfolder and replace its existing content with the following:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 定位到`src/main/kotlin/com/example/kotlinapi/`子文件夹中的Kotlin `KotlinApiApplication.kt`文件，并将其现有内容替换为以下内容：
- en: '![Figure 12.11 – Code in the KotlinApiApplication.kt file](img/B19199_12_11.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图12.11 – KotlinApiApplication.kt文件中的代码](img/B19199_12_11.jpg)'
- en: Figure 12.11 – Code in the KotlinApiApplication.kt file
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11 – KotlinApiApplication.kt文件中的代码
- en: You can also find this code in the `sample-solutions/ch12/kotlin-api` subfolder
    if you prefer not to type the example yourself.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想自己输入示例代码，也可以在`sample-solutions/ch12/kotlin-api`子文件夹中找到这段代码。
- en: In this example, a simple REST API with a single endpoint, `/`, was implemented.
    The endpoint increments a counter and exposes the count as a Prometheus metric
    named `api_requests_total`.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，实现了一个简单的REST API，只有一个端点`/`。该端点递增计数器并将计数作为Prometheus指标`api_requests_total`暴露。
- en: 'Add the following line to the `application.properties` file to use a different
    port than the default port, `8080`, which is already taken by cAdvisor in our
    stack. In our example, the port is `7000`:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下行添加到`application.properties`文件，以使用不同于默认端口`8080`的端口，`8080`端口已被我们堆栈中的cAdvisor占用。在我们的示例中，端口为`7000`：
- en: '[PRE34]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Step 4 – exposing metrics
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤4 – 暴露指标
- en: 'Add the following line to the `application.properties` file:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下行添加到`application.properties`文件：
- en: '[PRE35]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The above configuration should be all on a single line. It is shown on two lines
    here due to space limitations.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置应全部放在一行上。由于空间限制，这里显示为两行。
- en: This will expose the respective metrics on the `/actuator/health`, `/actuator/info`,
    `/actuator/metrics`, and `/``actuator/prometheus` endpoints.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这将暴露相应的指标，在`/actuator/health`、`/actuator/info`、`/actuator/metrics`和`/actuator/prometheus`端点上。
- en: Step 5 – creating a Dockerfile
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤5 – 创建Dockerfile
- en: 'Create a `multistage` Dockerfile in the project’s root directory with the following
    content:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目根目录中创建一个`multistage` Dockerfile，内容如下：
- en: '![Figure 12.12 – Dockerfile for the Kotlin API](img/B19199_12_12.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图12.12 – Kotlin API的Dockerfile](img/B19199_12_12.jpg)'
- en: Figure 12.12 – Dockerfile for the Kotlin API
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12 – Kotlin API的Dockerfile
- en: 'In this `multistage` Dockerfile, we have two stages:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`multistage` Dockerfile中，我们有两个阶段：
- en: '`gradle:jdk17` base image to build the Kotlin Spring Boot application. It sets
    the working directory, copies the source code, and runs the Gradle `build` command.
    This stage is named `build` using the `AS` keyword.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`gradle:jdk17`基础镜像来构建Kotlin Spring Boot应用程序。它设置工作目录，复制源代码，并运行Gradle `build`命令。此阶段使用`AS`关键字命名为`build`。
- en: '`openjdk:17-oracle` base image for the runtime environment, which is a smaller
    image without the JDK. It copies the built JAR file from the build stage and sets
    the entry point to run the Spring Boot application.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openjdk:17-oracle`基础镜像用于运行时环境，它是一个没有JDK的较小镜像。它从构建阶段复制构建的JAR文件，并将入口点设置为运行Spring
    Boot应用程序。'
- en: This multi-stage Dockerfile allows you to build the Kotlin Spring Boot application
    and create the final runtime image in one go. It also helps reduce the final image
    size by excluding unnecessary build tools and artifacts.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多阶段的Dockerfile允许你一次性构建Kotlin Spring Boot应用程序并创建最终的运行时镜像。它还通过排除不必要的构建工具和工件，帮助减少最终镜像的大小。
- en: Step 6 – integrating with the Docker Compose file
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第六步 – 与Docker Compose文件集成
- en: 'Update your existing `docker-compose.yml` file so that it includes the Kotlin
    Spring Boot API service, which resides in the `kotlin-api` subfolder:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 更新你现有的`docker-compose.yml`文件，以便它包含Kotlin Spring Boot API服务，该服务位于`kotlin-api`子文件夹中：
- en: '[PRE36]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, you can run `docker compose up -d` to build and start the Kotlin Spring
    Boot API service, along with the other services. The API will be accessible on
    port `8080`, and the Prometheus metrics can be collected.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以运行`docker compose up -d`来构建并启动Kotlin Spring Boot API服务以及其他服务。API将通过`8080`端口访问，Prometheus的度量标准可以被收集。
- en: Next, we will configure Prometheus to scrape all the metrics from our setup,
    including the Kotlin API we just created.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置Prometheus以抓取我们设置中的所有度量数据，包括我们刚创建的Kotlin API。
- en: Step 3 – configuring Prometheus to scrape your application metrics
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步 – 配置Prometheus抓取你的应用程序指标
- en: 'Update your `prometheus.yml` configuration file from the previous section so
    that it includes a new scrape job for your application. For example, since our
    Kotlin API sample application is running in a Docker container and exposing metrics
    on port `7000`, we will add the following to the `scrape_configs` section:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 更新你在前一部分中提到的`prometheus.yml`配置文件，以便它包括一个新的抓取任务，针对你的应用程序。例如，由于我们的Kotlin API示例应用程序在Docker容器中运行并在`7000`端口暴露度量标准，我们将以下内容添加到`scrape_configs`部分：
- en: '[PRE37]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Step 4 – setting up Grafana for visualization
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步 – 设置Grafana进行可视化
- en: 'Grafana is a popular open source visualization and analytics tool that can
    integrate with Prometheus to create interactive dashboards for your containerized
    application:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana是一个流行的开源可视化和分析工具，可以与Prometheus集成，创建适用于你的容器化应用程序的交互式仪表板：
- en: 'To the `docker-compose.yml` from the previous section, add this snippet to
    define a service for Grafana:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前一部分的`docker-compose.yml`中，添加以下代码段以定义Grafana服务：
- en: '[PRE38]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the `volumes:` section, add a volume called `grafana_data`.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`volumes:`部分，添加一个名为`grafana_data`的卷。
- en: 'Run cAdvisor, Prometheus, and Grafana with this command:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行cAdvisor、Prometheus和Grafana：
- en: '[PRE39]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Access Grafana by navigating to `http://localhost:3000` in your browser. The
    default username is `admin` and the default password is also `admin`.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在浏览器中导航到`http://localhost:3000`，你可以访问Grafana。默认的用户名是`admin`，默认的密码也是`admin`。
- en: Add **Prometheus** as a data source.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加**Prometheus**作为数据源。
- en: Click on the gear icon (**Configuration**) in the left sidebar.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧边栏中的齿轮图标（**Configuration**）。
- en: Select **Data Sources** and then click **Add** **data source**.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**Data Sources**，然后点击**Add** **data source**。
- en: Choose `http://host.docker.internal:9090` as the URL.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`http://host.docker.internal:9090`作为URL。
- en: Click **Save & Test** to verify the connection.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**Save & Test**以验证连接。
- en: Create a dashboard and panels to visualize your application metrics.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建仪表板和面板以可视化你的应用程序指标。
- en: Click on the **+** icon (**Create**) in the left sidebar and choose **Dashboard**.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧边栏中的**+**图标（**Create**），然后选择**Dashboard**。
- en: Click **Add new panel** to start creating panels for your metrics.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**Add new panel**开始为你的度量数据创建面板。
- en: Use the query editor to build queries based on your application metrics, and
    customize the visualization type, appearance, and other settings.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用查询编辑器基于你的应用程序指标构建查询，并自定义可视化类型、外观和其他设置。
- en: Save the dashboard by clicking the disk icon in the top-right corner.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击右上角的磁盘图标保存仪表板。
- en: With Grafana, you can create interactive dashboards that provide real-time insights
    into your containerized application’s performance, resource usage, and other critical
    metrics.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Grafana，你可以创建交互式仪表板，提供容器化应用程序的实时性能、资源使用情况和其他关键指标的洞察。
- en: Step 5 – setting up alerting (optional)
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五步 – 设置告警（可选）
- en: 'Grafana and Prometheus can be used to set up alerts based on your application
    metrics. This can help you proactively address issues before they impact your
    users:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana和Prometheus可以根据你的应用程序指标设置告警。这可以帮助你在问题影响用户之前主动处理问题：
- en: In Grafana, create a new panel or edit an existing one.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Grafana中，创建一个新面板或编辑现有面板。
- en: Switch to the **Alert** tab in the panel editor.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在面板编辑器中切换到**Alert**标签页。
- en: Click **Create Alert** and configure the alerting rules, conditions, and notification
    settings.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建警报**并配置警报规则、条件和通知设置。
- en: Save the panel and dashboard.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存面板和仪表板。
- en: 'You may also need to configure Grafana’s notification channels to send alerts
    via email, Slack, PagerDuty, or other supported services. To do this, follow these
    steps:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还需要配置 Grafana 的通知渠道，通过电子邮件、Slack、PagerDuty 或其他支持的服务发送警报。要做到这一点，请按照以下步骤操作：
- en: In Grafana, click on the bell icon (**Alerting**) in the left sidebar.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Grafana 中，点击左侧边栏的铃铛图标（**警报**）。
- en: Choose **Notification channels** and click **Add channel**.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**通知渠道**并点击**添加渠道**。
- en: Fill in the required information for your preferred notification service and
    click **Save**.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填入您偏好的通知服务所需的信息，然后点击**保存**。
- en: Now, when the alerting conditions specified in your panel are met, Grafana will
    send notifications through the configured channel.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当您的面板中指定的警报条件满足时，Grafana 将通过配置的渠道发送通知。
- en: Step 6 – monitoring your containerized application
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 6 步 – 监控您的容器化应用程序
- en: With Prometheus, Grafana, and cAdvisor set up, you can now effectively monitor
    your containerized application. Keep an eye on your Grafana dashboards, set up
    appropriate alerting rules, and use the collected data to identify performance
    bottlenecks, optimize resource usage, and improve the overall health of your application.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 配置了 Prometheus、Grafana 和 cAdvisor 后，您现在可以有效地监控您的容器化应用程序。请密切关注您的 Grafana 仪表板，设置适当的警报规则，并利用收集的数据识别性能瓶颈，优化资源使用，并改善应用程序的整体健康状况。
- en: Remember to continuously iterate and improve your monitoring setup by refining
    your application’s instrumentation, adjusting alerting rules, and adding new visualizations
    to your dashboards as your application evolves and grows.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 记得通过不断完善您的监控设置来持续迭代和改进，精炼应用程序的仪表化，调整警报规则，并随着应用程序的发展和增长，向仪表板中添加新的可视化内容。
- en: Summary
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned why logging and shipping the log to a central location
    is important. We then showed you how to set up an ELK Stack locally on our computer
    that can serve as a hub for logs. We generated a special version of this stack,
    including Filebeat, which can run on a Mac or Windows computer using the workaround
    of redirecting the standard log output to a file whose parent folder is mapped
    to a Docker volume. This volume is then mounted to Filebeat, which, in turn, forwards
    the logs to ElasticSearch. On a production or production-like system, the applications
    run on Linux servers or VMs and thus Filebeat can directly collect the logs from
    the default location, where Docker directs the logs at `/var/lib/docker/containers`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了为什么记录日志并将其发送到中央位置是很重要的。接着我们展示了如何在本地计算机上设置 ELK Stack，它可以作为日志的集线器。我们生成了这个堆栈的一个特殊版本，其中包括
    Filebeat，它可以通过重定向标准日志输出到一个文件并将其父文件夹映射到 Docker 卷，进而在 Mac 或 Windows 计算机上运行。在生产或类生产系统中，应用程序运行在
    Linux 服务器或虚拟机上，因此 Filebeat 可以直接从 Docker 将日志收集到默认位置 `/var/lib/docker/containers`。
- en: We also learned how to use Prometheus and Grafana to scrape, collect, and display
    the metrics of your applications centrally on a dashboard. We used a simple Kotlin
    application that exposed a counter to demonstrate this.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了如何使用 Prometheus 和 Grafana 来抓取、收集并集中显示您应用程序的指标，并在仪表板上展示这些数据。我们使用了一个简单的
    Kotlin 应用程序，暴露了一个计数器来演示这一过程。
- en: Lastly, we briefly mentioned how to define alerts based on the values of collected
    metrics.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要提到了如何根据收集的指标值定义警报。
- en: In the next chapter, we will introduce the concept of container orchestrators.
    It will teach us why orchestrators are needed, and how they work conceptually.
    The chapter will also provide an overview of the most popular orchestrators and
    list a few of their pros and cons.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍容器编排器的概念。它将教我们为什么需要编排器，以及编排器的工作原理。该章节还将概述最流行的编排器，并列出它们的一些优缺点。
- en: Questions
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Here are a few questions that you should try to answer to self-assess your
    learning progress:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个问题，您应该尝试回答它们以自我评估您的学习进度：
- en: What are Docker container logs, and why are they important?
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker 容器日志是什么？它们为什么重要？
- en: What is a daemon log in Docker, and how is it different from a container log?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker 中的守护进程日志是什么？它与容器日志有何不同？
- en: How can you monitor Docker containers?
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何监控 Docker 容器？
- en: How can you view the logs of a running Docker container?
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何查看正在运行的 Docker 容器的日志？
- en: What are some best practices for logging and monitoring Docker containers?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于记录和监控 Docker 容器，有哪些最佳实践？
- en: How can you collect logs from multiple Docker containers?
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何从多个 Docker 容器收集日志？
- en: Answers
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: 'Here are some sample answers to the questions for this chapter:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是本章问题的一些示例答案：
- en: Docker container logs are records of the events and messages generated by the
    applications running within a container. They are essential for monitoring performance,
    troubleshooting issues, and ensuring the smooth operation of the applications
    deployed in Docker containers.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker 容器日志是由容器内运行的应用程序生成的事件和消息记录。它们对于监控性能、故障排除问题以及确保 Docker 容器中部署的应用程序平稳运行至关重要。
- en: A daemon log in Docker refers to the log files generated by the Docker daemon,
    which manages Docker containers. These logs record system-wide events and messages
    related to the overall functioning of the Docker platform. In contrast, container
    logs are specific to individual containers and their applications.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker 中的守护程序日志指的是由管理 Docker 容器的 Docker 守护程序生成的日志文件。这些日志记录了与 Docker 平台整体功能相关的系统范围事件和消息。相比之下，容器日志是针对单个容器及其应用程序的特定日志。
- en: Monitoring Docker containers can be done using various methods, including command-line
    tools such as docker stats, third-party monitoring solutions such as Prometheus,
    and Docker’s built-in APIs. These tools help track resource usage, performance
    metrics, and the health status of containers.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过多种方法监控 Docker 容器，包括命令行工具如 docker stats、第三方监控解决方案如 Prometheus，以及 Docker 的内置
    API。这些工具帮助跟踪资源使用情况、性能指标和容器的健康状态。
- en: You can view the logs of a running Docker container using the `docker logs`
    command, followed by the container’s ID or name. This command retrieves the log
    messages generated by the container, which can help diagnose issues or monitor
    the container’s activities.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用 `docker logs` 命令查看运行中 Docker 容器的日志，后面跟上容器的 ID 或名称。该命令检索容器生成的日志消息，有助于诊断问题或监视容器的活动。
- en: 'Some best practices for logging and monitoring Docker containers include the
    following:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录和监控 Docker 容器的一些最佳实践包括以下几点：
- en: Centralize logs using a log management system
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用日志管理系统集中日志
- en: Configure log rotation and retention policies
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置日志轮转和保留策略
- en: Set up log filtering and alerting mechanisms
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置日志过滤和警报机制
- en: Monitor containers using a combination of built-in and third-party tools
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置和第三方工具组合监控容器
- en: Regularly review logs and metrics for anomalies
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期检查异常的日志和指标
- en: To collect logs from multiple Docker containers, you can use a log management
    system such as the ELK Stack or Splunk. You can also use tools such as Fluentd
    or Logspout to aggregate and forward logs from all containers to a centralized
    log management system for analysis and visualization.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从多个 Docker 容器收集日志，您可以使用日志管理系统，如 ELK Stack 或 Splunk。您还可以使用 Fluentd 或 Logspout
    等工具，将所有容器的日志聚合并转发到集中的日志管理系统，进行分析和可视化。
