<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Optimize for Scale and Cost"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Optimize for Scale and Cost</h1></div></div></div><p>On the subject of optimization, we shall start from the top, that is to say the earliest stage: the design stage.</p><p>Imagine iterating over your architecture plan time and time again, until you have convinced yourself and your colleagues that this is the best you can do with the information available at that time. Now imagine that, unless you have a very unusual use case, other people have already done similar iterations and have kindly shared the outcome.</p><p>Back to reality and fortunately, we were not far off. There is indeed a collective AWS knowledge base in the form of blog posts, case studies, and white papers available to anybody embarking on their first cloud deployment.</p><p>We are going to take a distilled sample of that knowledge base and apply it to a common architecture example, in an attempt to achieve maximum scalability, whilst remaining cost efficient.</p><p>The example is going to be one of a typical frontend (NGINX nodes), backend (DB cluster) and a storage layer deployment within a VPC:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_001.jpg" alt="Optimize for Scale and Cost"/></div><p>
</p><p>Whilst, technically, our whole deployment is on the Internet, the visual segregation above is to emphasize the network isolation properties of a VPC.</p><div class="section" title="Architectural considerations"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec29"/>Architectural considerations</h1></div></div></div><p>Let us now examine this deployment one component at a time, starting with the VPC itself.</p><div class="section" title="The VPC"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec53"/>The VPC</h2></div></div></div><p>I am proceeding under the assumption that if you are still holding this book, you have likely accepted the way of the VPC.</p><div class="section" title="CIDR"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec31"/>CIDR</h3></div></div></div><p>How many VPCs are you foreseeing having? Would they be linked (VPC peering) or would you be bridging other networks in (VPN)?</p><p>The answers to these questions play a role when choosing the CIDR for a VPC. As a general rule it is recommended to avoid common (household router) network addresses such as <code class="literal">192.168.1.0</code> or <code class="literal">10.0.0.0</code>.</p><p>Keep track of and assign different CIDRs if you have more than one VPC, even if you don't have an immediate need to peer them.</p><p>Consider a CIDR that will allow for large enough subnets to accommodate potential instance scaling with minimal fragmentation (number of subnets).</p></div><div class="section" title="Subnets and Availability Zones"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec32"/>Subnets and Availability Zones</h3></div></div></div><p>
<span class="strong"><strong>Availability Zones</strong></span> (<span class="strong"><strong>AZs</strong></span>) are how we add resilience to a deployment, so we should have at least two of those. There might be configurations in which you have to use three, for example where a cluster quorum is needed, such as <span class="strong"><strong>ZooKeeper</strong></span>. In that case, it is advisable to keep quorum members in separate zones in order to handle network partitions better. To accommodate this and to keep charges low, we could create subnets in three zones, deploy quorum clusters in all three, and other components (say <span class="strong"><strong>NGINX</strong></span> hosts) in only two of those.</p><p>Let us illustrate an example where we have a Zookeeper and a web server (NGINX) component within our VPC. We have decided to use three AZs and maintain two sets of subnets: <span class="strong"><strong>public</strong></span> and <span class="strong"><strong>private</strong></span>. The former routing through the IGW, the latter via NAT:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_002.jpg" alt="Subnets and Availability Zones"/></div><p>
</p><p>Here we have the ELB spanning across all three AZs and public subnets respectively. In the private subnet space, we find two web servers plus our cluster of three ZooKeeper nodes giving us a good balance of resilience at optimal cost.</p></div><div class="section" title="VPC limits"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec33"/>VPC limits</h3></div></div></div><p>AWS enforces certain initial limits on every account, which might catch you by surprise when your environment starts scaling up. Important ones to check are: <span class="strong"><strong>Instances</strong></span>, <span class="strong"><strong>EBS</strong></span> and <span class="strong"><strong>Networking</strong></span> limits found on the <span class="strong"><strong>EC2 dashboard</strong></span>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_003.jpg" alt="VPC limits"/></div><p>
</p><p>When requesting an increase, select a number that is high enough to provide a buffer for scaling, but not inadequately high as after all the limits are there to protect against accidental/erroneous overprovisioning.</p></div></div></div></div>
<div class="section" title="The frontend layer"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec30"/>The frontend layer</h1></div></div></div><p>With the subnets in place, we can start thinking about our VPC inhabitants.</p><p>The frontend or application layer consists of our Auto Scaling Groups and the first decision that we'll face would be that of an EC2 instance type.</p><p>The profile of the frontend application would very much dictate the choice between a memory, compute or a storage optimized instance. With some help from fellow developers (in the case of an in-house application) and a suitable performance testing tool (or service) you should be able to ascertain which system resource does the given application make most use of.</p><p>Let us assume we have picked the <span class="strong"><strong>C4 Compute Optimized</strong></span> instance class which AWS suggests for webservers. The next question will be - what size?</p><p>Well, one way to guess our way through, is to take the average number of requests per second that we would like to be able to support, deploy the minimum number of instances we can afford (two for resilience) of the smallest size available in the chosen class and run a load test against them. Ideally the average utilization across the two nodes would remain under 50% to allow for traffic spikes and events of failure where the remaining host takes all the load. If the results are far below that mark, then we should look for a different class with smaller instance types for better value. Otherwise we keep increasing the C4 size.</p><p>Next comes the question of Auto Scaling. We have the right class and instance size to work with, and now we need scaling thresholds. Firstly, if you are fortunate enough to have predicable loads, then your problems end here with the use of <span class="strong"><strong>Scheduled Actions</strong></span>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_004.jpg" alt="The frontend layer"/></div><p>
</p><p>You can simply tell AWS scale me up at <span class="emphasis"><em>X</em></span> o'clock then back down at <span class="emphasis"><em>Y</em></span>. The rest of us, we have to set alarms and thresholds.</p><p>We've already decided that a 50% average utilization (let us say CPU) is our upper limit and by that time we should already have scaling in progress. Otherwise, if one of our two nodes fails, at that rate the other one will have to work at maximum capacity. As an example a <span class="strong"><strong>CloudWatch</strong></span> alarm could be &gt;40% average CPU used for five minutes, triggering an Auto Scaling Group action to increase the group size by 50% (which is one instance).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip77"/>Tip</h3><p>In order to prevent unnecessary scaling events, it is important to adjust the value of the <span class="strong"><strong>Cooldown period</strong></span>. It should reflect the expected time a newly launched instance will take to become fully operational and start affecting the <span class="strong"><strong>CloudWatch</strong></span> metric.</p></div></div><p>For even finer control over how Auto Scaling reacts to the alarm we could use Step Scaling (ref: <a class="ulink" href="http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html">http://docs.aws.amazon.com/autoscaling/latest/userguide/as-scale-based-on-demand.html</a>). <span class="strong"><strong>Step Adjustments</strong></span> allow for a varied response based on the severity of the threshold breach. For example, if the load increases from 40% to 50%, then scale up with only a single instance, but if the hop is from 40% to 70%, go straight to two or more.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip78"/>Tip</h3><p>With Step Scaling the <span class="strong"><strong>Cooldown period</strong></span> is set via the <span class="strong"><strong>Instance Warmup</strong></span> option.</p></div></div><p>While we aim to scale up relatively quickly to prevent any service disruption, scaling down should be timely to save hourly charges, but not premature which could cause a scaling loop.</p><p>The <span class="strong"><strong>CloudWatch</strong></span> alarm for scaling down should act over a much longer period of time than the five minutes we observed earlier. Also the gap between the threshold for scaling up and the one for scaling down should be wide enough not to have instances launch, only to be terminated shortly after.</p><p>EC2 Instance utilization is just one example of a trigger; it is also worth considering ELB metrics such as sum of total request, non-2XX responses or response latency. If you choose to use any of those, ensure that your scale down alarms react to the <span class="strong"><strong>INSUFFICIENT_DATA</strong></span> state which is observed during periods of no traffic (perhaps late at night).</p></div>
<div class="section" title="The backend layer"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec31"/>The backend layer</h1></div></div></div><p>Behind the application we are likely to find a database cluster of some sort. For this example, we have chosen RDS (MySQL/PostgreSQL). However, the scaling and resilience ideas can be easily translated to suit a custom DB cluster on EC2 instances.</p><p>Starting with high-availability, in terms of RDS, the feature is called a <span class="strong"><strong>Multi-AZ</strong></span> deployment. This gives us a Primary RDS instance with a hot <span class="strong"><strong>STANDBY</strong></span> replica as a failover solution. Unfortunately, the Standby cannot be used for anything else, that is to say we cannot have it, for example, serving read-only queries.</p><p>A Multi-AZ setup within our VPC would look like this:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_005.jpg" alt="The backend layer"/></div><p>
</p><p>In the case of a <span class="strong"><strong>PRIMARY</strong></span> outage, RDS automatically fails over to the <span class="strong"><strong>STANDBY</strong></span>, updating relevant DNS records in the process. According to the documentation, a typical failover takes one to two minutes.</p><p>The triggers include the Primary becoming unavailable (thus failing AWS health-checks), a complete AZ outage, or a user interruption such as an RDS instance reboot.</p><p>So far, with Multi-AZ we have a reasonably resilient, but perhaps not very scalable setup. In a busy environment it is common to dedicate a primary DB node for writes, while reading is done off of replicas. The inexpensive option would be to add a single replica to our current arrangement:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_006.jpg" alt="The backend layer"/></div><p>
</p><p>Here we write to <span class="strong"><strong>PRIMARY</strong></span> and read from <span class="strong"><strong>REPLICA</strong></span>, or for read-intensive applications reads can go to both.</p><p>If our budget allows, we can take this a step further and provide a <span class="strong"><strong>REPLICA</strong></span> in both subnets in which we deploy frontend/application nodes:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_007.jpg" alt="The backend layer"/></div><p>
</p><p>Latency across AWS zones is already pretty low, but with such a per-zone RDS distribution, we reduce it even further. All hosts would write to the <span class="strong"><strong>PRIMARY</strong></span>. However they can assign a higher priority to their local (same zone) <span class="strong"><strong>REPLICA</strong></span> when reading.</p><p>And since we are on a spending spree, additional RDS performance boost can be gained with provisioned IOPS. This is something to consider if you are running a heavy workload and in need of high RDS Storage I/O.</p><p>Although indirectly, caching is another very effective way to increase RDS scalability by alleviating the load.</p><p>Popular software choices here are <span class="strong"><strong>Memcached</strong></span> and <span class="strong"><strong>Redis</strong></span>. Either is simple to setup locally (on each application host). If you would like to benefit from a shared cache then you could run a cluster on EC2 or use the AWS managed ElastiCache service. With the latter, we can have again a <span class="strong"><strong>Multi-AZ</strong></span> configuration plus multiple replicas for resilience and low-latency:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_008.jpg" alt="The backend layer"/></div><p>
</p><p>You will notice that the failover scenario differs from RDS in that there is no standby instance. In the event of a <span class="strong"><strong>PRIMARY</strong></span> failure <span class="strong"><strong>ELASTICACHE</strong></span> promotes the most up-to-date <span class="strong"><strong>REPLICA</strong></span> instead.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip79"/>Tip</h3><p>Note that after the promotion the <span class="strong"><strong>PRIMARY</strong></span> endpoint remains the same, however the promoted Replica's address changes.</p></div></div></div>
<div class="section" title="The object storage layer"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec32"/>The object storage layer</h1></div></div></div><p>In the effort of achieving effortless scalability, we must put emphasis on building stateless applications where possible. Not keeping state on our application nodes would mean storing any valuable data away from them. A classic example is <span class="strong"><strong>WordPress</strong></span>, where user uploads are usually kept locally, making it difficult to scale such a setup horizontally.</p><p>While it is possible to have a shared file system across your EC2 instances using <span class="strong"><strong>Elastic File System</strong></span> (<span class="strong"><strong>EFS</strong></span>), for reliability and scalability we are much better off using an object storage solution such as <span class="strong"><strong>AWS S3</strong></span>.</p><p>It is fair to say that accessing S3 objects is not as trivial as working with an EFS volume, however the AWS tools and SDKs lower the barrier considerably. For easy experimenting, you could start with the S3 CLI. Eventually you would want to build S3 capabilities into your application using one of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Java/.NET/PHP/Python/Ruby or other SDKs (ref: <a class="ulink" href="https://aws.amazon.com/tools/">https://aws.amazon.com/tools/</a>)</li><li class="listitem" style="list-style-type: disc">REST API (ref: <a class="ulink" href="http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAPI.html</a>)</li></ul></div><p>In previous chapters we examined IAM Roles as a convenient way of granting S3 bucket access to EC2 instances. We can also enhance the connectivity between those instances and S3 using VPC Endpoints:</p><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p>
<span class="emphasis"><em>A VPC endpoint enables you to create a private connection between your VPC and another AWS service without requiring access over the Internet, through a NAT device, a VPN connection, or AWS Direct Connect. Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components that allow communication between instances in your VPC and AWS services without imposing availability risks or bandwidth constraints on your network traffic.
</em></span>
</p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<span class="attribution"><a class="ulink" href="http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html</a></span></td></tr></table></div><p>If you have clients in a different geographic location uploading content to your bucket, then S3 transfer acceleration (ref: <a class="ulink" href="http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html">http://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a>) can be used to improve their experience. It is simply a matter of clicking <span class="strong"><strong>Enable</strong></span> on the bucket's settings page:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_009.jpg" alt="The object storage layer"/></div><p>
</p><p>We have now covered speed improvements; scalability comes built into the S3 service itself and for cost optimization we have the different storage classes.</p><p>S3 currently supports four types (classes) of storage. The most expensive and most durable being the <span class="strong"><strong>Standard class</strong></span>, which is also the default. This is followed by the <span class="strong"><strong>Infrequent Access class (Standard_IA)</strong></span> which is cheaper, however keep in mind that it is indeed intended for rarely accessed objects otherwise the associated retrieval cost would be prohibitive. Next is the <span class="strong"><strong>Reduced Redundancy class</strong></span> which, despite the scary name, is still pretty durable at 99.99%. And lastly, comes the <span class="strong"><strong>Glacier storage class</strong></span> which is akin to a tape backup in that objects are archived and there is a 3-5 hour retrieval time (with 1-5 minute urgent retrievals available at a higher cost).</p><p>You can specify the storage class (except for Glacier) of an object at time of upload or change it retrospectively using the AWS console, CLI or SDK. Archiving to Glacier is done using a bucket lifecycle policy (bucket's settings page):</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_010.jpg" alt="The object storage layer"/></div><p>
</p><p>We need to add a new rule, describing the conditions under which an object gets archived:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_011.jpg" alt="The object storage layer"/></div><p>
</p><p>Incidentally, Lifecycle rules can also help you clean up old files.</p><div class="section" title="The load balancing layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec54"/>The load balancing layer</h2></div></div></div><p>The days of the <span class="emphasis"><em>Wild Wild West</em></span> when one used to setup web servers with public IPs and DNS round-robin have faded away and the load balancer has taken over.</p><p>We are going to look at the AWS ELB service, but this is certainly not the only available option. As a matter of fact, if your use case is highly sensitive to latency or you observe frequent, short lived traffic surges then you might want to consider rolling your own EC2 fleet of load balancing nodes using NGINX or HAProxy.</p><p>The ELB service is priced at a flat per-hour fee plus bandwidth charges, so perhaps not much we can do to reduce costs, but we can explore ways of boosting performance.</p></div><div class="section" title="Cross-zone load balancing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec55"/>Cross-zone load balancing</h2></div></div></div><p>Under normal conditions, a Classic ELB would deploy its nodes within the zones which our backend (application) instances occupy and forward traffic according to those zones. That is to say, the ELB node in zone <span class="strong"><strong>A</strong></span> will talk to the backend instance in the same zone, and the same principle applies for zone <span class="strong"><strong>B</strong></span>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_012.jpg" alt="Cross-zone load balancing"/></div><p>
</p><p>This is sensible as it clearly ensures lowest latency, but there are a couple of things to note:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An equal number of backend nodes should be maintained in each zone for best load spread</li><li class="listitem" style="list-style-type: disc">Clients caching the IP address for an ELB node would stick to the respective backend instance</li></ul></div><p>To improve the situation at the expense of some (minimal) added latency, we can enable <span class="strong"><strong>Cross-Zone Load Balancing</strong></span> in the Classic ELB's properties:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_013.jpg" alt="Cross-zone load balancing"/></div><p>
</p><p>This will change the traffic distribution policy, so that requests to a given ELB node will be evenly spread across all registered (status: InService) backend instances, changing our earlier diagram to this:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_014.jpg" alt="Cross-zone load balancing"/></div><p>
</p><p>An unequal number of backend nodes per zone would no longer have an impact on load balancing, nor would external parties targeting a single ELB instance.</p></div><div class="section" title="ELB pre-warming"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec56"/>ELB pre-warming</h2></div></div></div><p>An important aspect of the ELB service is that it runs across a cluster of EC2 instances of a given type, very much like our backend nodes. With that in mind, it should not come as a surprise that ELB scales based on demand, again much like our Auto Scaling Group does.</p><p>This is all very well when incoming traffic fluctuates within certain boundaries, so that it can be absorbed by the ELB or increases gradually, allowing enough time for the ELB to scale and accommodate. However, sharp surges can result in ELB dropping connections if large enough.</p><p>This can be prevented with a technique called <span class="strong"><strong>pre-warming</strong></span> or essentially scaling up an ELB ahead of anticipated traffic spikes. Currently this is not something that can be done at the user end, meaning you would need to contact AWS Support with an ELB pre-warming request.</p></div><div class="section" title="The CDN layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec57"/>The CDN layer</h2></div></div></div><p>
<span class="strong"><strong>CloudFront</strong></span> or AWS's CDN solution is yet another method of improving the performance of the ELB and S3 services. If you are not familiar with CDN networks, those, generally speaking, provide faster access to any clients you might have in a different geographic location from your deployment location. In addition, a CDN would also cache data so that subsequent requests won't even reach your server (also called <span class="strong"><strong>origin</strong></span>) greatly reducing load.</p><p>So, given our VPC deployment in the US, if we were to setup a <span class="strong"><strong>CloudFront distribution</strong></span> in front of our ELB and/or S3 bucket, then requests from clients originating in say Europe would be routed to the nearest <span class="emphasis"><em>European CloudFront Point-of-Presence</em></span> which in turn would either serve a cached response or fetch the requested data from the ELB/S3 over a high-speed, internal AWS network.</p><p>To setup a basic <span class="strong"><strong>web distribution</strong></span> we can use the <span class="strong"><strong>CloudFront dashboard</strong></span>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_015.jpg" alt="The CDN layer"/></div><p>
</p><p>Once we <span class="strong"><strong>Get Started</strong></span> then the second page presents the distribution properties:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_016.jpg" alt="The CDN layer"/></div><p>
</p><p>Conveniently, resources within the same AWS account are suggested. The origin is the source of data that CloudFront needs to talk to, for example the ELB sitting in front of our application. In the <span class="strong"><strong>Alternate Domain Names</strong></span> field we would enter our website address (say <code class="literal">www.example.org</code>), the rest of the settings can remain with their defaults for now.</p><p>After the distribution becomes active all that is left to do is to update the DNS record for <code class="literal">www.example.org</code> currently pointing at the ELB to point to the distribution address instead.</p></div><div class="section" title="Spot instances"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec58"/>Spot instances</h2></div></div></div><p>Our last point is on making further EC2 cost savings using <span class="strong"><strong>Spot</strong></span> instances. These represent unused resources across the EC2 platform, which users can bid on at any given time. Once a user has placed a winning bid and has been allocated the EC2 instance, it remains theirs for as long as the current Spot price stays below their bid, else it gets terminated (a notice is served via the instance meta-data, ref: <a class="ulink" href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html</a>).</p><p>These conditions make Spot instances ideal for workflows, where the job start time is flexible and any tasks can be safely resumed in case of instance termination. For example, one can run short-lived Jenkins jobs on Spot instances (there is even a plugin for this) or use it to run a workflow which performs a series of small tasks that save state regularly to S3/RDS.</p></div><div class="section" title="AWS Calculators"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec59"/>AWS Calculators</h2></div></div></div><p>Lastly, a simple yet helpful tool to give you an idea of how much your planned deployment would cost: <a class="ulink" href="http://calculator.s3.amazonaws.com/index.html">http://calculator.s3.amazonaws.com/index.html</a> (remember to untick the <span class="strong"><strong>FREE USAGE TIER</strong></span> near the top of the page)</p><p>And if you were trying to compare the cost of on-premise to cloud, then this might be of interest: <a class="ulink" href="https://aws.amazon.com/tco-calculator/">https://aws.amazon.com/tco-calculator/</a>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec33"/>Summary</h1></div></div></div><p>In this chapter we examined different ways in which to optimize both the scalability and running costs of an AWS deployment.</p><p>We started with the underlying VPC and its core properties such as the CIDR, subnets and how to plan for growth. We covered methods of improving the performance of the frontend, backend, storage and load balancing components. Then we looked at AWS Spot instances as a very cost efficient solution for executing lower-priority, batch processing jobs.</p><p>In the next chapter we move into the realm of security and explore the topic of how to better harden an AWS environment.</p></div></body></html>