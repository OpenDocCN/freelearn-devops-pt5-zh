- en: Big Data Hadoop Ecosystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed the key concepts of **big data technologies** in the preceding chapters.
    In this chapter, we will cover the building of big data clusters, frameworks,
    key components, and the architecture of popular vendors. We will discuss big data
    DevOps concepts in successive chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Big data Hadoop ecosystems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types and application
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master, worker, edge nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Their roles
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloudera CDH Hadoop distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hortonworks Data Platform** (**HDP**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: MapR Hadoop distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivotal big data suite
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM open platform
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based Hadoop distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Elastic MapReduce
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Azure's HDInsight
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Capacity planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data Hadoop ecosystems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Hadoop is an open source software platform built from commodity hardware
    and used to scale clusters up to terabytes or petabytes for big data, spanning
    across thousands of servers. It is highly popular and efficient for distributed
    data storage and distributed processing of very large datasets. Hadoop offers
    a full scale of services such as data persistence, data processing, data access,
    data governance, data security, and operations. A few of the benefits associated
    with Hadoop clusters are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data scalability**:Big data volumes can grow exponentially to accommodate
    these large data volumes, Hadoop enables distributed processing of data; each
    node in the data cluster participates in storing, managing, processing, and analyzing
    data. The addition of nodes enables quick scaling of clusters to store data at
    a scale of petabytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data reliability**: Hadoop cluster configurations provide data redundancy.
    For example, in case of accidental failure of one or more nodes, Hadoop cluster
    management software will automatically replicate the data and processing to the
    rest of the active nodes. Thus business continuity is assured with the application
    and data functional, even during the non-availability of a few nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data** **flexibility**: In traditional relational database management systems,
    schema tables are created before storing structured data into the system commonly
    referred to as *schema on write*. Based on the processing application data requirements,
    diverse data formats can be loaded into Hadoop systems such as structured, semi-structured,
    or unstructured data. Hence, the schema is dynamically created during the data
    load, and referred to as *schema on read*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Economical**:Hadoop is open source and built on low-cost commodity hardware,
    and hence, is more economical than proprietary licensed software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizations adopt the Hadoop system for its versatility and its ability to
    persist on large data volumes, managing, visualizing and analyzing vast amounts
    of data quickly, reliably, efficiently, and with a low cost for a variety of data
    formats, with data governance, workflow, security, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Inbuilt tools and capabilities in Hadoop ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hadoop ecosystem offers many inbuilt tools, features, and capabilities, listed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/160dc7ec-fd30-448f-a257-4579441c6e09.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Data storage**: The **Hadoop Distributed File System** (**HDFS**) provides
    scalable, fault-tolerant, cost-efficient storage. Hadoop can handle exponential
    data growth by distributing storage across many nodes; the combined storage capacity
    can grow with demand while remaining economical per unit of storage. There are
    other storage managers, such as HBase, Solr, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data lake**: One of the key strengths of Hadoop is its ability to build a
    data lake economically. It will be a valuable asset for an organization to store
    all their relevant data needs, gathered and consolidated from various data sources.
    For example, in the manufacturing industry the machine maintenance data, inventory
    data, sales data, machine sensor data on performance, social media data on customer
    feedback, vendors and suppliers data, weather reports, and so on can be captured
    regularly as per the requirements of the data lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data processing**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop ecosystem**: Hadoop ecosystem offers data processing in batch, stream,
    and hybrid systems.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MapReduce**: MapReduce is an initial processing framework for batch jobs
    in Hadoop. MapReduce''s processing technique follows the map, shuffle and reduce
    algorithm using key-value pairs. Batch jobs are like monthly telephone invoices
    for customers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream processing**: Like stock price information and airline reservation
    data, Apache storm is well suited for processing data in streams.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid processing systems**: These processing frameworks can handle both
    batch and stream workloads, such as Apache Spark, Flink, and so on. One use case
    is the **Internet of Things** (**IoT**), truck sensor data capture, aggregation
    in the cloud and analytics to derive patterns, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data access**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop**: Hadoop offers multiple ways of accessing and processing the data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Solr**: Apache Solr provides indexing and search capabilities for
    the data stored in HDFS.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hive**: Hive provides data warehouse functionality for Hadoop with a simple
    SQL-like language called **HiveQL** that provides indexes, making querying faster.
    A standard SQL programming interface can be used and provides better integration
    with a few analytics packages such as Tableau, QlikView, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HBase**: A NoSQL columnar database that provides capabilities such as the
    columnar data storage model and storage for sparse data to Hadoop systems.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flume**: Flume collects data from the source systems like a web server log
    data from Flume **agents** which it then aggregates and moves into Hadoop.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mahout**: Mahout is a machine learning library with a collection of key algorithms
    for clustering, classification, and collaborative filtering.  These algorithms
    can be implemented from any processing framework or engines such as MapReduce,
    and are more efficient for in-memory data mining frameworks such as Spark.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sqoop**: Sqoop is a valuable tool for transitioning data from other database
    systems (mainly relational databases) into Hadoop.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pig**: Pig Latin is a Hadoop-based language that is adept at very deep, very
    long data pipelines (a limitation of SQL). It is relatively simple and easier
    to use than SQL.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource management**:YARN is a great enabler of dynamic resource utilization,
    an integral part of the Hadoop framework. It manages the increasing workloads
    of multi-tenant users, running various Hadoop applications without performance
    impact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unified administration**: Ambari is a RESTful API that provides a user-friendly
    web interface for Hadoop administration. It’s a tool for Apache Hadoop clusters''
    provisioning, managing, and monitoring. Provisioning tasks for a Hadoop cluster
    include installing Hadoop services across multiple hosts and configuring Hadoop
    services for the cluster. Ambari manages Hadoop cluster services such as starting,
    stopping, and reconfiguring Hadoop services across the entire cluster through
    the central management console. Ambari monitors a Hadoop cluster with a dashboard
    for monitoring the health and status of the Hadoop cluster, and integrates with
    the Ambari Metrics System for metrics collection and the Ambari alert framework.
    This alerting system will notify if a node goes down, or disk utilization is higher
    than a threshold, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow management**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Oozie**: A workflow processing system that manages and schedules a series
    of jobs. Jobs can be written in multiple languages such as MapReduce, Pig, and
    Hive, and linked logically to one another. Oozie allows scheduling dependent jobs
    as an output of one query to be completed to feed data into the next job as input.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ZooKeeper**: ZooKeeper is a centralized service for maintaining configuration
    information, naming, providing distributed synchronization, and providing group
    services. All of these kinds of services are used in some form or another by distributed
    applications.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comprehensive data security and governance:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security in Hadoop has three modes of implementation--authentication, authorization,
    and encryption.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Authentication ensures only genuine users have access to the Hadoop cluster.
    Currently, the tools used are MIT Kerberos, AD, OpenLDAP, and so on.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Authorization grants users data privileges such as read-only, write, modify,
    delete, and so on. The currently used tool is Apache Sentry.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data encryption ensures data protection from unauthorized access to data, both
    at rest and in transit. The encryption tool for data at rest is Navigator Encrypt
    and the tools for data in transit can be implemented by enabling TLS/SSL.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Access administration of Hadoop systems can be a challenging task in distributed
    environments that host the individual components of Hadoop on different clusters
    for optimal performance. For example, in a large production environment, there
    will be different cluster groups responsible for workflow, for data storage, data
    analytics, and so on; so managing the respective group access privileges could
    be a daunting task.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Hadoop cluster is a system comprising two or more computers or systems (called
    nodes). It represents a single unified system for the users. The nodes work together
    to execute applications or perform other tasks like a virtual machine. There are
    variants of Hadoop clusters that cater for different data needs. The key features
    in the construction of these platforms are reliability, load balancing, and performance.
  prefs: []
  type: TYPE_NORMAL
- en: The single node or pseudo-distributed cluster has the essential daemons such
    as NameNode, DataNode, JobTracker, and TaskTracker, all run on the same machine.
    A single node cluster is a simple configuration system used to test Hadoop applications
    by simulating a full cluster-like environment with a replication factor of 1.
  prefs: []
  type: TYPE_NORMAL
- en: A small Hadoop cluster comprises a single master and multiple worker nodes.
    The master node is comprised of a Job Tracker, Task Tracker, NameNode, and DataNode.
    A slave or worker node performs the roles of both a DataNode and TaskTracker if
    required; data-only worker nodes and compute-only worker nodes can be configured.
    Such nodes are used for full stack development of Hadoop application and projects
    with a replication factor of 3, such as a multi-node cluster for effective backup.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-node or fully distributed clusters follow the master-slave architecture
    pattern of Hadoop cluster. The NameNode and TaskTracker daemon runs on the master
    machine and the DataNode and TaskTracker daemon runs on one or more slave machines.
    It is deployed for full stack production deployment of the Hadoop application
    and for projects with a replication factor of 3.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop cluster attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the key attributes of Hadoop clusters, such
    as load balancing for high availability and distributed processing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: High availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**High availability** (**HA**) clusters provide services and resources in an
    uninterrupted manner through the redundancy built to the system. High availability
    is to be accomplished both within the cluster and between clusters too.'
  prefs: []
  type: TYPE_NORMAL
- en: High availability within a cluster is accomplished by a master node that monitors
    the worker nodes for any failure and ensures that the load is distributed to other
    active, working nodes.
  prefs: []
  type: TYPE_NORMAL
- en: High availability between cluster examples across regions is accomplished by
    having each cluster system monitor the others, and in the event of failover, replicating
    servers and services through redundant hardware and software reconfiguration.
    Fault tolerance for hardware is achieved with raid systems, and for network systems,
    in the event of link breakdown alternative link paths are provided for continuity
    of services.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load balancing among servers is a key valuable functionality in the increasing
    and explosive use of network and internet-based applications. It is an important
    feature that distributes incoming traffic requests from clients evenly across
    all the active node machines of the cluster that are allocated to the application.
    In case of a node failure, the requests are redistributed among the rest of the
    active nodes available and responsible for processing the orders. The web cluster,
    to be scalable, must ensure that each server is fully utilized, providing increased
    network capacity and improving performance. Web application servers based on load
    balancing are called **web farms**, and redirect requests independently as they
    arrive, based on a scheduler and an algorithm. A few popular algorithms for load
    balancing are least connections, round robin, and weighted fair; each have unique
    applicability.
  prefs: []
  type: TYPE_NORMAL
- en: High availability and load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: High availability and load balancing combines the features of both types of
    clusters, increasing the availability and scalability of services and resources.
    Consistent HA and load balancing is the backbone of the entire web hosting and
    e-commerce project; it needs to ensure support of the scalability of traffic volume
    on networks, without eventually becoming a bottleneck or single point of failure.
    Apart from simply redirecting client traffic to other servers, the web systems
    need to have verification of servers, redundancy, and balancing characteristics
    such as full-time communication checks. This type of cluster configuration is
    widely used in airline and train reservation systems, e-commerce, banking, email
    servers, and 24/7 applications.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed processing and parallel processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed processing involves dividing a large computational task into smaller
    tasks, for processing to run in parallel on individual smaller clusters of nodes.
    It represents a massively parallel supercomputer. This model of cluster is effective
    for application in large computational tasks, and improves the availability and
    performance. These cluster systems are used for scientific research-based computing,
    weather forecasts, and so on; tasks that require high-processing power.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of Hadoop big data cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data clusters are built for varied purposes, such as storage, analytics,
    testing, development, and so on. It is imperative to have the right size of the
    cluster for the right kind of workload, so capacity planning for a cluster is
    an important and critical task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Big data clusters catering to different purposes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Development cluster**: There are many requirements for building a development
    platform, such as the technological validation of porting an application to develop
    functionality on a big data platform, so as to develop advanced analytics use
    cases with machine learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test cluster**: A test platform is built to test the features and functionality
    developed in the development cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data lake cluster**: A build used to provide extensive storage capacity,
    data from different source systems, including third-party data sources, are gathered
    into the data lake. There will be preprocessing activities to filter and perform
    aggregations on the incoming data before it is loaded into the data lake. A data
    lake serves multipurpose data needs for different departments of an organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytical cluster**: A platform for performing advance analytics using appropriate
    algorithms, and publishing the generated results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop big data cluster nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will discuss the different types of nodes along with their role and usage
    in Hadoop Ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NameNode**: The NameNode is an important part of an HDFS file system. It
    keeps the directory tree of all files in the file system, and tracks across where 
    the cluster data files are stored. The data for these files is not stored at all.
    Client applications communicate with NameNode whenever there is a need to locate
    a file, or when they want to modify a file. The modifications are stored by NameNode
    as a log that is appended to a native file system file edits. When a NameNode
    starts up, it reads the HDFS state from an image file, fsimage, and then applies
    the edits to the log file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secondary NameNode**: Secondary NameNode''s whole purpose is to have a checkpoint
    in HDFS. The Secondary NameNode is just a helper node for NameNode; it merges
    the fsimage and the edits log files periodically and keeps edits log size within
    a limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataNode**: A DataNode stores data in HDFS. A functional file system has
    more than one DataNode, with data replicated across them. Client applications
    can talk directly to a DataNode, once the NameNode has provided the location of
    the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge/Hop Node**: Edge nodes are the interface between the Hadoop cluster
    and the outside network. Most commonly, edge nodes are used to run client applications
    and cluster administration tools. They''re also often used as staging areas for
    data being transferred into the Hadoop cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster management**: Cluster management software applications provide end-to-end
    functionality and features for managing cluster landscapes. It facilitates the
    improvement of performance, enhances the quality of service, increases compliance
    and reduces administrative costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security** for the HDFS file level and node level is depicted as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f49a9d39-e82a-4b2c-b762-5b600c511c7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Typically, the edge node is connected to the outside world through an external
    switch, that enables third-party systems to access through Kafka or STB-based,
    and SSH access for inbound users.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding entire mentioned cluster servers interact with each other by a
    dedicated network switch, isolating traffic from the outside world.
  prefs: []
  type: TYPE_NORMAL
- en: Types of nodes and their roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/b30dd897-1b90-4288-9038-aca34bdddafd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Nodes**: A Hadoop cluster can have a different configuration of servers based
    on the role they fulfill. This can be broadly divided into three types, with a
    different hardware configuration for each type:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Master** **n****ode** (also referred to as **n****ame node**): In an enterprise
    deployment, this runs crucial management services. These nodes only store metadata
    so do not need a lot of storage, but since these files are critical and important,
    master node services include as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enterprise manager
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource manager
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Standby resource manager
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NameNode
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Standby NameNode
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Journal nodes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: HBase master
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive server
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sqoop server
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ZooKeeper
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Oozie server
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark/Job history server
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloudera search
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloudera Navigator
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive metastore
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka master
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Flume master
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Worker node**: Worker/slave nodes in a Cloudera enterprise deployment are
    the ones that run worker services. Since tasks are performed by these nodes along
    with the storing of actual data, they are designed to be fault tolerant. Worker
    nodes can have the following roles:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data node
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Node manager
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: HBase region server
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Impala daemons
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Solr servers
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka broker
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Flume agent
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gateway/edge node:** These are where Hadoop client services run, and include:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Third-party tools
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop command-line clients
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Beeline
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Impala shell
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Flume agents
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hue server
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark and other gateway services
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: HA proxy
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Commercial Hadoop distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen in the previous section, Hadoop is open stack community distribution
    with an integration of multiple components or interface layers. Many commercial
    vendors have built on the basic Hadoop platform and customized it to offer in
    the market, both as a hardware product platform and a service. We will discuss
    a few of the popular options as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloudera CDH Hadoop distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hortonworks Data Platform** (**HDP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MapR Hadoop distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Elastic MapReduce
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM open platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Azure's HDInsight--cloud-based Hadoop distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivotal big data suite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop Cloudera enterprise distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The standard framework of Hadoop open source, with different layers and components,
    is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea1a7098-e85e-4626-ac75-887eee989597.png)'
  prefs: []
  type: TYPE_IMG
- en: The Cloudera proprietary distribution framework is built on the Hadoop open
    source code, customizing the services as shown in the following topics. We will
    discuss the various components that make it a leading enterprise product.
  prefs: []
  type: TYPE_NORMAL
- en: Data integration services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data from external source systems ingests into Hadoop systems can be through
    multiple modes based on the business need.
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch transfer**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Sqoop**: Sqoop is a command-line interface application for transferring
    data between relational databases and Hadoop. It supports the saved jobs that
    can be run multiple times helping us to import updates made to a database since
    the last import. It also supports the incremental loads of a single table or a
    free-form SQL query.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time data transfer**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kafka**: Kafka is an open source message broker project developed
    by the Apache Software Foundation, written in Scala. The project aims to provide
    a complete, high-throughput, low-latency platform for handling real-time data
    feeds. It’s a highly scalable pub/sub message queue architected as a distributed
    transaction log, making it very important for enterprise infrastructures to process
    streaming data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Flume**: Flume adopts a simple, flexible and distributed architecture
    for streaming data. It effectively ingests large amounts of log data reliably
    and aggregates it. It has a simple, extensible data model, flexible for building
    and supporting online analytical applications. Flume is a pretty robust, fault
    tolerant, reliable service with built-in failover and recovery features.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Chukwa**: Chukwa is a framework for data collection and analysis on distributed
    file systems such as Hadoop, simplifying log analysis, processing and monitoring.
    Chukwa agents run on respective machines to collect the logs generated from various
    applications. It offers a high degree of flexibility to ingest the huge log data
    generated by servers. Collectors receive the data from the agent and write them
    to HDFS, which serves as storage, and the MapReduce framework will process, analyze,
    and parse the jobs and archive the huge log data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Avro**: Avro is a language-neutral remote procedure call and data
    serialization framework developed within Apache''s Hadoop project. Since Hadoop
    writable classes lack language portability, Avro uses JSON for defining data types
    and protocols and serializes data in a compact binary format. Avro is quite helpful
    for dealing with data formats that can be processed by multiple languages such
    as Java, C, C++, C#, Python, and Ruby.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop data storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data storage in Hadoop is a key function and we will discuss various modes
    of accomplishing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache HDFS (Filesystem)**: The HDFS is a distributed, scalable, and portable
    file system written in Java for the Hadoop framework. HDFS stores large files--typically
    in the range of gigabytes to petabytes--across multiple machines and data nodes.
    Data nodes can talk to each other to rebalance data, to move copies around, and
    to keep the replication of data high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache HBase (NoSQL)**: HBase is an open source, non-relational, distributed
    data store running atop of HDFS providing a fault-tolerant means of storing large
    amount of sparse data. HBase is a column-oriented key-value data store and has
    been admired greatly due to its lineage with Hadoop and HDFS. It is suitable for
    faster read and write operations on extensive datasets with high throughput and
    low input/output latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Kudu (Relational)**: Apache Kudu is an open source storage engine
    intended for structured data that supports low-latency random access, together
    with efficient analytical access patterns. It bridges the gap between HDFS and
    the HBase NoSQL database. Kudu tables look like those in SQL relational databases,
    to act as a storage system for structured data. Like RDBMS principles, primary
    keys are made up of one or more columns that enforce uniqueness and act as an
    index for efficient updates and deletes, as a storage system for tables of structured
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data access services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Spark**: Spark is an open source framework for machine learning and
    stream processing based on in-memory technology for data processing. It provides
    programmers with a data structure called the resilient distributed dataset (RDD),
    an application programming interface. The RDD is read-only, distributing multiple
    sets of data items over a cluster of machines with fault tolerant features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Hive**: The Hive data warehouse software facilitates reading, writing,
    and managing large datasets that reside in distributed storage using SQL. The
    structure can be projected onto data already in storage. A command-line tool and
    JDBC driver are provided to connect users to Hive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impala**: Impala is Cloudera''s SQL query engine for data stored in an Apache
    Hadoop cluster and running open source massively parallel processing (MPP). Impala
    enables users to run low latency SQL queries on data stored in HDFS and Apache
    HBase, without requiring additional data movement or transformation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solr**: Apache Solr is a search platform for websites, popular for enterprise
    search because it can be used to index and search documents and email attachments.
    It''s built upon a Java library called **Lucene** ([http://whatis.techtarget.com/definition/Apache-Lucene](http://whatis.techtarget.com/definition/Apache-Lucene)),
    written in Java, and provides both a RESTful XML interface and a JSON API that
    are used to build search applications. Solr can search and index multiple websites,
    returning content related recommendations based on the taxonomy ([http://searchcontentmanagement.techtarget.com/definition/taxonomy](http://searchcontentmanagement.techtarget.com/definition/taxonomy))
    of the search query ([http://searchsqlserver.techtarget.com/definition/query](http://searchsqlserver.techtarget.com/definition/query)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Pig**: Pig provides a high-level language known as Pig Latin, a SQL-like
    language with many built-in operators for performing data operations such as joins,
    filters, ordering, and so on, and is used to perform all the data manipulation
    operations in Hadoop. The component of Apache Pig is Pig Engine that ingests the
    Pig Latin scripts as input and converts the scripts into MapReduce jobs. As a
    tool, it''s very efficient, reducing development and coding time. As a platform,
    it adopts multiple query paths, representing them as data flows to analyze large
    sets of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kite** is a high-level data layer for Hadoop, that provides an API and a
    set of tools to create logical abstractions on top of storage systems (such as
    HDFS) and operates in terms of records, datasets, and dataset repositories. It
    can access Mavin through plug-in and aid-in packaging, deploying and running distributed
    applications. It speeds up the development of stream processing ETL applications
    in Hadoop that extract, transform, and load data into target repositories such
    as Apache Solr, enterprise data warehouses, HDFS, HBase, and OLAP applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MapReduce** is a processing technique and framework based on Java for distributed
    computing. As the sequence of the name MapReduce implies, the reduce task is always
    performed after the map job. A map job usually splits the input dataset into independent
    chunks where individual elements are broken down into tuples (key/value pairs).
    The reduce framework sorts the outputs of the map jobs, which are then input to
    the reduce tasks. The tasks are processed in a completely parallel manner to scale
    data processing over multiple computing nodes. The input jobs and the output jobs
    are stored in a file system. The framework takes care of scheduling tasks, monitoring
    them and re-executing failed tasks. The MapReduce model framework can easily scale
    the application to run over tens of thousands of machines in a cluster through
    a configuration change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Cassandra architecture is a distributed NoSQL database management system
    known for its ability to scale, perform, and offer continuous uptime. Apache Cassandra
    is based on a ring design wherein all nodes play the equivalent role without any
    master concept. Compared to other architectures such as master-slave, legacy,
    or sharded design, Cassandra is quite easy to set up and maintain, and is designed
    for handling a high volume of structured data across commodity servers.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Cassandra's high availability and scalable architecture enable it to
    handle large amounts of data, and thousands of concurrent users and operations
    spread across multiple data centers to ensure high-performance by distributing
    user traffic. Cassandra has built-in features such as data modeling, high availability
    clusters, monitoring tools, query language, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4b779ed-b3ba-41a6-b928-78829e7ce525.png)'
  prefs: []
  type: TYPE_IMG
- en: Unified (common) services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Resource management**: Apache Hadoop **Yet Another Resource Negotiator**
    (**YARN**) is a cluster management technology. YARN is one of the key features
    of the second-generation Hadoop 2 version of the Apache Software Foundation''s
    open source distributed processing framework. It also enables versatility; the
    resource manager can support additional paradigms and not just map/reduce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Oozie**: Apache Oozie is a system to schedule a workflow to manage
    Hadoop jobs. The directed acyclic graph is the mode of representing workflows
    in Oozie, which are a collection of control flow and activity nodes. The beginning
    and the end of a workflow and the mechanism to control the workflow execution
    path are defined by control flow nodes. The execution of a computation processing
    task through workflow triggers happens on action nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Sentry**: Hadoop''s strong security at the file system level lacks
    the granular support to adequately secure row-level access to data by users and
    BI applications. Sentry allows access control at the server, database, and table,
    and grants different privilege levels including select, insert, and so on. It
    provides for authenticated users privileges on data, the ability to control and
    enforce access to data, and so on. It enables fine-grained access control to data
    and metadata in Hadoop. The column level security can be implemented by creating
    a view of a subset of allowed columns by restricting the base table and granted
    privileges. Sentry administration is simple and convenient through role-based
    authorization. It is a policy engine that can easily grant multiple groups access
    to the same data at different privilege levels such as resource, roles, users,
    and groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following diagram and table provide a comprehensive view of the security
    model adopted by Hadoop systems. There are security requirements at multiple levels,
    such as clusters, user level, and application level in an enterprise-wide implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloudera offers four layers of security as follows; perimeter, access, visibility,
    and data. Cloudera Enterprise Security can be classified into four broad categories;
    authentication, authorization, data protection and auditing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3090755d-043a-4806-9485-c26549aae793.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Security features offered by popular tools are listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7252d1d0-f435-4573-a00c-fe5de170c2bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloudera proprietary services and operations/cluster management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cloudera Navigator**: Cloudera Navigator is part of Cloudera Enterprise,
    and is a fully integrated data management and security system for the Hadoop platform.
    Cloudera Navigator is the data governance solution for Hadoop, presenting crucial
    capabilities such as data discovery, continuous optimization, audit, lineage,
    metadata management, and policy enforcement. Cloudera Navigator supports continuous
    data architecture optimization and meeting regulatory compliance requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloudera Manager**: Cloudera Manager is an end-to-end application for managing
    CDH clusters. Cloudera Manager sets the standard for enterprise deployment by
    delivering granular visibility into and control over every part of the CDH cluster—empowering
    operators to improve performance, enhance the quality of service, increase compliance
    and reduce administrative costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloudera Director**: Cloudera Director works with Cloudera Manager and the
    cloud service provider to provide centralized and programmatic administration
    of clusters in the cloud, including deployment, configuration, and maintenance
    of CDH clusters. With Cloudera Director, you can monitor and manage multiple Cloudera
    Manager and CDH deployments, across different cloud environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Hadoop Hortonworks framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image is the framework of Hortonworks. Many components are the
    same as the Hadoop stack, as seen previously; we will discuss the components unique
    to this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34af9bb3-26af-44c4-8b42-401d066fcfc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Data governance and schedule pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Falcon is a data management tool for managing dependencies between the
    system infrastructure, data, and processing logic. Data administrators can define
    operational and data governance policies for Hadoop workflows for overseeing data
    pipelines in Hadoop ([http://searchdatamanagement.techtarget.com/definition/Hadoop-2](http://searchdatamanagement.techtarget.com/definition/Hadoop-2))
    clusters. Using Falcon, we can manage thousands of compute nodes, with a large
    number of jobs typically running on a cluster at any given time, ensuring a consistent
    and dependable performance on complex processing jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Falcon relies on Oozie job scheduling software to generate the processing workflows,
    to set procedures for replication, and for the retention and archiving of incoming
    data. The data governance engine schedules and monitors data management policies
    such as enhanced monitoring, and so on. The other features are tracing jobs activities
    for failures, dependencies, audits, and lineage, and also tagging the data to
    comply with data retention and discovery requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7609f8d4-f321-4a37-9a77-799b8ca34371.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Knox is a bastion security server; shielding direct access to Hadoop
    cluster nodes helps setups be more secure for enterprise-ready installations.
    Knox can easily scale horizontally by supporting stateless protocols. Knox provides
    authentication functionality to be managed by users and groups using LDAP or active directory.
    Identity Federation is SSO and HTTP header based.
  prefs: []
  type: TYPE_NORMAL
- en: Authorization is supported through an **access control list** (**ACL**) on service
    levels. The Knox Policy enforcement ranges from authentication, federation, authorization,
    audit, dispatch, host mapping and content rewrite rules. The policy is enforced
    through a list of providers, defined within the topology and the cluster definition,
    for purposes of routing and translation between user-facing URLs and cluster internals.
  prefs: []
  type: TYPE_NORMAL
- en: Data access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Tez**: This is an application framework that allows for a complex
    directed-acyclic-graph of tasks for processing data. It is a resource-management
    framework, based on Apache YARN functionality, that is extensible for building
    high-performance batch and interactive data processing applications, leading to
    significant improvements in response times while maintaining MapReduce''s ability
    to scale to petabytes of data. Tez caters for cases that require the near-real-time
    performance of query processing and machine learning, and so on, with a powerful
    framework based on expressing computations as a data flow graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache HCatalog**: This is a storage management layer for Hadoop, that facilitates
    the easy reading and writing of data from the Hadoop cluster grid with different
    data processing tools such as Hive, Pig, MapReduce, and so on. For different kinds
    of data formats stored on HDFS, such as RCFile, Parquet, ORC files, or Sequence
    files, it uses Hive **Serializer-Deserializer** (**SerDe**) to enable a relational
    view. Apache HCatalog provides features such as table abstraction and data visibility
    to tools for cleaning and archiving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f11c4c6c-a956-47cb-9d52-2651bfaca05c.png)'
  prefs: []
  type: TYPE_IMG
- en: Data workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The WebHDFS protocol provides external applications over the internet, HTTP,
    or web access for managing files and data stored in the HDFS cluster, on a par
    with the high-performance native protocol or native Java API default with Hadoop
    Cluster. WebHDFS is based on an industry-standard RESTful mechanism that provides
    security on par with native Hadoop protocols. Using WebHDFS common tools such
    as `curl/wget`, users can access the HDFS for operations such as reading files,
    writing to files, making directories, changing permissions, renaming, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A Hadoop MapR framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MapR is a commercial distribution of Apache Hadoop with HDFS replaced with
    MapR-FS**.** The following is the MapR framework with common Hadoop open source
    components; we will now review the components unique to this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af731398-08f5-49d7-ae2a-a3c98b7178c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLLIB is a collection of machine-learning algorithms and utilities for prediction
    models and data sciences. There are some broad groups, such as classification,
    clustering, collaborative filtering, and so on. A few of the ML algorithms used
    for each category are listed following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: Logistic regression, naive Bayes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: Generalized linear regression, survival regression, decision
    trees, random forests, and gradient-boosted trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: K-means, Gaussian mixtures (GMMs), frequent itemsets, association
    rules, and sequential pattern mining'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX** is a new component in Spark for graphs and graph-parallel computation,
    used to implement new types of algorithms that require the modeling of relationships
    between objects. In many real-world applications, such as social networks, networking,
    and astrophysics, graph processing is very effective and efficient at representing
    a model relationship between entities visually.'
  prefs: []
  type: TYPE_NORMAL
- en: SQL stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Drill is a distributed SQL engine that enables data exploration and analytics
    on non-relational data stores such as Hadoop, MapR, CDH, NoSQL (MongoDB, HBase),
    cloud storage (Amazon S3, Google Cloud Storage, Azure Blog Storage, Swift), and
    so on. It uses a shredded, in-memory, columnar execution engine for distributed
    query optimization and execution, for complex data and schema-free data. By using
    a query engine that compiles and re-compiles queries at runtime, high performance
    is achieved for any structure of data. Using standard SQL and BI tools, users
    can query the data without having to create and manage schemas. It supports schema-free
    JSON document models, similar to MongoDB and Elasticsearch, and industry-standard
    APIs--ANSI SQL, ODBC, JDBC, and RESTful APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Shark** is a data warehouse-based system used with Apache Spark; its
    distributed query engine enhances high-end analytical results and the performance
    of Hive Queries multifold. Shark supports most of the Hive''s features, such as
    query language, metastore, serialization formats, and user-defined functions.
    Apache Shark is built on top of Apache Spark, which is a parallel data execution
    engine; hence, Shark can respond to complex queries in sub-second latency.  It
    provides the maximum performance gains offered by column-wise memory storage systems,
    as data is stored and processed within the cluster''s memory or in a database
    with in-memory materialized views.'
  prefs: []
  type: TYPE_NORMAL
- en: Storage, retrieval, and access control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Accumulo** provides fine-grained data access control and cell-level access
    control, with complex policies governing access to sensitive data. It is a low-latency,
    large table data storage and retrieval system, with a key/value store based design.
    Accumulo provides extremely fast access to data in massive HDFS tables, while
    also controlling access to its millions of rows and columns down to the individual
    cell. It enables the intermingling of different data sets with access control
    policies for fine-grained access to data sets by encoding the policy rules for
    each individual data cell, and controls fine-grained access.'
  prefs: []
  type: TYPE_NORMAL
- en: Data integration and access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hue is an open source web interface for analyzing data within any HDFS cluster
    through Apache Oozie. It comes with many built-in features such as Hue's Editor
    to build workflows and then schedule them to run regularly and automatically.
    It has a dashboard for data querying, monitoring progress and logs, and performing
    actions such as pausing or stopping jobs. The applications supported are Apache
    Hive, Apache Impala (incubating), MySQL, Oracle, PostgreSQL, SparkSQL, Apache
    Solr SQL, Apache Phoenix, Apache Solr, and Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '**HttpFS**--Apache Hadoop HttpFS is a service that provides HTTP access to
    HDFS through REST APIs, supporting all HDFS file system operations (both read
    and write). It supports data transfer between HDFS clusters running different
    versions of Hadoop (overcoming RPC versioning issues) or a cluster behind a firewall.'
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning and coordination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data provision tools enable cloud hosting and coordination services for
    Hadoop systems, two popular choices are discussed following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Juju**:The container option available with Hadoop distribution is the Juju
    framework. This allows users to deploy software built locally on a range of services,
    including MAAS, EC2, Azure, LXD containers. Juju can model, configure and manage
    services and deploy to all major public and private clouds with only a few commands.
    Hundreds of preconfigured services are available in the Juju store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Whirr--big data on clouds**: Apache Whirr can be used to define, provision
    and configure big data solutions on cloud platforms such as Amazon EC2, Rackspace
    servers, and CloudStack. Whirr automatically starts instances (set of libraries)
    in the cloud and bootstraps Hadoop through on them. It initiates cloud-neutral
    big data services to define and provision Hadoop clusters in the cloud, and adds
    packages such as Hive, HBase, and Yarn for MapReduce jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivotal Hadoop platform HD Enterprise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the Pivotal HD Enterprise framework. The open source Hadoop
    components in this framework were already discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55ad2cfb-01c9-4fa2-94fe-79f9a3131f90.png)'
  prefs: []
  type: TYPE_IMG
- en: A Hadoop ecosystem on IBM big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a Hadoop ecosystem based on IBM big data. We are already familiar
    with most of the open source Hadoop big data components listed in this framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c0369df-41b9-4d1e-86a0-ad2009d2a128.png)'
  prefs: []
  type: TYPE_IMG
- en: A Hadoop ecosystem on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon **Elastic MapReduce** (**EMR**) is a service that allows users to launch
    and scale Hadoop clusters inside of Amazon's web infrastructure. EMR instances
    use Amazon's prebuilt and customized EC2 instances, which greatly simplifies the
    setup and management of the cluster of Hadoop and MapReduce components. EMR can
    analyze large datasets on AWS cloud Hadoop clusters quite effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'An AWS EMR framework depicting multiple service layers is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37f71ce0-7ed8-4c92-b30a-fb3a40f49ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An AWS EMR framework that offers integrations with a wide choice of Hadoop
    open source components is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**R**: It is a GNU package, open source programming language and software environment
    for statistical computing and graphics, and is widely used among statisticians
    and data miners for developing statistical software and data analysis. R as a
    language is both flexible and powerful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Presto**:Apache Presto is a distributed parallel cross-platform query execution
    engine on the Hadoop platform. Presto supports using standard ANSI SQL to query
    multiple sources such as HDFS, MySQL, Cassandra, Hive, relational databases, and
    other data stores. Presto runs multiple analytic queries, optimized for low latency
    and interactive query analysis, and scales without downtime. Presto supports most
    of today''s best industrial applications such as Facebook, Teradata and Airbnb,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradle**:The landscape of modern software development is continuously evolving,
    and so are the needs for build automation. Projects involve large and diverse
    software stacks with multiple programming languages and a wide spectrum of testing
    strategies. Adopting agile practices leads to the early integration of code, as
    well as frequent and easy delivery to both test and production environments, as
    supported by builds. Gradle is an open source build automation system that builds
    upon the concepts of Apache Ant, and Apache Maven.  Gradle uses a directed acyclic
    graph to schedule the order of the tasks and introduces a Groovy-based domain-specific
    language for declaring the project configuration. Gradle was designed for multi-project
    builds for its ability to manage dependencies. Gradle can define and organize
    large project builds, as well as modeling dependencies between projects. It supports
    incremental builds by intelligently determining the build tree dependencies and
    need for re-execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cascading**:Cascading is an application development platform for building
    big data applications on Apache Hadoop, providing an abstraction layer for Apache Hadoop
    and Apache Flink. Cascading is used to create and execute complex data processing
    workflows on a Hadoop cluster, hiding the underlying complexity of MapReduce jobs.
    Cascading provides an optimal level of abstraction with the necessary options
    through a computation engine, systems integration framework, data processing,
    and scheduling capabilities. Cascading offers Hadoop development teams portability
    for simple or complex data applications without incurring the costs of rewriting
    them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Phoenix**:Apache Phoenix is an open source, massively parallel relational
    database engine that uses Apache HBase as a base to support OLTP for Hadoop. It
    provides random, real-time access to large datasets with a familiar SQL interface
    to Hadoop systems such as Spark, Hive, Pig, Flume, and MapReduce.  Apache Phoenix
    abstracts away the underlying data store. Aggregation queries are executed on
    the nodes where data is stored, reducing the need to send massive data over the
    network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Mahout**: Apache Mahout is a suite of scalable machine learning algorithms
    focused primarily in the areas of collaborative filtering, clustering, and classifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Hadoop platform is HDInsight hosted on Microsoft Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the architecture of an HDInsight Ecosystem hosted on Microsoft
    Azure. While some of the native open source layers are embedded as is, some others
    are tailored and customized as per Microsoft proprietary offerings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fc5e870-d38c-462e-a4cb-29a61e6ee0a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Capacity planning for systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sizing a Hadoop cluster is an important task as there are many factors influencing
    the performance. Capacity planning and the sizing of a Hadoop cluster are imperative
    for optimizing the distributed cluster environment with its related software.
    The number of machines, specifications of the machines, and effective process
    per node planning will allow you to optimize the performance effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the Hadoop ecosystems, different layers (components/services) interact
    with each other, leading to performance overheads associated within a complex
    cluster stack between any of the layers; hence the need for requisite performance
    tests at each interface and appropriate tuning, as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46744cf0-b7cc-460f-ac1b-92abbc66feba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many factors that influence the capacity planning, sizing, and performance
    of a complex Hadoop-distributed cluster. The following are a few factors for consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amount of data**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volume of data and growth
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The data retention policy of how many years to hold data before discarding
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Also the data storage mechanism (data container, type of compression used if
    any)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type of workload**:If workloads are CPU/IO /memory intensive, we will have
    to consider hardware accordingly. If processing might grow rapidly, we have to
    consider adding new data nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequency** **of workload**: If the data load is batch or real-time streaming
    data, would it be a few times a day, nightly, weekly or monthly loads?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type of security**:Authentication, authorization, and encryption'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type of services required**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the business SLA for the cluster? Is there is a requirement for real-time
    support?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What types of services are running other than core Hadoop services?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How many third-party tools will be installed/used?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice of an operating system**:Selecting an operating system depends on
    multiple factors, such as your team''s administration competency, the cost of
    procurement and maintenance, stability, performance, reliability, support availability,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CentOS**: Linux CentOS is functionally compatible with RHEL, and a popular
    choice towards work nodes in Hadoop clusters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RedHat Enterprise Linux (RHEL)**: Linux RHEL is widely used for servers in
    Hadoop clusters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ubuntu**: A very popular distribution, based on Debian –  both desktop and
    server versions available.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLSE**: A Linux Enterprise Server developed by SUSE. It is designed for servers,
    mainframes, and workstations but can be installed on desktop computers for testing
    as well.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solaris, OpenSolaris**: Not popular in production clusters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fedora Core:** Linux distributions for servers and workstations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network considerations**: Hadoop is very bandwidth-intensive, since most
    of the time all the nodes are communicating with one other simultaneously. Consider
    the usage of dedicated switches, a network Ethernet bandwidth of 10 GB/sec, and
    racks that are interconnected through switches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guideline for estimating and capacity planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proper cluster sizing includes selecting the right hardware for a master and
    a worker, as well as edge nodes, while keeping the cost low.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few factors to consider, along with some general guidelines for
    planning capacity sizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data size**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data sizing, the recommended replication factor is 3
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the total data to be stored is *Y* TB, it will become *3Y* TB after replication
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If any compression techniques are used to store data then a compression factor
    can be considered
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For disk efficiency, only 60-70% usage is recommended for the total disk availability
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: So, the total disk capacity including disk compression factor = *3 x Y x 7*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data growth rate**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering data growth factor, say 1 TB per year
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a 3 x replication factor
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk efficiency factor of 70%
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to calculate *1 TB * 3 / .6  = 4-5 TB*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage capacity per 1 TB of data growth
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is equivalent to adding a new node, considering the data processing needs
    and growth of data volume
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number of data nodes planner:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's consider that we need to store 200 TB of data in HDFS
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With 3 x replication, it will be 600 TB
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering the replication factor, it will be *600 * 1.3   = 780* TB storage
    (nearly)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming per node there are 12 disks with 2 TB capacity = 24 TB per node
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of nodes required would be *780/24* = *33* nodes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering data growth requirements, we need to plan for cluster expansion
    in terms of additional nodes required per month, week, year, and so on
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster-level sizing estimates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster sizing and capacity planning is important for various nodes as per their
    roles.
  prefs: []
  type: TYPE_NORMAL
- en: For master node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Capacity planning for master mode is critical and needs consideration of system
    resources and services hosted on it as detailed following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory**:Capacity sizing of the master name node memory is a very important
    task. The rule of thumb is to have 1 GB of heap size to store 1 million blocks.
    For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a 5-node cluster
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw storage on each node is 20 TB
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider that the HDFS block size is 128MB
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of blocks = *5 * (20*1024*1024)*  = *33* *million* (approximately)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total HDFS block size is 33 million
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on a replication factor of 3, the required heap size = *33/3* = *11*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of actual blocks would be approximately 11 GB
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other factors that we need to consider as well:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name node would be the stable entity, so you should plan for the future growth
    prospects as well, while planning the sizing configuration
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Services such as resource manager, HBase master, zookeeper, and so on would
    also run on this node
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule of thumb is to keep a minimum of 2 GB for services such as HBase and 4
    GB for the resource manager
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on normal usage, the memory configuration for the master node should be
    128 GB to 512 GB
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: Cores of 2 GHz or above of processor per node are reasonable, depending
    on the number of services running on the node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disk**: The local disks on the master node should be configured as RAID10
    with hot spares to give good performance and fast recovery on the loss of a disk.
    There should be a separate partition for `/var` with a minimum size of 1 TB, or
    based on the capacity required for log storage. It is strongly recommended to
    configure High Availability in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Capacity planning for worker node needs due consideration for system resources
    (memory, CPU, disk) as per the services it runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory**: Memory on the worker nodes is based on the type of workload and
    daemons they will be running. You should consider 256 GB to 512 GB of memory for
    each of the worker nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: CPU capacity is based on the type of workload and numbers of parallel
    MapReduce tasks that are planned. By enabling hyperthreading, the total number
    of MapReduce tasks should be 1.5 times of the number of the cores. Consider a
    minimum of 24 cores with processors greater than 2 GHz.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disk**: You should consider having a large number of small SATA disks (2
    to 4 TB), instead of a small number of large disks. All disks should be configured
    as JBODs with noatime, nodiratime along with `/var` as a separate partition from
    the root OS partition. It is recommended you add additional nodes to enhance storage,
    as it will increase processing power too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gateway node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gateway nodes do not run any specific Hadoop services; their configuration should
    be based on what jobs they will run.
  prefs: []
  type: TYPE_NORMAL
- en: '**Network**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network is a core component and should be considered very carefully as processing
    in Hadoop is based on data proximity.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If workloads consist of map-only jobs that are only transforming the data, there
    isn't a lot of data movement over the wire. If workloads have a lot of reduce
    actions such as aggregation, joins, and so on, then there is a lot of data movement
    between the nodes, in which case we should consider a minimum network capacity
    of 10 GB.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Running other services such as HBase, Spark should also be taken into account
    while considering network configuration.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider an average of 15 to 20 nodes per rack.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dual Ethernet cards bonded together are recommended to support failover.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It's good practice to set up data locality and replication configured across
    the rack (rack awareness), so that even if one rack goes down, data is still available.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Core switches connected to top of the rack switches should be of high bandwidth
    (10 GB/sec or  more). Consider redundancy for top of rack as well as core switches.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered big data Hadoop components, popular frameworks,
    and their unique components for various services. In the next chapter, we will
    cover the terminology and technologies of the cloud such as public, private, hybrid
    models, service offerings for infrastructure, platform, identity, software, and
    network as service. We will also present a few popular market vendors.
  prefs: []
  type: TYPE_NORMAL
