- en: Big Data Hadoop Ecosystems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据Hadoop生态系统
- en: We have discussed the key concepts of **big data technologies** in the preceding chapters.
    In this chapter, we will cover the building of big data clusters, frameworks,
    key components, and the architecture of popular vendors. We will discuss big data
    DevOps concepts in successive chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章讨论了**大数据技术**的关键概念。在本章中，我们将介绍构建大数据集群、框架、关键组件以及流行厂商的架构。我们将在后续章节中讨论大数据DevOps概念。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Big data Hadoop ecosystems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据Hadoop生态系统
- en: Big data clusters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据集群
- en: Types and application
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型与应用
- en: High availability
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性
- en: Load balancing
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Big data nodes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据节点
- en: Master, worker, edge nodes
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点、工作节点、边缘节点
- en: Their roles
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的角色
- en: Hadoop frameworks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop框架
- en: Cloudera CDH Hadoop distribution
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudera CDH Hadoop发行版
- en: '**Hortonworks Data Platform** (**HDP**)'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hortonworks数据平台** (**HDP**)'
- en: MapR Hadoop distribution
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapR Hadoop发行版
- en: Pivotal big data suite
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pivotal大数据套件
- en: IBM open platform
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM开放平台
- en: Cloud-based Hadoop distribution
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于云的Hadoop发行版
- en: Amazon Elastic MapReduce
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Elastic MapReduce
- en: Microsoft Azure's HDInsight
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft Azure的HDInsight
- en: Capacity planning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容量规划
- en: Factors
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因素
- en: Guidelines
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指导原则
- en: Big data Hadoop ecosystems
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据Hadoop生态系统
- en: 'Apache Hadoop is an open source software platform built from commodity hardware
    and used to scale clusters up to terabytes or petabytes for big data, spanning
    across thousands of servers. It is highly popular and efficient for distributed
    data storage and distributed processing of very large datasets. Hadoop offers
    a full scale of services such as data persistence, data processing, data access,
    data governance, data security, and operations. A few of the benefits associated
    with Hadoop clusters are listed as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop是一个开源软件平台，基于廉价硬件构建，用于将集群扩展到TB或PB级别，支持大数据处理，跨越成千上万的服务器。它在分布式数据存储和大规模数据集的分布式处理方面非常流行和高效。Hadoop提供了一整套服务，如数据持久化、数据处理、数据访问、数据治理、数据安全和运营。Hadoop集群的几个好处列举如下：
- en: '**Data scalability**:Big data volumes can grow exponentially to accommodate
    these large data volumes, Hadoop enables distributed processing of data; each
    node in the data cluster participates in storing, managing, processing, and analyzing
    data. The addition of nodes enables quick scaling of clusters to store data at
    a scale of petabytes.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可扩展性**：大数据量可以指数级增长以容纳这些大数据量，Hadoop支持数据的分布式处理；数据集群中的每个节点都参与存储、管理、处理和分析数据。节点的增加使得集群能够迅速扩展，支持PB级别的数据存储。'
- en: '**Data reliability**: Hadoop cluster configurations provide data redundancy.
    For example, in case of accidental failure of one or more nodes, Hadoop cluster
    management software will automatically replicate the data and processing to the
    rest of the active nodes. Thus business continuity is assured with the application
    and data functional, even during the non-availability of a few nodes.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可靠性**：Hadoop集群配置提供数据冗余。例如，在某些节点发生意外故障的情况下，Hadoop集群管理软件会自动将数据和处理任务复制到其余活跃节点上。即使在某些节点不可用的情况下，应用和数据的功能也能得到保证，从而确保了业务的持续性。'
- en: '**Data** **flexibility**: In traditional relational database management systems,
    schema tables are created before storing structured data into the system commonly
    referred to as *schema on write*. Based on the processing application data requirements,
    diverse data formats can be loaded into Hadoop systems such as structured, semi-structured,
    or unstructured data. Hence, the schema is dynamically created during the data
    load, and referred to as *schema on read*.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据** **灵活性**：在传统的关系数据库管理系统中，通常在存储结构化数据之前会创建模式表，这被称为*写时模式*。根据处理应用程序的数据需求，Hadoop系统可以加载多种数据格式，例如结构化、半结构化或非结构化数据。因此，模式是在数据加载过程中动态创建的，称为*读时模式*。'
- en: '**Economical**:Hadoop is open source and built on low-cost commodity hardware,
    and hence, is more economical than proprietary licensed software.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经济性**：Hadoop是开源的，基于低成本的廉价硬件构建，因此比专有的许可证软件更加经济。'
- en: Organizations adopt the Hadoop system for its versatility and its ability to
    persist on large data volumes, managing, visualizing and analyzing vast amounts
    of data quickly, reliably, efficiently, and with a low cost for a variety of data
    formats, with data governance, workflow, security, and so on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 组织采用Hadoop系统是因为其多功能性，能够在大数据量下持久化数据、快速、可靠和高效地管理、可视化和分析大量数据，并且支持各种数据格式，具有数据治理、工作流、安全性等功能，并且成本低廉。
- en: Inbuilt tools and capabilities in Hadoop ecosystem
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 生态系统中的内置工具和功能
- en: 'Hadoop ecosystem offers many inbuilt tools, features, and capabilities, listed
    as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 生态系统提供了许多内置工具、功能和能力，列举如下：
- en: '![](img/160dc7ec-fd30-448f-a257-4579441c6e09.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/160dc7ec-fd30-448f-a257-4579441c6e09.png)'
- en: '**Data storage**: The **Hadoop Distributed File System** (**HDFS**) provides
    scalable, fault-tolerant, cost-efficient storage. Hadoop can handle exponential
    data growth by distributing storage across many nodes; the combined storage capacity
    can grow with demand while remaining economical per unit of storage. There are
    other storage managers, such as HBase, Solr, and so on.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储**：**Hadoop 分布式文件系统**（**HDFS**）提供可扩展、容错且具有成本效益的存储。Hadoop 可以通过将存储分布到多个节点上来处理数据的指数级增长；组合的存储容量可以根据需求增长，同时单位存储成本保持经济。还有其他存储管理工具，如
    HBase、Solr 等。'
- en: '**Data lake**: One of the key strengths of Hadoop is its ability to build a
    data lake economically. It will be a valuable asset for an organization to store
    all their relevant data needs, gathered and consolidated from various data sources.
    For example, in the manufacturing industry the machine maintenance data, inventory
    data, sales data, machine sensor data on performance, social media data on customer
    feedback, vendors and suppliers data, weather reports, and so on can be captured
    regularly as per the requirements of the data lake.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据湖**：Hadoop 的一个关键优势是能够经济地构建数据湖。它将成为组织存储所有相关数据需求的宝贵资产，这些数据来自不同的数据源并被收集和整合。例如，在制造业中，机器维护数据、库存数据、销售数据、机器性能的传感器数据、客户反馈的社交媒体数据、供应商和供应商数据、天气报告等，可以根据数据湖的需求定期捕获。'
- en: '**Data processing**:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理**：'
- en: '**Hadoop ecosystem**: Hadoop ecosystem offers data processing in batch, stream,
    and hybrid systems.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 生态系统**：Hadoop 生态系统提供了批处理、流处理和混合系统的数据处理功能。'
- en: '**MapReduce**: MapReduce is an initial processing framework for batch jobs
    in Hadoop. MapReduce''s processing technique follows the map, shuffle and reduce
    algorithm using key-value pairs. Batch jobs are like monthly telephone invoices
    for customers.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce**：MapReduce 是 Hadoop 中用于批处理任务的初始处理框架。MapReduce 的处理技术遵循映射、洗牌和归约算法，使用键值对。批处理任务类似于客户的每月电话账单。'
- en: '**Stream processing**: Like stock price information and airline reservation
    data, Apache storm is well suited for processing data in streams.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流处理**：像股票价格信息和航空公司预定数据一样，Apache Storm 非常适合处理流数据。'
- en: '**Hybrid processing systems**: These processing frameworks can handle both
    batch and stream workloads, such as Apache Spark, Flink, and so on. One use case
    is the **Internet of Things** (**IoT**), truck sensor data capture, aggregation
    in the cloud and analytics to derive patterns, and so on.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合处理系统**：这些处理框架可以同时处理批量和流数据工作负载，例如 Apache Spark、Flink 等。一个典型用例是 **物联网**（**IoT**）中的卡车传感器数据捕获、在云中聚合并分析以推导模式等。'
- en: '**Data access**:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问**：'
- en: '**Hadoop**: Hadoop offers multiple ways of accessing and processing the data.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop**：Hadoop 提供了多种方式来访问和处理数据。'
- en: '**Apache Solr**: Apache Solr provides indexing and search capabilities for
    the data stored in HDFS.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Solr**：Apache Solr 提供了对存储在 HDFS 中数据的索引和搜索功能。'
- en: '**Hive**: Hive provides data warehouse functionality for Hadoop with a simple
    SQL-like language called **HiveQL** that provides indexes, making querying faster.
    A standard SQL programming interface can be used and provides better integration
    with a few analytics packages such as Tableau, QlikView, and so on.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hive**：Hive 为 Hadoop 提供数据仓库功能，采用一种类似 SQL 的简单语言叫做 **HiveQL**，该语言提供了索引，使查询更快。可以使用标准
    SQL 编程接口，并且提供了与一些分析软件包（如 Tableau、QlikView 等）更好的集成。'
- en: '**HBase**: A NoSQL columnar database that provides capabilities such as the
    columnar data storage model and storage for sparse data to Hadoop systems.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HBase**：一个 NoSQL 列式数据库，提供列式数据存储模型等功能，并为 Hadoop 系统提供稀疏数据存储。'
- en: '**Flume**: Flume collects data from the source systems like a web server log
    data from Flume **agents** which it then aggregates and moves into Hadoop.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flume**：Flume 从源系统（如 Web 服务器日志数据）收集数据，通过 Flume **代理**进行聚合并传输到 Hadoop。'
- en: '**Mahout**: Mahout is a machine learning library with a collection of key algorithms
    for clustering, classification, and collaborative filtering.  These algorithms
    can be implemented from any processing framework or engines such as MapReduce,
    and are more efficient for in-memory data mining frameworks such as Spark.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mahout**: Mahout 是一个机器学习库，包含了一系列用于聚类、分类和协同过滤的核心算法。这些算法可以在任何处理框架或引擎中实现，如 MapReduce，并且在像
    Spark 这样的内存数据挖掘框架中更加高效。'
- en: '**Sqoop**: Sqoop is a valuable tool for transitioning data from other database
    systems (mainly relational databases) into Hadoop.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sqoop**: Sqoop 是一个有价值的工具，用于将数据从其他数据库系统（主要是关系型数据库）迁移到 Hadoop 中。'
- en: '**Pig**: Pig Latin is a Hadoop-based language that is adept at very deep, very
    long data pipelines (a limitation of SQL). It is relatively simple and easier
    to use than SQL.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pig**: Pig Latin 是一种基于 Hadoop 的语言，擅长处理非常深且非常长的数据管道（这是 SQL 的一个限制）。它比 SQL 更简单，更易于使用。'
- en: '**Resource management**:YARN is a great enabler of dynamic resource utilization,
    an integral part of the Hadoop framework. It manages the increasing workloads
    of multi-tenant users, running various Hadoop applications without performance
    impact.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源管理**: YARN 是动态资源利用的一个重要推动力，是 Hadoop 框架的核心部分。它管理多租户用户的日益增加的工作负载，运行各种 Hadoop
    应用而不会影响性能。'
- en: '**Unified administration**: Ambari is a RESTful API that provides a user-friendly
    web interface for Hadoop administration. It’s a tool for Apache Hadoop clusters''
    provisioning, managing, and monitoring. Provisioning tasks for a Hadoop cluster
    include installing Hadoop services across multiple hosts and configuring Hadoop
    services for the cluster. Ambari manages Hadoop cluster services such as starting,
    stopping, and reconfiguring Hadoop services across the entire cluster through
    the central management console. Ambari monitors a Hadoop cluster with a dashboard
    for monitoring the health and status of the Hadoop cluster, and integrates with
    the Ambari Metrics System for metrics collection and the Ambari alert framework.
    This alerting system will notify if a node goes down, or disk utilization is higher
    than a threshold, and so on.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一管理**: Ambari 是一个 RESTful API，提供一个用户友好的网页界面用于 Hadoop 管理。它是 Apache Hadoop
    集群的配置、管理和监控工具。Hadoop 集群的配置任务包括在多个主机上安装 Hadoop 服务并为集群配置 Hadoop 服务。Ambari 通过中央管理控制台管理
    Hadoop 集群服务，例如启动、停止和重新配置 Hadoop 服务。Ambari 通过仪表盘监控 Hadoop 集群的健康状况和状态，并与 Ambari
    Metrics System 集成，用于指标收集，以及 Ambari 警报框架。该警报系统会在节点宕机、磁盘使用率超过阈值等情况下进行通知。'
- en: '**Workflow management**:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作流管理**:'
- en: '**Oozie**: A workflow processing system that manages and schedules a series
    of jobs. Jobs can be written in multiple languages such as MapReduce, Pig, and
    Hive, and linked logically to one another. Oozie allows scheduling dependent jobs
    as an output of one query to be completed to feed data into the next job as input.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Oozie**: Oozie 是一个工作流处理系统，管理和调度一系列任务。任务可以用多种语言编写，如 MapReduce、Pig 和 Hive，并且可以逻辑地相互链接。Oozie
    允许调度依赖任务，将一个查询的输出作为下一个任务的输入，确保数据流的顺利进行。'
- en: '**ZooKeeper**: ZooKeeper is a centralized service for maintaining configuration
    information, naming, providing distributed synchronization, and providing group
    services. All of these kinds of services are used in some form or another by distributed
    applications.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZooKeeper**: ZooKeeper 是一个集中式服务，用于维护配置文件、命名、提供分布式同步和提供组服务。所有这些服务都以某种形式被分布式应用程序使用。'
- en: '**Comprehensive data security and governance:**'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**综合数据安全与治理:**'
- en: Security in Hadoop has three modes of implementation--authentication, authorization,
    and encryption.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 的安全性有三种实现模式——身份验证、授权和加密。
- en: Authentication ensures only genuine users have access to the Hadoop cluster.
    Currently, the tools used are MIT Kerberos, AD, OpenLDAP, and so on.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份验证确保只有真实用户能够访问 Hadoop 集群。目前使用的工具有 MIT Kerberos、AD、OpenLDAP 等。
- en: Authorization grants users data privileges such as read-only, write, modify,
    delete, and so on. The currently used tool is Apache Sentry.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 授权授予用户数据权限，如只读、写、修改、删除等。目前使用的工具是 Apache Sentry。
- en: Data encryption ensures data protection from unauthorized access to data, both
    at rest and in transit. The encryption tool for data at rest is Navigator Encrypt
    and the tools for data in transit can be implemented by enabling TLS/SSL.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据加密确保数据在静态和传输过程中免受未经授权的访问。静态数据加密工具是 Navigator Encrypt，传输中的数据加密工具可以通过启用 TLS/SSL
    来实现。
- en: Access administration of Hadoop systems can be a challenging task in distributed
    environments that host the individual components of Hadoop on different clusters
    for optimal performance. For example, in a large production environment, there
    will be different cluster groups responsible for workflow, for data storage, data
    analytics, and so on; so managing the respective group access privileges could
    be a daunting task.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式环境中管理 Hadoop 系统的访问权限可能是一项挑战，因为这些环境将 Hadoop 的各个组件部署在不同的集群中，以优化性能。例如，在大型生产环境中，将会有不同的集群组负责工作流、数据存储、数据分析等；因此，管理各个组的访问权限可能会是一项艰巨的任务。
- en: Big data clusters
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据集群
- en: A Hadoop cluster is a system comprising two or more computers or systems (called
    nodes). It represents a single unified system for the users. The nodes work together
    to execute applications or perform other tasks like a virtual machine. There are
    variants of Hadoop clusters that cater for different data needs. The key features
    in the construction of these platforms are reliability, load balancing, and performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 集群是由两个或更多计算机或系统（称为节点）组成的系统。它为用户提供一个统一的系统。节点协同工作来执行应用程序或执行其他任务，类似于虚拟机。根据不同的数据需求，有多种
    Hadoop 集群变体。这些平台构建的关键特性是可靠性、负载均衡和性能。
- en: The single node or pseudo-distributed cluster has the essential daemons such
    as NameNode, DataNode, JobTracker, and TaskTracker, all run on the same machine.
    A single node cluster is a simple configuration system used to test Hadoop applications
    by simulating a full cluster-like environment with a replication factor of 1.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 单节点或伪分布式集群包含必要的守护进程，如 NameNode、DataNode、JobTracker 和 TaskTracker，这些进程都在同一台机器上运行。单节点集群是一种简单的配置系统，用于通过模拟完整集群环境来测试
    Hadoop 应用，且其副本因子为 1。
- en: A small Hadoop cluster comprises a single master and multiple worker nodes.
    The master node is comprised of a Job Tracker, Task Tracker, NameNode, and DataNode.
    A slave or worker node performs the roles of both a DataNode and TaskTracker if
    required; data-only worker nodes and compute-only worker nodes can be configured.
    Such nodes are used for full stack development of Hadoop application and projects
    with a replication factor of 3, such as a multi-node cluster for effective backup.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 小型 Hadoop 集群由一个主节点和多个工作节点组成。主节点包含 Job Tracker、Task Tracker、NameNode 和 DataNode。一个从节点或工作节点在需要时可以同时充当
    DataNode 和 TaskTracker；可以配置仅存储数据的工作节点和仅计算的工作节点。这些节点用于 Hadoop 应用和项目的全栈开发，副本因子为
    3，例如用于有效备份的多节点集群。
- en: Multi-node or fully distributed clusters follow the master-slave architecture
    pattern of Hadoop cluster. The NameNode and TaskTracker daemon runs on the master
    machine and the DataNode and TaskTracker daemon runs on one or more slave machines.
    It is deployed for full stack production deployment of the Hadoop application
    and for projects with a replication factor of 3.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 多节点或完全分布式集群遵循 Hadoop 集群的主从架构模式。NameNode 和 TaskTracker 守护进程运行在主机上，DataNode 和
    TaskTracker 守护进程则运行在一个或多个从机上。此部署用于 Hadoop 应用的全栈生产部署，并且副本因子为 3。
- en: Hadoop cluster attributes
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 集群特性
- en: In this section, we will discuss the key attributes of Hadoop clusters, such
    as load balancing for high availability and distributed processing, and so on.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论 Hadoop 集群的关键特性，如高可用性的负载均衡和分布式处理等。
- en: High availability
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性
- en: '**High availability** (**HA**) clusters provide services and resources in an
    uninterrupted manner through the redundancy built to the system. High availability
    is to be accomplished both within the cluster and between clusters too.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**高可用性**（**HA**）集群通过系统中的冗余设计提供不中断的服务和资源。高可用性不仅要在集群内实现，也需要在集群之间实现。'
- en: High availability within a cluster is accomplished by a master node that monitors
    the worker nodes for any failure and ensures that the load is distributed to other
    active, working nodes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 集群内的高可用性通过一个主节点实现，主节点监控工作节点的任何故障，确保负载分配到其他活跃工作节点。
- en: High availability between cluster examples across regions is accomplished by
    having each cluster system monitor the others, and in the event of failover, replicating
    servers and services through redundant hardware and software reconfiguration.
    Fault tolerance for hardware is achieved with raid systems, and for network systems,
    in the event of link breakdown alternative link paths are provided for continuity
    of services.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同区域的集群实例之间实现高可用性是通过使每个集群系统监控其他集群，并在故障转移发生时，通过冗余硬件和软件重新配置复制服务器和服务来完成的。硬件故障容忍通过RAID系统实现，而对于网络系统，在链路断开时，提供备用链路路径以保证服务的连续性。
- en: Load balancing
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Load balancing among servers is a key valuable functionality in the increasing
    and explosive use of network and internet-based applications. It is an important
    feature that distributes incoming traffic requests from clients evenly across
    all the active node machines of the cluster that are allocated to the application.
    In case of a node failure, the requests are redistributed among the rest of the
    active nodes available and responsible for processing the orders. The web cluster,
    to be scalable, must ensure that each server is fully utilized, providing increased
    network capacity and improving performance. Web application servers based on load
    balancing are called **web farms**, and redirect requests independently as they
    arrive, based on a scheduler and an algorithm. A few popular algorithms for load
    balancing are least connections, round robin, and weighted fair; each have unique
    applicability.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器之间的负载均衡是网络和基于互联网的应用程序日益广泛使用中的一个关键功能。它是一个重要特性，能够将来自客户端的流量请求均匀地分配到所有分配给应用程序的集群中的活跃节点机器上。在节点发生故障时，请求会重新分配到其余活跃的节点上，这些节点负责处理订单。为了可扩展，Web集群必须确保每个服务器得到充分利用，从而提高网络容量并改善性能。基于负载均衡的Web应用服务器被称为**Web农场**，它们根据调度器和算法独立地重定向请求。几种常见的负载均衡算法包括最少连接法、轮询法和加权公平法，每种算法都有其独特的适用性。
- en: High availability and load balancing
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性和负载均衡
- en: High availability and load balancing combines the features of both types of
    clusters, increasing the availability and scalability of services and resources.
    Consistent HA and load balancing is the backbone of the entire web hosting and
    e-commerce project; it needs to ensure support of the scalability of traffic volume
    on networks, without eventually becoming a bottleneck or single point of failure.
    Apart from simply redirecting client traffic to other servers, the web systems
    need to have verification of servers, redundancy, and balancing characteristics
    such as full-time communication checks. This type of cluster configuration is
    widely used in airline and train reservation systems, e-commerce, banking, email
    servers, and 24/7 applications.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性和负载均衡结合了两种类型集群的特性，增加了服务和资源的可用性和可扩展性。持续的高可用性和负载均衡是整个Web托管和电子商务项目的支柱；它需要确保支持网络流量的可扩展性，而不会最终成为瓶颈或单点故障。除了简单地将客户端流量重定向到其他服务器外，Web系统还需要验证服务器、冗余和负载均衡特性，如全天候的通信检查。这种类型的集群配置广泛应用于航空公司和火车订票系统、电子商务、银行、电子邮件服务器以及24/7应用程序中。
- en: Distributed processing and parallel processing
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式处理和并行处理
- en: Distributed processing involves dividing a large computational task into smaller
    tasks, for processing to run in parallel on individual smaller clusters of nodes.
    It represents a massively parallel supercomputer. This model of cluster is effective
    for application in large computational tasks, and improves the availability and
    performance. These cluster systems are used for scientific research-based computing,
    weather forecasts, and so on; tasks that require high-processing power.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式处理涉及将一个大型计算任务分解成多个较小的任务，以便在单独的小型节点集群上并行运行。它代表了一个大规模并行的超级计算机。这种集群模型适用于大规模计算任务的应用，并提高了可用性和性能。这些集群系统用于基于科学研究的计算、天气预报等需要高处理能力的任务。
- en: Usage of Hadoop big data cluster
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop大数据集群的使用
- en: Big data clusters are built for varied purposes, such as storage, analytics,
    testing, development, and so on. It is imperative to have the right size of the
    cluster for the right kind of workload, so capacity planning for a cluster is
    an important and critical task.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据集群用于多种目的，如存储、分析、测试、开发等。为正确的工作负载选择合适大小的集群至关重要，因此集群的容量规划是一项重要且关键的任务。
- en: 'Big data clusters catering to different purposes are as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 满足不同需求的大数据集群如下：
- en: '**Development cluster**: There are many requirements for building a development
    platform, such as the technological validation of porting an application to develop
    functionality on a big data platform, so as to develop advanced analytics use
    cases with machine learning.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发集群**：构建开发平台有许多需求，例如验证将应用程序迁移到大数据平台上开发功能的技术，以便开发带有机器学习的高级分析用例。'
- en: '**Test cluster**: A test platform is built to test the features and functionality
    developed in the development cluster.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集群**：构建一个测试平台，用于测试在开发集群中开发的功能和特性。'
- en: '**Data lake cluster**: A build used to provide extensive storage capacity,
    data from different source systems, including third-party data sources, are gathered
    into the data lake. There will be preprocessing activities to filter and perform
    aggregations on the incoming data before it is loaded into the data lake. A data
    lake serves multipurpose data needs for different departments of an organization.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据湖集群**：用于提供广泛存储容量的构建，来自不同源系统的数据（包括第三方数据源）被汇聚到数据湖中。在数据加载到数据湖之前，会进行预处理活动以过滤和执行聚合操作。数据湖为组织中不同部门提供多功能的数据需求。'
- en: '**Analytical cluster**: A platform for performing advance analytics using appropriate
    algorithms, and publishing the generated results.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析集群**：一个用于执行高级分析的平台，使用适当的算法并发布生成的结果。'
- en: Hadoop big data cluster nodes
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop大数据集群节点
- en: 'We will discuss the different types of nodes along with their role and usage
    in Hadoop Ecosystem:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论不同类型的节点及其在Hadoop生态系统中的角色和用途：
- en: '**NameNode**: The NameNode is an important part of an HDFS file system. It
    keeps the directory tree of all files in the file system, and tracks across where 
    the cluster data files are stored. The data for these files is not stored at all.
    Client applications communicate with NameNode whenever there is a need to locate
    a file, or when they want to modify a file. The modifications are stored by NameNode
    as a log that is appended to a native file system file edits. When a NameNode
    starts up, it reads the HDFS state from an image file, fsimage, and then applies
    the edits to the log file.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NameNode**：NameNode是HDFS文件系统的重要组成部分。它保存文件系统中所有文件的目录树，并跟踪集群数据文件的存储位置。该文件的数据本身并不存储。每当客户端应用程序需要查找文件，或想要修改文件时，都会与NameNode进行通信。这些修改会作为日志存储在NameNode中，并附加到本地文件系统的编辑文件中。当NameNode启动时，它会从一个镜像文件（fsimage）读取HDFS的状态，并将编辑应用到日志文件中。'
- en: '**Secondary NameNode**: Secondary NameNode''s whole purpose is to have a checkpoint
    in HDFS. The Secondary NameNode is just a helper node for NameNode; it merges
    the fsimage and the edits log files periodically and keeps edits log size within
    a limit.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Secondary NameNode**：Secondary NameNode的整个目的是在HDFS中设置检查点。Secondary NameNode仅是NameNode的辅助节点；它定期合并fsimage和编辑日志文件，并保持编辑日志的大小在一个限制内。'
- en: '**DataNode**: A DataNode stores data in HDFS. A functional file system has
    more than one DataNode, with data replicated across them. Client applications
    can talk directly to a DataNode, once the NameNode has provided the location of
    the data.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataNode**：DataNode在HDFS中存储数据。一个功能齐全的文件系统拥有多个DataNode，数据在它们之间进行复制。客户端应用程序可以直接与DataNode进行通信，一旦NameNode提供了数据的位置。'
- en: '**Edge/Hop Node**: Edge nodes are the interface between the Hadoop cluster
    and the outside network. Most commonly, edge nodes are used to run client applications
    and cluster administration tools. They''re also often used as staging areas for
    data being transferred into the Hadoop cluster.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘/跳跃节点**：边缘节点是Hadoop集群与外部网络之间的接口。通常，边缘节点用于运行客户端应用程序和集群管理工具。它们也常常作为数据传输到Hadoop集群的中转区。'
- en: '**Cluster management**: Cluster management software applications provide end-to-end
    functionality and features for managing cluster landscapes. It facilitates the
    improvement of performance, enhances the quality of service, increases compliance
    and reduces administrative costs.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群管理**：集群管理软件应用程序提供端到端的功能和特性，用于管理集群环境。它有助于提升性能，增强服务质量，提高合规性并降低管理成本。'
- en: '**Security** for the HDFS file level and node level is depicted as follows:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：以下是HDFS文件级别和节点级别的安全性：'
- en: '![](img/f49a9d39-e82a-4b2c-b762-5b600c511c7b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f49a9d39-e82a-4b2c-b762-5b600c511c7b.png)'
- en: Typically, the edge node is connected to the outside world through an external
    switch, that enables third-party systems to access through Kafka or STB-based,
    and SSH access for inbound users.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，边缘节点通过外部交换机连接到外部世界，允许第三方系统通过 Kafka 或基于 STB 的方式访问，并为入站用户提供 SSH 访问。
- en: The preceding entire mentioned cluster servers interact with each other by a
    dedicated network switch, isolating traffic from the outside world.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有集群服务器通过专用网络交换机相互连接，隔离与外部世界的流量。
- en: Types of nodes and their roles
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点类型及其角色
- en: '![](img/b30dd897-1b90-4288-9038-aca34bdddafd.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b30dd897-1b90-4288-9038-aca34bdddafd.png)'
- en: '**Nodes**: A Hadoop cluster can have a different configuration of servers based
    on the role they fulfill. This can be broadly divided into three types, with a
    different hardware configuration for each type:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：Hadoop 集群可以根据其所承担的角色配置不同的服务器。大致可以分为三种类型，每种类型的硬件配置不同：'
- en: '**Master** **n****ode** (also referred to as **n****ame node**): In an enterprise
    deployment, this runs crucial management services. These nodes only store metadata
    so do not need a lot of storage, but since these files are critical and important,
    master node services include as follows:'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主节点**（也称为**名称节点**）：在企业部署中，主节点运行关键的管理服务。这些节点只存储元数据，因此不需要大量存储空间，但由于这些文件至关重要，主节点服务包括以下内容：'
- en: Enterprise manager
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业管理器
- en: Resource manager
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源管理器
- en: Standby resource manager
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备用资源管理器
- en: NameNode
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NameNode
- en: Standby NameNode
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 备用 NameNode
- en: Journal nodes
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志节点
- en: HBase master
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase 主节点
- en: Hive server
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive 服务器
- en: Sqoop server
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sqoop 服务器
- en: ZooKeeper
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZooKeeper
- en: Oozie server
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oozie 服务器
- en: Spark/Job history server
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark/作业历史服务器
- en: Cloudera search
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudera 搜索
- en: Cloudera Navigator
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudera Navigator
- en: Hive metastore
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive 元存储
- en: Kafka master
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka 主节点
- en: Flume master
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flume 主节点
- en: '**Worker node**: Worker/slave nodes in a Cloudera enterprise deployment are
    the ones that run worker services. Since tasks are performed by these nodes along
    with the storing of actual data, they are designed to be fault tolerant. Worker
    nodes can have the following roles:'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作节点**：在 Cloudera 企业部署中，工作/从节点是运行工作服务的节点。由于这些节点执行任务并存储实际数据，因此它们设计为容错的。工作节点可以有以下角色：'
- en: Data node
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据节点
- en: Node manager
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点管理器
- en: HBase region server
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase 区域服务器
- en: Impala daemons
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala 守护进程
- en: Solr servers
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Solr 服务器
- en: Kafka broker
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka 经纪人
- en: Flume agent
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flume 代理
- en: '**Gateway/edge node:** These are where Hadoop client services run, and include:'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网关/边缘节点**：这些是运行 Hadoop 客户端服务的地方，包括：'
- en: Third-party tools
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方工具
- en: Hadoop command-line clients
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 命令行客户端
- en: Beeline
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeline
- en: Impala shell
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Impala Shell
- en: Flume agents
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flume 代理
- en: Hue server
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hue 服务器
- en: Spark and other gateway services
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 和其他网关服务
- en: HA proxy
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HA 代理
- en: Commercial Hadoop distributions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业 Hadoop 发行版
- en: 'As we have seen in the previous section, Hadoop is open stack community distribution
    with an integration of multiple components or interface layers. Many commercial
    vendors have built on the basic Hadoop platform and customized it to offer in
    the market, both as a hardware product platform and a service. We will discuss
    a few of the popular options as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Hadoop 是一个开源社区分发的堆栈，集成了多个组件或接口层。许多商业厂商在基本的 Hadoop 平台上进行定制，提供市场上的硬件产品平台和服务。我们将讨论以下几种流行的选项：
- en: Cloudera CDH Hadoop distribution
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudera CDH Hadoop 发行版
- en: '**Hortonworks Data Platform** (**HDP**)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hortonworks 数据平台**（**HDP**）'
- en: MapR Hadoop distribution
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapR Hadoop 发行版
- en: Amazon Elastic MapReduce
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊弹性 MapReduce
- en: IBM open platform
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM 开放平台
- en: Microsoft Azure's HDInsight--cloud-based Hadoop distribution
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微软 Azure 的 HDInsight——基于云的 Hadoop 发行版
- en: Pivotal big data suite
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键大数据套件
- en: Hadoop Cloudera enterprise distribution
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop Cloudera 企业版
- en: 'The standard framework of Hadoop open source, with different layers and components,
    is presented as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 开源的标准框架，包含不同的层和组件，展示如下：
- en: '![](img/ea1a7098-e85e-4626-ac75-887eee989597.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea1a7098-e85e-4626-ac75-887eee989597.png)'
- en: The Cloudera proprietary distribution framework is built on the Hadoop open
    source code, customizing the services as shown in the following topics. We will
    discuss the various components that make it a leading enterprise product.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera 专有的发行版框架是基于 Hadoop 开源代码构建的，定制化服务如以下主题所示。我们将讨论使其成为领先企业产品的各个组件。
- en: Data integration services
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集成服务
- en: Data from external source systems ingests into Hadoop systems can be through
    multiple modes based on the business need.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 外部源系统的数据导入到 Hadoop 系统可以通过多种方式，具体取决于业务需求。
- en: '**Batch transfer**:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量传输**：'
- en: '**Apache Sqoop**: Sqoop is a command-line interface application for transferring
    data between relational databases and Hadoop. It supports the saved jobs that
    can be run multiple times helping us to import updates made to a database since
    the last import. It also supports the incremental loads of a single table or a
    free-form SQL query.'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Sqoop**：Sqoop 是一个命令行界面应用程序，用于在关系型数据库和 Hadoop 之间传输数据。它支持保存的作业，可以多次运行，帮助我们导入自上次导入以来对数据库所做的更新。它还支持增量加载单个表或自由格式
    SQL 查询。'
- en: '**Real-time data transfer**:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时数据传输**：'
- en: '**Apache Kafka**: Kafka is an open source message broker project developed
    by the Apache Software Foundation, written in Scala. The project aims to provide
    a complete, high-throughput, low-latency platform for handling real-time data
    feeds. It’s a highly scalable pub/sub message queue architected as a distributed
    transaction log, making it very important for enterprise infrastructures to process
    streaming data.'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka**：Kafka 是一个由 Apache 软件基金会开发的开源消息代理项目，使用 Scala 编写。该项目旨在提供一个完整的、高吞吐量、低延迟的实时数据流处理平台。它是一个高度可扩展的发布/订阅消息队列，构建为分布式事务日志，这使得它在企业基础设施中处理流数据时非常重要。'
- en: '**Apache Flume**: Flume adopts a simple, flexible and distributed architecture
    for streaming data. It effectively ingests large amounts of log data reliably
    and aggregates it. It has a simple, extensible data model, flexible for building
    and supporting online analytical applications. Flume is a pretty robust, fault
    tolerant, reliable service with built-in failover and recovery features.'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Flume**：Flume 采用一种简单、灵活且分布式的架构来进行数据流处理。它能够可靠地摄取大量日志数据并进行聚合。它有一个简单且可扩展的数据模型，灵活地构建和支持在线分析应用程序。Flume
    是一个非常强大、容错性高且可靠的服务，具有内置的故障切换和恢复功能。'
- en: '**Apache Chukwa**: Chukwa is a framework for data collection and analysis on distributed
    file systems such as Hadoop, simplifying log analysis, processing and monitoring.
    Chukwa agents run on respective machines to collect the logs generated from various
    applications. It offers a high degree of flexibility to ingest the huge log data
    generated by servers. Collectors receive the data from the agent and write them
    to HDFS, which serves as storage, and the MapReduce framework will process, analyze,
    and parse the jobs and archive the huge log data.'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Chukwa**：Chukwa 是一个用于在分布式文件系统（如 Hadoop）上进行数据收集和分析的框架，简化了日志分析、处理和监控。Chukwa
    代理在相应的机器上运行，收集来自各种应用程序生成的日志。它提供了高度的灵活性来摄取由服务器生成的大量日志数据。收集器从代理接收数据并将其写入 HDFS，HDFS
    作为存储，MapReduce 框架将处理、分析和解析任务，并归档这些海量日志数据。'
- en: '**Apache Avro**: Avro is a language-neutral remote procedure call and data
    serialization framework developed within Apache''s Hadoop project. Since Hadoop
    writable classes lack language portability, Avro uses JSON for defining data types
    and protocols and serializes data in a compact binary format. Avro is quite helpful
    for dealing with data formats that can be processed by multiple languages such
    as Java, C, C++, C#, Python, and Ruby.'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Avro**：Avro 是一个语言中立的远程过程调用和数据序列化框架，开发于 Apache 的 Hadoop 项目中。由于 Hadoop
    可写类缺乏语言可移植性，Avro 使用 JSON 来定义数据类型和协议，并以紧凑的二进制格式序列化数据。Avro 对于处理可以被多种语言（如 Java、C、C++、C#、Python
    和 Ruby）处理的数据格式非常有帮助。'
- en: Hadoop data storage
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop 数据存储
- en: 'Data storage in Hadoop is a key function and we will discuss various modes
    of accomplishing this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 中的数据存储是一个关键功能，我们将讨论实现这一目标的各种方式：
- en: '**Apache HDFS (Filesystem)**: The HDFS is a distributed, scalable, and portable
    file system written in Java for the Hadoop framework. HDFS stores large files--typically
    in the range of gigabytes to petabytes--across multiple machines and data nodes.
    Data nodes can talk to each other to rebalance data, to move copies around, and
    to keep the replication of data high.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache HDFS（文件系统）**：HDFS 是一个分布式、可扩展且便携的文件系统，使用 Java 编写，旨在支持 Hadoop 框架。HDFS
    存储大型文件——通常是从几 GB 到 PB 级别——并分布在多台机器和数据节点上。数据节点之间可以相互通信，重新平衡数据，移动副本并保持数据的高复制性。'
- en: '**Apache HBase (NoSQL)**: HBase is an open source, non-relational, distributed
    data store running atop of HDFS providing a fault-tolerant means of storing large
    amount of sparse data. HBase is a column-oriented key-value data store and has
    been admired greatly due to its lineage with Hadoop and HDFS. It is suitable for
    faster read and write operations on extensive datasets with high throughput and
    low input/output latency.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache HBase (NoSQL)**：HBase 是一个开源的非关系型分布式数据存储系统，运行在 HDFS 之上，提供了一种容错的方式来存储大量稀疏数据。HBase
    是一个面向列的键值数据存储系统，由于与 Hadoop 和 HDFS 的渊源，它备受推崇。它适用于在大规模数据集上进行快速的读写操作，具有高吞吐量和低输入/输出延迟。'
- en: '**Apache Kudu (Relational)**: Apache Kudu is an open source storage engine
    intended for structured data that supports low-latency random access, together
    with efficient analytical access patterns. It bridges the gap between HDFS and
    the HBase NoSQL database. Kudu tables look like those in SQL relational databases,
    to act as a storage system for structured data. Like RDBMS principles, primary
    keys are made up of one or more columns that enforce uniqueness and act as an
    index for efficient updates and deletes, as a storage system for tables of structured
    data.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kudu (关系型)**：Apache Kudu 是一个开源存储引擎，专为结构化数据设计，支持低延迟随机访问，并且能够高效地执行分析访问模式。它填补了
    HDFS 和 HBase NoSQL 数据库之间的空白。Kudu 表看起来像 SQL 关系型数据库中的表，作为结构化数据的存储系统。像关系型数据库管理系统（RDBMS）一样，主键由一个或多个列组成，确保唯一性，并充当高效更新和删除操作的索引，作为结构化数据表的存储系统。'
- en: Data access services
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据访问服务
- en: '**Apache Spark**: Spark is an open source framework for machine learning and
    stream processing based on in-memory technology for data processing. It provides
    programmers with a data structure called the resilient distributed dataset (RDD),
    an application programming interface. The RDD is read-only, distributing multiple
    sets of data items over a cluster of machines with fault tolerant features.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Spark**：Spark 是一个开源框架，用于机器学习和基于内存技术的数据处理流处理。它为程序员提供了一种名为弹性分布式数据集（RDD）的数据结构，这是一种应用程序接口。RDD
    是只读的，可以将多个数据集分布到一群机器上，具有容错功能。'
- en: '**Apache Hive**: The Hive data warehouse software facilitates reading, writing,
    and managing large datasets that reside in distributed storage using SQL. The
    structure can be projected onto data already in storage. A command-line tool and
    JDBC driver are provided to connect users to Hive.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Hive**：Hive 数据仓库软件通过 SQL 便于对存储在分布式存储中的大型数据集进行读取、写入和管理。结构可以映射到已经存储的数据上。提供了命令行工具和
    JDBC 驱动程序，用于将用户连接到 Hive。'
- en: '**Impala**: Impala is Cloudera''s SQL query engine for data stored in an Apache
    Hadoop cluster and running open source massively parallel processing (MPP). Impala
    enables users to run low latency SQL queries on data stored in HDFS and Apache
    HBase, without requiring additional data movement or transformation.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Impala**：Impala 是 Cloudera 的 SQL 查询引擎，用于查询存储在 Apache Hadoop 集群中的数据，支持开源的大规模并行处理（MPP）。Impala
    使用户能够对存储在 HDFS 和 Apache HBase 中的数据执行低延迟 SQL 查询，而无需额外的数据移动或转换。'
- en: '**Solr**: Apache Solr is a search platform for websites, popular for enterprise
    search because it can be used to index and search documents and email attachments.
    It''s built upon a Java library called **Lucene** ([http://whatis.techtarget.com/definition/Apache-Lucene](http://whatis.techtarget.com/definition/Apache-Lucene)),
    written in Java, and provides both a RESTful XML interface and a JSON API that
    are used to build search applications. Solr can search and index multiple websites,
    returning content related recommendations based on the taxonomy ([http://searchcontentmanagement.techtarget.com/definition/taxonomy](http://searchcontentmanagement.techtarget.com/definition/taxonomy))
    of the search query ([http://searchsqlserver.techtarget.com/definition/query](http://searchsqlserver.techtarget.com/definition/query)).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Solr**：Apache Solr 是一个网站搜索平台，因其可以用来索引和搜索文档及电子邮件附件，广受企业搜索领域的欢迎。它基于一个名为 **Lucene**
    的 Java 库（[http://whatis.techtarget.com/definition/Apache-Lucene](http://whatis.techtarget.com/definition/Apache-Lucene)），用
    Java 编写，提供了 RESTful XML 接口和 JSON API，用于构建搜索应用程序。Solr 可以搜索和索引多个网站，根据搜索查询的分类法（[http://searchcontentmanagement.techtarget.com/definition/taxonomy](http://searchcontentmanagement.techtarget.com/definition/taxonomy)）返回相关内容推荐（[http://searchsqlserver.techtarget.com/definition/query](http://searchsqlserver.techtarget.com/definition/query)）。'
- en: '**Apache Pig**: Pig provides a high-level language known as Pig Latin, a SQL-like
    language with many built-in operators for performing data operations such as joins,
    filters, ordering, and so on, and is used to perform all the data manipulation
    operations in Hadoop. The component of Apache Pig is Pig Engine that ingests the
    Pig Latin scripts as input and converts the scripts into MapReduce jobs. As a
    tool, it''s very efficient, reducing development and coding time. As a platform,
    it adopts multiple query paths, representing them as data flows to analyze large
    sets of data.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Pig**：Pig 提供了一种高级语言，称为 Pig Latin，它是一种类似 SQL 的语言，拥有许多内置操作符，用于执行数据操作，如连接、过滤、排序等，并用于执行
    Hadoop 中的所有数据操作。Apache Pig 的组件是 Pig Engine，它将 Pig Latin 脚本作为输入，并将脚本转换为 MapReduce
    作业。作为一个工具，它非常高效，能减少开发和编码时间。作为一个平台，它采用多个查询路径，将其表示为数据流来分析大量数据集。'
- en: '**Kite** is a high-level data layer for Hadoop, that provides an API and a
    set of tools to create logical abstractions on top of storage systems (such as
    HDFS) and operates in terms of records, datasets, and dataset repositories. It
    can access Mavin through plug-in and aid-in packaging, deploying and running distributed
    applications. It speeds up the development of stream processing ETL applications
    in Hadoop that extract, transform, and load data into target repositories such
    as Apache Solr, enterprise data warehouses, HDFS, HBase, and OLAP applications.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kite** 是 Hadoop 的高级数据层，提供 API 和一组工具，用于在存储系统（如 HDFS）之上创建逻辑抽象，并以记录、数据集和数据集仓库的形式操作。它可以通过插件访问
    Mavin，并协助打包、部署和运行分布式应用程序。它加速了 Hadoop 中流处理 ETL 应用程序的开发，这些应用程序提取、转换和加载数据到目标仓库，如
    Apache Solr、企业数据仓库、HDFS、HBase 和 OLAP 应用程序。'
- en: '**MapReduce** is a processing technique and framework based on Java for distributed
    computing. As the sequence of the name MapReduce implies, the reduce task is always
    performed after the map job. A map job usually splits the input dataset into independent
    chunks where individual elements are broken down into tuples (key/value pairs).
    The reduce framework sorts the outputs of the map jobs, which are then input to
    the reduce tasks. The tasks are processed in a completely parallel manner to scale
    data processing over multiple computing nodes. The input jobs and the output jobs
    are stored in a file system. The framework takes care of scheduling tasks, monitoring
    them and re-executing failed tasks. The MapReduce model framework can easily scale
    the application to run over tens of thousands of machines in a cluster through
    a configuration change.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce** 是一种基于 Java 的分布式计算处理技术和框架。正如 MapReduce 名称的顺序所暗示的那样，reduce 任务始终在
    map 作业之后执行。map 作业通常将输入数据集拆分为独立的块，每个块中的单个元素被分解为元组（键/值对）。reduce 框架对 map 作业的输出进行排序，然后将其输入到
    reduce 任务中。这些任务完全并行处理，从而在多个计算节点上扩展数据处理。输入作业和输出作业存储在文件系统中。该框架负责任务调度、监控并重新执行失败的任务。通过配置更改，MapReduce
    模型框架可以轻松扩展应用程序，使其在集群中的数以万计的机器上运行。'
- en: Database
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库
- en: Apache Cassandra architecture is a distributed NoSQL database management system
    known for its ability to scale, perform, and offer continuous uptime. Apache Cassandra
    is based on a ring design wherein all nodes play the equivalent role without any
    master concept. Compared to other architectures such as master-slave, legacy,
    or sharded design, Cassandra is quite easy to set up and maintain, and is designed
    for handling a high volume of structured data across commodity servers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Cassandra 架构是一个分布式 NoSQL 数据库管理系统，以其可扩展性、性能和持续运行的能力而著称。Apache Cassandra
    基于环形设计，其中所有节点都扮演相同的角色，没有主节点的概念。与其他架构（如主从架构、传统架构或分片设计）相比，Cassandra 设置和维护都非常简单，并且设计用于处理跨商品服务器的大量结构化数据。
- en: Apache Cassandra's high availability and scalable architecture enable it to
    handle large amounts of data, and thousands of concurrent users and operations
    spread across multiple data centers to ensure high-performance by distributing
    user traffic. Cassandra has built-in features such as data modeling, high availability
    clusters, monitoring tools, query language, and so on.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Cassandra 的高可用性和可扩展架构使其能够处理大量数据，以及跨多个数据中心分布的数千个并发用户和操作，通过分发用户流量来确保高性能。Cassandra
    具有内置功能，如数据建模、高可用性集群、监控工具、查询语言等。
- en: '![](img/d4b779ed-b3ba-41a6-b928-78829e7ce525.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4b779ed-b3ba-41a6-b928-78829e7ce525.png)'
- en: Unified (common) services
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统一（公共）服务
- en: '**Resource management**: Apache Hadoop **Yet Another Resource Negotiator**
    (**YARN**) is a cluster management technology. YARN is one of the key features
    of the second-generation Hadoop 2 version of the Apache Software Foundation''s
    open source distributed processing framework. It also enables versatility; the
    resource manager can support additional paradigms and not just map/reduce.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源管理**：Apache Hadoop的**Yet Another Resource Negotiator**（**YARN**）是一种集群管理技术。YARN是Apache软件基金会第二代Hadoop
    2版本的关键特性之一，也是其开源分布式处理框架的一部分。它还具有多功能性；资源管理器可以支持其他范式，而不仅仅是Map/Reduce。'
- en: '**Apache Oozie**: Apache Oozie is a system to schedule a workflow to manage
    Hadoop jobs. The directed acyclic graph is the mode of representing workflows
    in Oozie, which are a collection of control flow and activity nodes. The beginning
    and the end of a workflow and the mechanism to control the workflow execution
    path are defined by control flow nodes. The execution of a computation processing
    task through workflow triggers happens on action nodes.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Oozie**：Apache Oozie是一个调度工作流以管理Hadoop作业的系统。Oozie中使用有向无环图（DAG）表示工作流，它由控制流节点和活动节点组成。工作流的开始和结束，以及控制工作流执行路径的机制，都是由控制流节点定义的。通过工作流触发计算处理任务的执行发生在动作节点上。'
- en: '**Apache Sentry**: Hadoop''s strong security at the file system level lacks
    the granular support to adequately secure row-level access to data by users and
    BI applications. Sentry allows access control at the server, database, and table,
    and grants different privilege levels including select, insert, and so on. It
    provides for authenticated users privileges on data, the ability to control and
    enforce access to data, and so on. It enables fine-grained access control to data
    and metadata in Hadoop. The column level security can be implemented by creating
    a view of a subset of allowed columns by restricting the base table and granted
    privileges. Sentry administration is simple and convenient through role-based
    authorization. It is a policy engine that can easily grant multiple groups access
    to the same data at different privilege levels such as resource, roles, users,
    and groups.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Sentry**：Hadoop在文件系统级别提供强大的安全性，但缺乏足够的粒度来有效地保护用户和商业智能应用程序对数据的行级访问。Sentry允许在服务器、数据库和表级别进行访问控制，并授予不同的权限级别，包括选择、插入等。它为经过身份验证的用户提供数据访问权限，能够控制和强制执行数据访问等。它能够实现Hadoop中数据和元数据的细粒度访问控制。列级安全性可以通过创建允许列子集的视图来实施，通过限制基础表和授予权限来实现。Sentry的管理简单且方便，通过基于角色的授权进行管理。它是一个策略引擎，可以轻松地授予多个组在不同权限级别（如资源、角色、用户和组）上对相同数据的访问权限。'
- en: The following diagram and table provide a comprehensive view of the security
    model adopted by Hadoop systems. There are security requirements at multiple levels,
    such as clusters, user level, and application level in an enterprise-wide implementation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表提供了Hadoop系统所采用的安全模型的全面视图。企业级实现中，在多个层面上都存在安全需求，例如集群级、用户级和应用级等。
- en: 'Cloudera offers four layers of security as follows; perimeter, access, visibility,
    and data. Cloudera Enterprise Security can be classified into four broad categories;
    authentication, authorization, data protection and auditing:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudera提供了四层安全性，分别是；边界安全、访问控制、安全可见性和数据保护。Cloudera企业安全可以分为四个广泛类别；身份验证、授权、数据保护和审计：
- en: '![](img/3090755d-043a-4806-9485-c26549aae793.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3090755d-043a-4806-9485-c26549aae793.png)'
- en: 'Security features offered by popular tools are listed in the following table:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了流行工具提供的安全特性：
- en: '![](img/7252d1d0-f435-4573-a00c-fe5de170c2bd.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7252d1d0-f435-4573-a00c-fe5de170c2bd.png)'
- en: Cloudera proprietary services and operations/cluster management
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cloudera专有服务与操作/集群管理
- en: '**Cloudera Navigator**: Cloudera Navigator is part of Cloudera Enterprise,
    and is a fully integrated data management and security system for the Hadoop platform.
    Cloudera Navigator is the data governance solution for Hadoop, presenting crucial
    capabilities such as data discovery, continuous optimization, audit, lineage,
    metadata management, and policy enforcement. Cloudera Navigator supports continuous
    data architecture optimization and meeting regulatory compliance requirements.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cloudera Navigator**：Cloudera Navigator是Cloudera Enterprise的一部分，是一个完全集成的数据管理和安全系统，旨在为Hadoop平台提供支持。Cloudera
    Navigator是Hadoop的数据治理解决方案，提供数据发现、持续优化、审计、数据血缘、元数据管理和策略执行等关键功能。Cloudera Navigator支持持续的数据架构优化，并满足合规性要求。'
- en: '**Cloudera Manager**: Cloudera Manager is an end-to-end application for managing
    CDH clusters. Cloudera Manager sets the standard for enterprise deployment by
    delivering granular visibility into and control over every part of the CDH cluster—empowering
    operators to improve performance, enhance the quality of service, increase compliance
    and reduce administrative costs.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloudera Director**: Cloudera Director works with Cloudera Manager and the
    cloud service provider to provide centralized and programmatic administration
    of clusters in the cloud, including deployment, configuration, and maintenance
    of CDH clusters. With Cloudera Director, you can monitor and manage multiple Cloudera
    Manager and CDH deployments, across different cloud environments.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Hadoop Hortonworks framework
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image is the framework of Hortonworks. Many components are the
    same as the Hadoop stack, as seen previously; we will discuss the components unique
    to this distribution:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34af9bb3-26af-44c4-8b42-401d066fcfc1.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: Data governance and schedule pipeline
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Falcon is a data management tool for managing dependencies between the
    system infrastructure, data, and processing logic. Data administrators can define
    operational and data governance policies for Hadoop workflows for overseeing data
    pipelines in Hadoop ([http://searchdatamanagement.techtarget.com/definition/Hadoop-2](http://searchdatamanagement.techtarget.com/definition/Hadoop-2))
    clusters. Using Falcon, we can manage thousands of compute nodes, with a large
    number of jobs typically running on a cluster at any given time, ensuring a consistent
    and dependable performance on complex processing jobs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Falcon relies on Oozie job scheduling software to generate the processing workflows,
    to set procedures for replication, and for the retention and archiving of incoming
    data. The data governance engine schedules and monitors data management policies
    such as enhanced monitoring, and so on. The other features are tracing jobs activities
    for failures, dependencies, audits, and lineage, and also tagging the data to
    comply with data retention and discovery requirements:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7609f8d4-f321-4a37-9a77-799b8ca34371.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Cluster management
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Knox is a bastion security server; shielding direct access to Hadoop
    cluster nodes helps setups be more secure for enterprise-ready installations.
    Knox can easily scale horizontally by supporting stateless protocols. Knox provides
    authentication functionality to be managed by users and groups using LDAP or active directory.
    Identity Federation is SSO and HTTP header based.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Authorization is supported through an **access control list** (**ACL**) on service
    levels. The Knox Policy enforcement ranges from authentication, federation, authorization,
    audit, dispatch, host mapping and content rewrite rules. The policy is enforced
    through a list of providers, defined within the topology and the cluster definition,
    for purposes of routing and translation between user-facing URLs and cluster internals.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Data access
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Tez**: This is an application framework that allows for a complex
    directed-acyclic-graph of tasks for processing data. It is a resource-management
    framework, based on Apache YARN functionality, that is extensible for building
    high-performance batch and interactive data processing applications, leading to
    significant improvements in response times while maintaining MapReduce''s ability
    to scale to petabytes of data. Tez caters for cases that require the near-real-time
    performance of query processing and machine learning, and so on, with a powerful
    framework based on expressing computations as a data flow graph.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache HCatalog**: This is a storage management layer for Hadoop, that facilitates
    the easy reading and writing of data from the Hadoop cluster grid with different
    data processing tools such as Hive, Pig, MapReduce, and so on. For different kinds
    of data formats stored on HDFS, such as RCFile, Parquet, ORC files, or Sequence
    files, it uses Hive **Serializer-Deserializer** (**SerDe**) to enable a relational
    view. Apache HCatalog provides features such as table abstraction and data visibility
    to tools for cleaning and archiving.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f11c4c6c-a956-47cb-9d52-2651bfaca05c.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Data workflow
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The WebHDFS protocol provides external applications over the internet, HTTP,
    or web access for managing files and data stored in the HDFS cluster, on a par
    with the high-performance native protocol or native Java API default with Hadoop
    Cluster. WebHDFS is based on an industry-standard RESTful mechanism that provides
    security on par with native Hadoop protocols. Using WebHDFS common tools such
    as `curl/wget`, users can access the HDFS for operations such as reading files,
    writing to files, making directories, changing permissions, renaming, and so on.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: A Hadoop MapR framework
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MapR is a commercial distribution of Apache Hadoop with HDFS replaced with
    MapR-FS**.** The following is the MapR framework with common Hadoop open source
    components; we will now review the components unique to this distribution:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af731398-08f5-49d7-ae2a-a3c98b7178c4.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: Machine learning
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLLIB is a collection of machine-learning algorithms and utilities for prediction
    models and data sciences. There are some broad groups, such as classification,
    clustering, collaborative filtering, and so on. A few of the ML algorithms used
    for each category are listed following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: Logistic regression, naive Bayes'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: Generalized linear regression, survival regression, decision
    trees, random forests, and gradient-boosted trees'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: K-means, Gaussian mixtures (GMMs), frequent itemsets, association
    rules, and sequential pattern mining'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX** is a new component in Spark for graphs and graph-parallel computation,
    used to implement new types of algorithms that require the modeling of relationships
    between objects. In many real-world applications, such as social networks, networking,
    and astrophysics, graph processing is very effective and efficient at representing
    a model relationship between entities visually.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: SQL stream
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Drill is a distributed SQL engine that enables data exploration and analytics
    on non-relational data stores such as Hadoop, MapR, CDH, NoSQL (MongoDB, HBase),
    cloud storage (Amazon S3, Google Cloud Storage, Azure Blog Storage, Swift), and
    so on. It uses a shredded, in-memory, columnar execution engine for distributed
    query optimization and execution, for complex data and schema-free data. By using
    a query engine that compiles and re-compiles queries at runtime, high performance
    is achieved for any structure of data. Using standard SQL and BI tools, users
    can query the data without having to create and manage schemas. It supports schema-free
    JSON document models, similar to MongoDB and Elasticsearch, and industry-standard
    APIs--ANSI SQL, ODBC, JDBC, and RESTful APIs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Shark** is a data warehouse-based system used with Apache Spark; its
    distributed query engine enhances high-end analytical results and the performance
    of Hive Queries multifold. Shark supports most of the Hive''s features, such as
    query language, metastore, serialization formats, and user-defined functions.
    Apache Shark is built on top of Apache Spark, which is a parallel data execution
    engine; hence, Shark can respond to complex queries in sub-second latency.  It
    provides the maximum performance gains offered by column-wise memory storage systems,
    as data is stored and processed within the cluster''s memory or in a database
    with in-memory materialized views.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Storage, retrieval, and access control
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Accumulo** provides fine-grained data access control and cell-level access
    control, with complex policies governing access to sensitive data. It is a low-latency,
    large table data storage and retrieval system, with a key/value store based design.
    Accumulo provides extremely fast access to data in massive HDFS tables, while
    also controlling access to its millions of rows and columns down to the individual
    cell. It enables the intermingling of different data sets with access control
    policies for fine-grained access to data sets by encoding the policy rules for
    each individual data cell, and controls fine-grained access.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Data integration and access
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hue is an open source web interface for analyzing data within any HDFS cluster
    through Apache Oozie. It comes with many built-in features such as Hue's Editor
    to build workflows and then schedule them to run regularly and automatically.
    It has a dashboard for data querying, monitoring progress and logs, and performing
    actions such as pausing or stopping jobs. The applications supported are Apache
    Hive, Apache Impala (incubating), MySQL, Oracle, PostgreSQL, SparkSQL, Apache
    Solr SQL, Apache Phoenix, Apache Solr, and Apache Spark.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '**HttpFS**--Apache Hadoop HttpFS is a service that provides HTTP access to
    HDFS through REST APIs, supporting all HDFS file system operations (both read
    and write). It supports data transfer between HDFS clusters running different
    versions of Hadoop (overcoming RPC versioning issues) or a cluster behind a firewall.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning and coordination
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data provision tools enable cloud hosting and coordination services for
    Hadoop systems, two popular choices are discussed following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '**Juju**:The container option available with Hadoop distribution is the Juju
    framework. This allows users to deploy software built locally on a range of services,
    including MAAS, EC2, Azure, LXD containers. Juju can model, configure and manage
    services and deploy to all major public and private clouds with only a few commands.
    Hundreds of preconfigured services are available in the Juju store.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Whirr--big data on clouds**: Apache Whirr can be used to define, provision
    and configure big data solutions on cloud platforms such as Amazon EC2, Rackspace
    servers, and CloudStack. Whirr automatically starts instances (set of libraries)
    in the cloud and bootstraps Hadoop through on them. It initiates cloud-neutral
    big data services to define and provision Hadoop clusters in the cloud, and adds
    packages such as Hive, HBase, and Yarn for MapReduce jobs.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivotal Hadoop platform HD Enterprise
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the Pivotal HD Enterprise framework. The open source Hadoop
    components in this framework were already discussed earlier:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55ad2cfb-01c9-4fa2-94fe-79f9a3131f90.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: A Hadoop ecosystem on IBM big data
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a Hadoop ecosystem based on IBM big data. We are already familiar
    with most of the open source Hadoop big data components listed in this framework:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c0369df-41b9-4d1e-86a0-ad2009d2a128.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: A Hadoop ecosystem on AWS
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon **Elastic MapReduce** (**EMR**) is a service that allows users to launch
    and scale Hadoop clusters inside of Amazon's web infrastructure. EMR instances
    use Amazon's prebuilt and customized EC2 instances, which greatly simplifies the
    setup and management of the cluster of Hadoop and MapReduce components. EMR can
    analyze large datasets on AWS cloud Hadoop clusters quite effectively.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'An AWS EMR framework depicting multiple service layers is shown in the following
    diagram:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37f71ce0-7ed8-4c92-b30a-fb3a40f49ea5.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: 'An AWS EMR framework that offers integrations with a wide choice of Hadoop
    open source components is presented as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '**R**: It is a GNU package, open source programming language and software environment
    for statistical computing and graphics, and is widely used among statisticians
    and data miners for developing statistical software and data analysis. R as a
    language is both flexible and powerful.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Presto**:Apache Presto is a distributed parallel cross-platform query execution
    engine on the Hadoop platform. Presto supports using standard ANSI SQL to query
    multiple sources such as HDFS, MySQL, Cassandra, Hive, relational databases, and
    other data stores. Presto runs multiple analytic queries, optimized for low latency
    and interactive query analysis, and scales without downtime. Presto supports most
    of today''s best industrial applications such as Facebook, Teradata and Airbnb,
    and so on.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradle**:The landscape of modern software development is continuously evolving,
    and so are the needs for build automation. Projects involve large and diverse
    software stacks with multiple programming languages and a wide spectrum of testing
    strategies. Adopting agile practices leads to the early integration of code, as
    well as frequent and easy delivery to both test and production environments, as
    supported by builds. Gradle is an open source build automation system that builds
    upon the concepts of Apache Ant, and Apache Maven.  Gradle uses a directed acyclic
    graph to schedule the order of the tasks and introduces a Groovy-based domain-specific
    language for declaring the project configuration. Gradle was designed for multi-project
    builds for its ability to manage dependencies. Gradle can define and organize
    large project builds, as well as modeling dependencies between projects. It supports
    incremental builds by intelligently determining the build tree dependencies and
    need for re-execution.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cascading**:Cascading is an application development platform for building
    big data applications on Apache Hadoop, providing an abstraction layer for Apache Hadoop
    and Apache Flink. Cascading is used to create and execute complex data processing
    workflows on a Hadoop cluster, hiding the underlying complexity of MapReduce jobs.
    Cascading provides an optimal level of abstraction with the necessary options
    through a computation engine, systems integration framework, data processing,
    and scheduling capabilities. Cascading offers Hadoop development teams portability
    for simple or complex data applications without incurring the costs of rewriting
    them.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Phoenix**:Apache Phoenix is an open source, massively parallel relational
    database engine that uses Apache HBase as a base to support OLTP for Hadoop. It
    provides random, real-time access to large datasets with a familiar SQL interface
    to Hadoop systems such as Spark, Hive, Pig, Flume, and MapReduce.  Apache Phoenix
    abstracts away the underlying data store. Aggregation queries are executed on
    the nodes where data is stored, reducing the need to send massive data over the
    network.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Mahout**: Apache Mahout is a suite of scalable machine learning algorithms
    focused primarily in the areas of collaborative filtering, clustering, and classifications.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Hadoop platform is HDInsight hosted on Microsoft Azure
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the architecture of an HDInsight Ecosystem hosted on Microsoft
    Azure. While some of the native open source layers are embedded as is, some others
    are tailored and customized as per Microsoft proprietary offerings:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fc5e870-d38c-462e-a4cb-29a61e6ee0a8.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Capacity planning for systems
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sizing a Hadoop cluster is an important task as there are many factors influencing
    the performance. Capacity planning and the sizing of a Hadoop cluster are imperative
    for optimizing the distributed cluster environment with its related software.
    The number of machines, specifications of the machines, and effective process
    per node planning will allow you to optimize the performance effectively.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the Hadoop ecosystems, different layers (components/services) interact
    with each other, leading to performance overheads associated within a complex
    cluster stack between any of the layers; hence the need for requisite performance
    tests at each interface and appropriate tuning, as depicted in the following diagram:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46744cf0-b7cc-460f-ac1b-92abbc66feba.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: 'There are many factors that influence the capacity planning, sizing, and performance
    of a complex Hadoop-distributed cluster. The following are a few factors for consideration:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '**Amount of data**:'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The volume of data and growth
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The data retention policy of how many years to hold data before discarding
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Also the data storage mechanism (data container, type of compression used if
    any)
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type of workload**:If workloads are CPU/IO /memory intensive, we will have
    to consider hardware accordingly. If processing might grow rapidly, we have to
    consider adding new data nodes.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequency** **of workload**: If the data load is batch or real-time streaming
    data, would it be a few times a day, nightly, weekly or monthly loads?'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type of security**:Authentication, authorization, and encryption'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type of services required**:'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the business SLA for the cluster? Is there is a requirement for real-time
    support?
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What types of services are running other than core Hadoop services?
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How many third-party tools will be installed/used?
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice of an operating system**:Selecting an operating system depends on
    multiple factors, such as your team''s administration competency, the cost of
    procurement and maintenance, stability, performance, reliability, support availability,
    and so on.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CentOS**: Linux CentOS is functionally compatible with RHEL, and a popular
    choice towards work nodes in Hadoop clusters.'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RedHat Enterprise Linux (RHEL)**: Linux RHEL is widely used for servers in
    Hadoop clusters.'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ubuntu**: A very popular distribution, based on Debian –  both desktop and
    server versions available.'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLSE**: A Linux Enterprise Server developed by SUSE. It is designed for servers,
    mainframes, and workstations but can be installed on desktop computers for testing
    as well.'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solaris, OpenSolaris**: Not popular in production clusters.'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fedora Core:** Linux distributions for servers and workstations.'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network considerations**: Hadoop is very bandwidth-intensive, since most
    of the time all the nodes are communicating with one other simultaneously. Consider
    the usage of dedicated switches, a network Ethernet bandwidth of 10 GB/sec, and
    racks that are interconnected through switches.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guideline for estimating and capacity planning
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Proper cluster sizing includes selecting the right hardware for a master and
    a worker, as well as edge nodes, while keeping the cost low.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few factors to consider, along with some general guidelines for
    planning capacity sizing:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '**Data size**:'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data sizing, the recommended replication factor is 3
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the total data to be stored is *Y* TB, it will become *3Y* TB after replication
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If any compression techniques are used to store data then a compression factor
    can be considered
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For disk efficiency, only 60-70% usage is recommended for the total disk availability
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: So, the total disk capacity including disk compression factor = *3 x Y x 7*
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data growth rate**:'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering data growth factor, say 1 TB per year
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a 3 x replication factor
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk efficiency factor of 70%
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to calculate *1 TB * 3 / .6  = 4-5 TB*
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage capacity per 1 TB of data growth
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is equivalent to adding a new node, considering the data processing needs
    and growth of data volume
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number of data nodes planner:'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's consider that we need to store 200 TB of data in HDFS
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: With 3 x replication, it will be 600 TB
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering the replication factor, it will be *600 * 1.3   = 780* TB storage
    (nearly)
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming per node there are 12 disks with 2 TB capacity = 24 TB per node
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of nodes required would be *780/24* = *33* nodes
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering data growth requirements, we need to plan for cluster expansion
    in terms of additional nodes required per month, week, year, and so on
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster-level sizing estimates
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster sizing and capacity planning is important for various nodes as per their
    roles.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: For master node
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Capacity planning for master mode is critical and needs consideration of system
    resources and services hosted on it as detailed following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory**:Capacity sizing of the master name node memory is a very important
    task. The rule of thumb is to have 1 GB of heap size to store 1 million blocks.
    For example:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a 5-node cluster
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw storage on each node is 20 TB
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider that the HDFS block size is 128MB
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of blocks = *5 * (20*1024*1024)*  = *33* *million* (approximately)
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total HDFS block size is 33 million
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on a replication factor of 3, the required heap size = *33/3* = *11*
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The total number of actual blocks would be approximately 11 GB
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other factors that we need to consider as well:'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name node would be the stable entity, so you should plan for the future growth
    prospects as well, while planning the sizing configuration
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Services such as resource manager, HBase master, zookeeper, and so on would
    also run on this node
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule of thumb is to keep a minimum of 2 GB for services such as HBase and 4
    GB for the resource manager
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on normal usage, the memory configuration for the master node should be
    128 GB to 512 GB
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: Cores of 2 GHz or above of processor per node are reasonable, depending
    on the number of services running on the node.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disk**: The local disks on the master node should be configured as RAID10
    with hot spares to give good performance and fast recovery on the loss of a disk.
    There should be a separate partition for `/var` with a minimum size of 1 TB, or
    based on the capacity required for log storage. It is strongly recommended to
    configure High Availability in production.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker node
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Capacity planning for worker node needs due consideration for system resources
    (memory, CPU, disk) as per the services it runs:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory**: Memory on the worker nodes is based on the type of workload and
    daemons they will be running. You should consider 256 GB to 512 GB of memory for
    each of the worker nodes.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU**: CPU capacity is based on the type of workload and numbers of parallel
    MapReduce tasks that are planned. By enabling hyperthreading, the total number
    of MapReduce tasks should be 1.5 times of the number of the cores. Consider a
    minimum of 24 cores with processors greater than 2 GHz.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disk**: You should consider having a large number of small SATA disks (2
    to 4 TB), instead of a small number of large disks. All disks should be configured
    as JBODs with noatime, nodiratime along with `/var` as a separate partition from
    the root OS partition. It is recommended you add additional nodes to enhance storage,
    as it will increase processing power too.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gateway node
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gateway nodes do not run any specific Hadoop services; their configuration should
    be based on what jobs they will run.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '**Network**:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network is a core component and should be considered very carefully as processing
    in Hadoop is based on data proximity.
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If workloads consist of map-only jobs that are only transforming the data, there
    isn't a lot of data movement over the wire. If workloads have a lot of reduce
    actions such as aggregation, joins, and so on, then there is a lot of data movement
    between the nodes, in which case we should consider a minimum network capacity
    of 10 GB.
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Running other services such as HBase, Spark should also be taken into account
    while considering network configuration.
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider an average of 15 to 20 nodes per rack.
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dual Ethernet cards bonded together are recommended to support failover.
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It's good practice to set up data locality and replication configured across
    the rack (rack awareness), so that even if one rack goes down, data is still available.
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Core switches connected to top of the rack switches should be of high bandwidth
    (10 GB/sec or  more). Consider redundancy for top of rack as well as core switches.
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered big data Hadoop components, popular frameworks,
    and their unique components for various services. In the next chapter, we will
    cover the terminology and technologies of the cloud such as public, private, hybrid
    models, service offerings for infrastructure, platform, identity, software, and
    network as service. We will also present a few popular market vendors.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
