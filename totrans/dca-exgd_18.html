<html><head></head><body>
        

                            
                    Summarizing Important Concepts
                
            
            
                
<p>In this chapter, you will learn which topics are most important for the exam and you will get a good idea of the knowledge required to pass the <strong>Docker Certified Associate</strong> (<strong>DCA</strong>) exam.</p>
<p>We will recap all the topics we have learned regarding orchestration, image management, Docker platform component installation and configuration, networking implementations for standalone and cluster environments, and security features and data management strategies in container-based applications. All these concepts were already covered in different chapters.</p>
<p>We will summarize the following topics in this chapter:</p>
<ul>
<li>Reviewing orchestration concepts</li>
<li>A brief summary of Docker image concepts</li>
<li>A summary of the Docker architecture, installation, and configuration topics</li>
<li>A summary of the networking topics</li>
<li>Understanding security concepts and related Docker features</li>
<li>Quickly summarizing Docker storage and volumes</li>
</ul>
<p class="mce-root">By the end of this chapter, you will be ready for some exam-like questions, which have been prepared for you in the next chapter. Before looking at some sample questions, let's start talking about the orchestration concepts we have learned.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-8c040d52-abec-4fa8-a342-3396dd14558b" class="mce-root">Reviewing orchestration concepts</h1>
<p>Orchestration is an important topic for the DCA exam. It represents 25% of the questions you have to pass to get this certification. In the second section of the book, we introduced orchestration and we covered Docker Swarm and Kubernetes.</p>
<p>Orchestration concepts were covered in <a href="3b13261c-9b4d-46e2-b115-fc323563f646.xhtml">Chapter 7</a>, <em>Introduction to Orchestration</em>, <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>, <a href="abcbf266-c469-4d84-ad4f-abd321a64b53.xhtml">Chapter 9</a>,  <em>Orchestration Using Kubernetes</em>, <a href="3a2d6b8e-40d0-44f2-8a07-75969a28cc6b.xhtml">Chapter 10</a>, <em>Introduction to the Docker Enterprise Platform</em>, and <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>.</p>
<p>This is a quick summary of the Docker Swarm features. We recommend you read this summary to remember the concepts we have learned:</p>
<ul>
<li>We started talking about multi-container applications before introducing orchestration because it is the first approach to container orchestration. They work locally, using Docker Compose (the <kbd>docker-compose</kbd> tool) and application components, and their interaction is described using <kbd>docker-compose.yml</kbd> YAML files. Multi-container applications run all of their components together on a Docker host, but we can scale their components up and down, as well as interacting with them and reviewing their logs.</li>
<li>Docker Swarm orchestrates Docker services to provide them with resilience, internal discovery, and load balancing in cluster environments. Our applications' workloads will be distributed cluster-wide.</li>
<li>We will use two kinds of node roles within Docker Swarm—managers and workers—which can be modified.</li>
<li>We will deploy more than one manager and more than one worker to provide high availability to the cluster and workloads deployed on top of it.</li>
<li>One of the managers is also the leader of the cluster and will update all cluster resource changes in an internal database, synced between manager nodes. Docker Swarm uses the Raft algorithm to update changes, hence a quorum between managers is required before changes are committed.</li>
<li>Docker Swarm has a management plane, a control plane, and a data plane. The management and control planes can be isolated from the data plane, and they work encrypted out of the box. The data plane can also be encrypted but not by default (we have to encrypt each custom network).</li>
<li>Docker Swarm issues and maintains an internal <strong>Certificate Authority</strong> (<strong>CA</strong>) and manages certificates for all cluster components. We can lock this information to keep it safe.</li>
<li>A minimum of (<em>number of managers / 2 + 1</em>) healthy manager nodes is required to maintain the cluster health. If we have less than the required number, no changes can be made within the cluster but application workloads will continue working. If a service fails, it will not be recovered if the cluster is unhealthy.</li>
<li>Docker Swarm uses the Raft log to maintain internal key-value store synchronization between nodes. Therefore, an odd number of managers is required to keep a quorum. This also applies to Kubernetes, but it uses <kbd>etcd</kbd> as a key-value store.</li>
<li>All nodes can run application workloads, but we can change this behavior whenever we need draining nodes or to disallow new workloads without interrupting already-running ones. </li>
<li>Cluster workloads are declared as services, with a required number of instances or replicas to be healthy. These resources are tasks and they will run one container.</li>
<li>Docker Swarm does not manage containers; it only manages services. Therefore, we deploy applications based on services. We do not deploy standalone containers.</li>
<li>Services receive one virtual IP address by default and this address does not change during their lifetime. Tasks run only one container; they do not have an associated IP address, and they always keep their names. If a task's container dies or needs to be modified with some updates, a new task will be created with the original name. The container will receive a new IP, but the internal load balancer will associate it as a backend endpoint for the service.</li>
<li>We can scale up or down the number of tasks required for a service whenever we need to. However, Docker will not manage our application behavior under these circumstances.</li>
<li>Tasks are scheduled automatically on healthy nodes if they have enough resources to run associated services' tasks, but we can force a task location on specific nodes.</li>
<li>Docker provides some template tools to help us format, filter, and create unique resources using Docker Swarm variables.</li>
<li>Networking in Docker Swarm uses bridged interfaces, as we also learned with Docker containers. We deploy overlay networks distributed cluster-wide using VXLAN technology to provide communication between containers running on different hosts and other network features.</li>
<li>Docker Swarm provides a router mesh as a default strategy to publish cluster services for users and other applications. By default, services' ports will be published on all cluster nodes even if they do not run any services' tasks. Internal routing guides service requests to appropriate backend containers. We can change these behaviors with common Docker Swarm command options.</li>
<li>As we learned with Docker Engine, services are not published to be consumed by default. We need to manually publish service ports and processes.</li>
<li>Publishing applications to the world can be done using the router mesh on Docker Swarm, or Interlock in Docker Enterprise. Interlock provides an integrated and automated reverse-proxy solution to secure your application's backends. We just publish the <kbd>interlock-proxy</kbd> component while the Docker Swarm services receive requests internally. Hence, no additional publishing is required for services; just configure a few labels to inform Interlock about the required forwarding for an application.</li>
<li>We can create as many overlay networks as required and they will be isolated from each other, as we also learned with custom bridge networks.</li>
<li>Orchestration introduced some new concepts, such as secrets and configurations, to provide stored information that is distributed cluster-wide. Secrets are secured and encrypted by Docker Swarm and we use them to configure passwords, certificates, or tokens using on-memory filesystems. Configuration objects help us to distribute configurations on containers running on different hosts without having to sync files between nodes manually.</li>
<li>In Docker Swarm, we deploy an application using Docker stacks. These resources allow us to deploy multi-service applications that are distributed cluster-wide. We will define all the required application resources (services, secrets, networks, configurations, volumes, and so on) in a <kbd>docker-compose</kbd>-like file and we will use these files to deploy the complete application. All changes or updates in application components should be written in these files because it allows us to manage the application's deployments as code.</li>
<li>Application component updates are managed using rolling updates. We can deploy changes manually or using Docker stacks. In both cases, we can deploy changes smoothly, avoiding service interruption and user impact. If an update goes bad, we can easily execute a rollback to run the previous service configuration.</li>
<li>We also reviewed the Kubernetes orchestrator because it is included in Docker Enterprise. This orchestrator has many differences from Docker Swarm, although both manage containers at the end. We learned all about Kubernetes components and their interactions.</li>
<li>Docker Enterprise deploys full vanilla (non-customized) Kubernetes for us out of the box, including Calico as a <strong>Container Network Interface</strong> (<strong>CNI</strong>) by default. All worker nodes (DTR requires dedicated workers) can be set to run either Docker Swarm, Kubernetes workloads, or even both orchestrator workloads.</li>
<li>Pods are the smallest scheduling unit in Kubernetes but they do not provide resilience. We need to integrate them on orchestrated templated resources, such as ReplicaSets, DaemonSets, or deployments.</li>
<li>Kubernetes provides a flat network. This means that all pods that are deployed will see each other. Service-to-service communications are always allowed by default. To ensure security in this situation, we need to deploy NetworkPolicy resources to allow only specific component communications.</li>
</ul>
<p>Although Docker stacks and multi-container applications using <kbd>docker-compose</kbd> use the same type of YAML files, some keys are only valid for one of them. For example, keys such as <kbd>depends_on</kbd>, <kbd>build</kbd>, or <kbd>volumes_from</kbd> are only available for Docker Compose multi-container applications; therefore, we will receive a warning message indicating this issue when we try to use them for Docker stacks.</p>
<p>Let's review the required topics for the exam.</p>
<h2 id="uuid-b4e32d2c-e522-42cc-bb94-57766b827245" class="mce-root">Required knowledge for the exam</h2>
<p>The exam will verify our knowledge of the following topics, among others:</p>
<ul>
<li>Completing the setup of a Swarm mode cluster, with managers and worker nodes</li>
<li>Describing and demonstrating how to extend the instructions to run individual containers into running services under Swarm</li>
<li>Describing the importance of a quorum in a Swarm cluster</li>
<li>Describing the difference between running a container and running a service</li>
<li>Interpreting the output of the <kbd>docker inspect</kbd> commands</li>
<li>Converting an application deployment into a stack file using a YAML compose file with <kbd>docker stack deploy</kbd></li>
<li>Manipulating a running stack of services</li>
<li>Describing and demonstrating orchestration activities</li>
<li>Increasing the number of replicas</li>
<li>Adding networks and publishing ports</li>
<li>Mounting volumes</li>
<li>Describing and demonstrating how to run replicated and global services</li>
<li>Applying node labels to demonstrate the placement of tasks</li>
<li>Describing and demonstrating how to use templates with <kbd>docker service create</kbd></li>
<li>Identifying the steps needed to troubleshoot a service that is not deploying</li>
<li>Describing how a Dockerized application communicates with legacy systems</li>
<li>Describing how to deploy containerized workloads as Kubernetes pods and deployments</li>
<li>Describing how to provide configuration for Kubernetes pods using ConfigMap and secret resources.</li>
</ul>
<p>These topics are extracted from Docker's official study guide, which can be found at <a href="https://success.docker.com/certification">https://success.docker.com/certification</a>.</p>
<h1 id="uuid-b87ba231-d279-4c3f-af3c-16071adafdc6" class="mce-root">A brief summary of Docker image concepts</h1>
<p>Images are fundamental to creating containers, and this topic represents around 20% of the DCA exam questions. We covered Docker images in depth in <a href="3952ec16-ca49-4bc2-b7e6-d6f17fec3fab.xhtml">Chapter 2</a>, <em>Building Docker Images</em>, but we also talked about them in <a href="c2dd78c4-066f-40b4-94e7-a7e2904d7ec2.xhtml">Chapter 3</a>, <em>Running Docker Containers</em>, <a href="e9fd3807-5bbd-4ea8-84f7-ee02d288643d.xhtml">Chapter 6</a>, <em>Introduction to Docker Content Trust</em>, and <em><a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13</a>, Implementing an Enterprise-Grade Registry with DTR</em>. In this chapter, we will quickly review all of the concepts seen in those chapters as a summary.</p>
<p>Let's review the most important concepts, features, and actions associated with Docker images before getting into the required knowledge section:</p>
<ul>
<li>Images are based on copy-on-write filesystem strategies. They are based on different overlapping layers, applied using different union filesystems and storage drivers. Currently, the most used filesystem driver for containers is <kbd>overlay2</kbd>. Docker Engine chooses the most adequate graph driver for our system, although we can change it.</li>
<li>Containers are just isolated processes running on Docker hosts. We use images as templates to provide a root filesystem and meta-information to control processes' behavior.</li>
<li>There are three methods for creating images:
<ul>
<li><strong>Using a Dockerfile</strong>: This file contains all the steps required to install our application with all its dependencies, as well as how it should be started. We also provide which ports and protocols should be used to communicate with the container's processes. This method is reproducible and it provides infrastructure-as-code behavior.</li>
<li><strong>Running containers and committing</strong>: In this case, we run a container, and inside, we run commands to install and configure our applications. When all the changes are made in the container's filesystem, we commit those changes to make an image. This method is not reproducible. We usually use this workflow when application installation cannot be automated, for example.</li>
<li><strong>Images from scratch</strong>: In this case, images are lightweight because they just include an empty root filesystem with application binaries and dependencies. This root filesystem does not include any non-required operating system files. We add our binaries using Dockerfile copy keys.</li>
</ul>
</li>
<li>Multi-stage building can also be included as an alternative method for creating images. In this case, we declare different build processes in just one Dockerfile. We define a descriptive name for each one and we define a workflow, copying files from different builds. This allows us to define a phase to compile an application using the required compilers, headers, or libraries on an application-development image and just copy the final development product to another phase, with a runtime environment. As a result, the runtime image is much smaller than the development one.</li>
<li>Dockerfiles create images by executing containers. Each container makes changes in its root filesystem and these changes will be committed (stored) for subsequent containers, using the previous container's layers for execution.</li>
<li>Smaller images are more secure because having non-required binaries, libraries, and configurations inside images is risky. Images should only contain the required content for our application.</li>
<li>There are a few important practices to follow to build better images:
<ul>
<li>Never add debugging tools or compilers to production images.</li>
<li>Declare all required resources on your images, such as exposed ports, the user required for the main process execution, and the directories that will be used as volumes. These will help other users to easily understand how your application works and should be used.</li>
<li>Do not use root on your application's images unless it is strictly required by processes to work.</li>
<li>Build your images to run just one process per container. With many processes per container, it is hard to maintain and verify their health.</li>
<li>We always have to choose between the portability of layers between images and the images' sizes. There are cases where it is better to have fewer layers, while at other times it is better to have more layers because others will reuse them. Image layer caching is key to speeding up the image building process.</li>
<li>Always add health checks inside your Dockerfiles to help Docker Engine verify the container's health.</li>
</ul>
</li>
<li>Docker provides all of the required commands for building and shipping Docker images. We can also inspect their content or build history to help us debug their processes or create new images.</li>
<li>It is key to understand that dangling images, unreferenced layers from previous builds, will stay in your Docker hosts until you remove them. Administrators should keep the Docker platform clean to avoid hosts degrading due to disk space being lost.</li>
<li>Good image tagging is fundamental on container platforms. We can also use labels on Dockerfiles to add meta-information to Docker images. You should try to uniquely identify images by their tags, but remember that only an image's ID will really identify an image uniquely. An image can have many names and tags, but only one ID.</li>
<li>We can include variables inside Dockerfiles. This will help us to build images with special features for different stages. We can deliver a production-ready image into production systems while having debugging and instrumentation tools on testing images. They will still have common application binaries but we will use a debugging version to review some specific problems. Variables can also be modified as arguments for the <kbd>docker build</kbd> command line.</li>
</ul>
<p>Let's get an idea of the topics required for the exam.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h2 id="uuid-41b9c2c1-2af0-4d86-95f8-d7b1d46bda84" class="mce-root">Required image management knowledge for the exam</h2>
<p>The exam will verify our knowledge of the following topics, among others:</p>
<ul>
<li>Describing the use of a Dockerfile</li>
<li>Describing options, such as <kbd>add</kbd>, <kbd>copy</kbd>, <kbd>volumes</kbd>, <kbd>expose</kbd>, and <kbd>entrypoint</kbd></li>
<li>Identifying and displaying the main parts of a Dockerfile</li>
<li>Describing and demonstrating how to create an efficient image via a Dockerfile</li>
<li>Describing and demonstrating how to use CLI commands to manage images, such as <kbd>list</kbd>, <kbd>delete</kbd>, <kbd>prune</kbd>, and <kbd>rmi</kbd></li>
<li>Describing and demonstrating how to inspect images and report specific attributes using <kbd>filter</kbd> and <kbd>format</kbd></li>
<li>Describing and demonstrating how to tag an image</li>
<li>Describing and demonstrating how to apply a file to create a Docker image</li>
<li>Describing and demonstrating how to display the layers of a Docker image</li>
<li>Describing and demonstrating how to modify an image to a single layer</li>
<li>Describing and demonstrating registry functions</li>
<li>Deploying a registry</li>
<li>Logging into a registry</li>
<li>Utilizing search in a registry</li>
<li>Pushing an image to a registry</li>
<li>Signing an image in a registry</li>
<li>Pulling and deleting images from a registry</li>
</ul>
<p>These topics are extracted from Docker's official study guide, which can be found at <a href="https://success.docker.com/certification">https://success.docker.com/certification</a>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-cf18f26b-9571-48c8-8f6b-6f49c2d49317" class="mce-root">A summary of the Docker architecture, installation, and configuration topics</h1>
<p>The installation and configuration of the Docker platform are key to every Docker Enterprise administrator. These topics represent 15% of the exam content. They were covered in multiple chapters for standalone and cluster environments. We learned about these concepts in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>, <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml"/><a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>, <a href="3a2d6b8e-40d0-44f2-8a07-75969a28cc6b.xhtml">Chapter 10</a>, <em>The Docker Enterprise Platform</em>, <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml"/><a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>, and <a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml"/><a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13</a>, <em>Implementing an Enterprise-Grade Registry with DTR</em>.</p>
<p>This is a quick summary of special characteristics and tips for the installation and configuration of the Docker platform. We recommend that you read this summary to ensure you remember the concepts learned:</p>
<ul>
<li><strong>Docker components on standalone and cluster environments</strong>: We should have a good idea of Docker Enterprise component distribution and features.</li>
<li><strong>The installation processes for each component on different platforms</strong>: We have seen that installation is easy in both the Docker Community and Docker Enterprise environments. Review the installation processes for the different platforms and ensure that you have a good idea of the configuration file locations.</li>
<li>You must know all the components' requirements and the steps required to deploy a <strong>Container-as-a-Service</strong> (<strong>CaaS</strong>) enterprise-ready solution, with high availability on core components.</li>
<li><a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>, showed many configuration procedures for Docker Engine. By default, Docker will choose the best storage driver for your Docker layers. Remember that we used <kbd>overlay2</kbd> because we should be able to change it if our installation has different requirements.</li>
<li>Ensure that you have a good understanding of what files are under the <kbd>/var/lib/docker</kbd> directory (or the one configured) and what should be stored in your Docker Engine's backups. You also learned about the procedures to create <strong>Universal Control Plane</strong> (<strong>UCP</strong>) and <strong>Docker Trusted Registry</strong> (<strong>DTR</strong>) backups and the steps and cases where restoration is required.</li>
<li>Only Docker Enterprise and Kubernetes provide role-based access. We covered basic Docker Enterprise permissions and configurations for UCP and DTR in <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>, and <a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13</a>, <em>Implementing an Enterprise-Grade Registry with DTR</em>, respectively.</li>
<li>Review how we configured TLS communications for client authentication when we do not need different levels of authorization. This was covered in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>.</li>
<li>Cgroups and kernel namespaces provide container isolation. These are key to ensuring processes have enough resources without any non-authorized communication with other processes running on the same host.</li>
</ul>
<p>We will now review which topics should be known about for the exam.</p>
<h2 id="uuid-e893b3a2-41ba-41d5-ad7b-d17fb5dc900b" class="mce-root">The knowledge required about the Docker platform for the exam</h2>
<p>The exam will verify our knowledge of the following topics, among others:</p>
<ul>
<li>Describing sizing requirements for installation</li>
<li>Describing and demonstrating the setup of a repo, the selection of a storage driver, and the installation of the Docker engine on multiple platforms</li>
<li>Describing and demonstrating the configuration of logging drivers (<kbd>splunk</kbd>, <kbd>journald</kbd>, and so on)</li>
<li>Describing and demonstrating how to set up Swarm, configure managers, add nodes, and set up the backup schedule</li>
<li>Describing and demonstrating how to create and manage users and teams</li>
<li>Describing and demonstrating how to configure the Docker daemon to start on boot</li>
<li>Describing and demonstrating how to use certificate-based client-server authentication to ensure a Docker daemon has the right to access images on a registry</li>
<li>Describing the use of namespaces, cgroups, and certificate configuration</li>
<li>Describing and interpreting errors to troubleshoot installation issues without assistance</li>
<li>Describing and demonstrating the steps to deploy Docker Engine, UCP, and DTR on <strong>Amazon Web Services</strong> (<strong>AWS</strong>) and on-premises with high availability</li>
<li>Describing and demonstrating how to configure backups for UCP and DTR</li>
</ul>
<p>These topics are extracted from Docker's official study guide, which can be found at <a href="https://success.docker.com/certification">https://success.docker.com/certification</a>.</p>
<p class="mce-root"/>
<h1 id="uuid-d30b8a93-45e8-486e-baa6-77e5010d3ebb" class="mce-root">A summary of the networking topics</h1>
<p>Networking is one of the core components of microservice application architecture. Faster networks allowed the evolution of distributed architectures. High availability and resilience can be provided using modern infrastructures, even on cloud or cloud-hybrid architectures. Containers work like small virtual nodes and they get virtual interfaces. We learned that network namespaces allow us to isolate processes on the same host, even if they use the same bridge interface to communicate with the real network, out of the host's network namespaces. Distributed networking on clusters is also simple because Docker Swarm manages all the internal infrastructures and processes required to allow communication between containers on different hosts. Overlay networks in Docker Swarm, distributed cluster-wide, use VXLAN to encapsulate traffic and can even be encrypted. By default, the Docker Swarm control plane's components are secured using <strong>Mutual TLS</strong> (<strong>MTLS</strong>) communication and we can isolate application data from network management.</p>
<p>All of these topics were covered in multiple chapters on Docker Engine, Docker Swarm, and Kubernetes. We learned about these topics in <a href="e7804d8c-ed8c-4013-8449-b746ee654210.xhtml">Chapter 4</a>, <em>Container Persistency and Networking</em>, <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>, <a href="abcbf266-c469-4d84-ad4f-abd321a64b53.xhtml">Chapter 9</a>, <em>Orchestration Using Kubernetes</em>, <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>, and <a href="ab131f1f-ca6e-4815-9a3a-8c92c93c9dbc.xhtml">Chapter 12</a>, <em>Publishing Applications in Docker Enterprise.</em></p>
<p>In <a href="abcbf266-c469-4d84-ad4f-abd321a64b53.xhtml">Chapter 9</a>, <em>Orchestration Using Kubernetes</em>, we learned how Kubernetes implements network features cluster-wide. We also reviewed these features side by side against Docker Swarm implementations to have a good idea of how we can use both or make container workloads that can run on any of them.</p>
<p>We also learned that containers can expose their application processes internally. Other containers can consume their services but we need to publish their ports for external users and applications. This is very important because security is ensured in Docker Swarm for containers working in the same network. They are isolated, hence we can publish only frontend applications' components.</p>
<p>Let's look at some network topics as a summary:</p>
<ul>
<li>Docker Engine networking is based on bridge networking, although we can use MacVLAN interfaces (with real IP addresses), underlying the host's networking (using its network namespace), and can even extend default behavior using plugins. We can use default or custom bridge networking. Custom networks also deploy internal DNS facilities, hence, containers running on these networks will know each other by their names. In some special cases, it is useful to deploy containers without networking features.</li>
<li>Networking in Docker Swarm is easy because Docker creates new virtual networks (overlay networks) and deploys VXLAN tunnels to encapsulate all hosts' traffic. Containers deployed for services' tasks can see each other if they are working on the same overlay network.</li>
<li>The Kubernetes network model is even easier. It is based on a flat network where services and pods are always reachable by default. For this to work, we need to integrate a CNI. Each CNI has its own implementation of this flat network model and Docker Enterprise deploys Calico (<a href="https://www.projectcalico.org/">https://www.projectcalico.org/</a>) by default.</li>
<li>A flat network is unsecured by default because applications' components are not isolated. We will use network policies to isolate applications, grouping them by namespaces, labels, and so on. NetworkPolicy resources manage connection rules to allow or disallow specific pods' connections and hence their traffic.</li>
<li>Docker Swarm nodes use encrypted TLS communications by default (mutual TLS). Docker manages all of the required certificates. Users' communications with the cluster are not secure, but we can also create secure communications manually (we have a complete example explaining all of the required steps in <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration  Using Docker Swarm</em>) or use UCP-integrated <strong>Role-Based Access Control</strong> (<strong>RBAC</strong>). UCP provides users with bundles, containing all the required files to create secure TLS tunnels.</li>
<li>Kubernetes also encrypts its control plane. Docker Enterprise does all the deployment work for us and a fully functional Kubernetes cluster will be up and running after its installation. Certificates will be used to deploy TLS tunnels between Kubernetes components and users by default.</li>
<li>Internal DNS is deployed for local custom bridge and overlay networks. Therefore, containers and services can be discovered by their names. Containers use an internal DNS and an external resolution will be forwarded to a specific external DNS. By default, containers receive the host's DNS configuration, but we can change this behavior.</li>
<li>Kubernetes also integrates an internal DNS. In this case, the <kbd>kube-dns</kbd> component will manage all service entries.</li>
<li>Internal load balancing is also deployed in overlay networks. Remember that services can be replicated or global. In both cases, <kbd>vip</kbd> mode is used by default, and services get an IP address in the special ingress network. This IP address is registered and the internal load balancer will route requests to all available services' replicas. We can avoid this behavior using the <kbd>dns-round-robin</kbd> mode.</li>
<li>Kubernetes' internal load balancing has similar behavior. All services will receive an internal virtual IP address by default (a ClusterIP). Services in Kubernetes are logical groups of pods and services' requests will be forwarded by default to all associated pods.</li>
<li>As mentioned before, an application deployed within containers will not publish their ports unless we declare this behavior. Publishing ports on Docker Engine is easy and we can ensure that only specific IP addresses will listen on a published port on multi-homed nodes. Bridge networking uses NAT for publishing an application's ports. Docker creates all of the required hosts' firewall rules to allow and route this traffic. If we use the host's networking, all container-exposed ports will be published and applications will be directly accessible.</li>
<li>We also learned that services in Docker Swarm will be published by default in all nodes, even if they do not run any services' tasks. This feature is known as a router mesh and application ports will be available in all of the clusters' hosts. Internal load balancing will also be applied using an ingress overlay network and instances in different hosts will be reachable. This can be insecure because all application ports will be accessible on all hosts.</li>
<li>Kubernetes' <kbd>NodePort</kbd> services have equivalent behavior to Docker Swarm's router mesh. Services declared as <kbd>NodePort</kbd> will publish their ports on all cluster nodes. However, Kubernetes also has the <kbd>LoadBalancer</kbd> service type. These services will be published directly using infrastructure load balancers. This integration only works on some cloud providers.</li>
<li>UCP provides Interlock as a solution to avoid unsecured router mesh publishing. We have learned about Interlock's components and deployment and how we publish applications using this tool. Interlock's ports must be published, but all other applications' services can be accessed through Interlock. Therefore, we do not need to publish applications' ports. This improves security because Interlock acts as a reverse proxy, providing TLS security, host- and content-based services routing, and sticky sessions, among others. The Interlock proxy component will be updated using services' labels; therefore, only services with specific labels will be published. We have learned about these required labels and reviewed a few examples of their usage.</li>
<li>Kubernetes can integrate ingress controllers to avoid <kbd>NodePort</kbd> cluster-wide application publishing. Ingress controllers deploy reverse-like proxy features to route requests to appropriate services matching specific headers or content rules. This improves security because services should not be published. We just publish ingress controllers (using service strategies, for example), and ingress resources manage the necessary rules for reaching out to the desired services, although they are not externally published.</li>
</ul>
<p>As mentioned, networking is critical in cluster environments. Let's review some of the topics required to pass the exam.</p>
<h2 id="uuid-ad71bcff-6499-4498-bdff-c81599218a00" class="mce-root">The Docker networking knowledge required for the exam</h2>
<p>The exam will verify our knowledge of the following topics, among others:</p>
<ul>
<li>Describing the container network model and how it interfaces with Docker Engine and network and IPAM drivers</li>
<li>Describing the different types and use cases for built-in network drivers</li>
<li>Describing the types of traffic that flow between Docker Engine, registry, and UCP controllers</li>
<li>Describing and demonstrating how to create a Docker bridge network for developers to use for their containers</li>
<li>Describing and demonstrating how to publish a port so that an application is accessible externally</li>
<li>Identifying which IP and port a container is externally accessible on</li>
<li>Comparing and contrasting host and ingress publishing modes</li>
<li>Describing and demonstrating how to configure Docker to use an external DNS</li>
<li>Describing and demonstrating how to use Docker to load balance HTTP/HTTPS traffic to an application (configuring L7 load balancing with Docker EE).</li>
<li>Understanding and describing the types of traffic that flow between Docker Engine, registry, and UCP controllers</li>
<li>Describing and demonstrating how to deploy a service on a Docker overlay network</li>
<li>Describing and demonstrating how to troubleshoot container and engine logs to resolve connectivity issues between containers</li>
<li>Describing how to route traffic to Kubernetes pods using the ClusterIP and NodePort services</li>
<li>Describing the Kubernetes container network model.</li>
</ul>
<p>These topics are extracted from Docker's official study guide, which can be found at <a href="https://success.docker.com/certification">https://success.docker.com/certification</a>.</p>
<p>The next section will help you by presenting the required knowledge in relation to security on the Docker platform.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-f67bdb7a-630c-4ddd-bdbf-adc1782278a9">Understanding security concepts and related Docker features</h1>
<p class="mce-root">Security is crucial when you are running applications in production. We have learned about many security features provided by Docker and its components. We started by reviewing how containers are isolated from other host processes and we also learned how we can ensure security in Docker Engine. Then, we moved on to Docker Swarm, where security must be applied cluster-wide. Users' access must also be managed and we need to provide authentication and authorization mechanisms. Docker Enterprise provides a higher level of security. It includes a complete RBAC environment, which allows us to manage fine-grained permissions to objects and cluster resources.</p>
<p>All of these topics were covered in multiple chapters on Docker Engine, Docker Swarm, Kubernetes, and the Docker Enterprise platform. We learned about security in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>, <a href="3952ec16-ca49-4bc2-b7e6-d6f17fec3fab.xhtml">Chapter 2</a>, <em>Building Docker Images</em>, <a href="e7804d8c-ed8c-4013-8449-b746ee654210.xhtml">Chapter 4</a>, <em>Container Persistency and Networking</em>, <a href="e9fd3807-5bbd-4ea8-84f7-ee02d288643d.xhtml">Chapter 6</a>, <em>Introduction to Docker Content Trust</em>, <a href="78af3b70-773d-4f5d-9835-71d1c15a104a.xhtml">Chapter 8</a>, <em>Orchestration Using Docker Swarm</em>, <a href="abcbf266-c469-4d84-ad4f-abd321a64b53.xhtml">Chapter 9</a>, <em>Orchestration Using Kubernetes</em>, <a href="1879ea92-ae47-4230-ac84-784d4bc73185.xhtml">Chapter 11</a>, <em>Universal Control Plane</em>, <a href="ab131f1f-ca6e-4815-9a3a-8c92c93c9dbc.xhtml">Chapter 12</a>, <em>Publishing Applications in Docker Enterprise</em>, and <a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13,</a><a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml"/><a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml"> </a><em>Implementing an Enterprise-Grade Registry with DTR</em><em>.</em></p>
<p>We have to remember that containers are created using images, so securing images is also critical. Following good practices is key to developing safe images. Docker Enterprise provides several strategies to validate image precedence, immutability, and content security.</p>
<p>Let's review some of these security topics:</p>
<ul>
<li>Docker is a client-server application. The server will publish its API on local (by default) and remotely accessible sockets. We can limit Docker Engine access by limiting access to these sockets. Locally, only users with filesystem permissions to a defined socket file will be allowed to run Docker commands on the local Docker engine.</li>
<li>Docker Engine can be integrated with operating system-provided security modules, such as SELinux or AppArmor. Docker provides integration and default profiles to use with our containers. Docker also integrates with the Linux kernel to allow the adding or removing of specific system calls using capabilities. There are also simpler security tips, such as using read-only root filesystems and non-root users within containers, that will also help us to provide secure applications.</li>
<li>Images should be secure to create secure containers. Images should only contain the required binaries, libraries, and configurations for our processes. Everything irrelevant to the application should be avoided. Docker Enterprise provides an image's content security scanner. It compares relevant content file hashes against a database of well-known published vulnerabilities and exploits (internet <strong>Common Vulnerabilities and Exposures </strong>(<strong>CVE</strong>)). We learned how this process works and how we can integrate tag promotions to ensure that only allowed users get the appropriate access to their images. These are some of the DTR features.</li>
<li>We can also sign images. This process ensures image content immutability and ownership. If we integrate image building into our continuous integration and continuous deployment, we can ensure that images were created using an appropriate workflow. We can also improve our CaaS security, allowing only containers based on images signed by specific teams or users within your organization.</li>
<li>We learned about all the automatic steps to be followed to sign an image and all the keys integrated into the process. Image signing is based on Content Trust logic, and we learned how it is integrated in Docker in <a href="e9fd3807-5bbd-4ea8-84f7-ee02d288643d.xhtml">Chapter 6</a>, <em>Introduction to Docker Content Trust</em>.</li>
<li>We mentioned some simple practices that increase security in our workloads, such as running read-only root filesystems or using non-root users for applications (or user namespaces). We should review an image's specifications using <kbd>docker image inspect</kbd> to have a good idea of exposed ports, applications' users, and commands that will be executed inside containers.</li>
<li>As mentioned in this chapter, neither Docker Engine nor Docker Swarm have any RBAC integration. On the other hand, Docker Enterprise components have integrated role-based access. UCP provides different accesses to Docker Swarm resources based on roles, grants, and collections. We can configure fine-grained access to volumes, secrets, configs, networks, and so on, so users will only be able to execute allowed actions on their resources. Users will connect to the cluster to execute, review, and modify their resources by using either the provided web UI or their Docker command line, using their Docker client software and their UCP's bundle. This compressed file contains user certificates and environment scripts prepared to help users connect easily to the cluster.</li>
<li>DTR has its own RBAC environment, isolated from that of UCP. DTR is a registry, therefore its RBAC environment is dedicated to managing access to the images stored within your CaaS. We have fine-grained permissions to allow a group of users to use or modify images, while other images are public within teams or the full organization.</li>
<li>DTR and UCP are integrated by default in a single sign-on solution, although we can change this behavior. We can also integrate them into our organization user management solution, Active Directory, or any compatible <strong>Lightweight Directory Access Protocol</strong> (<strong>LDAP</strong>).</li>
<li>We learned how to deploy Docker Enterprise components and how to manage users, roles, and different levels of access to resources and images. They will be deployed with high availability using an odd number of software nodes and we will require an external load balancer to provide users' access. We can integrate our corporate certificates, but we can also use autogenerated ones. In this case, we will need to integrate DTR's CA in our organization server and client hosts.</li>
<li>Although Docker Swarm requires UCP to integrate user management, Kubernetes implements its own RBAC system. We will be allowed to authenticate and authorize users using tokens and certificates. Kubernetes RBAC will work for applications and users and it is integrated into Docker Enterprise.</li>
<li>Docker Swarm and Kubernetes provide secure storage for certificates, passwords, tokens, and so on. Both provide secret resources to manage any file (or variable) that should be protected from suspicious users. But while secrets are encrypted in Docker Swarm, they are not encrypted in Kubernetes by default. Secret resources are encoded using Base64 in Kubernetes, and additional configuration must be performed to encrypt them.</li>
<li>Kubernetes has advanced features regarding security, such as PodSecurityPolicy resources, which allow us to force security on pods, allowing or disallowing specific behaviors (root processes and read-only filesystems). Admission controllers can also be implemented (there are a few already configured by default in UCP's Kubernetes deployment) to force pod security policies and other security features by default to any workload deployed in our Kubernetes cluster.</li>
<li>We will use RBAC for either UCP and DTR user accesses. First, we will ensure only authorized users will be able to manage and use cluster resources for their applications. DTR's RBAC will protect images, allowing only authorized users to manipulate and update their content.</li>
</ul>
<p>The next section will highlight the knowledge required to pass the exam.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h2 id="uuid-60423001-07fc-470b-9745-823a6a0e64be" class="mce-root">The knowledge of Docker security required for the exam</h2>
<p>The exam will verify our knowledge of the following topics, among others:</p>
<ul>
<li>Describing security administration and tasks</li>
<li>Describing the process of signing an image</li>
<li>Describing default engine security</li>
<li>Describing Swarm default security</li>
<li>Describing MTLS</li>
<li>Describing identity roles</li>
<li>Comparing and contrasting UCP workers and managers</li>
<li>Describing the process of using external certificates with UCP and DTR</li>
<li>Describing and demonstrating how an image passes a security scan</li>
<li>Describing and demonstrating how to enable Docker Content Trust</li>
<li>Describing and demonstrating how to configure RBAC with UCP</li>
<li>Describing and demonstrating how to integrate UCP with LDAP/AD</li>
<li>Describing and demonstrating how to create UCP client bundles</li>
</ul>
<p>These topics are extracted from Docker's official study guide, which can be found at <a href="https://success.docker.com/certification">https://success.docker.com/certification</a>.</p>
<h1 id="uuid-b0aefcbd-1408-4d76-8640-63ba2cf737cb" class="mce-root">Quickly summarizing Docker storage and volumes</h1>
<p>Using Docker containers requires different storage solutions, as we have learned through this book. Images and containers are created using multiple-layer filesystem strategies. However, we also have to manage persistence in our container-based applications. This persistence can be associated with application data, but we also have to be able to manage configurations and states cluster-wide.</p>
<p>We learned about security in <a href="c5ecd7bc-b7ed-4303-89a8-e487c6a220ed.xhtml">Chapter 1</a>, <em>Modern Infrastructures and Applications with Docker</em>, <a href="3952ec16-ca49-4bc2-b7e6-d6f17fec3fab.xhtml">Chapter 2</a>, <em>Building Docker Images</em>, <a href="e7804d8c-ed8c-4013-8449-b746ee654210.xhtml">Chapter 4</a>, <em>Container Persistency and Networking</em>, and <a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13</a>, <em>Implementing an Enterprise-Grade Registry with DTR.</em></p>
<p class="mce-root"/>
<p>This is a quick summary of the topics looked at in this book regarding storage and volume management within containers. We recommend you read through this summary to ensure you remember the concepts learned:</p>
<ul>
<li>We learned that containers are based on different filesystems and solutions with a common feature – copy-on-write. This allows us to create multiple immutable layers to group files. Each layer is the base for another, and file modifications will be stored in the last layer where they were changed. All immutable layers are considered as the image for the creation of new containers. We will add a new read-and-write layer for the container. These layers rely on host storage. This storage is known as graph storage and we will use different strategies to manage it, depending on the host operating system. Docker will choose the best driver for your host according to your kernel features and installed drivers. The most popular and most widely used today is <kbd>overlay2</kbd>, which is the default graph driver for many Linux distributions. <kbd>docker info</kbd> provides information about the driver used.</li>
<li>We have also learned that images are stored locally for fast usage on your host. When these images must be shared with cluster nodes, things get difficult, although we can export and import image layers. We will use image registries to store images and share their content with hosts and users. We learned how to deploy Docker Registry (Community Edition) as well as DTR, which is recommended for enterprise environments. We can use different storage solutions for registry volumes, depending on whether we are using cloud environments or on-premises installations. As reviewed in <a href="108b5948-15a9-40fb-b8dd-5a44c54efd7d.xhtml">Chapter 13</a>, <em>Implementing an Enterprise-Grade Registry with DTR</em>, object storage is quite good for storing images based on big layers, which is the most common way of creating images.</li>
<li>Images can occupy a lot of space in your host. We should take care of this and review dead containers and unused images that are consuming space with <kbd>docker system df</kbd>. We should remove dangling images not used as a layer within any other images. We also have to take care of the space on our registries. Only keep required images, but remember to verify which containers or applications will use different old image versions. We learned how to filter this information in <a href="3952ec16-ca49-4bc2-b7e6-d6f17fec3fab.xhtml">Chapter 2</a>, <em>Building Docker Images</em>.</li>
<li>Volumes, on the other hand, are different from image and container storage. They are used to bypass container storage. These help us to improve performance when a lot of disk I/O is required, and also allow us to store persistent data. By default, we can use on-memory filesystems, a host's local directories (bind mounts), NFS, and Docker volumes for storage. Docker volumes are associated with a container's life cycle when they are created during their execution.</li>
<li>As mentioned, Docker provides some volume solutions by default. We can extend them using plugins and third-party integrations. Using distributed storage with Docker Swarm and UCP is critical if we need to provide high availability to our applications using resilience. If one cluster host dies, another will take its workloads by default, but storage must follow this behavior.</li>
<li>Kubernetes has a different approach to persistent data. We talked about volumes and persistent volumes (<kbd>persistentVolumes</kbd>). The former are used to share and manage data associated with pods' containers. On the other hand, persistent volumes are used to manage and persist data cluster-wide. There are different retention policies to manage their recycling cycles. Persistent volume claims (<kbd>persistentVolumeClaims</kbd>) are used to link pods with volumes using labels and required space among other parameters. Therefore, instead of using persistent volumes directly attached to pods, we will use <kbd>persistentVolumeClaims</kbd> inside pods' configurations as volumes. Administrators should create these resources, but they can avoid this behavior by using <kbd>storageClass</kbd> resources. They will just configure <kbd>storageClass</kbd> resources using labels, storage providers, and other advanced profiles to allow dynamic storage allocation for persistent volumes.</li>
<li>We learned that Docker provides <kbd>Config</kbd> and <kbd>Secret</kbd> objects to allow us to manage information in cluster nodes. These help us to configure applications and ensure that applications' containers receive appropriate configurations, passwords, certificates, and so on. Kubernetes has its own configuration and secret resources. To manage configurations, we will use ConfigMaps for storing an application's configuration files and managing environment variables. Secret resources are used to store secured data, but they are not encrypted by default in Kubernetes. They are stored using the Base64 format and can be used for either storing keys and values or files.</li>
</ul>
<p>Storing data and states is quite important and is part of the exam. Let's review what concepts you are required to understand to pass the exam.</p>
<h2 id="uuid-f3dc4698-95d9-4df7-9b4d-b103714003cc" class="mce-root">The storage and volume knowledge required for the exam</h2>
<p>The exam will verify our knowledge of the following topics, among others:</p>
<ul>
<li>Identifying the correct graph drivers to use with various operating systems</li>
<li>Describing and demonstrating how to configure a device mapper</li>
<li>Comparing and contrasting object and block storage and when they should be used</li>
<li>Describing how an application is composed of layers and where those layers reside on the filesystem</li>
<li>Describing the use of volumes with Docker for persistent storage</li>
<li>Identifying the steps to take to clean up unused images on a filesystem and DTR</li>
<li>Describing and demonstrating how storage can be used across cluster nodes</li>
<li>Describing how to provision persistent storage to a Kubernetes pod using <kbd>persistentVolume</kbd> resources.</li>
<li>Describing the relationship between container storage interface drivers, <kbd>storageClass</kbd>, <kbd>persistentVolumeClaim</kbd>, and <kbd>volume</kbd> objects in Kubernetes.</li>
</ul>
<p>We will look at some final notes and sample exam questions to help us prepare for the DCA exam in the next chapter.</p>
<h1 id="uuid-25162f1b-e9d0-491f-93de-2f52c41c5419">Summary</h1>
<p class="mce-root">This chapter was a summary of the topics required to pass the exam. We reviewed the topic distribution and their approximate value in the exam. This should give you a good idea of what sections are more important than others. We recommend that you review this chapter before reading all the exam-like questions set out in the next chapter.</p>
<p>We covered a brief summary of orchestration's most important topics. We also reviewed some of the installation and configuration tips required for Docker Engine, Docker Swarm, and Enterprise components. We looked at a summary of the features and processes involved in the creation of images. Security is always critical and we looked at a summary of the features provided by different Docker components that help us to provide a CaaS platform in production. Container networking and the different storage implementations for containers and images and for data management were also reviewed. It is recommended that you review any chapters that were not clear in these summaries and review the labs provided in this book to reinforce your knowledge of all the exam topics.</p>
<p>The next chapter provides some exam-like questions that will prepare you for the exam.</p>


            

            
        
    </body></html>