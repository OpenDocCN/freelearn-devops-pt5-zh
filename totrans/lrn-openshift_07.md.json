["```\n172.24.0.11 openshift.example.com openshift\n172.24.0.12 storage.example.com storage\n```", "```\n$ cat Vagrantfile \n$lab_script = <<SCRIPT\ncat <<EOF >> /etc/hosts\n172.24.0.11 openshift.example.com openshift\n172.24.0.12 storage.example.com storage\nEOF\nSCRIPT\n\n$lab_openshift = <<SCRIPT\nsystemctl disable firewalld\nsystemctl stop firewalld\nyum install -y epel-release git\nyum install -y docker\ncat << EOF >/etc/docker/daemon.json\n{\n \"insecure-registries\": [\n \"172.30.0.0/16\"\n ]\n}\nEOF\nsystemctl start docker\nsystemctl enable docker\nyum -y install centos-release-openshift-origin39\nyum -y install origin-clients\noc cluster up\nSCRIPT\n\nVagrant.configure(2) do |config|\n config.vm.define \"openshift\" do |conf|\n conf.vm.box = \"centos/7\"\n conf.vm.hostname = 'openshift.example.com'\n conf.vm.network \"private_network\", ip: \"172.24.0.11\"\n conf.vm.provider \"virtualbox\" do |v|\n v.memory = 4096\n v.cpus = 2\n end\n conf.vm.provision \"shell\", inline: $lab_script\n conf.vm.provision \"shell\", inline: $lab_openshift\n end\n\n config.vm.define \"storage\" do |conf|\n conf.vm.box = \"centos/7\"\n conf.vm.hostname = 'storage.example.com'\n conf.vm.network \"private_network\", ip: \"172.24.0.12\"\n conf.vm.provider \"virtualbox\" do |v|\n v.memory = 2048\n v.cpus = 1\n end\n conf.vm.provision \"shell\", inline: $lab_script\n end\nend\n```", "```\n$ vagtrant up\n$ vagrant ssh storage\n```", "```\n# yum install -y nfs-utils\n…\n<output omitted>\n…\nUpdated:\n  nfs-utils.x86_64 1:1.3.0-0.54.el7\nComplete!\n```", "```\n# mkdir -p /exports/{nfsvol1,nfsvol2,nfsvol3}\n# chmod 0700 /exports/{nfsvol1,nfsvol2,nfsvol3}\n# chown nfsnobody:nfsnobody /exports/{nfsvol1,nfsvol2,nfsvol3}\n```", "```\n# firewall-cmd --perm --add-service={nfs,mountd,rpc-bind}\nsuccess\n\n# firewall-cmd --reload\nsuccess\n```", "```\n# cat <<EOF > /etc/exports\n/exports/nfsvol1 openshift.example.com(rw,sync,all_squash)\n/exports/nfsvol2 openshift.example.com(rw,sync,all_squash)\n/exports/nfsvol3 openshift.example.com(rw,sync,all_squash)\nEOF\n```", "```\n# cat <<EOF > /etc/exports\n/exports/nfsvol1 172.24.0.11(rw,sync,all_squash)\n/exports/nfsvol2 172.24.0.11(rw,sync,all_squash)\n/exports/nfsvol3 172.24.0.11(rw,sync,all_squash)\nEOF\n```", "```\n# systemctl enable rpcbind nfs-server\nCreated symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.\n# systemctl start rpcbind nfs-server\n```", "```\n# exportfs -v\n/exports/nfsvol1 openshift.example.com(rw,sync,wdelay,hide,no_subtree_check,sec=sys,secure,root_squash,all_squash)\n/exports/nfsvol2 openshift.example.com(rw,sync,wdelay,hide,no_subtree_check,sec=sys,secure,root_squash,all_squash)\n/exports/nfsvol3 openshift.example.com(rw,sync,wdelay,hide,no_subtree_check,sec=sys,secure,root_squash,all_squash)\n```", "```\n# yum install -y centos-release-gluster312 ...\n<output omitted>\n...\n# yum install -y glusterfs-server\n...\n<output omitted>\n...\nDependency Installed:\n attr.x86_64 0:2.4.46-12.el7\n glusterfs.x86_64 0:3.12.6-1.el7\n glusterfs-api.x86_64 0:3.12.6-1.el7\n glusterfs-cli.x86_64 0:3.12.6-1.el7\n glusterfs-client-xlators.x86_64 0:3.12.6-1.el7\n glusterfs-fuse.x86_64 0:3.12.6-1.el7\n glusterfs-libs.x86_64 0:3.12.6-1.el7\n psmisc.x86_64 0:22.20-15.el7\n userspace-rcu.x86_64 0:0.10.0-3.el7\n\nComplete!\n```", "```\n# systemctl enable glusterd\nCreated symlink from /etc/systemd/system/multi-user.target.wants/glusterd.service to /usr/lib/systemd/system/glusterd.service.\n# systemctl start glusterd\n```", "```\n# mkdir /exports/gluster\n# gluster volume create gvol1 storage.example.com:/exports/gluster force\nvolume create: gvol1: success: please start the volume to access data\n```", "```\n# gluster volume create gvol1 storage.example.com:/exports/gluster\nvolume create: gvol1: failed: The brick storage.example.com:/exports/gluster is being created in the root partition. It is recommended that you don't use the system's root partition for storage backend. Or use 'force' at the end of the command if you want to override this behavior.\n```", "```\n# gluster volume start gvol1\nvolume start: gvol1: success\n```", "```\n# yum install -y targetcli\n```", "```\n# systemctl enable target; systemctl start target\n```", "```\n# firewall-cmd --permanent --add-port=3260/tcp\n# firewall-cmd --reload\n```", "```\n# targetcli\ntargetcli shell version 2.1.fb46\nCopyright 2011-2013 by Datera, Inc and others.\nFor help on commands, type 'help'.\n\n/> /backstores/fileio create iscsivol1 /exports/iscsivol1.raw 1g\nCreated fileio iscsivol1 with size 1073741824\n/> /iscsi create iqn.2018-04.com.example.storage:disk1\nCreated target iqn.2018-04.com.example.storage:disk1.\nCreated TPG 1.\nGlobal pref auto_add_default_portal=true\nCreated default portal listening on all IPs (0.0.0.0), port 3260.\n/> cd iscsi/iqn.2018-04.com.example.storage:disk1/tpg1/\n/iscsi/iqn.20...ge:disk1/tpg1> luns/ create /backstores/fileio/iscsivol1\nCreated LUN 0.\n/iscsi/iqn.20...ge:disk1/tpg1> set attribute authentication=0 demo_mode_write_protect=0 generate_node_acls=1 cache_dynamic_acls=1\nParameter generate_node_acls is now '1'.\n/iscsi/iqn.20...ge:disk1/tpg1> exit\nGlobal pref auto_save_on_exit=true\nLast 10 configs saved in /etc/target/backup.\nConfiguration saved to /etc/target/saveconfig.json\n```", "```\n$ vagrant ssh openshift\nLast login: Sun Jul 8 22:24:44 2018 from 10.0.2.2\n[vagrant@openshift ~]$ sudo -i\n```", "```\n# yum install -y nfs-utils\n... <output omitted>\n...\n# showmount -e storage.example.com\nExport list for storage.example.com:\n/exports/nfsvol3 openshift.example.com\n/exports/nfsvol2 openshift.example.com\n/exports/nfsvol1 openshift.example.com\n# mkdir /mnt/{nfsvol1,nfsvol2,nfsvol3}\n# mount storage.example.com:/exports/nfsvol1 /mnt/nfsvol1\n# mount storage.example.com:/exports/nfsvol2 /mnt/nfsvol2\n# mount storage.example.com:/exports/nfsvol3 /mnt/nfsvol3\n# df -h|grep nfsvol\nstorage.example.com:/exports/nfsvol1 38G 697M 37G 2% /mnt/nfsvol1\nstorage.example.com:/exports/nfsvol2 38G 697M 37G 2% /mnt/nfsvol2\nstorage.example.com:/exports/nfsvol3 38G 697M 37G 2% /mnt/nfsvol3\n# umount /mnt/nfsvol1 /mnt/nfsvol2 /mnt/nfsvol3\n```", "```\n# yum install centos-release-gluster312 -y\n# yum install glusterfs-fuse -y\n# mkdir /mnt/gvol1\n# mount -t glusterfs storage.example.com:/gvol1 /mnt/gvol1 \n```", "```\n# echo \"Persistent data on GlusterFS\" > /mnt/gvol1/index.html\n```", "```\n# df -h /mnt/gvol1/\nFilesystem Size Used Avail Use% Mounted on\nstorage.example.com:/gvol1 38G 713M 37G 2% /mnt/gvol1\n# umount /mnt/gvol1\n```", "```\n# yum install -y iscsi-initiator-utils\n... <output omitted>\n...\n# iscsiadm --mode discoverydb --type sendtargets --portal storage.example.com --discover 172.24.0.12:3260,1 iqn.2018-04.com.example.storage:disk1 # iscsiadm --mode node --login Logging in to [iface: default, target: iqn.2018-04.com.example.storage:disk1, portal: 172.24.0.12,3260] (multiple)\nLogin to [iface: default, target: iqn.2018-04.com.example.storage:disk1, portal: 172.24.0.12,3260] successful.\n# cat /proc/partitions major minor #blocks name\n\n   8 0 41943040 sda\n   8 1 1024 sda1\n   8 2 1048576 sda2\n   8 3 40892416 sda3\n 253 0 39288832 dm-0\n 253 1 1572864 dm-1\n   8 16 1048576 sdb\n\n# iscsiadm --mode node --logout Logging out of session [sid: 2, target: iqn.2018-04.com.example.storage:disk1, portal: 172.24.0.12,3260]\nLogout of [sid: 2, target: iqn.2018-04.com.example.storage:disk1, portal: 172.24.0.12,3260] successful.\n\n# iscsiadm --mode node -T iqn.2018-04.com.example.storage:disk1 --op delete\n```", "```\n# oc login -u system:admin \n```", "```\n# oc new-project persistent-storage\n```", "```\n# cat pv-nfsvol1.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-nfsvol1\nspec:\n  capacity:\n    storage: 2Gi \n  accessModes:\n    - ReadWriteMany \n  persistentVolumeReclaimPolicy: Retain \n  nfs: \n    path: /exports/nfsvol1\n    server: storage.example.com\n    readOnly: false\n```", "```\n# oc create -f pv-nfsvol1.yaml persistentvolume \"pv-nfsvol1\" created\n```", "```\n# oc describe pv pv-nfsvol1\nName: pv-nfsvol1\nLabels: <none>\nAnnotations: <none>\nStorageClass:\nStatus: Available\nClaim:\nReclaim Policy: Retain\nAccess Modes: RWX\nCapacity: 2Gi\nMessage:\nSource:\n    Type: NFS (an NFS mount that lasts the lifetime of a pod)\n    Server: storage.example.com\n    Path: /exports/nfsvol1\n    ReadOnly: false\nEvents: <none>\n```", "```\n# oc get pv | egrep \"^NAME|^pv-\"\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE\npv-nfsvol1 2Gi RWX Retain Available 37s\n```", "```\n# yum install -y centos-release-gluster312\n# yum install -y glusterfs-fuse\n```", "```\n# cat gluster-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: glusterfs-cluster\nspec:\n  ports:\n    - port: 1\n```", "```\n# oc create -f gluster-service.yaml\nservice \"glusterfs-cluster\" created\n# oc get svc\nNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nglusterfs-cluster 172.30.193.29 <none> 1/TCP 3s\n```", "```\n# cat gluster-endpoint.yaml\napiVersion: v1\nkind: Endpoints\nmetadata:\n  name: glusterfs-cluster \nsubsets:\n  - addresses:\n      - ip: 172.24.0.12\n    ports:\n      - port: 1\n\n# oc create -f gluster-endpoint.yaml\nendpoints \"glusterfs-cluster\" created\n\n# oc get endpoints\nNAME ENDPOINTS AGE\nglusterfs-cluster 172.24.0.12:1 17s\n```", "```\n# cat pv-gluster.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-gluster \nspec:\n  capacity:\n    storage: 3Gi \n  accessModes: \n    - ReadWriteMany\n  glusterfs: \n    endpoints: glusterfs-cluster \n    path: gvol1 \n    readOnly: false\n  persistentVolumeReclaimPolicy: Retain \n\n# oc create -f pv-gluster.yaml\npersistentvolume \"pv-gluster\" created\n```", "```\n# oc get pv | egrep \"^NAME|^pv-\"\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE\npv-gluster 3Gi RWX Retain Available 2s\npv-nfsvol1 2Gi RWX Retain Available 2m\n```", "```\n# cat pv-iscsi.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-iscsi\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n  iscsi:\n     targetPortal: storage.example.com\n     iqn: iqn.2018-04.com.example.storage:disk1\n     lun: 0\n     fsType: 'ext4'\n     readOnly: false\n```", "```\n# oc create -f pv-iscsi.yaml\npersistentvolume \"pv-iscsi\" created\n```", "```\n# oc get pv | egrep \"^NAME|^pv-\"\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE\npv-gluster 3Gi RWX Retain Available 1m\npv-iscsi   1Gi RWO Retain Available 6s\npv-nfsvol1 2Gi RWX Retain Available 3m\n```", "```\n# cat pvc-db.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-db \nspec:\n  accessModes:\n  - ReadWriteOnce \n  resources:\n     requests:\n       storage: 1Gi \n```", "```\n# oc create -f pvc-db.yaml\npersistentvolumeclaim \"pvc-db\" created\n```", "```\n# oc get pv | egrep \"^NAME|^pv-\"\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE\npv-gluster 3Gi RWX Retain Available 2m\npv-iscsi   1Gi RWO Retain Bound persistent-storage/pvc-db 1m\npv-nfsvol1 2Gi RWX Retain Available 4m\n\n# oc get pvc\nNAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE\npvc-db Bound pv-iscsi 1Gi RWO 28s\n```", "```\n# cat pvc-web.yaml\napiVersion: \"v1\"\nkind: \"PersistentVolumeClaim\"\nmetadata:\n  name: \"pvc-web\"\nspec:\n  accessModes:\n    - \"ReadWriteMany\"\n  resources:\n    requests:\n      storage: \"1Gi\"\n  volumeName: \"pv-gluster\"\n```", "```\n# oc create -f pvc-web.yaml\npersistentvolumeclaim \"pvc-web\" created\n\n# oc get pv | egrep \"^NAME|^pv-\"\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE\npv-gluster 3Gi RWX Retain Bound persistent-storage/pvc-web 3m\npv-iscsi   1Gi RWO Retain Bound persistent-storage/pvc-db 2m\npv-nfsvol1 2Gi RWX Retain Available 5m\n```", "```\n# cat pvc-data.yaml\napiVersion: \"v1\"\nkind: \"PersistentVolumeClaim\"\nmetadata:\n  name: \"pvc-data\"\nspec:\n  accessModes:\n    - \"ReadWriteMany\"\n  resources:\n    requests:\n      storage: \"100Mi\"\n\n# oc create -f pvc-data.yaml\npersistentvolumeclaim \"pvc-data\" created\n\n# oc get pv | egrep \"^NAME|^pv-\"\nNAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE\npv-gluster 3Gi RWX Retain Bound persistent-storage/pvc-web 4m\npv-iscsi 1Gi RWO Retain Bound persistent-storage/pvc-db 2m\npv-nfsvol1 2Gi RWX Recycle Bound persistent-storage/pvc-data 6m\n```", "```\n# cat pod-webserver.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mywebserverpod\n  labels:\n    name: webeserver\nspec:\n  containers:\n    - name: webserver\n      image: docker.io/centos/httpd\n      ports:\n        - name: web\n          containerPort: 80\n      volumeMounts:\n        - name: volume-webroot\n          mountPath: /var/www/html\n volumes:\n - name: volume-webroot\n persistentVolumeClaim:\n claimName: pvc-web\n```", "```\n# oc create -f pod-webserver.yaml\npod \"mywebserverpod\" created\n```", "```\n# oc describe pod mywebserverpod | grep -A 4 Volumes:\nVolumes:\n  nfsvol:\n    Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n    ClaimName: pvc-web\n    ReadOnly: false\n```", "```\n# df -h | grep gvol1\n172.24.0.12:gvol1 38G 720M 37G 2% /var/lib/origin/openshift.local.volumes/pods/e2ca34d3-4823-11e8-9445-5254005f9478/volumes/kubernetes.io~glusterfs/pv-gluster\n```", "```\n# oc describe pod mywebserverpod | grep IP:\nIP: 172.17.0.2\n```", "```\n# curl http://172.17.0.2\nPersistent data on GlusterFS\n```", "```\n[root@storage ~]# cat /exports/gluster/index.html Persistent data on GlusterFS\n```", "```\n# oc delete pod mywebserverpod\npod \"mywebserverpod\" deleted\n# oc create -f pod-webserver.yaml\npod \"mywebserverpod\" created\n# oc describe pod mywebserverpod | grep IP:\nIP: 172.17.0.2\n# curl http://172.17.0.2:80 Persistent data on GlusterFS\n```", "```\n# oc new-app httpd ...\n<output omitted>\n...\n```", "```\n# oc get pod | egrep \"^NAME|httpd\"\nNAME           READY STATUS   RESTARTS  AGE\nhttpd-1-qnh5k   1/1  Running    0       49s\n```", "```\n# oc volume dc/httpd --add --name=demovolume -t pvc --claim-name=pvc-data --mount-path=/var/www/html\ndeploymentconfig \"httpd\" updated\n\n# oc get pod | egrep \"^NAME|httpd\" NAME READY STATUS RESTARTS AGE\nhttpd-2-bfbft 1/1 Running 0 40s\n\n# oc describe pod httpd-2-bfbft | grep -A 4 Volumes:\nVolumes:\n demovolume:\n Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n ClaimName: pvc-data\n ReadOnly: false\n```", "```\n# df -h | grep nfsvol1\nstorage.example.com:/exports/nfsvol1 38G 720M 37G 2% /var/lib/origin/openshift.local.volumes/pods/12cfe985-482b-11e8-9445-5254005f9478/volumes/kubernetes.io~nfs/pv-nfsvol1\n```", "```\n[root@storage ~]# echo \"New NFS data\" >/exports/nfsvol1/index.html\n```", "```\n# oc describe pod httpd-2-bfbft | grep IP:\nIP: 172.17.0.3\n\n# curl http://172.17.0.3:8080\nNew NFS data\n```", "```\n# oc volume dc/httpd --remove --name=demovolume\ndeploymentconfig \"httpd\" updated\n```", "```\n# oc get pod\nNAME READY STATUS RESTARTS AGE\nhttpd-3-fbq74 1/1 Running 0 1m\n\n# oc describe pod httpd-3-fbq74 | grep IP:\nIP: 172.17.0.4\n\n# curl http://172.17.0.4:8080\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\" \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n\n<html  xml:lang=\"en\">\n <head>\n <title>Test Page for the Apache HTTP Server on Red Hat Enterprise Linux</title>\n...\n<output omitted>\n...\n```", "```\n# oc new-app \\\n-e MYSQL_USER=openshift \\\n-e MYSQL_PASSWORD=openshift \\\n-e MYSQL_DATABASE=openshift \\\nmariadb\nFound image a339b72 (10 days old) in image stream \"openshift/mariadb\" under tag \"10.1\" for \"mariadb\"\n\nMariaDB 10.1\n------------\nMariaDB is a multi-user, multi-threaded SQL database server. The container image provides a containerized packaging of the MariaDB mysqld daemon and client application. The mysqld server daemon accepts connections from clients and provides access to content from MariaDB databases on behalf of the clients.\n\nTags: database, mysql, mariadb, mariadb101, rh-mariadb101, galera\n\n* This image will be deployed in deployment config \"mariadb\"\n* Port 3306/tcp will be load balanced by service \"mariadb\"\n* Other containers can access this service through the hostname \"mariadb\"\n* This image declares volumes and will default to use non-persistent, host-local storage.\nYou can add persistent volumes later by running 'volume dc/mariadb --add ...'\n\n--> Creating resources ...\n\ndeploymentconfig \"mariadb\" created\nservice \"mariadb\" created\n--> Success\nApplication is not exposed. You can expose services to the outside world by executing one or more of the commands below:\n'oc expose svc/mariadb'\nRun 'oc status' to view your app.\n```", "```\n# oc get pod | egrep \"^NAME|mariadb\"\nNAME            READY STATUS   RESTARTS AGE\nmariadb-1-lfmrn 1/1   Running   0       1m\n```", "```\n# oc describe dc mariadb\nName: mariadb\n...\n<output omitted>\n...\n Mounts:\n /var/lib/mysql/data from mariadb-volume-1 (rw)\n Volumes:\n mariadb-volume-1:\n Type: EmptyDir (a temporary directory that shares a pod's lifetime)\n Medium:\n...\n<output omitted>\n...\n```", "```\n# oc volume dc/mariadb --add --name=mariadb-volume-1 -t pvc --claim-name=pvc-db --mount-path=/var/lib/mysql --overwrite deploymentconfig \"mariadb\" updated\n```"]