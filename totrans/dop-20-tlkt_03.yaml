- en: Chapter 3. System Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From here on, the whole book will be one big project. We'll go through all the
    stages starting from development all the way until production deployment and monitoring.
    Each phase will begin with a discussion about different paths we can take to accomplish
    the goal. We'll choose the best given our needs and implement it. The objective
    is to learn techniques that you can apply to your projects so please feel free
    to adapt instructions to fit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: As most other projects, this one will start with high-level requirements. Our
    goal is to create an online shop. The complete plan is still not available, but
    we do know that selling books has priority. We should design services and a Web
    application in a way that it can easily be extended. We do not have the whole
    set of requirements in front of us, so we need to be prepared for the unknown.
    Besides books, we'll be selling other types of goods, and there will be other
    kinds of functionality like a shopping cart, registration and login, and so on.
    Our job is to develop bookstore and be able to respond to the future requirements
    in a fast manner. Since it is a new endeavor, not much traffic is expected at
    the beginning, but we should be prepared to scale easily and quickly if the service
    becomes successful. We want to ship new features as fast as possible without any
    downtime and to be able to recuperate from failures.
  prefs: []
  type: TYPE_NORMAL
- en: Let us start working on the architecture. It is clear that requirements are
    very general and do not provide many details. That means that we should be prepared
    for very likely changes in the future as well as well as requests for new features.
    At the same time, business requires us to build something small but be prepared
    to grow. How should we solve the problems given to us?
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we should decide is how to define the architecture of the application
    we're about to build. Which approach will allow us possible changes of the direction,
    additional (but at this moment unknown) requirements and the need to be ready
    to scale? We should start by examining two most common approaches to applications
    architecture; monoliths and microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monolithic applications are developed and deployed as a single unit. In the
    case of Java, the result is often a single WAR or JAR file. Similar statement
    is true for C++, .Net, Scala and many other programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the short history of software development is marked by a continuous
    increment in size of the applications we are developing. As time passes, we're
    adding more and more to our applications continuously increasing their complexity
    and size and decreasing our development, testing and deployment speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We started dividing our applications into layers: presentation layer, business
    layer, data access layer, and so on. This separation is more logical than physical,
    and each of those layers tends to be in charge of one particular type of operations.
    This kind of architecture often provided immediate benefits since it made clear
    the responsibility of each layer. We got separation of concerns on a high level.
    Life was good. Productivity increased, time-to-market decreased and overall clarity
    of the code base was better. Everybody seemed to be happy, for a while:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monolithic Applications](img/B05848_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1 – Monolithic application
  prefs: []
  type: TYPE_NORMAL
- en: 'With time, the number of features our application was required to support was
    increasing and with that comes increased complexity. One feature on UI level would
    need to speak with multiple business rules that in turn require multiple DAO classes
    that access many different database tables. No matter how hard we try, the sub-division
    within each layer and communication between them gets ever more complicated and,
    given enough time, developers start straying from the initial path. After all,
    a design made initially often does not pass the test of time. As a result, modifications
    to any given sub-section of a layer tends to be more complicated, time demanding
    and risky since they might affect many different parts of the system with often
    unforeseen effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monolithic Applications](img/B05848_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2 – Monolithic application with increased number of features
  prefs: []
  type: TYPE_NORMAL
- en: As time passes, things start getting worse. In many cases, the number of layers
    increases. We might decide to add a layer with a rules engine, API layer, and
    so on. As things usually go, the flow between layers is in many cases mandatory.
    That results in situations where we might need to develop a simple feature that
    under different circumstances would require only a few lines of code but, due
    to the architecture we have, those few lines turn up to be hundreds or even thousands
    because all layers need to be passed through.
  prefs: []
  type: TYPE_NORMAL
- en: The development was not the only area that suffered from monolithic architecture.
    We still needed to test and deploy everything every time there was a change or
    a release. It is not uncommon in enterprise environments to have applications
    that take hours to test, build and deploy. Testing, especially regression, tends
    to be a nightmare that in some cases last for months. As time passes, our ability
    to make changes that affect only one module is decreasing. The primary objective
    of layers is to make them in a way that they can be easily replaced or upgraded.
    That promise is almost never actually fulfilled. Replacing something in big monolithic
    applications is hardly ever easy and without risks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling monoliths often mean scaling the entire application thus producing
    very unbalanced utilization of resources. If we need more resources, we are forced
    to duplicate everything on a new server even if a bottleneck is one module. In
    that case, we often end up with a monolith replicated across multiple nodes with
    a load balancer on top. This setup is sub-optimum at best:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monolithic Applications](img/B05848_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3 – Scaling monolithic application
  prefs: []
  type: TYPE_NORMAL
- en: Services Split Horizontally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Service-oriented architecture** (**SOA**) was created as a way to solve problems
    created by, often tightly coupled, monolithic applications. The approach is based
    on four main concepts we should implement:'
  prefs: []
  type: TYPE_NORMAL
- en: Boundaries are explicit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services are autonomous
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services share schema and contract but not class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services compatibility is based on policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SOA was such a big hit that many software provides jumped right in and created
    products that should help us in the transition. The most used type born out of
    SOA movement is **Enterprise Service Bus** (**ESB**). At the same time, companies
    that experienced problems with monolithic applications and big systems jumped
    into the train and started the SOA transition with ESB as the locomotive. However,
    the common problem with this move is the way we are used working that often resulted
    in an intention to artificially apply SOA architecture into the existing model.
  prefs: []
  type: TYPE_NORMAL
- en: We continued having the same layers as we had before, but this time physically
    separated from each other. There is an apparent benefit from this approach in
    that we can, at least, develop and deploy each layer independently from others.
    Another improvement is scaling. With the physical separation between what used
    to be layers, we are allowed to scale better. That approach was often combined
    with acquisitions of one of the **enterprise service bus** (**ESB**) products.
    In between services we would put ESB that would be in charge of transformation
    and redirection of requests from one service to another. ESB and similar products
    are beasts of their own and we often end up with another monolithic application
    that is as big or even bigger than the one we tried to split. What we needed was
    to break services by bounded contexts and separate them physically with each running
    in their own process and with clearly defined communication between them. Thus,
    microservices were born.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservices are an approach to architecture and development of a single application
    composed of small services. The key to understanding microservices is their independence.
    Each is developed, tested and deployed separately from each other. Each service
    runs as a separate process. The only relation between different microservices
    is data exchange accomplished through APIs they are exposing. They inherit, in
    a way, the idea of small programs and pipes used in Unix/Linux. Most Linux programs
    are small and produce some output. That output can be passed as input to other
    programs. When chained, those programs can perform very complex operations. It
    is complexity born from a combination of many simple units.
  prefs: []
  type: TYPE_NORMAL
- en: In a way, microservices use the concepts defined by SOA. Then why are the called
    differently? SOA implementations went astray. That is especially true with the
    emergence of ESB products that themselves become big and complex enterprise applications.
    In many cases, after adopting one of the ESB products, the business went as usual
    with one more layer sitting on top of what we had before. Microservices movement
    is, in a way, reaction to misinterpretation of SOA and the intention to go back
    to where it all started. The main difference between SOA and microservices is
    that the latter should be self-sufficient and deployable independently of each
    other while SOA tends to be implemented as a monolith.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what Gartner has to say about microservices. While I'm not a big fan
    of their predictions, they do strike the important aspect of the market by appealing
    to big enterprise environments. Their evaluations of market tendencies usually
    mean that we passed the adoption by greenfield projects, and the technology is
    ready for the big enterprises. Here's what Gary Olliffe said about microservices
    at the beginning of 2015.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architectures promise to deliver flexibility and scalability to
    the development and deployment of service-based applications. But how is that
    promise delivered? In short, by adopting an architecture that allows individual
    services to be built and deployed independently and dynamically; an architecture
    that embraces DevOps practices.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices are simpler, developers get more productive and systems can be
    scaled quickly and precisely, rather than in large monolithic globs. And I haven't
    even mentioned the potential for polyglot coding and data persistence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key aspects of microservices are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: They do one thing or are responsible for one functionality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each microservice can be built by any set of tools or languages since each is
    independent of others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are truly loosely coupled since each microservice is physically separated
    from others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relative independence between different teams developing different microservices
    (assuming that APIs they expose are defined in advance).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easier testing and continuous delivery or deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the problems with microservices is the decision when to use them. In
    the beginning, while the application is still small, problems that microservices
    are trying to solve do not exist. However, once the application grows and the
    case for microservices can be made, the cost of switching to a different architecture
    style might be too big. Experienced teams tend to use microservices from the very
    start knowing that technical debt they might have to pay later will be more expensive
    than working with microservices from the very beginning. Often, as it was the
    case with Netflix, eBay, and Amazon, monolithic applications start evolving towards
    microservices gradually. New modules are developed as microservices and integrated
    with the rest of the system. Once they prove their worth, parts of the existing
    monolithic application gets refactored into microservices as well.
  prefs: []
  type: TYPE_NORMAL
- en: One of the things that often gets most critique from developers of enterprise
    applications is decentralization of data storage. While microservices can work
    (with few adjustments) using centralized data storage, the option to decentralize
    that part as well should, at least, be explored. The option to store data related
    to some service in a separate (decentralized) storage and pack it together into
    the same container or as a separate one and link them together is something that
    in many cases could be a better option than storing that data in a centralized
    database. I am not proposing always to use decentralized storage but to have that
    option in account when designing microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we often employ some kind of a lightweight proxy server that is in
    charge of the orchestration of all requests no matter whether they come from outside
    or from one microservice to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Microservices](img/B05848_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4 – Microservices with a proxy service
  prefs: []
  type: TYPE_NORMAL
- en: Armed with a basic knowledge about monoliths and microservices, let us compare
    the two and evaluate their strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: Monolithic Applications and Microservices Compared
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From what we learned by now, seems that microservices are a better option than
    monoliths. Indeed, in many (but far from all) cases they are. However, there is
    no such thing as a free lunch. Microservices have their set of disadvantages with
    increased operational and deployment complexity, and remote process calls being
    the most common.
  prefs: []
  type: TYPE_NORMAL
- en: Operational and Deployment Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary argument against microservices is increased operational and deployment
    complexity. This argument is correct, but thanks to relatively new tools it can
    be mitigated. **Configuration Management** (**CM**) tools can handle environment
    setups and deployments with relative ease. Utilization of containers with Docker
    significantly reduces deployment pains that microservices can cause. CM tools
    together with containers allow us to deploy and scale microservices quickly.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, increased deployment complexity argument usually does not take
    into account advances we saw during last years and is greatly exaggerated. That
    does not mean that part of the work is not shifted from development to DevOps.
    It is. However, benefits are in many cases bigger than the inconvenience that
    change produces.
  prefs: []
  type: TYPE_NORMAL
- en: Remote Process Calls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another argument for monolithic applications is reduced performance produced
    by microservices remote process calls. Internal calls through classes and methods
    are faster and this problem cannot be removed. How much that loss of performance
    affects a system depends on case to case basis. The important factor is how we
    split our system. If we take it towards the extreme with very small microservices
    (some propose that they should not have more than 10-100 lines of code), this
    impact might be considerable. I like to create microservices organized around
    bounded contexts or functionality like users, shopping cart, products, and so
    on. That reduces the number of remote process calls but still keep services organization
    within healthy boundaries. Also, it's important to note that if calls from one
    microservice to another are going through a fast internal LAN, the negative impact
    is relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are the advantages microservices have over monoliths? The following
    list is by no means final nor it represents advantages only available with microservices.
    While many of them are valid for other types of architecture, they are more prominent
    with microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling microservices is much easier than monolithic applications. With monoliths,
    we duplicate the whole application into a new machine. On the other hand, with
    microservices, we duplicate only those that need scaling. Not only that we can
    scale what needs to be scaled but we can distribute things better. We can, for
    example, put a service that has heavy utilization of CPU together with another
    one that uses a lot of RAM while moving the other CPU demanding service to a different
    hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Innovation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monolithic applications, once the initial architecture is made, do not leave
    much space for innovation. I'd go even further and claim that monoliths are innovation
    killers. Due to their nature, changing things takes time, and experimentation
    is perilous since it potentially affects everything. One cannot, for example,
    change Apache Tomcat for NodeJS just because it would better suit one particular
    module.
  prefs: []
  type: TYPE_NORMAL
- en: I'm not suggesting that we should change programming language, server, persistence,
    and other architecture aspects for each module. However, monolithic servers tend
    to go to an opposite extreme where changes are risky if not unwelcome. With microservices,
    we can choose what we think is the best solution for each service separately.
    One might use Apache Tomcat while the other could use NodeJS. One can be written
    in Java and the other in Scala. I'm not advocating that each service is different
    from the rest but that each can be made in a way we think is best suited for the
    goal at hand. On top of that, changes and experiments are much easier to do. After
    all, whatever we do affects only one out of many microservices and not the system
    as a whole as long as the API is respected.
  prefs: []
  type: TYPE_NORMAL
- en: Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since microservices are small, they are much easier to understand. There is
    much less code to go through to see what one microservice is doing. That in itself
    greatly simplifies development, especially when newcomers join the project. On
    top of that, everything else tends to be much faster. IDEs work faster with a
    small project when compared to big ones used in monolithic applications. They
    start faster since there are no huge servers nor an enormous number of libraries
    to load.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment, Rollback, and Fault Isolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deployment is much faster and easier with microservices. Deploying something
    small is always quicker (if not easier) than deploying something big. In case
    we realized that there is a problem, that problem has potentially limited effect
    and can be rolled back much easier. Until we roll back, the fault is isolated
    to a small part of the system. Continuous delivery or deployment can be done with
    speed and frequencies that would not be possible with big applications.
  prefs: []
  type: TYPE_NORMAL
- en: Commitment Term
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the common problems with monolithic applications is commitment. We are
    often forced to choose from the start the architecture and the technologies that
    will last for a long time. After all, we're building something big that should
    last for a long time. With microservices that need for a long-term commitment
    is much smaller. Change the programming language in one microservice and if it
    turns out to be a good choice, apply it to others. If the experiment failed or
    is not the optimum, there's only one small part of the system that needs to be
    redone. Same applies to frameworks, libraries, servers, and so on. We can even
    use different databases. If some lightweight NoSQL seems like the best fit for
    a particular microservice, why not use it and pack it into the container?
  prefs: []
  type: TYPE_NORMAL
- en: Let us go one step back and look at this subject from the prism of deployment.
    How do those two architectural approaches differ when the time comes to deploy
    our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already discussed that continuous delivery and deployment strategies require
    us to rethink all aspects of the application lifecycle. That is nowhere more noticeable
    than at the very beginning when we are faced with architectural choices. We won't
    go into details of every possible deployment strategy we could face but limit
    the scope to two major decisions that we should make. First one is architecturally
    related to the choice between monolithic applications and microservices. The second
    one is related to how we package the artifacts that should be deployed. More precisely,
    whether we should perform mutable or immutable deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Mutable Monster Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, the most common way to build and deploy applications is as a mutable
    monster server. We create a web server that has the whole application and update
    it every time there is a new release. Changes can be in configuration (properties
    file, XMLs, DB tables, and so on), code artifacts (JARs, WARs, DLLs, static files,
    and so on) and *database schemas and data*. Since we are changing it on every
    release, it is mutable.
  prefs: []
  type: TYPE_NORMAL
- en: With mutable servers, we cannot know for sure that development, test, and production
    environments are the same. Even different nodes in the production might have undesirable
    differences. Code, configuration or static files might not have been updated in
    all instances.
  prefs: []
  type: TYPE_NORMAL
- en: It is a monster server since it contains everything we need as a single instance.
    Back-end, front-end, APIs, and so on. Moreover, it grows over time. It is not
    uncommon that after some time no one is sure what is the exact configuration of
    all pieces in production and the only way to accurately reproduce it somewhere
    else (new production node, test environment, and so on) is to copy the VM where
    it resides and start fiddling with configurations (IPs, host file, DB connections,
    and so on). We just keep adding to it until we lose the track of what it has.
    Given enough time, your "perfect" design and impressive architecture will become
    something different. New layers will be added, the code will be coupled, patches
    on top of patches will be created and people will start losing themselves in the
    maze the code start looking like. Your beautiful little project will become a
    big monster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pride you have will become a subject people talk about on coffee breaks.
    People will start saying that the best thing they could do is to throw it to trash
    and start over. But, the monster is already too big to start over. Too much is
    invested. Too much time would be needed to rewrite it. Too much is at stake. Our
    monolith might continue existing for a long time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mutable Monster Server](img/B05848_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5 – Mutable application server as initially designed
  prefs: []
  type: TYPE_NORMAL
- en: Mutable deployments might look simple, but they are usually not. By coupling
    everything into one place, we are hiding complexity thus increasing the chance
    of discrepancies between different instances.
  prefs: []
  type: TYPE_NORMAL
- en: Time to restart such a server when it receives a new release can be considerable.
    During that time server is usually not operational. Downtime that the new release
    provokes is a loss of money and trust. Today's business expects us to operate
    24/7 without any downtime, and it is not uncommon that a release to production
    means night work of the team during which our services are not available. Given
    such a situation, applying continuous deployment is a dream out of the reach.
    It is a dream that can not become a reality.
  prefs: []
  type: TYPE_NORMAL
- en: Testing is also a problem. No matter how much we tested the release on development
    and test environments, the first time it will be tried in production is when we
    deploy it and make it available not only to our testers but also to all our users.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, fast rollback of such a server is close to impossible. Since it is
    mutable, there is no "photo" of the previous version unless we create a snapshot
    of a whole virtual machine that brings up a whole new set of problems.
  prefs: []
  type: TYPE_NORMAL
- en: By having architecture like this, we cannot fulfill all, if any, of the requirements
    described earlier. We cannot deploy often, due to inability to produce zero-downtime
    and easily rollback. Full automation is risky due to mutable nature of its architecture
    thus preventing us to be fast.
  prefs: []
  type: TYPE_NORMAL
- en: By not deploying often we are accumulating changes that will be released and,
    in that way, we are increasing the probability of a failure.
  prefs: []
  type: TYPE_NORMAL
- en: To solve those problems, deployments should be immutable and composed of small,
    independent, and self-sufficient applications. Remember, our goals are to deploy
    often, have zero-downtime, be able to rollback any release, be automated and be
    fast. Moreover, we should be able to test the release on production environment
    before users see it.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable Server and Reverse Proxy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each traditional deployment introduces a risk tied with changes that need to
    be performed on the server. If we change our architecture to immutable deployments,
    we gain immediate benefits. Provisioning of environments becomes much simpler
    since there is no need to think about applications (they are unchangeable). Whenever
    we deploy an image or a container to the production server, we know that it is
    precisely the same as the one we built and tested. Immutable deployments reduce
    the risk tied to unknown. We know that each deployed instance is exactly the same
    as the other. Unlike mutable deployment, when a package is immutable and contains
    everything (application server, configurations, and artifacts) we stop caring
    about all those things. They were packaged for us throughout the deployment pipeline
    and all we have to do is make sure that the immutable package is sent to the destination
    server. It is the same package as the one we already tested in other environments
    and inconsistencies that could be introduced by mutable deployments are gone.
  prefs: []
  type: TYPE_NORMAL
- en: A reverse proxy can be used to accomplish zero-downtime. Immutable servers together
    with a reverse proxy in a simplified form can be as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we start with a reverse proxy that points to our fully self-sufficient
    immutable application package. This package could be a virtual machine or a container.
    We''ll refer to this application as application image to establish a clear distinction
    from mutable applications. On top of the application is a proxy service that routes
    all the traffic towards the final destination instead of exposing the server directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Server and Reverse Proxy](img/B05848_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-6: Immutable application server deployed as an image (a virtual machine
    or a container)'
  prefs: []
  type: TYPE_NORMAL
- en: Once we decide to deploy a new version, we do it by deploying a separate image
    to a separate server. While in some cases we could deploy this image to the same
    server, more often than not, monolithic applications are very resource demanding
    and we cannot have both on the same node without affecting the performance. At
    this moment, we have two instances. One old (previous release) and one new (latest
    release). All traffic still goes to the old server through the reverse proxy so
    users of our application still do not notice any change. For them, we're still
    running the old and proven software. This is the right moment to execute the final
    set of tests. Preferably those tests are automatic and part of the deployment
    process but manual verification is not excluded. For example, if changes were
    done to front-end, we might want to do the final round of user experience tests.
    No matter what types of tests are performed, they should all attack the new release
    bypassing the reverse proxy. The good thing about those tests is that we are working
    with the future production version of the software that resides on production
    hardware. We are testing production software and hardware without affecting our
    users (they are still being redirected to the old version). We could even enable
    our new release only to a limited number of users in the form of A/B testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, at this stage we have two instances of the server, one (the previous
    release) used by our users and the other (the latest release) used for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Server and Reverse Proxy](img/B05848_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7 – New release of the immutable application deployed to a separate
    node
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we are finished with tests and are confident that the new release works
    as expected, all we have to do is change the reverse proxy to point to the new
    release. The old one can stay for a while in case we need to rollback the changes.
    However, for our users, it does not exist. All traffic is routed to the new release.
    Since the latest release was up-and-running before we changed the routing, the
    switch itself will not interrupt our service (unlike, for example, if we would
    need to restart the server in case of mutable deployments). When the route is
    changed we need to reload our reverse proxy. As an example, *nginx* maintains
    old connections until all of them are switched to the new route:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Server and Reverse Proxy](img/B05848_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-8: Poxy is rerouted to point to the new release'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, when we do not need the old version, we can remove it. Even better,
    we can let the next release remove it for us. In the latter case, when the time
    comes, release process will remove the older release and start the process all
    over again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Server and Reverse Proxy](img/B05848_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9 – The old release is removed
  prefs: []
  type: TYPE_NORMAL
- en: The technique described above is called blue-green deployment and has been in
    use for a long time. We'll be practicing it later on when we reach the Docker
    packaging and deployment examples.
  prefs: []
  type: TYPE_NORMAL
- en: Immutable Microservices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can do even better than this. With immutable deployments, we can easily accomplish
    automatism of the process. Reverse proxy gives us zero-downtime and, having two
    releases up and running allows us to rollback easily. However, since we're still
    dealing with one big application, deployment and tests might take a long time
    to run. That in itself might prevent us from being fast and thus from deploying
    as often as needed. Moreover, having everything as one big server increases development,
    testing and deployment complexity. If things could be split into smaller pieces,
    we might divide complexity into easily manageable chunks. As a bonus, having small
    independent services would allow us to scale more easily. They can be deployed
    to the same machine, scaled out across the network or multiplied if the performance
    of one of them becomes the bottleneck. Microservices to the rescue!
  prefs: []
  type: TYPE_NORMAL
- en: With monster applications we tend to have decoupled layers. Front-end code should
    be separated from the back-end, business layer from data access layer, and so
    on. With microservices, we should start thinking in a different direction. Instead
    of having the business layer separated from the data access layer, we would separate
    services. For example, user's management could be split from the sales service.
    Another difference is physical. While traditional architecture separates on a
    level of packages and classes but still deploys everything together, microservices
    are split physically; they might not even reside on the same physical machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment of microservices follows the same pattern as previously described.
    We deploy our microservice immutable image as any other software:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Microservices](img/B05848_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10 – Immutable microservice deployed as an image (a virtual machine
    or a container)
  prefs: []
  type: TYPE_NORMAL
- en: 'When the time comes to release a new version of some microservice we deploy
    it alongside the older version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Microservices](img/B05848_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11 – New release of the immutable microservice deployed alongside the
    old release
  prefs: []
  type: TYPE_NORMAL
- en: When that microservice release is properly tested we change the proxy route.
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Microservices](img/B05848_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12 – Poxy is re-configured to point to the new release
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we remove the older version of the microservice.
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable Microservices](img/B05848_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13 – The old release is removed
  prefs: []
  type: TYPE_NORMAL
- en: The only significant difference is that due to the size of microservices, we
    often do not need a separate server to deploy the new release in parallel with
    the old one. Now we can truly deploy often automatically, be fast with zero-downtime
    and rollback in case something goes wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Technologically, this architecture might pose particular problems that will
    be the subject of the next chapters. For now, let's just say that those problems
    are easy to solve with the tools and processes we have at our disposal.
  prefs: []
  type: TYPE_NORMAL
- en: Given our requirements that are poor at best and the advantages microservices
    bring over monoliths, the choice is clear. We will be building our application
    using immutable microservices approach. That decision calls for a discussion about
    the best practices we should follow.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the following best practices can be applied to services oriented architecture
    in general. However, with microservices, they become even more significant or
    beneficial. Following is a very brief description that will be extended later
    on throughout the book when the time comes to apply them.
  prefs: []
  type: TYPE_NORMAL
- en: Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dealing with many microservices can quickly become a very complex endeavor.
    Each can be written in a different programming language, can require a different
    (hopefully light) application server or can use a different set of libraries.
    If each service is packed as a container, most of those problems will go away.
    All we have to do is run the container with, for example, Docker and trust that
    everything needed is inside it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers are self-sufficient bundles that contain everything we need (with
    the exception of the kernel), run in an isolated process and are immutable. Being
    self-sufficient means that a container commonly has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Runtime libraries (JDK, Python, or any other library required for the application
    to run)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application server (Tomcat, nginx, and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database (preferably lightweight)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artifact (JAR, WAR, static files, and so on)![Containers](img/B05848_03_14.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 3-14 – Self-sufficient microservice inside a container
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fully self-sufficient containers are the easiest way to deploy services but
    pose a few problems with scaling. If we'd like to scale such a container on multiple
    nodes in a cluster, we'd need to make sure that databases embedded into those
    containers are synchronized or that their data volumes are located on a shared
    drive. The first option often introduces unnecessary complexity while shared volumes
    might have a negative impact on performance. Alternative is to make containers
    almost self-sufficient by externalizing database into a separate container. In
    such a setting there would be two different containers per each service. One for
    the application and the other for the database. They would be linked (preferably
    through a proxy service). While such a combination slightly increases deployment
    complexity, it provides greater freedom when scaling. We can deploy multiple instances
    of the application container or several instances of the database depending performance
    testing results or increase in traffic. Finally, nothing prevents us to scale
    both if such a need arises.
  prefs: []
  type: TYPE_NORMAL
- en: '![Containers](img/B05848_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15 – Microservice inside a container with the separate database
  prefs: []
  type: TYPE_NORMAL
- en: Being self-sufficient and immutable allows us to move containers across different
    environments (development, testing, production, and so on) and always expect the
    same results. Those same characteristics combined with microservices approach
    of building small applications allows us to deploy and scale containers with very
    little effort and much lower risk than other methods would allow us.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a third commonly used combination when dealing with legacy
    systems. Even though we might decide to gradually move from monolithic applications
    towards microservices, databases tend to be the last parts of the system to be
    approved for refactoring. While this is far from the optimal way to perform the
    transition, the reality, especially in big enterprises is that data is the most
    valuable asset. Rewriting an application poses much lower risk than the one we'd
    be facing if we decide to restructure data. It's often understandable that management
    is very skeptical of such proposals. In such a case we might opt for a shared
    database (probably without containers). While such a decision would be partly
    against what we're trying to accomplish with microservices, the pattern that works
    best is to share the database but make sure that each schema or a group of tables
    is exclusively accessed by a single service. The other services that would require
    that data would need to go through the API of the service assigned to it. While
    in such a combination we do not accomplish clear separation (after all, there
    is no clearer more apparent than physical), we can at least control who accesses
    the data subset and have a clear relation between them and the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, that is very similar to what is commonly the idea behind horizontal
    layers. In practice, as the monolithic application grows (and with it the number
    of layers) this approach tends to get abused and ignored. Vertical separation
    (even if a database is shared), helps us keep much clearer bounded context each
    service is in charge of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Containers](img/B05848_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16 – Microservices inside containers accessing the shared database
  prefs: []
  type: TYPE_NORMAL
- en: Proxy Microservices or API Gateway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big enterprise front-ends might need to invoke tens or even hundreds of HTTP
    requests (as is the case with Amazon.com). Requests often take more time to be
    invoked than to receive response data. Proxy microservices might help in that
    case. Their goal is to invoke different microservices and return an aggregated
    service. They should not contain any logic but only group several responses together
    and respond with aggregated data to the consumer.
  prefs: []
  type: TYPE_NORMAL
- en: Reverse Proxy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Never expose microservice API directly. If there isn't some orchestration, the
    dependency between the consumer and the microservices becomes so big that it might
    remove freedom that microservices are supposed to give us. Lightweight servers
    like *nginx*, *Apache Tomcat*, and *HAProxy* are excellent at performing reverse
    proxy tasks and can easily be employed with very little overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Minimalist Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservices should contain only packages, libraries, and frameworks that they
    truly need. The smaller they are, the better. That is quite in contrast to the
    approach used with monolithic applications. While previously we might have used
    JEE servers like JBoss that packed all the tools that we might or might not need,
    microservices work best with much more minimalistic solutions. Having hundreds
    of microservices with each of them having a full JBoss server becomes overkill.
    *Apache Tomcat*, for example, is a much better option. I tend to go for even smaller
    solutions with, for instance, *Spray* as a very lightweight RESTful API server.
    Don't pack what you don't need.
  prefs: []
  type: TYPE_NORMAL
- en: The same approach should be applied to OS level as well. If we're deploying
    microservices as *Docker* containers, *CoreOS* might be a better solution than,
    for example, *Red Hat* or *Ubuntu*. It's free from things we do not need allowing
    us to obtain better utilization of resources. However, as we'll see later, choosing
    OS is not always that simple.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the number of microservices grows, the need for **Configuration Management**
    (**CM**) increases. Deploying many microservices without tools like *Puppet*,
    *Chef or Ansible* (just to name few) quickly becomes a nightmare. Actually, not
    using CM tools for any but simplest solutions is a waste, with or without microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Functional Teams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While no rule dictates what kinds of teams are utilized, microservices are done
    best when the team working on one is multifunctional. A single team should be
    responsible for it from the start (design) until the finish (deployment and maintenance).
    They are too small to be handled from one team to another (architecture/design,
    development, testing, deployment and maintenance teams). Preference is to have
    a team that is in charge of the full lifecycle of a microservice. In many cases,
    one team might be in charge of multiple microservices, but multiple teams should
    not be in charge of one.
  prefs: []
  type: TYPE_NORMAL
- en: API Versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Versioning should be applied to any API, and this holds true for microservices
    as well. If some change breaks the API format, it should be released as a separate
    version. In the case of public APIs as well as those used by other internal services,
    we cannot be sure who is using them and, therefore, must maintain backward compatibility
    or, at least, give consumers enough time to adapt.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Microservices as a concept existed for a long time. Take a look at the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The command listed above is an example of the usage of *pipes* in Unix/Linux.
    It consists of four programs. Each of them is expecting an input (`stdin`) and/or
    an output (`stdout`). Each of them is highly specialized and performs one or very
    few functions. While simple by themselves, when combined those programs are capable
    performing some very complex operations. Same holds true for most of the programs
    found in today's Unix/Linux distributions. In this particular case, we're running
    `ps aux` that retrieves the list of all running processes and passing the output
    to the next in line. That output is used by `grep jav[a]` to limit the results
    to only Java processes. Again, the output is passed to whoever needs it. In this
    particular example, next in line is `awk '{print $2}'` that does, even more, filtering
    and returns only the second column that happens to be the process ID. Finally,
    `xargs kill` takes the output of *awk* as input and kills all processes that match
    IDs we retrieved previously.
  prefs: []
  type: TYPE_NORMAL
- en: Those not used to Unix/Linux might think that the command we just examined is
    an overkill. However, after a bit of practice, those working with Linux commands
    find this approach very flexible and useful. Instead of having "big" programs
    that need to contemplate all possible use cases, we have a lot of small programs
    that can be combined to fulfill almost any task we might need. It is a power born
    out of utmost simplicity. Each program is small and created to achieve a very
    specific objective. More importantly, they all accept clearly defined input and
    produce well-documented output.
  prefs: []
  type: TYPE_NORMAL
- en: Unix is, as far as I know, the oldest example of microservices still in use.
    A lot of small, specific, easy to reason with services with well-defined interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Even though microservices exist for a long time, it is not a chance that they
    become popular only recently. Many things needed to mature and be available for
    microservices to be useful to all but selected few. Some of the concepts that
    made microservices widely used are domain-driven design, continuous delivery,
    containers, small autonomous teams, scalable systems, and so on. Only when all
    those are combined into a single framework microservices start to shine truly.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices are used to create complex systems composed of small and autonomous
    services that exchange data through their APIs and limit their scope to a very
    specific bounded context. From a certain point of view, microservices are what
    object-oriented programming was initially designed to be. When you read thoughts
    of some of the leaders of our industry and, especially, object-oriented programming,
    their descriptions of best practices when absorbed for their logic and not the
    way authors implemented them initially, are the reminiscence of what microservices
    are today. The following quotes correctly describe some of the aspects of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *The big idea is ''messaging''. The key in making great and growable
    systems is much more to design how its modules communicate rather than what their
    internal properties and behaviors should be.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Alan Kay* |'
  prefs: []
  type: TYPE_TB
- en: '|   | *Gather together those things that change for the same reason, and separate
    those things that change for different reasons.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Robert C. Martin* |'
  prefs: []
  type: TYPE_TB
- en: When implementing microservices, we tend to organize them to do only one thing
    or perform only one function. This allows us to pick the best tools for each of
    the jobs. For example, we can code them in a language that best suits the objective.
    Microservices are truly loosely coupled due to their physical separation and provide
    a great level of independence between different teams as long as APIs are clearly
    defined in advance. On top of that, with microservices, we have much faster and
    easier testing and continuous delivery or deployment due to their decentralized
    nature. When concepts we discussed are combined with the emergence of new tools,
    especially *Docker*, we can see microservices in a new light and remove part of
    the problems their development and deployment was creating earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Still, do not take bits of advice from this book as something that should be
    applied to all cases. Microservices are not an answer to all our problems. Nothing
    is. They are not the way all applications should be created and no single solution
    fits all cases. With microservices, we are trying to solve very specific problems
    and not to change the way all applications are designed.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with the decision to develop our application around microservices, it
    is time to do something practical. There is no coding without development environment
    so that will be our first goal. We'll create a development environment for our
    fancy books store service.
  prefs: []
  type: TYPE_NORMAL
- en: We had enough theory and the time is ripe to put this book in front of a computer.
    From now on, most of the book will be a hands-on experience.
  prefs: []
  type: TYPE_NORMAL
