<html><head></head><body><div class="chapter" title="Chapter&#xA0;13.&#xA0;Blue-Green Deployment"><div class="titlepage"><div><div><h1 class="title"><a id="ch13"/>Chapter 13. Blue-Green Deployment</h1></div></div></div><p>Traditionally, we deploy a new release by replacing the current one. The old release is stopped, and the new one is brought up in its place. The problem with this approach is the downtime occurring from the moment the old release is stopped until the new one is fully operational. No matter how quickly you try to do this process, there will be some downtime. That might be only a millisecond, or it can last for minutes or, in extreme situations, even hours. Having monolithic applications introduces additional problems like, for example, the need to wait a considerable amount of time until the application is initialized. People tried to solve this issue in various ways, and most of them used some variation of the <span class="emphasis"><em>blue-green deployment process</em></span>. The idea behind it is simple. At any time, one of the releases <a class="indexterm" id="id534"/>should be running meaning that, during the deployment process, we must deploy a new release in parallel with the old one. The new and the old releases are called blue and green.</p><div class="mediaobject"><img alt="Blue-Green Deployment" src="graphics/B05848_13_01.jpg"/><div class="caption"><p>Figure 13-1 – At any given moment, at least, one service release is up and running</p></div></div><p>We run one color as a current release, bring up the other color as a new release and, once it is fully operational, switch all the traffic from the current to the new release. This switch is often made with a router or a proxy service.</p><p>With the blue-green process, not only that we are removing the deployment downtime, but we are also reducing the risk the deployment might introduce. No matter how well we tested our software before it reached the production node(s), there is always a chance that something will go wrong. When that happens, we still have the current version to rely on. There is no real reason to switch the traffic to the new release until it is tested enough that any reasonable possibility of a failure due to some specifics of the production node is verified. That usually means that integration testing is performed after the deployment and before the "switch" is made. Even if those verifications returned false negatives and there is a failure after the traffic is redirected, we can quickly switch back to the old release and restore the system to the previous state. We can roll back much faster than if we'd need to restore the application from some backup or do another deployment.</p><p>If we combine the blue-green process with immutable deployments (through VMs in the past and though containers today), the result is a very powerful, secure and reliable deployment procedure that can be performed much more often. If architecture is based on microservices in conjunction with containers, we don't need two nodes to perform the procedure and can run both releases side by side.</p><p>The significant challenges with this approach are databases. In many cases, we need to upgrade a database schema in a way that it supports both releases and then proceed with the deployment. The problems that might arise from this database upgrade are often related to the time that passes between releases. When releases are done often, changes to the database schema tend to be small, making it easier to maintain compatibility across two releases. If weeks, or months, passed between releases, database changes could be so big that backward compatibility might be impossible or not worthwhile doing. If we are aiming towards continuous delivery or deployment, the period between two releases should be short or, if it isn't, involve a relatively small amount of changes to the code base.</p><div class="section" title="The blue-green deployment process"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec31"/>The blue-green deployment process</h1></div></div></div><p>The blue-green <a class="indexterm" id="id535"/>deployment procedure, when applied to microservices packed as containers, is as follows.</p><p>The current release (for example blue), is running on the server. All traffic to that release is routed through a proxy service. Microservices are immutable and deployed as containers.</p><div class="mediaobject"><img alt="The blue-green deployment process" src="graphics/B05848_13_02.jpg"/><div class="caption"><p>Figure 13-2 – Immutable microservice deployed as a container</p></div></div><p>When a new release (for example green) is ready to be deployed, we run it in parallel with the current <a class="indexterm" id="id536"/>release. This way we can test the new release without affecting the users since all the traffic continues being sent to the current release.</p><div class="mediaobject"><img alt="The blue-green deployment process" src="graphics/B05848_13_03.jpg"/><div class="caption"><p>Figure 13-3 – New release of the immutable microservice deployed alongside the old release</p></div></div><p>Once we think that the new release is working as expected, we change the proxy service configuration so that the traffic is redirected to that release. Most proxy services will let the existing requests finish their execution using the old proxy configuration so that there is no interruption.</p><div class="mediaobject"><img alt="The blue-green deployment process" src="graphics/B05848_13_04.jpg"/><div class="caption"><p>Figure 13-4 – Proxy is reconfigured to point to the new release</p></div></div><p>When all the<a class="indexterm" id="id537"/> requests sent to the old release received responses, the previous version of a service can be removed or, even better, stopped from running. If the latter option is used, rollback in case of a failure of the new release will be almost instantaneous since all we have to do is bring the old release back up.</p><div class="mediaobject"><img alt="The blue-green deployment process" src="graphics/B05848_13_05.jpg"/><div class="caption"><p>Figure 13-5 – The old release is removed</p></div></div><p>Equipped with the basic logic behind the blue-green process, we can try setting it up. We'll start with manual commands and, once we're familiar with the practical part of the process, we'll attempt to automate the procedure.</p><p>We'll need the <a class="indexterm" id="id538"/>usual two nodes (<code class="literal">cd</code> and <code class="literal">prod</code>) to be up and running so let us create and provision the VMs.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd prod</strong></span>

<span class="strong"><strong>vagrant ssh cd</strong></span>

<span class="strong"><strong>ansible-playbook /vagrant/ansible/prod2.yml \</strong></span>
<span class="strong"><strong>-i /vagrant/ansible/hosts/prod</strong></span>
</pre></div></div></div>
<div class="section" title="Manually running the blue-green deployment"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec32"/>Manually running the blue-green deployment</h1></div></div></div><p>Please note that we'll go through the whole blue-green process within the context of what we tried to accomplish earlier. We will not only run two releases in parallel but make sure that, among <a class="indexterm" id="id539"/>other things, everything is thoroughly tested during multiple phases. That will complicate the process more than if we follow the blue-green procedure assuming that everything works. Most implementations do not take into account the need for testing before making the change to the proxy service. We can, and will, do better than that. Another thing to note is that we'll explore manual steps for you to understand the process. Later on, we'll automate everything using the tools we're already familiar with. I choose this approach in order to be sure that you grasp the complexity behind the combination of the continuous deployment and the blue-green processes. By truly understanding how to do it manually, you will be able to make an informed decision whether benefits of tools we're will explore throughout the rest of the book are greater than things they are missing.</p><p>We'll start by downloading the Docker Compose and nginx configurations that we used in the previous chapter.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir books-ms</strong></span>

<span class="strong"><strong>cd books-ms</strong></span>

<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/docker-compose.yml</strong></span>

<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/nginx-includes.conf</strong></span>

<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/nginx-upstreams-blue.ctmpl</strong></span>

<span class="strong"><strong>wget https://raw.githubusercontent.com/vfarcic\</strong></span>
<span class="strong"><strong>/books-ms/master/nginx-upstreams-green.ctmpl</strong></span>
</pre></div><p>With all the <a class="indexterm" id="id540"/>configuration files available, let us deploy the first release. The tools we explored earlier will come in handy. We'll use Consul as the service registry, Registrator to register and de-register containers, nginx as a proxy service and Consul Template to generate configurations and reload nginx.</p><div class="section" title="Deploying the blue release"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec64"/>Deploying the blue release</h2></div></div></div><p>Since, at this<a class="indexterm" id="id541"/> moment, we do not have the <code class="literal">books-ms</code> service up and running, we'll call the first release <code class="literal">blue</code>. The only thing we need to do for now is to make sure that the name of the container we are about to run contains the word <code class="literal">blue</code> so that it does not collide with the next release. We'll be using Docker Compose to run containers so let us take a quick look at the targets defined in the <code class="literal">docker-compose.yml</code> file that we just downloaded (only relevant targets are presented).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>base:</strong></span>
<span class="strong"><strong>  image: 10.100.198.200:5000/books-ms</strong></span>
<span class="strong"><strong>  ports:</strong></span>
<span class="strong"><strong>    - 8080</strong></span>
<span class="strong"><strong>  environment:</strong></span>
<span class="strong"><strong>    - SERVICE_NAME=books-ms</strong></span>

<span class="strong"><strong>app-blue:</strong></span>
<span class="strong"><strong>  extends:</strong></span>
<span class="strong"><strong>    service: base</strong></span>
<span class="strong"><strong>  environment:</strong></span>
<span class="strong"><strong>    - SERVICE_NAME=books-ms-blue</strong></span>
<span class="strong"><strong>  links:</strong></span>
<span class="strong"><strong>    - db:db</strong></span>

<span class="strong"><strong>app-green:</strong></span>
<span class="strong"><strong>  extends:</strong></span>
<span class="strong"><strong>    service: base</strong></span>
<span class="strong"><strong>  environment:</strong></span>
<span class="strong"><strong>    - SERVICE_NAME=books-ms-green</strong></span>
<span class="strong"><strong>  links:</strong></span>
<span class="strong"><strong>    - db:db</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>We cannot use the <code class="literal">app</code> target directly since we'll be deploying two different targets (one for each color) and in that way avoid them overriding each other. Also, we'll want to differentiate them in Consul as well, so the <code class="literal">SERVICE_NAME</code> environment variable should be unique. To<a class="indexterm" id="id542"/> accomplish that, we have two new targets called <code class="literal">app-blue</code> and <code class="literal">app-green</code>. Those targets extend the <code class="literal">base</code> service in the same way the <code class="literal">app</code> target extended it in previous chapters. The only difference between the targets <code class="literal">app-blue</code> and <code class="literal">app-green</code> on one hand and the <code class="literal">base</code> on the other is (besides the name of the target) the environment variable <code class="literal">SERVICE_NAME</code>.</p><p>With those two targets defined, we can deploy the blue release.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://prod:2375</strong></span>

<span class="strong"><strong>docker-compose pull app-blue</strong></span>

<span class="strong"><strong>docker-compose up -d app-blue</strong></span>
</pre></div><p>We pulled the latest version from the registry and brought it up as the blue release of the service. Just to be on the safe side, let us quickly check whether the service is running and is registered in Consul.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose ps</strong></span>

<span class="strong"><strong>curl prod:8500/v1/catalog/service/books-ms-blue \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>
</pre></div><p>The output of both commands combined is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>   Name              Command     State                     Ports</strong></span>
<span class="strong"><strong>----------------------------------------------------------------</strong></span>
<span class="strong"><strong>booksms_app-blue_1   /run.sh      Up      0.0.0.0:32768-&gt;8080/tcp</strong></span>
<span class="strong"><strong>booksms_db_1         /entrypoint.sh mongod   Up         27017/tcp</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    "ModifyIndex": 38,</strong></span>
<span class="strong"><strong>    "CreateIndex": 38,</strong></span>
<span class="strong"><strong>    "Node": "prod",</strong></span>
<span class="strong"><strong>    "Address": "10.100.198.201",</strong></span>
<span class="strong"><strong>    "ServiceID": "prod:booksms_app-blue_1:8080",</strong></span>
<span class="strong"><strong>    "ServiceName": "books-ms-blue",</strong></span>
<span class="strong"><strong>    "ServiceTags": [],</strong></span>
<span class="strong"><strong>    "ServiceAddress": "10.100.198.201",</strong></span>
<span class="strong"><strong>    "ServicePort": 32768,</strong></span>
<span class="strong"><strong>    "ServiceEnableTagOverride": false</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>]</strong></span>
</pre></div><p>The first command showed that both the <code class="literal">app-blue</code> and the <code class="literal">db</code> containers are running. The second command displayed the details of the <code class="literal">books-ms-blue</code> service registered in Consul. Now we have the first release of our service up and running but still not integrated <a class="indexterm" id="id543"/>with nginx and, therefore, not accessible through the port 80. We can confirm that by sending a request to the service.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>
</pre></div><p>The output is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>HTTP/1.1 404 Not Found</strong></span>
<span class="strong"><strong>Server: nginx/1.9.9</strong></span>
<span class="strong"><strong>Date: Sun, 03 Jan 2016 20:47:59 GMT</strong></span>
<span class="strong"><strong>Content-Type: text/html</strong></span>
<span class="strong"><strong>Content-Length: 168</strong></span>
<span class="strong"><strong>Connection: keep-alive</strong></span>
</pre></div><p>The request response is the <code class="literal">404 Not Found</code> error message proving that we are yet to configure the proxy.</p><div class="mediaobject"><img alt="Deploying the blue release" src="graphics/B05848_13_06.jpg"/><div class="caption"><p>Figure 13-6 – The blue container is deployed</p></div></div></div><div class="section" title="Integrating the blue release"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec65"/>Integrating the blue release</h2></div></div></div><p>We can integrate<a class="indexterm" id="id544"/> the service in a similar way as we did before. The only difference is in the target of the service we registered in Consul.</p><p>Let us start by taking a look at the nginx Consul template <code class="literal">nginx-upstreams-blue.ctmpl</code> that we downloaded earlier.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>upstream books-ms {</strong></span>
<span class="strong"><strong>    {{range service "books-ms-blue" "any"}}</strong></span>
<span class="strong"><strong>    server {{.Address}}:{{.Port}};</strong></span>
<span class="strong"><strong>    {{end}}</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>The service name is <code class="literal">books-ms-blue</code> and we can proceed by running Consul Template that will<a class="indexterm" id="id545"/> generate the final nginx upstreams configuration.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>consul-template \</strong></span>
<span class="strong"><strong>    -consul prod:8500 \</strong></span>
<span class="strong"><strong>    -template "nginx-upstreams-blue.ctmpl:nginx-upstreams.conf" \</strong></span>
<span class="strong"><strong>    -once</strong></span>
</pre></div><p>The command run Consul Template that produced the nginx upstreams configuration file and reloaded the service.</p><p>Let's check whether the configuration file was indeed created correctly.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat nginx-upstreams.conf</strong></span>
</pre></div><p>The output is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>upstream books-ms {</strong></span>
<span class="strong"><strong>    server 10.100.198.201:32769;</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>Finally, all that's left is to copy the configuration files to the <code class="literal">prod</code> server and reload <code class="literal">nginx</code>. When asked, please use <code class="literal">vagrant</code> as the password.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scp nginx-includes.conf \</strong></span>
<span class="strong"><strong>    prod:/data/nginx/includes/books-ms.conf</strong></span>

<span class="strong"><strong>scp nginx-upstreams.conf \</strong></span>
<span class="strong"><strong>    prod:/data/nginx/upstreams/books-ms.conf</strong></span>

<span class="strong"><strong>docker kill -s HUP nginx</strong></span>
</pre></div><p>We copied the two configuration files to the server and reloaded <code class="literal">nginx</code> by sending the <code class="literal">HUP</code> signal.</p><p>Let's check whether our service is indeed integrated with the proxy.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>
</pre></div><p>The output is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>HTTP/1.1 200 OK</strong></span>
<span class="strong"><strong>Server: nginx/1.9.9</strong></span>
<span class="strong"><strong>Date: Sun, 03 Jan 2016 20:51:12 GMT</strong></span>
<span class="strong"><strong>Content-Type: application/json; charset=UTF-8</strong></span>
<span class="strong"><strong>Content-Length: 2</strong></span>
<span class="strong"><strong>Connection: keep-alive</strong></span>
<span class="strong"><strong>Access-Control-Allow-Origin: *</strong></span>
</pre></div><p>This time, the response code is <code class="literal">200 OK</code> indicating that the service indeed responded to the request.</p><div class="mediaobject"><img alt="Integrating the blue release" src="graphics/B05848_13_07.jpg"/><div class="caption"><p>Figure 13-7 – The blue container integrated with the proxy service</p></div></div><p>We finished<a class="indexterm" id="id546"/> the simplest scenario by deploying the first (blue) release. As you will soon see, the process of deploying the second (green) release will not be much different.</p></div><div class="section" title="Deploying the green release"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec66"/>Deploying the green release</h2></div></div></div><p>Deployment <a class="indexterm" id="id547"/>of the second (green) release can be done using the same steps as those we executed for the first (blue) release. The only difference is that this time we'll deploy the <code class="literal">books-ms-green</code> instead of the <code class="literal">books-ms-blue</code> target.</p><p>Unlike the previous deployment, this time, the new release (green) will run in parallel with the current release (blue).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose pull app-green</strong></span>

<span class="strong"><strong>docker-compose up -d app-green</strong></span>
</pre></div><p>The new release has been pulled and run. We can confirm that by running the <code class="literal">docker-compose ps</code> command.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose ps</strong></span>
</pre></div><p>The result is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>       Name           Command     State                     Ports</strong></span>
<span class="strong"><strong>-----------------------------------------------------------------</strong></span>
<span class="strong"><strong>booksms_app-blue_1     /run.sh    Up      0.0.0.0:32769-&gt;8080/tcp</strong></span>
<span class="strong"><strong>booksms_app-green_1    /run.sh    Up      0.0.0.0:32770-&gt;8080/tcp</strong></span>
<span class="strong"><strong>booksms_db_1           /entrypoint.sh mongod   Up      27017/tcp</strong></span>
</pre></div><p>The output shows that the two services (blue and green) are running in parallel. Similarly, we can <a class="indexterm" id="id548"/>confirm that both releases are registered in Consul.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl prod:8500/v1/catalog/services \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>
</pre></div><p>The output is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "dockerui": [],</strong></span>
<span class="strong"><strong>  "consul": [],</strong></span>
<span class="strong"><strong>  "books-ms-green": [],</strong></span>
<span class="strong"><strong>  "books-ms-blue": []</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>As before, we can also check the details of the newly deployed service.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl prod:8500/v1/catalog/service/books-ms-green \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>
</pre></div><p>Finally, we can confirm that the old release is still accessible through the proxy.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>

<span class="strong"><strong>docker logs nginx</strong></span>
</pre></div><p>The output of the last command should be similar to the following (timestamps are removed for brevity).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 201 "-" "curl/7.35.0" "-" 10.100.198.201:32769</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 201 "-" "curl/7.35.0" "-" 10.100.198.201:32769</strong></span>
</pre></div><p>Please keep in mind that the port of the service deployed on your computer might be different than the one from the preceding example.</p><p>The output of nginx logs should display that the request we made is redirected to the port of the blue release. That can be observed by checking that the last request went to the same port as the one we made before deploying the <code class="literal">green</code> release.</p><div class="mediaobject"><img alt="Deploying the green release" src="graphics/B05848_13_08.jpg"/><div class="caption"><p>Figure 13-8 – The green container is deployed in parallel with the blue</p></div></div><p>Right now, we <a class="indexterm" id="id549"/>have two releases (blue and green) running in parallel and the proxy service is still redirecting all requests to the old release (blue). The next step should be to test the new release before we change the proxy configuration. We'll skip testing until we reach the automation part and dive straight into the integration of the green release with nginx.</p></div><div class="section" title="Integrating the green release"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec67"/>Integrating the green release</h2></div></div></div><p>The process<a class="indexterm" id="id550"/> to integrate the second (green) release with the proxy service is similar to the one we already did.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>consul-template \</strong></span>
<span class="strong"><strong>    -consul prod:8500 \</strong></span>
<span class="strong"><strong>    -template "nginx-upstreams-green.ctmpl:nginx-upstreams.conf" \</strong></span>
<span class="strong"><strong>    -once</strong></span>

<span class="strong"><strong>scp nginx-upstreams.conf \</strong></span>
<span class="strong"><strong>    prod:/data/nginx/upstreams/books-ms.conf</strong></span>

<span class="strong"><strong>docker kill -s HUP nginx</strong></span>
</pre></div><p>We can send a request to the proxy and check its logs to see whether it truly points to the new (green) release.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>

<span class="strong"><strong>docker logs nginx</strong></span>
</pre></div><p>The nginx logs should be similar to the following (timestamps are removed for brevity).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 201 "-" "curl/7.35.0" "-" 10.100.198.201:32769</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 201 "-" "curl/7.35.0" "-" 10.100.198.201:32769</strong></span>
<span class="strong"><strong>"GET /api/v1/books HTTP/1.1" 200 201 "-" "curl/7.35.0" "-" 10.100.198.201:32770</strong></span>
</pre></div><p>It is obvious that the last request went to a different port (<code class="literal">32770</code>) than those we made before (<code class="literal">32769</code>). We switched the proxy from the blue to the green release. There was no downtime during this process since we waited until the new release is fully up and running <a class="indexterm" id="id551"/>before changing the proxy. Also, nginx is intelligent enough not to apply the configuration change to all requests but only to those made after the reload. In other words, all requests started before the reload continued using the old release while all those initiated afterward were sent to the new release. We managed to accomplish zero-downtime with minimum effort and without resorting to any new tool. We used nginx as a proxy and Consul (together with Registrator and Consul Template) to store and retrieve service information.</p><div class="mediaobject"><img alt="Integrating the green release" src="graphics/B05848_13_09.jpg"/><div class="caption"><p>Figure 13-9 – The green container integrated with the proxy service</p></div></div><p>As a result of what we did by now, the new release was deployed in parallel with the old one and proxy was changed to point to that new release. Now we can safely remove the old release.</p></div><div class="section" title="Removing the blue release"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec68"/>Removing the blue release</h2></div></div></div><p>Removing a<a class="indexterm" id="id552"/> release is easy, and we did it many times before. All we have to do is make sure that the correct target is used when running the <code class="literal">stop</code> command.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose stop app-blue</strong></span>

<span class="strong"><strong>docker-compose ps</strong></span>
</pre></div><p>The first command stopped the blue release, and the second listed all processes specified as Docker Compose targets. The output of the command that list processes is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Name               Command      State                       Ports</strong></span>
<span class="strong"><strong>-----------------------------------------------------------------</strong></span>
<span class="strong"><strong>booksms_app-blue_1    /run.sh                            Exit 137</strong></span>
<span class="strong"><strong>booksms_app-green_1   /run.sh    Up       0.0.0.0:32770-&gt;8080/tcp</strong></span>
<span class="strong"><strong>booksms_db_1   /entrypoint.sh mongod     Up             27017/tcp</strong></span>
</pre></div><p>Please note that the state of the <code class="literal">booksms_app-blue_1</code> is <code class="literal">Exit 137</code>. Only the green release and the database containers are running.</p><p>We can also <a class="indexterm" id="id553"/>confirm the same by sending a request to Consul.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl prod:8500/v1/catalog/services | jq '.'</strong></span>
</pre></div><p>The Consul response is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "dockerui": [],</strong></span>
<span class="strong"><strong>  "consul": [],</strong></span>
<span class="strong"><strong>  "books-ms-green": []</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>Registrator detected the removal of the blue release and removed it from Consul.</p><p>We should also check that the green release is still integrated with the proxy service.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>
</pre></div><p>As expected, nginx is still sending all requests to the green release and our work is done (for now). To summarize, we deployed a new release in parallel with the old one, changed the proxy service to point to the new release and, once all requests invoked with the old release received their responses, removed the old release.</p><div class="mediaobject"><img alt="Removing the blue release" src="graphics/B05848_13_10.jpg"/><div class="caption"><p>Figure 13-10 – The blue container is removed</p></div></div><p>The only thing left, before we proceed with the automation, is to find a better way to discover which release to deploy (blue or green). While running manually, we can easily find that information by simply listing docker processes or services registered in Consul and observing which color is not running. The automated deployment will require a bit different <a class="indexterm" id="id554"/>approach. We should discover which release to run.</p><p>Let us remove the containers and start over.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker-compose stop</strong></span>

<span class="strong"><strong>docker-com</strong></span>
<span class="strong"><strong>pose rm -f</strong></span>
</pre></div></div><div class="section" title="Discovering which release to deploy and rolling back"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec69"/>Discovering which release to deploy and rolling back</h2></div></div></div><p>One way to know which color to deploy next would be to store the deployed color to Consul and use that information for the next deployment. In other words, we should have two processes; color discovery and color registration.</p><p>Let's think<a class="indexterm" id="id555"/> about use cases of the color discovery. There are three possible combinations:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We are deploying the first release, and no color is stored in the registry.</li><li class="listitem">The blue release is running and stored in the registry.</li><li class="listitem">The green release is running and stored in the registry.</li></ol></div><p>We can reduce those combinations to two. If blue color is registered, the next one is green. Otherwise, the next color is blue covering both the case when the current color is green or when no color is registered (when service has never been deployed). With this strategy, we can create the following bash script (please do not run it yet).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#!/usr/bin/env bash</strong></span>

<span class="strong"><strong>SERVICE_NAME=$1</strong></span>
<span class="strong"><strong>PROD_SERVER=$2</strong></span>

<span class="strong"><strong>CURR_COLOR=`curl \</strong></span>
<span class="strong"><strong>    http://$PROD_SERVER:8500/v1/kv/$SERVICE_NAME/color?raw`</strong></span>

<span class="strong"><strong>if [ "$CURR_COLOR" == "blue" ]; then</strong></span>
<span class="strong"><strong>    echo "green"</strong></span>
<span class="strong"><strong>else</strong></span>
<span class="strong"><strong>    echo "blue"</strong></span>
<span class="strong"><strong>fi</strong></span>
</pre></div><p>Since we could use the same script for many services, it accepts two arguments; the name of the service we are about to deploy and the destination (production) server. Then, we query Consul on the production server and put the result into the <code class="literal">CURR_COLOR</code> variable. That is followed by a simple <code class="literal">if…else</code> statement that sends the <code class="literal">green</code> or the <code class="literal">blue</code> string to <code class="literal">STDOUT</code>. With <a class="indexterm" id="id556"/>such a script, we can easily retrieve the color we should use to deploy a service.</p><p>Let's create the script:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo '#!/usr/bin/env bash</strong></span>

<span class="strong"><strong>SERVICE_NAME=$1</strong></span>
<span class="strong"><strong>PROD_SERVER=$2</strong></span>

<span class="strong"><strong>CURR_COLOR=`curl \</strong></span>
<span class="strong"><strong>    http://$PROD_SERVER:8500/v1/kv/$SERVICE_NAME/color?raw`</strong></span>

<span class="strong"><strong>if [ "$CURR_COLOR" == "blue" ]; then</strong></span>
<span class="strong"><strong>    echo "green"</strong></span>
<span class="strong"><strong>else</strong></span>
<span class="strong"><strong>    echo "blue"</strong></span>
<span class="strong"><strong>fi</strong></span>
<span class="strong"><strong>' | tee get-color.sh</strong></span>

<span class="strong"><strong>chmod +x get-color.sh</strong></span>
</pre></div><p>We created the <code class="literal">get-color.sh</code> script and gave it executable permissions. Now we can use it to retrieve the next color and repeat the procedure we practiced before.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NEXT_COLOR=`./get-color.sh books-ms prod`</strong></span>

<span class="strong"><strong>export DOCKER_HOST=tcp://prod:2375</strong></span>

<span class="strong"><strong>docker-compose pull app-$NEXT_COLOR</strong></span>

<span class="strong"><strong>docker-compose up -d app-$NEXT_COLOR</strong></span>
</pre></div><p>The only difference when compared with the commands we run earlier, is that we're using the <code class="literal">NEXT_COLOR</code> variable instead of hard-coded values <code class="literal">blue</code> and <code class="literal">green</code>. As a result, we have the first release (blue) up and running.</p><div class="mediaobject"><img alt="Discovering which release to deploy and rolling back" src="graphics/B05848_13_11.jpg"/><div class="caption"><p>Figure 13-11 – The color of the current release is retrieved from Consul</p></div></div><p>Let's use this<a class="indexterm" id="id557"/> opportunity to have a short discussion about testing. On one hand, we want to test as much as possible before we change the proxy to point to the new release. On the other hand, we still need to make one round of tests, after the proxy is changed, to be sure that everything (including the change of the proxy) is running as expected. We'll call those two types pre-integration tests and post-integration tests. Keep in mind that their scope should be limited to those cases that could not be covered with pre-deployment tests. In the case of the (relatively small) <code class="literal">books-ms</code> service, it should be enough if pre-integration tests verify that the service can communicate with the database. In such a case, the only thing left to check after the integration with the proxy service, is that nginx has been reconfigured correctly.</p><p>Let's start with pre-integration tests. We'll simulate testing using <code class="literal">curl</code>. Since the proxy is still not changed to point to the newly deployed service, we need to find out what the port the newly released service is. We can find the port from Consul and create a script similar to the <code class="literal">get-color.sh</code>. The script can be created with the following command.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo '#!/usr/bin/env bash</strong></span>

<span class="strong"><strong>SERVICE_NAME=$1</strong></span>
<span class="strong"><strong>PROD_SERVER=$2</strong></span>
<span class="strong"><strong>COLOR=$3</strong></span>

<span class="strong"><strong>echo `curl \</strong></span>
<span class="strong"><strong>  $PROD_SERVER:8500/v1/catalog/service/$SERVICE_NAME-$COLOR \</strong></span>
<span class="strong"><strong>  | jq ".[0].ServicePort"`</strong></span>
<span class="strong"><strong>' | tee get-port.sh</strong></span>

<span class="strong"><strong>chmod +x get-port.sh</strong></span>
</pre></div><p>This time, we created the script named <code class="literal">get-port.sh</code> with three arguments; the name of the service, the address of the production server, and the color. With those three arguments, we <a class="indexterm" id="id558"/>are querying the information from Consul and sending the result to STDOUT.</p><p>Let's try it out.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NEXT_PORT=`./get-port.sh books-ms prod $NEXT_COLOR`</strong></span>

<span class="strong"><strong>echo $NEXT_PORT</strong></span>
</pre></div><p>The output will vary from case to case depending on the random port Docker assigned to our service. With the port stored inside the variable, we can test the service before integrating it with the proxy.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I prod:$NEXT_PORT/api/v1/books</strong></span>
</pre></div><p>Service returned the status code <code class="literal">200 OK</code> so we can proceed with the integration in a similar way we did before. When asked, please use <code class="literal">vagrant</code> as the password.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>consul-template \</strong></span>
<span class="strong"><strong>    -consul prod:8500 \</strong></span>
<span class="strong"><strong>    -template "nginx-upstreams-$NEXT_COLOR.ctmpl:nginx-upstreams.conf" \</strong></span>
<span class="strong"><strong>    -once</strong></span>

<span class="strong"><strong>scp nginx-upstreams.conf \</strong></span>
<span class="strong"><strong>    prod:/data/nginx/upstreams/books-ms.conf</strong></span>

<span class="strong"><strong>docker kill -s HUP nginx</strong></span>
</pre></div><p>With the service integrated, we can test it again but this time without the port.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>
</pre></div><p>Finally, we should stop one of the containers. Which one should be stopped depends on the testing results. If pre-integration tests failed, we should stop the new release. There is no need to do anything with the proxy since, at this time, it is still sending all requests to the old release. On the other hand, if post-integration tests failed, not only that the new release should be stopped, but we should also revert changes to the proxy service so that all traffic goes back to the old release. At this moment we won't go through all the paths we might need to take in case of tests failures. That will be reserved for the automation that we will explore soon. For now, we'll put the color to Consul registry and stop the old release.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -X PUT -d $NEXT_COLOR \</strong></span>
<span class="strong"><strong>    prod:8500/v1/kv/books-ms/color</strong></span>

<span class="strong"><strong>CURR_COLOR=`./get-color.sh books-ms prod`</strong></span>

<span class="strong"><strong>docker-compose stop app-$CURR_COLOR</strong></span>
</pre></div><p>This set of commands put the new color to the registry, obtained the next color that should be equivalent to the color of the old release, and, finally, stopped the old release. Since we <a class="indexterm" id="id559"/>started over and this is the first release, there was no old release to be stopped. Never the less, the next time we run the process, the old release will indeed be stopped.</p><div class="mediaobject"><img alt="Discovering which release to deploy and rolling back" src="graphics/B05848_13_12.jpg"/><div class="caption"><p>Figure 13-12 – The color of the current release is sent to Consul</p></div></div><p>With this, we concluded the manual process of blue-green deployment. It is done in a way that it can easily be automated. Before we move forward, let's run all those commands few more times and observe that the color changes from blue to green, from green to blue and so on. All the commands grouped together are as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NEXT_COLOR=`./get-color.sh books-ms prod`</strong></span>

<span class="strong"><strong>docker-compose pull app-$NEXT_COLOR</strong></span>

<span class="strong"><strong>docker-compose up -d app-$NEXT_COLOR</strong></span>

<span class="strong"><strong>NEXT_PORT=`./get-port.sh books-ms prod $NEXT_COLOR`</strong></span>

<span class="strong"><strong>consul-template \</strong></span>
<span class="strong"><strong>    -consul prod:8500 \</strong></span>
<span class="strong"><strong>    -template "nginx-upstreams-$NEXT_COLOR.ctmpl:nginx-upstreams.conf" \</strong></span>
<span class="strong"><strong>    -once</strong></span>

<span class="strong"><strong>scp nginx-upstreams.conf \</strong></span>
<span class="strong"><strong>    prod:/data/nginx/upstreams/books-ms.conf</strong></span>

<span class="strong"><strong>docker kill -s HUP nginx</strong></span>

<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>

<span class="strong"><strong>curl -X PUT -d $NEXT_COLOR \</strong></span>
<span class="strong"><strong>    prod:8500/v1/kv/books-ms/color</strong></span>

<span class="strong"><strong>CURR_COLOR=`./get-color.sh books-ms prod`</strong></span>

<span class="strong"><strong>docker-compose stop app-$CURR_COLOR</strong></span>

<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>

<span class="strong"><strong>docker-compose ps</strong></span>
</pre></div><p>The last<a class="indexterm" id="id560"/> command showed Docker processes. You will see that, after the first run, the green release will be running, and the blue will be in Exited state, and then, after the next run, the blue release will be running, and the green will be in the Exited state, and so on. We managed to deploy new releases without any downtime. The only exception is if post-integration tests fail, which is very unlikely to happen since the only cause for that would be a failure of the proxy service itself due to the wrong configuration. Since the process will soon be fully automated, such a thing is indeed very unlikely to happen. Another reason for post-integration tests to fail would be if proxy service itself fails. The only way to remove this possibility is to have multiple instances of the proxy service (out of the scope of this book).</p><p>That being said, let's see the nginx logs.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker logs nginx</strong></span>
</pre></div><p>You'll notice that each request we made was sent to a different port meaning that a new container was indeed deployed and running on a new port.</p><p>Now, after all those commands and experiments, we are ready to start working on the automation of the blue-green deployment procedure.</p><p>We'll destroy the virtual machines and start over to be sure that everything works correctly.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>

<span class="strong"><strong>vagr</strong></span>
<span class="strong"><strong>ant destroy -f</strong></span>
</pre></div></div></div>
<div class="section" title="Automating the blue-green deployment with Jenkins workflow"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec33"/>Automating the blue-green deployment with Jenkins workflow</h1></div></div></div><p>We'll <a class="indexterm" id="id561"/>start by creating the <a class="indexterm" id="id562"/>VMs, provisioning the <code class="literal">prod</code> node, and bringing up Jenkins, our deployment tool of choice.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd prod</strong></span>

<span class="strong"><strong>vagrant ssh cd</strong></span>

<span class="strong"><strong>ansible-playbook /vagrant/ansible/prod2.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>

<span class="strong"><strong>ansible-playbook /vagrant/ansible/jenkins-node.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/prod</strong></span>

<span class="strong"><strong>ansible-playbook /vagrant/ansible/jenkins.yml \</strong></span>
<span class="strong"><strong>    -c local</strong></span>
</pre></div><p>Since it will take a couple of minutes until everything is set, let us discuss what should be automated and how. We are already familiar with the Jenkins Workflow. It served us well, so there is no real reason to change the tool at this time. We'll use it to automate the blue-green deployment procedure. The flow will have quite a lot of steps so we'll break them into functions to digest the process more easily and, at the same time, to extend our workflow utilities script. More detailed discussion and implementation of those functions follow.</p><div class="mediaobject"><img alt="Automating the blue-green deployment with Jenkins workflow" src="graphics/B05848_13_13.jpg"/><div class="caption"><p>Figure 13-13 – Blue-green deployment automation flow</p></div></div><div class="section" title="Blue-green deployment role"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec70"/>Blue-green deployment role</h2></div></div></div><p>We'll use the <span class="emphasis"><em>Multibranch Workflow</em></span> Jenkins job <code class="literal">books-ms-blue-green</code>. It filters branches of the <code class="literal">vfarcic/books-ms</code> repository so that only those containing <code class="literal">blue-green</code> in their names are included.</p><p>Since the <a class="indexterm" id="id563"/>first run might take a considerable amount of time, let's index branches so that Jenkins can run the subprojects while we explore the script.</p><p>Please open the Jenkins Multibranch Workflow job <code class="literal">books-ms-blue-green</code>, click the <span class="strong"><strong>Branch Indexing</strong></span> and, then, <span class="strong"><strong>Run Now</strong></span> links from the left-hand menu. Once branches are indexed, Jenkins will find that the <code class="literal">blue-green</code> branch matches the filter set inside the job, create the subproject with the same name and start running it. The indexing status can be seen in the <code class="literal">master</code> node executor located in the bottom-left part of the screen.</p><div class="mediaobject"><img alt="Blue-green deployment role" src="graphics/B05848_13_14.jpg"/><div class="caption"><p>Figure 13-14 – The Jenkins Multibranch Workflow job books-ms-blue-green with the blue-green subproject</p></div></div><p>We'll leave<a class="indexterm" id="id564"/> Jenkins running the build and explore the <code class="literal">Jenkinsfile</code> inside the <code class="literal">blue-green</code> branch.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>node("cd") {</strong></span>
<span class="strong"><strong>    def serviceName = "books-ms"</strong></span>
<span class="strong"><strong>    def prodIp = "10.100.198.201"</strong></span>
<span class="strong"><strong>    def proxyIp = "10.100.198.201"</strong></span>
<span class="strong"><strong>    def proxyNode = "prod"</strong></span>
<span class="strong"><strong>    def registryIpPort = "10.100.198.200:5000"</strong></span>

<span class="strong"><strong>    def flow = load "/data/scripts/workflow-util.groovy"</strong></span>

<span class="strong"><strong>    git url: "https://github.com/vfarcic/${serviceName}.git"</strong></span>
<span class="strong"><strong>    flow.provision("prod2.yml")</strong></span>
<span class="strong"><strong>    flow.buildTests(serviceName, registryIpPort)</strong></span>
<span class="strong"><strong>    flow.runTests(serviceName, "tests", "")</strong></span>
<span class="strong"><strong>    flow.buildService(serviceName, registryIpPort)</strong></span>

<span class="strong"><strong>    def currentColor = flow.getCurrentColor(serviceName, prodIp)</strong></span>
<span class="strong"><strong>    def nextColor = flow.getNextColor(currentColor)</strong></span>

<span class="strong"><strong>    flow.deployBG(serviceName, prodIp, nextColor)</strong></span>
<span class="strong"><strong>    flow.runBGPreIntegrationTests(serviceName, prodIp, nextColor)</strong></span>
<span class="strong"><strong>    flow.updateBGProxy(serviceName, proxyNode, nextColor)</strong></span>
<span class="strong"><strong>    flow.runBGPostIntegrationTests(serviceName, prodIp, proxyIp, proxyNode, currentColor, nextColor)</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>The file starts with the declaration of a few variables followed by the load of the <code class="literal">workflow-util.groovy</code> script. That is followed with invocations of the functions that provision the environments, build and run tests, and build the service. Up until now, the script is the same as the one we explored in the previous chapter.</p><p>The first new<a class="indexterm" id="id565"/> additions are invocations of the utilities functions <code class="literal">getCurrentColor</code> and <code class="literal">getNextColor</code> and assignment of values they return to the <code class="literal">currentColor</code> and the <code class="literal">nextColor</code> variables. The functions are as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def getCurrentColor(serviceName, prodIp) {</strong></span>
<span class="strong"><strong>    try {</strong></span>
<span class="strong"><strong>        return sendHttpRequest("http://${prodIp}:8500/v1/kv/${serviceName}/color?raw")</strong></span>
<span class="strong"><strong>    } catch(e) {</strong></span>
<span class="strong"><strong>        return ""</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}</strong></span>

<span class="strong"><strong>def getNextColor(currentColor) {</strong></span>
<span class="strong"><strong>    if (currentColor == "blue") {</strong></span>
<span class="strong"><strong>        return "green"</strong></span>
<span class="strong"><strong>    } else {</strong></span>
<span class="strong"><strong>        return "blue"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>As you can see, those functions follow the same logic as the one we practiced with manual commands but, this time, translated to Groovy. The current color is retrieved from Consul and used to deduce the next color we should deploy.</p><p>Now that we know what the currently running color is as well as what the next color should be, we can deploy the new release using the <code class="literal">deployBG</code>. The function is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def deployBG(serviceName, prodIp, color) {</strong></span>
<span class="strong"><strong>    stage "Deploy"</strong></span>
<span class="strong"><strong>    withEnv(["DOCKER_HOST=tcp://${prodIp}:2375"]) {</strong></span>
<span class="strong"><strong>        sh "docker-compose pull app-${color}"</strong></span>
<span class="strong"><strong>        sh "docker-compose -p ${serviceName} up -d app-${color}"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>We created the <code class="literal">DOCKER_HOST</code> environment variable pointing to Docker CLI running on the production node. The variable scope is limited to the commands within its curly braces. Inside them, we are pulling the latest release and running it through Docker Compose. The only important difference when, compared with the <code class="literal">Jenkinsfile</code> script we explored in the previous chapter, is the dynamic generation of the target through the <code class="literal">color</code> variable. The target that will be used depends on the actual value of the <code class="literal">nextColor</code> used to invoke this function.</p><p>At this point in the script, a new release is deployed but still not integrated with the proxy service. The <a class="indexterm" id="id566"/>users of our service would still be using the old release thus giving us the opportunity to test the newly deployed version before making it publicly available. We'll call them pre-integration tests. They are run by invoking the utility function <code class="literal">runBGPreIntegrationTests</code> located in the <code class="literal">workflow-util.groovy</code> script.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def runBGPreIntegrationTests(serviceName, prodIp, color) {</strong></span>
<span class="strong"><strong>    stage "Run pre-integration tests"</strong></span>
<span class="strong"><strong>    def address = getAddress(serviceName, prodIp, color)</strong></span>
<span class="strong"><strong>    try {</strong></span>
<span class="strong"><strong>        runTests(serviceName, "integ", "-e DOMAIN=http://${address}")</strong></span>
<span class="strong"><strong>    } catch(e) {</strong></span>
<span class="strong"><strong>        stopBG(serviceName, prodIp, color);</strong></span>
<span class="strong"><strong>        error("Pre-integration tests failed")</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>The function starts by retrieving the address of the newly deployed service from Consul. This retrieval is accomplished through invocation of the <code class="literal">getAddress</code> function. Please consult the details of the function by examining the <code class="literal">workflow-util.groovy</code> script. Next, we run the tests inside a <code class="literal">try…catch</code> block. Since the new release is still not integrated with nginx and, therefore, not accessible through the port <code class="literal">80</code>, we are passing the <code class="literal">address</code> of the release as an environment variable <code class="literal">DOMAIN</code>. If the execution of tests fails, the script will jump to the <code class="literal">catch</code> block and call the <code class="literal">stopBG</code> function that will stop the new release. Since our servers are running [Registrator], once the new release is stopped, its data will be removed from Consul. There's nothing else to be done. Proxy service will continue pointing to the old release, and, through it, our users will continue using the old version of our service that is proven to work correctly. Please consult the <code class="literal">workflow-util.groovy</code> script to see details of the <code class="literal">stopBG</code> function.</p><p>If the pre-integration tests passed, we are invoking the <code class="literal">updateBGProxy</code> function that updates the proxy service thus making our new release available to our users. The function is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def updateBGProxy(serviceName, proxyNode, color) {</strong></span>
<span class="strong"><strong>    stage "Update proxy"</strong></span>
<span class="strong"><strong>    stash includes: 'nginx-*', name: 'nginx'</strong></span>
<span class="strong"><strong>    node(proxyNode) {</strong></span>
<span class="strong"><strong>        unstash 'nginx'</strong></span>
<span class="strong"><strong>        sh "sudo cp nginx-includes.conf /data/nginx/includes/${serviceName}.conf"</strong></span>
<span class="strong"><strong>        sh "sudo consul-template \</strong></span>
<span class="strong"><strong>            -consul localhost:8500 \</strong></span>
<span class="strong"><strong>            -template \"nginx-upstreams-${color}.ctmpl:/data/nginx/upstreams/${serviceName}.conf:docker kill -s HUP nginx\" \</strong></span>
<span class="strong"><strong>            -once"</strong></span>
<span class="strong"><strong>        sh "curl -X PUT -d ${color} http://localhost:8500/v1/kv/${serviceName}/color"</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>The major<a class="indexterm" id="id567"/> difference, when compared with the <code class="literal">updateProxy</code> function we used in the previous chapter, is the usage of <code class="literal">nginx-upstreams-${color}.ctmpl</code> as the name of the template. Depending on the value we pass to the function, <code class="literal">nginx-upstreams-blue.ctmpl</code>, or <code class="literal">nginx-upstreams-green.ctmpl</code> will be used. As an additional instruction, we are sending a request to Consul to store the color related to the newly deployed release. The rest of this function is the same as the <code class="literal">updateProxy</code>.</p><p>Finally, now that the new release is deployed, and the proxy service has been reconfigured, we are doing another round of testing to confirm that the integration with the proxy was indeed correct. We're doing that by invoking the <code class="literal">runBGPostIntegrationTests</code> function located in the <code class="literal">workflow-util.groovy</code> script.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>def runBGPostIntegrationTests(serviceName, prodIp, proxyIp, proxyNode, currentColor, nextColor) {</strong></span>
<span class="strong"><strong>    stage "Run post-integration tests"</strong></span>
<span class="strong"><strong>    try {</strong></span>
<span class="strong"><strong>        runTests(serviceName, "integ", "-e DOMAIN=http://${proxyIp}")</strong></span>
<span class="strong"><strong>    } catch(e) {</strong></span>
<span class="strong"><strong>        if (currentColor != "") {</strong></span>
<span class="strong"><strong>            updateBGProxy(serviceName, proxyNode, currentColor)</strong></span>
<span class="strong"><strong>        }</strong></span>
<span class="strong"><strong>        stopBG(serviceName, prodIp, nextColor);</strong></span>
<span class="strong"><strong>        error("Post-integration tests failed")</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>    stopBG(serviceName, prodIp, currentColor);</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><p>We start by running integration tests that are, this time, using the public domain that points to the proxy. If tests fail, we are reverting the changes to the proxy service by invoking the <code class="literal">updateBGProxy</code> function. By passing the <code class="literal">currentColor</code> as the variable, <code class="literal">updateBGProxy</code> will reconfigure nginx to work with the old release of the service. The second instruction in case of a failure of tests is to stop the new release by invoking the <code class="literal">stopBG</code> function with <code class="literal">nextColor</code>. On the other hand, if all tests passed, we are stopping the old release.</p><p>If you are new to Groovy, this script might have been overwhelming. However, with a little bit of practice, you'll see that, for our purposes, Groovy is very simple and with the addition of Jenkins Workflow DSL, many things are made even easier.</p><p>It is worth noting that the Workflow plugin is restrictive. For security reasons, invocation of some <a class="indexterm" id="id568"/>Groovy classes and functions needs to be approved. I already did that for you as part of the provisioning and configuration process defined through the <code class="literal">jenkins.yml</code> Ansible playbook. If you'd like to see the end result or would need to make new approvals, please open <span class="strong"><strong>In-process Script Approval</strong></span> screen located inside <span class="strong"><strong>Manage Jenkins</strong></span>. At first, those security restrictions might seem over-the-top, but the reasoning behind them is essential. Since Workflow scripts can access almost any part of the Jenkins platform, letting anything run inside it might have very severe consequences. For that reason, some instructions are allowed by default while others need to be approved. If a Workflow script fails due to this restriction, you'll see a new entry in the <span class="strong"><strong>In-process Script Approval</strong></span> screen waiting for your approval (or disapproval). The XML behind those approvals is located in the <code class="literal">/data/jenkins/scri</code>
<code class="literal">ptApproval.xml</code> file.</p></div><div class="section" title="Running the blue-green deployment"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec71"/>Running the blue-green deployment</h2></div></div></div><p>Hopefully, by <a class="indexterm" id="id569"/>this time, the subproject<a class="indexterm" id="id570"/> finished running. You can monitor the process by opening blue-green subproject Console screen. Once the first run of the subproject is finished, we can manually confirm that everything run correctly. We'll use this opportunity to showcase few <code class="literal">ps</code> arguments we haven't used. The first one will be <code class="literal">--filter</code> that can be used to (you guessed it) filter containers returned with the <code class="literal">ps</code> command. The second one is <code class="literal">--format</code>. Since the standard output of the <code class="literal">ps</code> command can be very long, we'll use it to retrieve only names of the containers.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://prod:2375</strong></span>

<span class="strong"><strong>docker ps -a --filter name=books --format "table {{.Names}}"</strong></span>
</pre></div><p>The output of the <code class="literal">ps</code> command is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NAMES</strong></span>
<span class="strong"><strong>booksms_app-blue_1</strong></span>
<span class="strong"><strong>booksms_db_1</strong></span>
</pre></div><p>We can see that the <code class="literal">blue</code> release has been deployed together with the linked database. We can also confirm that the service has been stored in Consul.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl prod:8500/v1/catalog/services | jq '.'</strong></span>

<span class="strong"><strong>curl prod:8500/v1/catalog/service/books-ms-blue | jq '.'</strong></span>
</pre></div><p>The combined output of the two requests to Consul is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>  "dockerui": [],</strong></span>
<span class="strong"><strong>  "consul": [],</strong></span>
<span class="strong"><strong>  "books-ms-blue": []</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[</strong></span>
<span class="strong"><strong>  {</strong></span>
<span class="strong"><strong>    "ModifyIndex": 461,</strong></span>
<span class="strong"><strong>    "CreateIndex": 461,</strong></span>
<span class="strong"><strong>    "Node": "prod",</strong></span>
<span class="strong"><strong>    "Address": "10.100.198.201",</strong></span>
<span class="strong"><strong>    "ServiceID": "prod:booksms_app-blue_1:8080",</strong></span>
<span class="strong"><strong>    "ServiceName": "books-ms-blue",</strong></span>
<span class="strong"><strong>    "ServiceTags": [],</strong></span>
<span class="strong"><strong>    "ServiceAddress": "10.100.198.201",</strong></span>
<span class="strong"><strong>    "ServicePort": 32780,</strong></span>
<span class="strong"><strong>    "ServiceEnableTagOverride": false</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>]</strong></span>
</pre></div><p>The <code class="literal">books-ms-blue</code> has been registered as a service besides the <code class="literal">dockerui</code> and <code class="literal">consul</code>. The <a class="indexterm" id="id571"/>second output shows all the details <a class="indexterm" id="id572"/>of the service.</p><p>Finally, we should verify that the color has been stored in Consul and that the service itself is indeed integrated with nginx.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl prod:8500/v1/kv/books-ms/color?raw</strong></span>

<span class="strong"><strong>curl -I prod/api/v1/books</strong></span>
</pre></div><p>The first command returned <code class="literal">blue</code>, and the status of the request to the service through the proxy is <code class="literal">200 OK</code>. Everything seems to be working correctly.</p><p>Please run the job a couple of more times by opening the <code class="literal">books-ms-blue-green</code> job and clicking the <span class="strong"><strong>Schedule a build for blue-green</strong></span> icon located on the right-hand side.</p><p>You can monitor the process by opening the blue-green subproject Console screen.</p><div class="mediaobject"><img alt="Running the blue-green deployment" src="graphics/B05848_13_15.jpg"/><div class="caption"><p>Figure 13-15 – The Jenkins blue-green subproject Console screen</p></div></div><p>If you<a class="indexterm" id="id573"/> repeat the manual verifications, you'll <a class="indexterm" id="id574"/>notice that the second time the <code class="literal">green</code> release will be running, and the <code class="literal">blue</code> will be stopped. The third run will invert colors and the <code class="literal">blue</code> release will be running while the <code class="literal">green</code> will be stopped. The correct color will be stored in Consul, proxy service will always redirect requests to the latest release, and there will be no downtime during the deployment process.</p><p>Even though we are reaching the end of this chapter, we are not finished practicing the blue-green deployment. Even though we will change the way we are running the procedure, it will be the integral part of a couple of more practices we'll explore throughout the rest of this book. We accomplished zero-downtime deployments, but there is still a lot of work left before we reach zero-downtime system. The fact that our current process does not produce downtime during deployments does not mean that the whole system is fault tolerant.</p><p>We reached a significant milestone, yet there are still a lot of obstacles left to overcome. One of them is clustering and scaling. The solution we have works well on a single server. We <a class="indexterm" id="id575"/>could easily extend it to support<a class="indexterm" id="id576"/> a few more, maybe even ten. However, the bigger the number of our servers, the greater the need to look for a better way to manage clustering and scaling. That will be the subject of the next chapter. Until then, let us destroy the environments we've been using so that we can start fresh.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>

<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div></div></div></body></html>