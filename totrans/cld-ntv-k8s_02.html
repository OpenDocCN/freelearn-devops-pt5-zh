<html><head></head><body>
		<div><h1 id="_idParaDest-17"><em class="italic"><a id="_idTextAnchor016"/>Chapter 1</em>: Communicating with Kubernetes </h1>
			<p>This chapter contains an explanation of container orchestration, including its benefits, use cases, and popular implementations. We'll also review Kubernetes briefly, including a layout of the architectural components, and a primer on authorization, authentication, and general communication with Kubernetes. By the end of this chapter, you'll know how to authenticate and communicate with the Kubernetes API.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>A container orchestration primer </li>
				<li>Kubernetes' architecture </li>
				<li>Authentication and authorization on Kubernetes </li>
				<li>Using kubectl and YAML files </li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Technical requirements</h1>
			<p>In order to run the commands detailed in this chapter, you will need a computer running Linux, macOS, or Windows. This chapter will teach you how to install the <code>kubectl</code> command-line tool that you will use in all later chapters.</p>
			<p>The code used in this chapter can be found in the book's GitHub repository at the following link: </p>
			<p><a href="https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter1">https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter1</a></p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Introducing container orchestration</h1>
			<p>We cannot talk about Kubernetes without an introduction of its purpose. Kubernetes is a container orchestration framework, so let's review what that means in the context of this book.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>What is container orchestration?</h2>
			<p>Container orchestration <a id="_idIndexMarker000"/>is a popular pattern for running modern applications both in the cloud and the data center. By using containers – preconfigured application units with bundled dependencies – as a base, developers can run many instances of an application in parallel.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Benefits of container orchestration </h2>
			<p>There are quite a<a id="_idIndexMarker001"/> few benefits that container orchestration offers, but we will highlight the main ones. First, it allows developers to easily build <strong class="bold">high-availability</strong> applications. By having multiple instances of an application running, a container orchestration system can be configured in a way that means it will automatically replace any failed instances of the application with new ones.</p>
			<p>This can be extended to the cloud by having those multiple instances of the application spread across physical data centers, so if one data center goes down, other instances of the application will remain, and prevent downtime.</p>
			<p>Second, container orchestration allows for highly <strong class="bold">scalable</strong> applications. Since new instances of the application can be created and destroyed easily, the orchestration tool can auto-scale up and down to meet demand. Either in a cloud or data center environment, new <strong class="bold">Virtual Machines</strong> (<strong class="bold">VMs</strong>) or physical machines can be added to the orchestration tool to give it a bigger pool of compute to manage. This process can be completely automated in a cloud setting to allow for completely hands-free scaling, both at the micro and macro level.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Popular orchestration tools</h2>
			<p>There are several highly popular container orchestration tools available in the ecosystem:</p>
			<ul>
				<li><strong class="bold">Docker Swarm</strong>: Docker Swarm<a id="_idIndexMarker002"/> was created by the team behind the Docker<a id="_idIndexMarker003"/> container engine. It is easier to set up and run compared to Kubernetes, but somewhat less flexible.</li>
				<li><strong class="bold">Apache Mesos</strong>: Apache Mesos<a id="_idIndexMarker004"/> is a lower-level orchestration tool that manages<a id="_idIndexMarker005"/> compute, memory, and storage, in both data center and cloud environments. By default, Mesos does not manage containers, but Marathon – a framework that runs on top of Mesos – is a fully fledged container orchestration tool. It is even possible to run Kubernetes on top of Mesos.</li>
				<li><strong class="bold">Kubernetes</strong>: As of 2020, much of the work in container orchestration has consolidated around Kubernetes (koo-bur-net-ees), often shortened to k8s. Kubernetes is an open <a id="_idIndexMarker006"/>source container orchestration tool that was originally<a id="_idIndexMarker007"/> created by Google, with learnings from internal orchestration tools Borg and Omega, which had been in use at Google for years. Since Kubernetes became open source, it has risen in popularity to become the de facto way to run and orchestrate containers in an enterprise environment. There are a few reasons for this, including that Kubernetes is a mature product that has an extremely large open source community. It is also simpler to operate than Mesos, and more flexible than Docker Swarm.</li>
			</ul>
			<p>The most important thing to take away from this comparison is that although there are multiple relevant options for container orchestration and some are indeed better in certain ways, Kubernetes has emerged as the de facto standard. With this in mind, let's take a look at how Kubernetes works.</p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Kubernetes' architecture</h1>
			<p>Kubernetes<a id="_idIndexMarker008"/> is an orchestration tool that can run on cloud VMs, on VMs running in your data center, or on bare metal servers. In general, Kubernetes runs on a set of nodes, each of which can each be a VM or a physical machine.</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Kubernetes node types</h2>
			<p>Kubernetes nodes can <a id="_idIndexMarker009"/>be many different things – from a VM, to a bare metal host, to a Raspberry Pi. Kubernetes nodes are split into two distinct categories: first, the master nodes, which run the Kubernetes control plane applications; second, the worker nodes, which run the applications that you deploy onto Kubernetes.</p>
			<p>In general, for high availability, a production deployment of Kubernetes should have a minimum of three master nodes and three worker nodes, though most large deployments have many more workers than masters.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>The Kubernetes control plane</h2>
			<p>The Kubernetes control plane<a id="_idIndexMarker010"/> is a suite of applications and services that run on the master nodes. There are several highly specialized services at play that form the core of Kubernetes functionality. They are as follows:</p>
			<ul>
				<li><strong class="bold">kube-apiserver</strong>: This is the Kubernetes API server. This application handles instructions sent to<a id="_idIndexMarker011"/> Kubernetes. </li>
				<li><strong class="bold">kube-scheduler</strong>: This is the <a id="_idIndexMarker012"/>Kubernetes scheduler. This component handles the work of deciding which nodes to place workloads on, which can become quite complex.</li>
				<li><strong class="bold">kube-controller-manager</strong>: This is the Kubernetes controller manager. This component provides a<a id="_idIndexMarker013"/> high-level control loop that ensures that the desired configuration of the cluster and applications running on it is implemented. </li>
				<li><strong class="bold">etcd</strong>: This is a<a id="_idIndexMarker014"/> distributed key-value store that contains the cluster configuration.</li>
			</ul>
			<p>Generally, all of these components take the form of system services that run on every master node. They can be started manually if you wanted to bootstrap your cluster entirely by hand, but through the use of a cluster creation library or cloud provider-managed service such as <strong class="bold">Elastic Kubernetes Service (EKS)</strong>, this will usually be done automatically in a production setting.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>The Kubernetes API server</h2>
			<p>The Kubernetes API server is a<a id="_idIndexMarker015"/> component that accepts HTTPS requests, typically on port <code>443</code>. It presents a certificate, which can be self-signed, as well as authentication and authorization mechanisms, which we will cover later in this chapter.</p>
			<p>When a configuration request is made to the Kubernetes API server, it will check the current cluster configuration in <code>etcd</code> and change it if necessary.</p>
			<p>The Kubernetes API is generally a RESTful API, with endpoints for each Kubernetes resource type, along with an API version that is passed in the query path; for instance, <code>/api/v1</code>.</p>
			<p>For the purposes of extending Kubernetes (see <a href="B14790_13_Final_PG_ePub.xhtml#_idTextAnchor289"><em class="italic">Chapter 13</em></a>, <em class="italic">Extending Kubernetes with CRDs</em>), the API also has a set of dynamic endpoints based on API groups, which can expose the same RESTful API functionality to custom resources.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>The Kubernetes scheduler</h2>
			<p>The Kubernetes scheduler <a id="_idIndexMarker016"/>decides where instances of a workload should be run. By default, this decision is influenced by workload resource requirements and node status. You can also influence the scheduler via placement controls that are configurable in Kubernetes (see <a href="B14790_08_Final_PG_ePub.xhtml#_idTextAnchor186"><em class="italic">Chapter 8</em></a>, <em class="italic">Pod Placement Controls</em>). These controls can act on node labels, which other pods are already running on a node, and many other possibilities.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>The Kubernetes controller manager</h2>
			<p>The Kubernetes <a id="_idIndexMarker017"/>controller manager is a component that runs several controllers. Controllers run control loops that ensure that the actual state of the cluster matches that stored in the configuration. By default, these include the following:</p>
			<ul>
				<li>The node controller, which ensures that nodes are up and running</li>
				<li>The replication controller, which ensures that each workload is scaled properly</li>
				<li>The endpoints controller, which handles communication and routing configuration for each workload (see <a href="B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 5</em></a><em class="italic">, Services and Ingress – Communicating with the Outside World</em>)</li>
				<li>Service <a id="_idIndexMarker018"/>account and token controllers, which handle the creation of API access tokens and default accounts</li>
			</ul>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>etcd</h2>
			<p>etcd is a distributed<a id="_idIndexMarker019"/> key-value store that houses the configuration of the cluster in a highly available way. An <code>etcd</code> replica runs on each master node and uses the Raft consensus algorithm, which ensures that a quorum is maintained before allowing any changes to the keys or values.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>The Kubernetes worker nodes</h2>
			<p>Each<a id="_idIndexMarker020"/> Kubernetes worker node contains components that allow it to communicate with the control plane and handle networking.</p>
			<p>First, there is the <strong class="bold">kubelet</strong>, which makes sure that containers are running on the node as dictated by the cluster configuration. Second, <strong class="bold">kube-proxy</strong> provides<a id="_idIndexMarker021"/> a network proxy layer to workloads running on each node. And finally, the <strong class="bold">container runtime</strong> is used to run the <a id="_idIndexMarker022"/>workloads on each node.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>kubelet</h2>
			<p>The kubelet <a id="_idIndexMarker023"/>is an agent that runs on every node (including master nodes, though it has a different configuration in that context). Its main purpose is to receive a list of PodSpecs (more on those later) and ensure that the containers prescribed by them are running on the node. The kubelet gets these PodSpecs through a few different possible mechanisms, but the main way is by querying the Kubernetes API server. Alternately, the kubelet can be started with a file path, which it will monitor for a list of PodSpecs, an HTTP endpoint to monitor, or its own HTTP endpoint to receive requests on.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>kube-proxy</h2>
			<p>kube-proxy is a<a id="_idIndexMarker024"/> network proxy that runs on every node. Its main purpose is to do TCP, UDP, and SCTP forwarding (either via stream or round-robin) to workloads running on its node. kube-proxy supports the Kubernetes <code>Service</code> construct, which we will discuss in <a href="B14790_05_Final_PG_ePub.xhtml#_idTextAnchor127"><em class="italic">Chapter 5</em></a><em class="italic">, Services and Ingress – Communicating with the Outside World</em>.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>The container runtime</h2>
			<p>The container runtime<a id="_idIndexMarker025"/> runs on each node and is the component that actually runs your workloads. Kubernetes supports CRI-O, Docker, containerd, rktlet, and<a id="_idIndexMarker026"/> any valid <strong class="bold">Container Runtime Interface</strong> (<strong class="bold">CRI</strong>) runtime. As of Kubernetes v1.14, the RuntimeClass feature has been moved from alpha to beta and allows for workload-specific runtime selection.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Addons</h2>
			<p>In addition to the core cluster components, a typical Kubernetes<a id="_idIndexMarker027"/> installation includes addons, which are additional components that provide cluster functionality.</p>
			<p>For example, <code>Calico</code>, <code>Flannel</code>, or <code>Weave</code> provide overlay network functionality that adheres to Kubernetes' networking requirements.</p>
			<p>CoreDNS, on the other hand, is a popular addon for in-cluster DNS and service discovery. There are also tools such as Kubernetes Dashboard, which provides a GUI for viewing and interacting with your cluster.</p>
			<p>At this point, you should have a high-level idea of the major components of Kubernetes. Next, we will review how a user interacts with Kubernetes to control those components.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Authentication and authorization on Kubernetes</h1>
			<p>Namespaces are an extremely important concept in Kubernetes, and since they can affect API access as well as authorization, we'll cover them now.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Namespaces</h2>
			<p>A namespace<a id="_idIndexMarker029"/> in Kubernetes is a construct that allows you to group Kubernetes resources in your cluster. They are a method of separation with many possible uses. For instance, you could have a namespace in your cluster for each environment – dev, staging, and production.</p>
			<p>By default, Kubernetes will create the default namespace, the <code>kube-system</code> namespace, and the <code>kube-public</code> namespace. Resources created without a specified namespace will be created in the default namespace. <code>kube-system</code> contains the cluster services such as <code>etcd</code>, the scheduler, and any resource created by Kubernetes itself and not users. <code>kube-public</code> is readable by all users by default and can be used for public resources.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Users</h2>
			<p>There are two types of users<a id="_idIndexMarker030"/> in Kubernetes – regular users and service accounts.</p>
			<p>Regular users are generally managed by a service outside the cluster, whether they be private keys, usernames and passwords, or some form of user store. Service accounts however are managed by Kubernetes and restricted to specific namespaces. To create a service account, the Kubernetes API may automatically make one, or they can be made manually through calls to the Kubernetes API. </p>
			<p>There are three possible types of requests to the Kubernetes API – those associated with a regular user, those associated with a service account, and anonymous requests.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Authentication methods</h2>
			<p>In order to <a id="_idIndexMarker031"/>authenticate requests, Kubernetes provides several different options: HTTP basic authentication, client certificates, bearer tokens, and proxy-based authentication.</p>
			<p>To use HTTP authentication, the requestor sends requests with an <code>Authorization</code> header that will have the value bearer <code>"token value"</code>.</p>
			<p>In order to specify which tokens are valid, a CSV file can be provided to the API server application when it starts using the <code>--token-auth-file=filename</code> parameter. A new beta feature (as of the writing of this book), called <em class="italic">Bootstrap Tokens</em>, allows for the dynamic swapping and changing of tokens while the API server is running, without restarting it.</p>
			<p>Basic username/password <a id="_idIndexMarker032"/>authentication is also possible via the <code>Authorization</code> token, by using the header value <code>Basic base64encoded(username:password)</code>.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Kubernetes' certificate infrastructure for TLS and security</h2>
			<p>In order<a id="_idIndexMarker033"/> to use client certificates (X.509 certificates), the API server<a id="_idIndexMarker034"/> must be started using the <code>--client-ca-file=filename</code> parameter. This file needs to contain one or more <strong class="bold">Certificate Authorities</strong> (<strong class="bold">CAs</strong>) that will be<a id="_idIndexMarker035"/> used when validating certificates passed with API requests.</p>
			<p>In addition to the <code>groups</code> can <a id="_idIndexMarker036"/>be included, which we will discuss in the <em class="italic">Authorization</em> options section.</p>
			<p>For instance, you can use the following:</p>
			<pre>openssl req -new -key myuser.pem -out myusercsr.pem -subj "/CN=myuser/0=dev/0=staging"</pre>
			<p>This will create a CSR for the user <code>myuser</code> who is part of groups named <code>dev</code> and <code>staging</code>.</p>
			<p>Once the CA and CSR are created, the actual client and server certificates can be created using <code>openssl</code>, <code>easyrsa</code>, <code>cfssl</code>, or any certificate generation tool. TLS certificates for the Kubernetes API can also be created at this point.</p>
			<p>Since our aim is to get you started running workloads on Kubernetes as soon as possible, we will leave all the various possible certificate configurations out of this book – but both the Kubernetes documentation and the article <em class="italic">Kubernetes The Hard Way</em> have some great tutorials on setting up a cluster from scratch. In the majority of production settings, you will not be doing these steps manually. </p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Authorization options</h2>
			<p>Kubernetes<a id="_idIndexMarker037"/> provides several authorization methods: nodes, webhooks, RBAC, and ABAC. In this book, we will focus on RBAC and ABAC as they are the ones used most often for user authorization. If you extend your cluster with other services and/or custom features, the other authorization modes may become more important.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>RBAC</h2>
			<p><code>Role</code>, <code>ClusterRole</code>, <code>RoleBinding</code>, and <code>ClusterRoleBinding</code>. To enable RBAC mode, the API server can be started with the <code>--authorization-mode=RBAC</code> parameter.</p>
			<p><code>Role</code> and <code>ClusterRole</code> resources specify a set of permissions, but do not assign those permissions to any specific users. Permissions are specified using <code>resources</code> and <code>verbs</code>. Here is a sample YAML file specifying a <code>Role</code>. Don't worry too much about the first few lines of the YAML file – we'll get to those soon. Focus on the <code>resources</code> and <code>verbs</code> lines to see how the actions can be applied to resources:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Read-only-role.yaml</p>
			<pre>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: read-only-role
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]</pre>
			<p>The only difference between a <code>Role</code> and <code>ClusterRole</code> is that a <code>Role</code> is restricted to a particular namespace (in this case, the default namespace), while a <code>ClusterRole</code> can affect access to all resources of that type in the cluster, as well as cluster-scoped resources such as nodes.</p>
			<p><code>RoleBinding</code> and <code>ClusterRoleBinding</code> are resources that associate a <code>Role</code> or <code>ClusterRole</code> with a user<a id="_idIndexMarker040"/> or a list of users. The following file represents a <code>RoleBinding</code> resource to connect our <code>read-only-role</code> with a <a id="_idIndexMarker041"/>user, <code>readonlyuser</code>:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Read-only-rb.yaml</p>
			<pre>apiVersion: rbac.authorization.k8s.io/v1namespace.
kind: RoleBinding
metadata:
  name: read-only
  namespace: default
subjects:
- kind: User
  name: readonlyuser
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: read-only-role
  apiGroup: rbac.authorization.k8s.io</pre>
			<p>The <code>subjects</code> key contains a list of all entities to associate a role with; in this case, the user <code>alex</code>. <code>roleRef</code> contains the name of the role to associate, and the type (either <code>Role</code> or <code>ClusterRole</code>).</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>ABAC</h2>
			<p><code>--authorization-mode=ABAC</code> and <code>--authorization-policy-file=filename</code> parameters.</p>
			<p>In the policy file, each policy object contains information about a single policy: firstly, which subjects it corresponds to, which can be either users or groups, and secondly, which resources can be accessed via the policy. Additionally, a Boolean <code>readonly</code> value can be included to limit the policy to <code>list</code>, <code>get</code>, and <code>watch</code> operations.</p>
			<p>A secondary type of policy is associated not with a resource, but with types of non-resource requests, such as calls to the <code>/version</code> endpoint.</p>
			<p>When a request to the API is made in ABAC mode, the API server will check the user and any group it is a part of against the list in the policy file, and see if any policies match the resource or endpoint that the user is trying to access. On a match, the API server will authorize the request.</p>
			<p>You should have a good understanding now of how the Kubernetes API handles authentication and authorization. The good news is that while you can directly access the API, Kubernetes provides an excellent command-line tool to simply authenticate and make Kubernetes API requests.</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Using kubectl and YAML</h1>
			<p>kubectl is the <a id="_idIndexMarker044"/>officially supported command-line tool for accessing the Kubernetes API. It can be installed on Linux, macOS, or Windows.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Setting up kubectl and kubeconfig</h2>
			<p>To install the newest <a id="_idIndexMarker045"/>release of kubectl, you can use the installation instructions at <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a>.</p>
			<p>Once kubectl<a id="_idIndexMarker046"/> is installed, it needs to be set up to authenticate with one or more clusters. This is done using the <code>kubeconfig</code> file, which looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Example-kubeconfig</p>
			<pre>apiVersion: v1
kind: Config
preferences: {}
clusters:
- cluster:
    certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
users:
- name: alex
  user:
    password: mypass
    username: alex
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: development</pre>
			<p>This file is <a id="_idIndexMarker047"/>written in YAML and is very similar to other Kubernetes resource specifications that we will get to shortly – except that this file lives only on your local machine.</p>
			<p>There are three sections to a <code>Kubeconfig</code> YAML file: <code>clusters</code>, <code>users</code>, and <code>contexts</code>: </p>
			<ul>
				<li>The <code>clusters</code> section is a list of clusters that you will be able to access via kubectl, including the CA filename and server API endpoint.</li>
				<li>The <code>users</code> section lists users that you will be able to authorize with, including any user certificates or username/password combinations for authentication.</li>
				<li>Finally, the <a id="_idIndexMarker048"/><code>contexts</code> section lists combinations of a cluster, a namespace, and a user that combine to make a context. Using the <code>kubectl config use-context</code> command, you can easily switch between contexts, which allows easy switching between cluster, user, and namespace combinations. </li>
			</ul>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Imperative versus declarative commands</h2>
			<p>There are<a id="_idIndexMarker049"/> two paradigms for talking to the<a id="_idIndexMarker050"/> Kubernetes API: imperative and declarative. Imperative commands allow you to dictate to Kubernetes "what to do" – that is, "spin up two copies of Ubuntu," "scale this application to five copies," and so on.</p>
			<p>Declarative commands, on the other hand, allow you to write a file with a specification of what should be running on the cluster, and have the Kubernetes API ensure that the configuration matches the cluster configuration, updating it if necessary.</p>
			<p>Though imperative commands allow you to quickly get started with Kubernetes, it is far better to write some YAML and use a declarative configuration when running production workloads, or workloads of any complexity. The reason for this is that it makes it easier to track changes, for instance via a GitHub repo, or introduce Git-driven <strong class="bold">Continous Integration/Continuous</strong> Delivery<a id="_idIndexMarker051"/> (<strong class="bold">CI/CD</strong>) to your cluster.</p>
			<p>Some basic kubectl commands</p>
			<p>kubectl provides <a id="_idIndexMarker052"/>many convenient commands for checking the current state of your cluster, querying resources, and creating new ones. kubectl is structured so most commands can access resources in the same way.</p>
			<p>First, let's learn how to see Kubernetes resources in your cluster. You can do this by using <code>kubectl get resource_type</code> where <code>resource_type</code> is the full name of the Kubernetes resource, or alternately, a shorter alias. A full list of aliases (and <code>kubectl</code> commands) can be found in the kubectl documentation at <a href="https://kubernetes.io/docs/reference/kubectl/overview">https://kubernetes.io/docs/reference/kubectl/overview</a>.</p>
			<p>We already know about nodes, so let's start with that. To find which nodes exist in a cluster, we can use <code>kubectl get nodes</code> or the alias <code>kubectl get no</code>.</p>
			<p>kubectl's <code>get</code> commands return a list of Kubernetes resources that are currently in the cluster. We can run this command with any Kubernetes resource type. To add additional information to the list, you can add the <code>wide</code> output flag: <code>kubectl get nodes -o wide</code>.</p>
			<p>Listing resources<a id="_idIndexMarker053"/> isn't enough, of course – we need to be able to see the details of a particular resource. For this, we use the <code>describe</code> command, which works similarly to <code>get</code>, except that we can optionally pass the name of a specific resource. If this last parameter is omitted, Kubernetes will return the details of all resources of that type, which will probably result in a lot of scrolling in your terminal.</p>
			<p>For example, <code>kubectl describe nodes</code> will return details for all nodes in the cluster, while <code>kubectl describe nodes node1</code> will return a description of the node named <code>node1</code>.</p>
			<p>As you've probably noticed, these commands are all in the imperative style, which makes sense since we're just fetching information about existing resources, not creating new ones. To create a Kubernetes resource, we can use the following:</p>
			<ul>
				<li><code>kubectl create -f /path/to/file.yaml</code>, which is an imperative command</li>
				<li><code>kubectl apply -f /path/to/file.yaml</code>, which is declarative</li>
			</ul>
			<p>Both commands take a path to a file, which can be either YAML or JSON – or you can just use <code>stdin</code>. You can also pass in the path to a folder instead of a file, which will create or apply all YAML or JSON files in that folder. <code>create</code> works imperatively, so it will create a new resource, but if you run it again with the same file, the command will fail since the resource already exists. <code>apply</code> works declaratively, so if you run it the first time it will create the resource, and subsequent runs will update the running resource in Kubernetes with any changes. You can use the <code>--dry-run</code> flag to see the output of the <code>create</code> or <code>apply</code> commands (that is, what resources will be created, or any errors if they exist).</p>
			<p>To update existing resources imperatively, use the <code>edit</code> command like so: <code>kubectl edit resource_type resource_name</code> – just like with our <code>describe</code> command. This will open up the default terminal editor with the YAML of the existing resource, regardless of whether you created it imperatively or declaratively. You can edit this and save as usual, which will trigger an automatic update of the resource in Kubernetes.</p>
			<p>To update existing <a id="_idIndexMarker054"/>resources declaratively, you can edit your local YAML resource file that you used to create the resource in the first place, then run <code>kubectl apply -f /path/to/file.yaml</code>. Deleting resources is best accomplished via the imperative command <code>kubectl delete resource_type resource_name</code>.</p>
			<p>The last command we'll talk about in this section is <code>kubectl cluster-info</code>, which will show the IP addresses where the major Kubernetes cluster services are running.</p>
			<h2 id="_idParaDest-46">Writing<a id="_idTextAnchor045"/> Kubernetes resource YAML files</h2>
			<p>For communicating with <a id="_idIndexMarker055"/>the Kubernetes API declaratively, formats of both YAML and JSON are allowed. For the purposes of this book, we will stick to YAML since it is a bit cleaner and takes up less space on the page. A typical Kubernetes resource YAML file looks like this: </p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">resource.yaml</p>
			<pre>apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: ubuntu
    image: ubuntu:trusty
    command: ["echo"]
    args: ["Hello Readers"]</pre>
			<p>A valid Kubernetes YAML file has four top-level keys at a minimum. They are <code>apiVersion</code>, <code>kind</code>, <code>metadata</code>, and <code>spec</code>.</p>
			<p><code>apiVersion</code> dictates which version of the Kubernetes API will be used to create the resource. <code>kind</code> specifies what type of resource the YAML file is referencing. <code>metadata</code> provides a location to <a id="_idIndexMarker056"/>name the resource, as well as adding annotations and name-spacing information (more on that later). And finally, the <code>spec</code> key will contain all the resource-specific information that Kubernetes needs to create the resource in your cluster.</p>
			<p>Don't worry about <code>kind</code> and <code>spec</code> quite yet – we'll get to what a <code>Pod</code> is in <a href="B14790_03_Final_PG_ePub.xhtml#_idTextAnchor091"><em class="italic">Chapter 3</em></a>, <em class="italic">Running Application Containers on Kubernetes</em>.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Summary</h1>
			<p>In this chapter, we learned the background behind container orchestration, an architectural overview of a Kubernetes cluster, how a cluster authenticates and authorizes API calls, and how to communicate with the API via imperative and declarative patterns using kubectl, the officially supported command-line tool for Kubernetes.</p>
			<p>In the next chapter, we'll learn several ways to get started with a test cluster, and master harnessing the kubectl commands you've learned so far.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Questions</h1>
			<ol>
				<li>What is container orchestration?</li>
				<li>What are the constituent parts of the Kubernetes control plane, and what do they do?</li>
				<li>How would you start the Kubernetes API server in ABAC authorization mode?</li>
				<li>Why is it important to have more than one master node for a production Kubernetes cluster?</li>
				<li>What is the difference between <code>kubectl apply</code> and <code>kubectl create</code>?</li>
				<li>How would you switch between contexts using <code>kubectl</code>?</li>
				<li>What are the downsides of creating a Kubernetes resource declaratively and then editing it imperatively?</li>
			</ol>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Further reading</h1>
			<ul>
				<li>The official Kubernetes documentation: <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
				<li><em class="italic">Kubernetes The Hard Way</em>: <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a></li>
			</ul>
		</div>
	</body></html>