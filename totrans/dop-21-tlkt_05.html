<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Continuous Delivery and Deployment with Docker Containers</h1>
            </header>

            <article>
                
<div class="packt_quote">In software, when something is painful, the way to reduce the pain is to do it more frequently, not less.<br/>
                                                                                                            – David Farley</div>
<p>At the time, we could not convert the <strong>Continuous Integration</strong> (<strong>CI</strong>) into the <strong>Continuous Delivery</strong> (<strong>CD</strong>) process because we were missing some essential knowledge. Now that we understand the basic principles and commands behind Docker Swarm, we can go back to the end of the <a href="9730e69c-a698-40c2-91ce-b92e48328a93.xhtml">Chapter 1</a>, <em>Continuous Integration with Docker Containers</em>. We can define the steps that will let us perform the full CD process.</p>
<p>I won't go into Continuous Delivery details. Instead, I'll pitch it as a single sentence. <em>Continuous Delivery is a process applied to every commit in a code repository and results in every successful build being ready for deployment to production.</em></p>
<p>CD means that anyone, at any time, can click a button, and deploy a build to production without the fear that something will go wrong. It means that the process is so robust that we have full confidence that "almost" any problem will be detected before the deployment to production. Needless to say, CD is an entirely automated process. There is no human involvement from the moment a commit is sent to a code repository, all the way until a build is ready to be deployed to production. The only manual action is that someone needs to press the button that will run a script that performs the deployment.<br/>
<strong>Continuous Deployment</strong> (<strong>CDP</strong>) is one step forward. It is Continuous Delivery without the button. <em>Continuous Deployment is a process applied to every commit in a code repository and results with every successful build being Deployed to production.</em></p>
<p>No matter which process you choose, the steps are the same. The only difference is whether there is a button that deploys the release to production.</p>
<p>At this point, it is safe to assume that we'll use Docker whenever convenient and that we'll use Swarm clusters to run the services in production and production-like environments.</p>
<p>Let's start by specifying the steps that could define one possible implementation of the CD/CDP process:</p>
<ol>
<li>Check out the code.</li>
<li>Run unit tests.</li>
<li>Build binaries and other required artifacts.</li>
<li>Deploy the service to the staging environment.</li>
<li>Run functional tests.</li>
<li>Deploy the service to the production-like environment.</li>
<li>Run production readiness tests.</li>
<li>Deploy the service to the production environment.</li>
<li>Run production readiness tests.</li>
</ol>
<p>Now, let's get going and set up the environment we'll need for practicing the CD flow.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Defining the Continuous Delivery environment</h1>
            </header>

            <article>
                
<p>A minimum requirement for a Continuous Delivery environment is two clusters. One should be dedicated to running tests, building artifacts and images, and all other CD tasks. We can use it for simulating a production cluster. The second cluster will be used for deployments to production.</p>
<p>Why do we need two clusters? Can't we accomplish the same with only one?</p>
<p>While we certainly could get away with only one cluster, having two will simplify quite a few processes and, more importantly, provide better isolation between production and non-production services and tasks.<br/>
The more we minimize the impact on the production cluster, the better. By not running non-production services and tasks inside the production cluster, we are reducing the risk. Therefore, we should have a production cluster separated from the rest of the environment.</p>
<p>Now let's get started and fire up those clusters.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up Continuous Delivery clusters</h1>
            </header>

            <article>
                
<p>What is a minimum number of servers required for a production-like cluster? I'd say two. If there's only one server, we would not be able to test whether networking and volumes work across nodes. So, it has to be plural. On the other hand, I don't want to push your laptop too much so we'll avoid increasing the number unless necessary.</p>
<p>For the production-like cluster, two nodes should be enough. We should add one more node that we'll use for running tests and building images. The production cluster should probably be a bit bigger since it will have more services running. We'll make it three nodes big. If needed, we can increase the capacity later on. As you already saw, adding nodes to a Swarm cluster is very easy.</p>
<p>By now, we set up a Swarm cluster quite a few times so we'll skip the explanation and just do it through a script.</p>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>05-continuous-delivery.sh</kbd> (<a href="https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe">https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe</a>) Gist.</div>
<p>Let's go back to the cloud-provisioning directory we created in the previous chapter and run the <kbd>scripts/dm-swarm.sh</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh">https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh</a>) script. It will create the production nodes and join them into a cluster. The nodes will be called <kbd>swarm-1</kbd>, <kbd>swarm-2</kbd>, and <kbd>swarm-3</kbd>:</p>
<pre>
<strong><span class="hljs-built_in">cd</span> cloud-provisioning<br/><br/>scripts/dm-swarm.sh<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker node ls</strong>
</pre>
<p>The output of the <kbd><strong>node ls</strong></kbd> command is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS<br/>swarm-<span class="hljs-number">2</span>   Ready   <span class="hljs-keyword">Active</span>        Reachable<br/>swarm-<span class="hljs-number">1</span>   Ready   <span class="hljs-keyword">Active</span>        Leader<br/>swarm-<span class="hljs-number">3</span>   Ready   <span class="hljs-keyword">Active</span>        Reachable</strong>
</pre>
<p>Next, we'll create the second cluster. We'll use it for running CD tasks as well as a simulation of a production environment. Three nodes should be enough, for now. We'll call them <kbd>swarm-test-1, swarm-test-2,</kbd> and <kbd>swarm-test-3.</kbd></p>
<p>We'll create the cluster by executing the <kbd>scripts/dm-test-swarm.sh</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh">https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh</a>) script:</p>
<pre>
<strong>scripts/dm-test-swarm.sh<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-test-<span class="hljs-number">1</span>)<br/><br/>docker node ls</strong>
</pre>
<p>The output of the <kbd>node ls</kbd> command is as follows (IDs were removed for brevity):</p>
<pre>
<strong>HOSTNAME      STATUS  AVAILABILITY  MANAGER STATUS<br/>swarm-test-<span class="hljs-number">2</span>  Ready   <span class="hljs-keyword">Active</span>        Reachable<br/>swarm-test-<span class="hljs-number">1</span>  Ready   <span class="hljs-keyword">Active</span>        Leader<br/>swarm-test-<span class="hljs-number">3</span>  Ready   <span class="hljs-keyword">Active</span>        Reachable</strong>
</pre>
<p>The only thing left, for now, is to create Docker registry services. We'll create one in each cluster. That way, there will be no direct relation between them, and they will be able to operate independently one from another. For registries running on separate clusters to share the same data, we'll mount the same host volume to both services. That way, an image pushed from one cluster will be available from the other, and vice versa. Please note that the volumes we're creating are still a workaround. Later on, we'll explore better ways to mount volumes.</p>
<p>Let's start with the production cluster.</p>
<p>We've already run the registry in the <a href="44df5a4c-1e47-4de0-9442-660034287e66.xhtml">Chapter 1</a>, <em>Continuous Integration with Docker Containers</em> . Back then, we had a single node, and we used Docker Compose to deploy services. Registry was not an option.</p>
<div class="packt_tip"><strong>A note to Windows users<br/></strong> Git Bash has a habit of altering file system paths. To stop this, execute the following before running the code block: <br/>
<kbd>export MSYS_NO_PATHCONV=1</kbd></div>
<p>This time, we'll run the registry as a Swarm service:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker service create --name registry \<br/>    -p <span class="hljs-number">5000</span>:<span class="hljs-number">5000</span> \<br/>    --reserve-memory <span class="hljs-number">100</span>m \<br/>    --mount <span class="hljs-string">"type=bind,source=<span class="hljs-variable">$PWD</span>,target=/var/lib/registry"</span> \<br/>    registry:<span class="hljs-number">2.5</span>.<span class="hljs-number">0</span></strong>
</pre>
<p>We exposed port <kbd>5000</kbd> and reserved <kbd>100</kbd> MB of memory. We used the <kbd>--mount</kbd> argument to expose a volume. This argument is, somewhat, similar to the Docker Engine argument <kbd>--volume</kbd> or the volumes argument in Docker compose files. The only significant difference is in the format. In this case, we specified that the current host directory <kbd>source=$PWD</kbd> should be mounted inside the container <kbd>target=/var/lib/registry</kbd>.</p>
<p>Please note that, from now on, we'll always run specific versions. While, until now, the latest was alright as a demonstration, now we're trying to simulate CD processes we'll run in "real" clusters. We should always be explicit which version of a service we want to run. That way, we can be sure that the same service is tested and deployed to production. Otherwise, we could run into a situation where one version was deployed and tested in a production-like environment, but a different one was deployed to production.</p>
<p>The benefits behind specific versions are even more apparent when we use images from Docker Hub. For example, if we just run the latest release of the registry, there is no guarantee that, later on, when we run it in the second cluster, the latest release will not be updated. We could, easily, end up getting different releases of the registry in different clusters. That could lead to some really hard-to-detect bugs.</p>
<p>I won't bother you more with versioning. I'm sure you know what's it for and when to use it.<br/>
<br/>
Let's get back to the <kbd>registry</kbd> service. We should create it inside the second cluster as well:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-test-<span class="hljs-number">1</span>)<br/><br/>docker service create --name registry \<br/>    -p <span class="hljs-number">5000</span>:<span class="hljs-number">5000</span> \<br/>    --reserve-memory <span class="hljs-number">100</span>m \<br/>    --mount <span class="hljs-string">"type=bind,source=<span class="hljs-variable">$PWD</span>,target=/var/lib/registry"</span> \<br/>    registry:<span class="hljs-number">2.5</span>.<span class="hljs-number">0</span></strong>
</pre>
<p>Now we have the <kbd>registry</kbd> service running inside both clusters.</p>
<div class="CDPAlignCenter CDPAlign"> <img class="image-border" height="173" src="assets/cd-environment-registry-1.png" width="505"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-1: CD and production clusters with the registry service</div>
<p>At the moment, we do not know in which servers the registries are running. All we know is that there is an instance of the service in each cluster. Normally, we'd have to configure Docker Engine to treat the registry service as insecure and allow the traffic. To do that, we'd need to know the IP of the server the registry is running in. However, since we ran it as a Swarm service and exposed port <kbd>5000</kbd>, the routing mesh will make sure that the port is open in every node of the cluster and forward requests to the service. That allows us to treat the registry as localhost. We can pull and push images from any node as if the registry is running in each of them. Moreover, Docker Engines default behavior is to allow only localhost traffic to the registry. That means that we do not need to change its configuration.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using node labels to constrain services</h1>
            </header>

            <article>
                
<p>Labels are defined as key-value sets. We'll use the key <kbd>env</kbd> (short for environment). At the moment, we don't need to label the nodes used for CD tasks since we are not yet running them as services. We'll change that in one of the chapters that follow. For now, we only need to label the nodes that will be used to run our services in the production-like environment.</p>
<p>We'll use the nodes <kbd>swarm-test-2</kbd> and <kbd>swarm-test-3</kbd> as our production-like environment so we'll label them with the key <kbd>env</kbd> and the value <kbd>prod-like</kbd>.</p>
<p>Let's start with the node <kbd>swarm-test-2</kbd>:</p>
<pre>
<strong>docker node update \<br/>    --label-add env=prod-like \<br/>    swarm-test-<span class="hljs-number">2</span></strong>
</pre>
<p>We can confirm that the label was indeed added by inspecting the node:</p>
<pre>
<strong>docker node inspect --pretty swarm-test-<span class="hljs-number">2</span></strong>
</pre>
<p>The output of the node <kbd>inspect</kbd> command is as follows:</p>
<pre>
<strong><span class="hljs-label">ID:</span>                vq5hj3lt7dskh54mr1jw4zunb<br/><span class="hljs-label">Labels:</span><br/> - env = prod-like<br/><span class="hljs-label">Hostname:</span>          swarm-test-<span class="hljs-number">2</span><br/>Joined at:         <span class="hljs-number">2017</span>-<span class="hljs-number">01</span>-<span class="hljs-number">21</span> <span class="hljs-number">23</span>:<span class="hljs-number">01</span>:<span class="hljs-number">40.557959238</span> +<span class="hljs-number">0000</span> utc<br/><span class="hljs-label">Status:</span><br/> State:            Ready<br/> Availability:     Active<br/> Address:          <span class="hljs-number">192.168</span><span class="hljs-number">.99</span><span class="hljs-number">.104</span><br/>Manager Status:<br/> Address:          <span class="hljs-number">192.168</span><span class="hljs-number">.99</span><span class="hljs-number">.104</span>:<span class="hljs-number">2377</span><br/> Raft Status:      Reachable<br/> Leader:           No<br/><span class="hljs-label">Platform:</span><br/> Operating System: linux<br/> Architecture:     x86_64<br/><span class="hljs-label">Resources:</span><br/> CPUs:             <span class="hljs-number">1</span><br/> Memory:           <span class="hljs-number">492.5</span> MiB<br/><span class="hljs-label">Plugins:</span><br/> Network:         bridge, host, macvlan, null, overlay<br/> Volume:          local<br/>Engine Version:    <span class="hljs-number">1.13</span><span class="hljs-number">.0</span><br/>Engine Labels:<br/> - provider = virtualbox</strong>
</pre>
<p>As you can see, one of the labels is <kbd>env</kbd> with the value <kbd>prod-like</kbd>.</p>
<p>Let's add the same label to the second node:</p>
<pre>
<strong>docker node update \<br/>    --label-add env=prod-like \<br/>    swarm-test-<span class="hljs-number">3</span></strong>
</pre>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" src="assets/cd-environment-labels.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-2: CD cluster with labeled nodes</div>
<p>Now that we have a few nodes labeled as production-like, we can create services that will run only on those servers.</p>
<p>Let's create a service with the <kbd>alpine</kbd> image and constrain it to one of the <kbd>prod-like</kbd> nodes:</p>
<pre>
<strong>docker service create --name util \<br/>    --constraint <span class="hljs-string">'node.labels.env == prod-like'</span> \<br/>    alpine sleep </strong><span class="hljs-number"><strong>10000000</strong>00</span>
</pre>
<p>We can list the processes of the <kbd>util</kbd> service and confirm that it is running on one of the <kbd>prod-like</kbd> nodes:</p>
<pre>
<strong>docker service ps util</strong>
</pre>
<p>The output of the <kbd>service ps</kbd> command is as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME    IMAGE   NODE          DESIRED STATE  CURRENT STATE               <br/>util<span class="hljs-number">.1</span>  alpine  swarm-test-<span class="hljs-number">2</span>  Running        Running about a <span class="hljs-built_in">minute</span> ago</strong>
</pre>
<p>As you can see, the service is running inside the <kbd>swarm-test-2</kbd> node which is labeled as <kbd>env=prod-like</kbd>.</p>
<p>That, in itself, does not prove that labels work. After all, two out of three nodes are labeled as production-like, so there was a 66% chance that the service would run on one of them if labels did not work. So, let's spice it up a bit.</p>
<p>We'll increase the number of instances to six:</p>
<pre>
<strong>docker service scale util=<span class="hljs-number">6</span></strong>
</pre>
<p>Let us take a look at the <kbd>util</kbd> processes:</p>
<pre>
<strong>docker service ps util</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME    IMAGE   NODE          DESIRED STATE  CURRENT STATE           <br/>util<span class="hljs-number">.1</span>  alpine  swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">2</span>  Running        Running <span class="hljs-number">15</span> minutes ago<br/>util<span class="hljs-number">.2</span>  alpine  swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">2</span>  Running        Running <span class="hljs-number">21</span> seconds ago<br/>util<span class="hljs-number">.3</span>  alpine  swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">3</span>  Running        Running <span class="hljs-number">21</span> seconds ago<br/>util<span class="hljs-number">.4</span>  alpine  swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">3</span>  Running        Running <span class="hljs-number">21</span> seconds ago<br/>util<span class="hljs-number">.5</span>  alpine  swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">2</span>  Running        Running <span class="hljs-number">21</span> seconds ago<br/>util<span class="hljs-number">.6</span>  alpine  swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">3</span>  Running        Running <span class="hljs-number">21</span> seconds ago</strong>
</pre>
<p>As you can see, all six instances are running on nodes labeled <kbd>env=prod-like</kbd> (nodes <kbd>swarm-test-2</kbd> and <kbd>swarm-test-3</kbd>).</p>
<p>We can observe a similar result if we would run the service in the global mode:</p>
<pre>
<strong>docker service create --name util-<span class="hljs-number">2</span> \<br/>    --mode global \<br/>    --constraint <span class="hljs-string">'node.labels.env == prod-like'</span> \<br/>    alpine sleep <span class="hljs-number">1000000000</span></strong>
</pre>
<p>Let's take a look at the <kbd>util-2</kbd> processes:</p>
<pre>
<strong>docker service ps util-<span class="hljs-number">2</span></strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong><span class="hljs-tag">NAME</span>      <span class="hljs-tag">IMAGE</span>         <span class="hljs-tag">NODE</span>         <span class="hljs-tag">DESIRED</span> <span class="hljs-tag">STATE</span> <span class="hljs-tag">CURRENT</span> <span class="hljs-tag">STATE</span>         <br/><span class="hljs-tag">util-2</span>... <span class="hljs-tag">alpine</span><span class="hljs-pseudo">:latest</span> <span class="hljs-tag">swarm-test-3</span> <span class="hljs-tag">Running</span>       <span class="hljs-tag">Running</span> 3 <span class="hljs-tag">seconds</span> <span class="hljs-tag">ago</span><br/><span class="hljs-tag">util-2</span>... <span class="hljs-tag">alpine</span><span class="hljs-pseudo">:latest</span> <span class="hljs-tag">swarm-test-2</span> <span class="hljs-tag">Running</span>       <span class="hljs-tag">Running</span> 2 <span class="hljs-tag">seconds</span> <span class="hljs-tag">ago</span></strong>
</pre>
<p>Since we told Docker that we want the service to be global, the desired state is <kbd>Running</kbd> on all nodes. However, since we specified the constraint <kbd>node.labels.env == prod-like</kbd>, replicas are running only on the nodes that match it. In other words, the service is running only on nodes <kbd>swarm-test-2</kbd> and <kbd>swarm-test-3</kbd>. If we would add the label to the node <kbd>swarm-test-1</kbd>, Swarm would run the service on that node as well.</p>
<p>Before we move on, let's remove the <kbd>util</kbd> services:</p>
<pre>
<strong>docker service rm util util-<span class="hljs-number">2</span></strong>
</pre>
<p>Now that we know how to constrain services to particular nodes, we must create a service before proceeding with the Continuous Delivery steps.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating services</h1>
            </header>

            <article>
                
<p>Before we continue exploring the Continuous Delivery steps, we should discuss a deployment change introduced with Docker Swarm. We thought that each release means a new deployment. That is not true with Docker Swarm. Instead of deploying each release, we are now updating services. After building Docker images, all we have to do is update the service that is already running. In most cases, all there is to do is to run the <kbd>docker service update --image &lt;IMAGE&gt; &lt;SERVICE_NAME&gt;</kbd> command. The service already has all the information it needs and all we have to do is to change the image to the new release.</p>
<p>For service update to work, we need to have a service. We need to create it and make sure that it has all the information it needs. In other words, we create a service once and update it with each release. That greatly simplifies the release process.</p>
<p>Since a service is created only once, the <strong>Return On Investment</strong> (<strong>ROI</strong>) is too low for us to automate this step. Remember, we want to automate processes that are done many times. Things that are done once and never again are not worth automating. One of those things is the creation of services. We are still running all the commands manually so consider this as a note for the next chapter that will automate the whole process.<br/>
<br/>
Let us create the services that form the <em>go-demo</em> application. We'll need the <kbd>proxy</kbd>, the <kbd>go-demo</kbd> service, and the accompanying database. As before, we'll have to create the <kbd>go-demo</kbd> and the <kbd>proxy</kbd> networks. Since we already did that a couple of times, we'll run all the commands through the <kbd>scripts/dm-test-swarm-services.sh</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh">https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh</a>) script. It creates the services in almost the same way as before. The only difference is that it uses the <kbd>prod-like</kbd> label to restrict services only to the nodes that should be utilized for production-like deployments.</p>
<pre>
<strong>scripts/dm-test-swarm-services.sh<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-test-<span class="hljs-number">1</span>)<br/><br/>docker service ls</strong>
</pre>
<p>The output of the <kbd>service ls</kbd> command is as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME       MODE       REPLICAS IMAGE<br/>proxy      replicated <span class="hljs-number">2</span>/<span class="hljs-number">2</span>      vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-proxy</span>:latest<br/>go<span class="hljs-attribute">-demo</span>    replicated <span class="hljs-number">2</span>/<span class="hljs-number">2</span>      vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span><br/>go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-db</span> replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      mongo:<span class="hljs-number">3.2</span><span class="hljs-number">.10</span><br/>registry   replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      registry:<span class="hljs-number">2.5</span><span class="hljs-number">.0</span></strong>
</pre>
<p><img class="image-border" src="assets/cd-environment-prod-like-services.png"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 5-3: CD cluster with services running in nodes labeled as prod-like</div>
<p>Please note that the proxy reconfiguration port was set to <kbd>8090</kbd> on the localhost. We had to differentiate it from the port <kbd>8080</kbd> that we'll use when running the <kbd>go-demo</kbd> service in the staging environment.<br/>
<br/>
On one hand, we want the services in the production-like cluster to resemble those in the production cluster. On the other hand, we do not want to waste resources in replication of the full production environment. For that reason, we are running two instances (replicas) of the <kbd>proxy</kbd> and <kbd>go-demo</kbd> services. Running only one would deviate too much from the idea that services should be scaled in production. Two of each gives us the ability to test that scaled services work as expected. Even if we run many more instances in production, two is just enough to replicate scaled behavior. Since we still did not manage to set up database replication, MongoDB is, for now, running as only one instance.</p>
<p>We can confirm that all services were indeed created and integrated successfully by sending a request to <kbd>go-demo</kbd>:</p>
<pre>
<strong>curl -i <span class="hljs-string">"<span class="hljs-variable">$(docker-machine ip swarm-test-1)</span>/demo/hello"</span></strong>
</pre>
<p>We'll also create the same services in the production cluster. The only difference will be in the number of replicas (we'll have more) and that we won't constrain them. Since there is no significant difference from what we did before, we'll use <kbd>scripts/dm-swarm-services.sh</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh">https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh</a>) script to speed up the process:</p>
<pre>
<strong>scripts/dm-swarm-services.sh<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker service ls</strong>
</pre>
<p>The output of the <kbd>service ls</kbd> is as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME       MODE       REPLICAS IMAGE<br/>go<span class="hljs-attribute">-demo</span><span class="hljs-attribute">-db</span> replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      mongo:<span class="hljs-number">3.2</span><span class="hljs-number">.10</span><br/>go<span class="hljs-attribute">-demo</span>    replicated <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span><br/>registry   replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      registry:<span class="hljs-number">2.5</span><span class="hljs-number">.0</span><br/>proxy      replicated <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-proxy</span>:latest</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="183" src="assets/cd-environment-services.png" width="518"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-4: CD and production clusters with services</div>
<p>Now that we have the services created in both clusters, we can start working on the Continuous Delivery steps.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Walking through Continuous Delivery steps</h1>
            </header>

            <article>
                
<p>We already know all the steps required for the Continuous Delivery process. We did each of them at least once. We got introduced to some of them in the <a href="44df5a4c-1e47-4de0-9442-660034287e66.xhtml">Chapter 1</a>, <em>Continuous Integration with Docker Containers</em>. After all, Continuous Delivery is Continuous Integration "extended". It's what Continuous Integration would be if it would have a clear objective.</p>
<p>We ran the rest of the steps throughout the chapters that lead to this point. We know how to create, and, more importantly, update a service inside a Swarm cluster. Therefore, I won't go into many details. Consider this sub-chapter a refreshment of everything we did by now.</p>
<p>We'll start by checking out the code of a service we want to move through the CD flow:</p>
<pre>
<strong>git clone https://github.com/vfarcic/go-demo.git<br/><br/><span class="hljs-built_in">cd</span> go-demo</strong>
</pre>
<p>Next, we should run the <kbd>unit</kbd> tests and compile the service binary:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-test-<span class="hljs-number">1</span>)<br/><br/>docker-compose \<br/><span class="hljs-operator">    -f</span> docker-compose-test-local.yml \<br/>    run --rm unit</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="170" src="assets/cd-environment-unit.png" width="407"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-5: Unit tests run inside the swarm-test-1 node</div>
<p>Please note that we used the <kbd>swarm-test-1</kbd> node. Even though it belongs to the Swarm cluster, we used it in the "traditional" mode.</p>
<p>With the binary compiled, we can build Docker images:</p>
<pre>
<strong>docker-compose \<br/><span class="hljs-operator">    -f</span> docker-compose-test-local.yml \<br/>    build app</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="168" src="assets/cd-environment-build.png" width="402"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-6: Build run inside the swarm-test-1 node</div>
<p>With the image built, we can run <kbd>staging</kbd> dependencies, functional tests, and tear-down everything once we're done:</p>
<pre>
<strong>docker-compose \<br/><span class="hljs-operator">    -f</span> docker-compose-test-local.yml \<br/>    up <span class="hljs-operator">-d</span> staging-dep<br/><br/>docker-compose \<br/><span class="hljs-operator">    -f</span> docker-compose-test-local.yml \<br/>    run --rm staging<br/><br/>docker-compose \<br/><span class="hljs-operator">    -f</span> docker-compose-test-local.yml \<br/>    down</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="214" src="assets/cd-environment-staging.png" width="511"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-7: Staging or functional tests run inside the swarm-test-1 node</div>
<p>Now that we are confident that the new release is likely to work as expected, we can push the result to the registry:</p>
<pre>
<strong>docker tag go-demo localhost:<span class="hljs-number">5000</span>/go-demo:<span class="hljs-number">1.1</span><br/><br/>docker push localhost:<span class="hljs-number">5000</span>/go-demo:<span class="hljs-number">1.1</span></strong>
</pre>
<p>We ran the unit tests, built the binary, built the images, ran functional tests, and pushed the images to the registry. The release is very likely to work as expected. However, the only true validation is whether the release works correctly in production. There is no more reliable or worthy criteria than that. On the other hand, we want to reach production with as much confidence as we can. We'll balance those two needs by using the <kbd>swarm-test</kbd> cluster that is as close to production as we can reasonably get.</p>
<p>Right now, the <kbd>go-demo</kbd> service is running the release 1.0 inside the <kbd>swarm-test</kbd> cluster. We can confirm that by observing the output of the <kbd>service ps</kbd> command:</p>
<pre>
<strong>docker service ps go-demo <span class="hljs-operator">-f</span> desired-state=running</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME      IMAGE               NODE         DESIRED STATE              <br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.1</span> vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span> swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">2</span> Running       <br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.2</span> vfarcic/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.0</span> swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">3</span> Running       <br/>------------------------------------<br/>CURRENT STATE<br/>Running about an hour ago<br/>Running about an hour ago<br/></strong>
</pre>
<p>Let's update the currently running release to the version we just built that is 1.1:</p>
<pre>
<strong>docker service update \<br/>    --image=localhost:<span class="hljs-number">5000</span>/go-demo:<span class="hljs-number">1.1</span> \<br/>    go-demo<br/><br/>docker service ps go-demo <span class="hljs-operator">-f</span> desired-state=running</strong>
</pre>
<p>Please note that the service was initially created with the <kbd>--update-delay 5s</kbd> argument. That means that each update will last for five seconds on each replica set (plus a few moments to pull the image and initialize containers).</p>
<p>After a few moments (approximately 6 seconds), the output of the <kbd>service ps</kbd> command should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME      IMAGE                      NODE         DESIRED STATE <br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.1</span> localhost:<span class="hljs-number">5000</span>/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.1</span> swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">3</span> Running       <br/>go<span class="hljs-attribute">-demo</span><span class="hljs-number">.2</span> localhost:<span class="hljs-number">5000</span>/go<span class="hljs-attribute">-demo</span>:<span class="hljs-number">1.1</span> swarm<span class="hljs-attribute">-test</span><span class="hljs-subst">-</span><span class="hljs-number">2</span> Running <br/>-----------------------------------<br/>CURRENT STATE ERROR PORTS<br/>Running <span class="hljs-number">8</span> seconds ago<br/>Running <span class="hljs-number">2</span> seconds ago</strong>
</pre>
<p>If the output on your laptop is different, please wait for a few moments and repeat the <kbd>service ps</kbd> command.</p>
<p>As you can see, the image changed to <kbd>localhost:5000/go-demo:1.1</kbd> indicating that the new release is indeed up and running.</p>
<p>Please note that, since the service was created with the <kbd>--constraint 'node.labels.env == prod-like'</kbd> argument, new releases are still running only in the nodes marked as <kbd>prod-like</kbd>. That shows one of the big advantages Docker Swarm provides. We create a service with all the arguments that define its complete behavior. From there on, all we have to do is update the image with each release. Things will get more complicated later on when we start scaling and doing a few other operations. However, the logic is still essentially the same. Most of the arguments we need are defined only once through the service creation command.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="179" src="assets/cd-environment-prod-like-update.png" width="428"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-8: The service is updated inside the prod-like nodes in the CD cluster</div>
<p>Now we are ready to run some production tests. We are still not confident enough to run them against the production environment. First, we want to see whether they will pass when executed against the production-like cluster.</p>
<p>We'll run production tests like other types we've already run. Our Docker client is still pointing to the <kbd>swarm-test-1</kbd> node so anything we run with Docker Compose will continue being executed inside that server.</p>
<p>Let's take a quick look the definition of the production service inside the <kbd>docker-compose-test-local.yml</kbd> (<a href="https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml">https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml</a>) file:</p>
<pre>
<strong>production:<br/>  extends:<br/>    service: unit<br/>  environment:<br/>    -<span class="ruby"> <span class="hljs-constant">HOST_IP</span>=<span class="hljs-variable">${</span><span class="hljs-constant">HOST_IP</span>}<br/></span>  network_mode: host<br/>  command: bash -c "go get -d -v -t &amp;&amp; go test --tags integration -v"</strong>
</pre>
<p>The <kbd>production</kbd> service <kbd>extends</kbd> the <kbd>unit</kbd> service. That means that it inherits all the properties of the <kbd>unit</kbd> service, allowing us to avoid repeating ourselves.</p>
<p>Further on, we are adding the environment variable <kbd>HOST_IP.</kbd> The tests we are about to run will use that variable to deduce the address of the service under test <kbd>go-demo</kbd>.</p>
<p>Finally, we are overwriting the command used in the <kbd>unit</kbd> service. The new command downloads <em>go</em> dependencies <kbd>go get -d -v -t</kbd> and executes all the tests tagged as integration <kbd>go test --tags integration -v</kbd>.</p>
<p>Let's see whether the service indeed works inside the <kbd>swarm-test</kbd> cluster:</p>
<pre>
<strong><span class="hljs-keyword">export</span> HOST_IP=localhost<br/><br/>docker-compose \<br/><span class="hljs-operator">    -f</span> docker-compose-test-local.yml \<br/>    run --rm production</strong>
</pre>
<p>We specified that the IP of the service under test is localhost. Since the node where tests are running <kbd>swarm-test-1</kbd> belongs to the cluster, the ingress network will forward the request to the <kbd>proxy</kbd> service which, in turn, will forward it to the <kbd>go-demo</kbd> service.</p>
<p>The last lines of the output are as follows:</p>
<pre>
<strong><span class="hljs-variable">PASS</span><br/><span class="hljs-ok">ok</span>      _/<span class="hljs-function_or_atom">usr</span>/<span class="hljs-function_or_atom">src</span>/<span class="hljs-function_or_atom">myapp</span> <span class="hljs-number">0.019</span>s</strong>
</pre>
<p>All integration tests passed, and the whole operation took less than 0.2 seconds.</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="210" src="assets/cd-environment-prod-like-tests.png" width="502"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-9: Production tests are run against the updated service inside the CD cluster</div>
<p>From now on, we should be fairly confident that the release is ready for production. We ran the pre-deployment unit tests, built the images, ran staging tests, updated the production-like cluster, and ran a set of integration tests.</p>
<p>Our Continuous Delivery steps are officially done. The release is ready and waiting for someone to make a decision to update the service running in production. In other words, at this point, the Continuous Delivery is finished, and we would be waiting for someone to press the button to update the service in the production cluster.</p>
<p>There is no reason to stop now. We have all the knowledge we would need to convert this process from Continuous Delivery to Continuous Deployment. All we have to do is repeat the last few commands inside the production cluster.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Walking the extra mile from Continuous Delivery to Continuous Deployment</h1>
            </header>

            <article>
                
<p>If we do have a comprehensive set of tests that give us confidence that each commit to the code repository is working as expected and if there is a repeatable and reliable Deployment process, there is no real reason for not taking that extra mile and automatically deploying each release to production.</p>
<p>You might choose not to do Continuous Deployment. Maybe your process requires us to cherry pick features. Maybe our marketing department does not want new features to be available before their campaign starts. There are plenty of reasons why one would choose to stop at Continuous Delivery. Nevertheless, from the technical perspective, the process is the same. The only difference is that Continuous Delivery requires us to press the button that deploys the selected release to production while Continuous Deployment does the deployment as part of the same automated flow. In other words, the steps we are about to run are the same, with or without a button in between.</p>
<p>This will probably be the shortest sub-chapter in the book. We are only a few commands short of converting the Continuous Delivery process into Continuous Deployment. We need to update the service in the production cluster (swarm) and go back to the <kbd>swarm-test-1</kbd> node and execute another round of tests. Since we already did all that, there is no strong reason to go into details. We'll just do it:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker service update \<br/>    --image=localhost:<span class="hljs-number">5000</span>/go-demo:<span class="hljs-number">1.1</span> \<br/>    go-demo</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="228" src="assets/cd-environment-prod-update.png" width="546"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-10: The service is updated inside the production cluster</div>
<p>Now that the service is updated inside the production cluster, we can execute the last round of tests:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-test-<span class="hljs-number">1</span>)<br/><br/><span class="hljs-keyword">export</span> HOST_IP=$(docker-machine ip swarm-<span class="hljs-number">1</span>)<br/><br/>docker-compose \<br/><span class="hljs-operator">    -f</span> docker-compose-test-local.yml \<br/>    run --rm production</strong>
</pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="205" src="assets/cd-environment-prod-tests.png" width="489"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 5-11: Production tests are run against the updated service inside the production cluster</div>
<p>We updated the release running inside the production cluster and ran another round of integration tests. Nothing failed, indicating that the new release is, indeed, running in production correctly.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">What now?</h1>
            </header>

            <article>
                
<p>Are we done with Continuous Deployment? The answer is no. We did not create the automated Continuous Deployment flow, but defined the steps that will help us run the process automatically. For the process to be fully automated and executed on each commit, we need to use one of the CD tools.</p>
<p>We'll use Jenkins to transform manual steps into a fully automated Continuous Deployment flow. For the whole process to work, we'll need to set up Jenkins master, a few agents, and a deployment pipeline job.</p>
<p>Now is the time to take a break before diving into the next chapter. As before, we'll destroy the machines we created and start fresh:</p>
<pre>
<strong>docker-machine rm <span class="hljs-operator">-f</span> \<br/>    swarm-<span class="hljs-number">1</span> swarm-<span class="hljs-number">2</span> swarm-<span class="hljs-number">3</span> \<br/>    swarm-test-<span class="hljs-number">1</span> swarm-test-<span class="hljs-number">2</span> swarm-test-<span class="hljs-number">3</span></strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>