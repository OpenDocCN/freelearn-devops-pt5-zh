- en: Introduction to Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes简介
- en: In the previous chapter, we learned how SwarmKit uses rolling updates to achieve
    zero downtime deployments. We were also introduced to Docker configs, which are
    used to store nonsensitive data in clusters and use this to configure application
    services, as well as Docker secrets, which are used to share confidential data
    with an application service running in a Docker Swarm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了SwarmKit如何通过滚动更新实现零停机部署。我们还介绍了Docker配置，它用于在集群中存储非敏感数据，并使用这些数据配置应用服务，以及Docker机密，它用于与在Docker
    Swarm中运行的应用服务共享机密数据。
- en: In this chapter, we're going to introduce Kubernetes. Kubernetes is currently
    the clear leader in the container orchestration space. We will start with a high-level
    overview of the architecture of a Kubernetes cluster and then discuss the main
    objects used in Kubernetes to define and run containerized applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将介绍Kubernetes。Kubernetes目前是容器编排领域的领头羊。我们将从Kubernetes集群架构的高层次概述开始，然后讨论Kubernetes中用于定义和运行容器化应用程序的主要对象。
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下内容：
- en: Kubernetes architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: Kubernetes master nodes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes主节点
- en: Cluster nodes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群节点
- en: Introduction to MiniKube
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MiniKube简介
- en: Kubernetes support in Docker for Desktop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker for Desktop中的Kubernetes支持
- en: Introduction to pods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod简介
- en: Kubernetes ReplicaSet
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes ReplicaSet
- en: Kubernetes deployment
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes部署
- en: Kubernetes service
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes服务
- en: Context-based routing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于上下文的路由
- en: Comparing SwarmKit with Kubernetes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将SwarmKit与Kubernetes进行比较
- en: 'After finishing this chapter, you will be able to do the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你将能够完成以下任务：
- en: Draft the high-level architecture of a Kubernetes cluster on a napkin
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在餐巾纸上绘制Kubernetes集群的高层架构
- en: Explain three to four main characteristics of a Kubernetes pod
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Kubernetes Pod的三到四个主要特性
- en: Describe the role of Kubernetes ReplicaSets in two to three short sentences
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用两到三句话描述Kubernetes ReplicaSets的作用
- en: Explain two or three main responsibilities of a Kubernetes service
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Kubernetes服务的两到三项主要职责
- en: Create a pod in Minikube
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Minikube中创建一个Pod
- en: Configure Docker for Desktop in order to use Kubernetes as an orchestrator
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Docker for Desktop，以便使用Kubernetes作为编排器
- en: Create a deployment in Docker for Desktop
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Docker for Desktop中创建一个部署
- en: Create a Kubernetes service to expose an application service internally (or
    externally) to the cluster
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个Kubernetes服务，将应用服务暴露到集群的内部（或外部）
- en: Technical requirements
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code files for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition).
    Alternatively, if you cloned the GitHub repository that accompanies this book
    to your computer, as described in [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml),
    *Setting Up a Working Environment*, then you can find the code at `~/fod-solution/ch15`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到，链接：[https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition](https://github.com/PacktPublishing/Learn-Docker---Fundamentals-of-Docker-19.x-Second-Edition)。或者，如果你按照[第2章](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml)《设置工作环境》的描述，将本书附带的GitHub仓库克隆到你的计算机上，你可以在`~/fod-solution/ch15`找到代码。
- en: Kubernetes architecture
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes架构
- en: A Kubernetes cluster consists of a set of servers. These servers can be VMs
    or physical servers. The latter are also called *bare metal*. Each member of the
    cluster can have one of two roles. It is either a Kubernetes master or a (worker)
    node. The former is used to manage the cluster, while the latter will run an application
    workload. I have put the worker in parentheses since, in Kubernetes parlance,
    you only talk about a node when you're talking about a server that runs application
    workloads. But in Docker parlance and in Swarm, the equivalent is a *worker node*.
    I think that the notion of a worker node better describes the role of the server
    than a simple *node*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Kubernetes集群由一组服务器组成。这些服务器可以是虚拟机（VM）或物理服务器，后者也称为*裸金属*。集群的每个成员都有两个角色之一。它要么是Kubernetes主节点，要么是（工作）节点。前者用于管理集群，而后者将运行应用程序工作负载。我把工作节点放在括号中，因为在Kubernetes术语中，只有在讨论运行应用程序工作负载的服务器时，才会提到节点。但在Docker术语和Swarm中，相应的概念是*工作节点*。我认为“工作节点”的概念比简单的*节点*更能描述服务器的角色。
- en: In a cluster, you have a small and odd number of masters and as many worker
    nodes as needed. Small clusters might only have a few worker nodes, while more
    realistic clusters might have dozens or even hundreds of worker nodes. Technically,
    there is no limit to how many worker nodes a cluster can have; in reality, though,
    you might experience a significant slowdown in some management operations when
    dealing with thousands of nodes. All members of the cluster need to be connected
    by a physical network, the so-called **underlay network**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中，你会有一个小而奇数的master节点，以及根据需要数量的worker节点。小型集群可能只有几个worker节点，而更现实的集群可能有几十个甚至上百个worker节点。从技术上讲，集群中worker节点的数量没有限制；但实际上，当处理成千上万个节点时，某些管理操作可能会显著变慢。集群中的所有成员需要通过一个物理网络连接，这就是所谓的**底层网络**。
- en: Kubernetes defines one flat network for the whole cluster. Kubernetes does not
    provide any networking implementation out of the box; instead, it relies on plugins
    from third parties. Kubernetes just defines the **Container Network Interface** (**CNI**)
    and leaves the implementation to others. The CNI is pretty simple. It basically
    states that each pod running in the cluster must be able to reach any other pod
    also running in the cluster without any **Network** **Address** **Translation**
    (**NAT**) happening in-between. The same must be true between cluster nodes and
    pods, that is, applications or daemons running directly on a cluster node must
    be able to reach each pod in the cluster and vice versa.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为整个集群定义了一个扁平网络。Kubernetes本身并不提供任何内建的网络实现，而是依赖于第三方的插件。Kubernetes只是定义了**容器网络接口**（**CNI**），并将实现留给其他人。CNI非常简单，基本上规定了集群中运行的每个pod必须能够访问集群中运行的任何其他pod，且中间不能发生**网络地址转换**（**NAT**）。集群节点与pod之间也必须满足相同的要求，即直接在集群节点上运行的应用或守护进程必须能够访问集群中的每个pod，反之亦然。
- en: 'The following diagram illustrates the high-level architecture of a Kubernetes
    cluster:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了Kubernetes集群的高级架构：
- en: '![](img/8f31f291-f723-4876-8ff9-349f2c42d114.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f31f291-f723-4876-8ff9-349f2c42d114.jpg)'
- en: High-level architecture diagram of Kubernetes
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的高级架构图
- en: 'The preceding diagram is explained as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图解说明如下：
- en: On the top, in the middle, we have a cluster of **etcd** nodes. **etcd** is
    a distributed key-value store that, in a Kubernetes cluster, is used to store
    all the state of the cluster. The number of **etcd** nodes has to be odd, as mandated
    by the Raft consensus protocol, which states which nodes are used to coordinate
    among themselves. When we talk about the **Cluster State**, we do not include
    data that is produced or consumed by applications running in the cluster; instead,
    we're talking about all the information on the topology of the cluster, what services
    are running, network settings, secrets used, and more. That said, this **etcd**
    cluster is really mission-critical to the overall cluster and thus, we should
    never run only a single **etcd** server in a production environment or any environment
    that needs to be highly available.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在顶部中间，我们有一个**etcd**节点集群。**etcd**是一个分布式的键值存储，在Kubernetes集群中，用于存储集群的所有状态。**etcd**节点的数量必须是奇数，这是Raft共识协议的要求，该协议规定哪些节点用于相互协调。当我们谈论**集群状态**时，并不包括在集群中运行的应用所生产或消耗的数据；而是指集群拓扑结构、运行的服务、网络设置、使用的密钥等所有信息。也就是说，这个**etcd**集群对于整个集群至关重要，因此，在生产环境或任何需要高可用性的环境中，我们绝不应该只运行一个**etcd**服务器。
- en: Then, we have a cluster of Kubernetes **master** nodes, which also form a **Consensus**
    **Group** among themselves, similar to the **etcd** nodes. The number of master
    nodes also has to be odd. We can run cluster with a single master but we should
    never do that in a production or mission-critical system. There, we should always
    have at least three master nodes. Since the master nodes are used to manage the
    whole cluster, we are also talking about the management plane. Master nodes use
    the **etcd** cluster as their backing store. It is good practice to put a **load**
    **balancer** (**LB**) in front of master nodes with a well-known **Fully Qualified
    Domain Name** (**FQDN**), such as `https://admin.example.com`. All tools that
    are used to manage the Kubernetes cluster should access it through this LB rather
    than using the public IP address of one of the master nodes. This is shown on
    the left upper side of the preceding diagram.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们有一个 Kubernetes **主** 节点的集群，它们之间也会形成一个 **共识** **组**，类似于 **etcd** 节点。主节点的数量也必须是奇数。我们可以用单个主节点运行集群，但在生产环境或关键任务系统中永远不应这样做。此时，我们应该始终至少有三个主节点。由于主节点用于管理整个集群，所以我们也在讨论管理平面。主节点使用
    **etcd** 集群作为其后备存储。将 **负载均衡器**（**LB**）放置在主节点前面，并使用一个众所周知的 **完全合格域名**（**FQDN**），例如
    `https://admin.example.com`，是一种良好的实践。所有用于管理 Kubernetes 集群的工具应通过这个 LB 访问，而不是直接使用某个主节点的公共
    IP 地址。这在前述图的左上方有所展示。
- en: Toward the bottom of the diagram, we have a cluster of **worker** nodes. The
    number of nodes can be as low as one and does not have an upper limit. Kubernetes
    master and worker nodes communicate with each other. It is a bidirectional form
    of communication that is different from the one we know from Docker Swarm. In
    Docker Swarm, only manager nodes communicate with worker nodes and never the other
    way around. All ingress traffic accessing applications running in the cluster
    should go through another **load balancer**. This is the application **load**
    **balancer** or reverse proxy. We never want external traffic to directly access
    any of the worker nodes.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图的底部，我们有一个 **工作** 节点的集群。节点的数量可以低至一个，没有上限。Kubernetes 主节点和工作节点之间相互通信。这是一种双向通信形式，不同于我们在
    Docker Swarm 中所知的通信方式。在 Docker Swarm 中，只有管理节点与工作节点进行通信，反之则不行。所有访问集群中运行的应用程序的入口流量应通过另一个
    **负载均衡器**。这就是应用程序 **负载** **均衡器** 或反向代理。我们绝不希望外部流量直接访问任何工作节点。
- en: Now that we have an idea about the high-level architecture of a Kubernetes cluster,
    let's delve a bit more deeply and look at the Kubernetes master and worker nodes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Kubernetes 集群的高层架构有了一个大致了解，接下来我们深入探讨 Kubernetes 主节点和工作节点。
- en: Kubernetes master nodes
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 主节点
- en: 'Kubernetes master nodes are used to manage a Kubernetes cluster. The following
    is a high-level diagram of such a master:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 主节点用于管理 Kubernetes 集群。以下是该主节点的高层次示意图：
- en: '![](img/36896fcb-357f-497a-822d-6d08c3a72c1e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36896fcb-357f-497a-822d-6d08c3a72c1e.png)'
- en: Kubernetes master
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 主节点
- en: 'At the bottom of the preceding diagram, we have the **Infrastructure**, which
    can be a VM on-premise or in the cloud or a server (often called bare metal) on-premise
    or in the cloud. Currently, Kubernetes masters only run on **Linux**. The most
    popular Linux distributions, such as RHEL, CentOS, and Ubuntu, are supported.
    On this Linux machine, we have at least the following four Kubernetes services
    running:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图的底部，我们有 **基础设施**，它可以是本地或云端的虚拟机，也可以是本地或云端的服务器（通常称为裸机服务器）。目前，Kubernetes 主节点仅能运行在
    **Linux** 系统上。主流的 Linux 发行版，如 RHEL、CentOS 和 Ubuntu，均得到支持。在这台 Linux 机器上，我们至少运行以下四个
    Kubernetes 服务：
- en: '**API server**: This is the gateway to Kubernetes. All requests to list, create,
    modify, or delete any resources in the cluster must go through this service. It
    exposes a REST interface that tools such as `kubectl` use to manage the cluster
    and applications in the cluster.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 服务器**：这是访问 Kubernetes 的网关。所有列出、创建、修改或删除集群中任何资源的请求都必须经过此服务。它暴露一个 REST
    接口，工具如 `kubectl` 使用这个接口来管理集群和集群中的应用程序。'
- en: '**Controller**: The controller, or more precisely the controller manager, is a
    control loop that observes the state of the cluster through the API server and
    makes changes, attempting to move the current or effective state toward the desired
    state if they differ.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器**：控制器，或者更准确地说是控制器管理器，是一个控制循环，它通过 API 服务器观察集群的状态，并进行更改，试图将当前状态或有效状态与期望状态对齐，如果它们之间存在差异。'
- en: '**Scheduler**: The scheduler is a service that tries its best to schedule pods
    on worker nodes while considering various boundary conditions, such as resource
    requirements, policies, quality of service requirements, and more.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器**：调度器是一个服务，尽力在考虑各种边界条件（如资源需求、策略、服务质量要求等）的情况下，将 Pod 调度到工作节点上。'
- en: '**Cluster Store**: This is an instance of etcd that is used to store all information
    about the state of the cluster.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群存储**：这是一个 etcd 实例，用于存储集群状态的所有信息。'
- en: To be more precise, etcd, which is used as a cluster store, does not necessarily
    have to be installed on the same node as the other Kubernetes services. Sometimes,
    Kubernetes clusters are configured to use standalone clusters of etcd servers,
    as shown in the architecture diagram in the previous section. But which variant
    to use is an advanced management decision and is outside the scope of this book.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，作为集群存储使用的 etcd 并不一定需要安装在与其他 Kubernetes 服务相同的节点上。有时，Kubernetes 集群被配置为使用独立的
    etcd 服务器集群，如前一部分的架构图所示。但选择使用哪种变体是一个高级管理决策，超出了本书的范围。
- en: We need at least one master, but to achieve high availability, we need three
    or more master nodes. This is very similar to what we have learned about the manager
    nodes of a Docker Swarm. In this regard, a Kubernetes master is equivalent to
    a Swarm manager node.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们至少需要一个主节点，但为了实现高可用性，我们需要三个或更多的主节点。这与我们在 Docker Swarm 中学到的管理节点非常相似。在这方面，Kubernetes
    的主节点相当于 Swarm 的管理节点。
- en: Kubernetes masters never run application workloads. Their sole purpose is to
    manage the cluster. Kubernetes masters build a Raft consensus group. The Raft
    protocol is a standard protocol used in situations where a group of members needs
    to make decisions. It is used in many well-known software products such as MongoDB,
    Docker SwarmKit, and Kubernetes. For a more thorough discussion of the Raft protocol,
    see the link in the *Further reading* section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 主节点从不运行应用工作负载。它们的唯一目的是管理集群。Kubernetes 主节点构建一个 Raft 一致性协议组。Raft 协议是一个标准协议，用于需要集体决策的情况。它被许多著名的软件产品使用，如
    MongoDB、Docker SwarmKit 和 Kubernetes。关于 Raft 协议的更深入讨论，请参见 *进一步阅读* 部分的链接。
- en: As we mentioned in the previous section, the state of the Kubernetes cluster
    is stored in etcd. If the Kubernetes cluster is supposed to be highly available,
    then etcd must also be configured in HA mode, which normally means that we have
    at least three etcd instances running on different nodes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一部分提到的，Kubernetes 集群的状态存储在 etcd 中。如果 Kubernetes 集群需要高度可用，那么 etcd 也必须配置为高可用模式，这通常意味着我们至少有三个
    etcd 实例在不同的节点上运行。
- en: Let's state once again that the whole cluster state is stored in etcd. This
    includes all the information about all the cluster nodes, all the replica sets,
    deployments, secrets, network policies, routing information, and so on. It is,
    therefore, crucial that we have a robust backup strategy in place for this key-value
    store.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再一次强调，整个集群的状态存储在 etcd 中。这包括所有集群节点的信息、所有副本集、部署、机密、网络策略、路由信息等。因此，确保我们有一个稳健的备份策略来保护这个键值存储至关重要。
- en: Now, let's look at the nodes that will be running the actual workload of the
    cluster.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看那些将运行集群实际工作负载的节点。
- en: Cluster nodes
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群节点
- en: 'Cluster nodes are the nodes with which Kubernetes schedules application workloads.
    They are the workhorses of the cluster. A Kubernetes cluster can have a few, dozens,
    hundreds, or even thousands of cluster nodes. Kubernetes has been built from the
    ground up for high scalability. Don''t forget that Kubernetes was modeled after
    Google Borg, which has been running tens of thousands of containers for years:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 集群节点是 Kubernetes 调度应用工作负载的节点。它们是集群的主力军。一个 Kubernetes 集群可以拥有少量、几十个、几百个甚至几千个集群节点。Kubernetes
    从一开始就为了高扩展性而构建。别忘了，Kubernetes 是以 Google Borg 为模型的，后者已经运行了数万个容器多年：
- en: '![](img/995b10fd-c87a-4b18-b291-104acf1bd1e8.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/995b10fd-c87a-4b18-b291-104acf1bd1e8.png)'
- en: Kubernetes worker node
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 工作节点
- en: A worker node can run on a VM, bare metal, on-premise, or in the cloud. Originally,
    worker nodes could only be configured on Linux. But since version 1.10 of Kubernetes,
    worker nodes can also run on Windows Server. It is perfectly fine to have a mixed
    cluster with Linux and Windows worker nodes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个工作节点可以运行在虚拟机、裸金属、内部服务器或云环境中。最初，工作节点只能在 Linux 上配置。但是从 Kubernetes 1.10 版本开始，工作节点也可以运行在
    Windows Server 上。拥有一个包含 Linux 和 Windows 工作节点的混合集群是完全可以的。
- en: 'On each node, we have three services that need to run, as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点上，我们需要运行三个服务，具体如下：
- en: '**Kubelet**: This is the first and foremost service. Kubelet is the primary
    node agent. The kubelet service uses pod specifications to make sure all of the
    containers of the corresponding pods are running and healthy. Pod specifications
    are files written in YAML or JSON format and they declaratively describe a pod.
    We will get to know what pods are in the next section. PodSpecs are provided to
    kubelet primarily through the API server.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubelet**：这是第一个也是最重要的服务。Kubelet是主要的节点代理。kubelet服务使用Pod规范确保对应Pod的所有容器都在运行且健康。Pod规范是用YAML或JSON格式编写的文件，它们声明性地描述一个Pod。我们将在下一节中了解什么是Pod。PodSpec主要通过API服务器提供给kubelet。'
- en: '**Container runtime**: The second service that needs to be present on each
    worker node is a container runtime. Kubernetes, by default, has used `containerd` since
    version 1.9 as its container runtime. Prior to that, it used the Docker daemon.
    Other container runtimes, such as rkt or CRI-O, can be used. The container runtime
    is responsible for managing and running the individual containers of a pod.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器运行时**：在每个工作节点上需要存在的第二个服务是容器运行时。Kubernetes从版本1.9开始默认使用`containerd`作为其容器运行时。在此之前，它使用的是Docker守护进程。还可以使用其他容器运行时，例如rkt或CRI-O。容器运行时负责管理和运行Pod的各个容器。'
- en: '**kube-proxy**: Finally, there is the kube-proxy. It runs as a daemon and is
    a simple network proxy and load balancer for all application services running
    on that particular node.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kube-proxy**：最后是kube-proxy。它作为守护进程运行，是一个简单的网络代理和负载均衡器，用于管理该节点上运行的所有应用服务。'
- en: Now that we have learned about the architecture of Kubernetes and the master
    and worker nodes, it is time to introduce the tooling that we can use to develop
    applications targeted at Kubernetes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了Kubernetes的架构以及主节点和工作节点的概念，是时候介绍我们可以用来开发面向Kubernetes应用的工具了。
- en: Introduction to Minikube
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Minikube简介
- en: Minikube is a tool that creates a single-node Kubernetes cluster in VirtualBox
    or Hyper-V (other hypervisors are supported too) ready to be used during the development
    of a containerized application. In [Chapter 2](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml), *Setting
    Up a Working Environment,* we learned how Minikube and `kubectl` can be installed
    on our macOS or Windows laptop. As stated there, Minikube is a single-node Kubernetes
    cluster and thus the node is, at the same time, a Kubernetes master as well as
    a worker node.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube是一个在VirtualBox或Hyper-V（也支持其他虚拟化程序）中创建单节点Kubernetes集群的工具，适用于容器化应用开发。在[第2章](99a92fe1-4652-4934-9c33-f3e19483afcd.xhtml)《设置工作环境》中，我们学习了如何在macOS或Windows笔记本上安装Minikube和`kubectl`。如前所述，Minikube是一个单节点Kubernetes集群，因此该节点既是Kubernetes主节点也是工作节点。
- en: 'Let''s make sure that Minikube is running with the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下命令确保Minikube正在运行：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once Minikube is ready, we can access its single node cluster using `kubectl`.
    We should see something similar to the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Minikube准备就绪，我们可以使用`kubectl`访问其单节点集群。我们应该看到类似如下的内容：
- en: '![](img/ee27b902-c2d7-4d1f-8c7d-8412d06b788b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee27b902-c2d7-4d1f-8c7d-8412d06b788b.png)'
- en: Listing all nodes in Minikube
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列出Minikube中的所有节点
- en: As we mentioned previously, we have a single-node cluster with a node called `minikube`.
    The version of Kubernetes that Minikube is using is `v1.16.2` in my case.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们有一个名为`minikube`的单节点集群。Minikube使用的Kubernetes版本在我这里是`v1.16.2`。
- en: Now, let's try to deploy a pod to this cluster. Don't worry about what a pod
    is for now; we will delve into all the details about it later in this chapter.
    For the moment, just take it as-is.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将一个Pod部署到这个集群。现在不必担心Pod是什么；我们将在本章后面深入探讨它的所有细节。暂时，先按原样理解它即可。
- en: 'We can use the `sample-pod.yaml` file in the `ch15` subfolder of our `labs` folder
    to create such a pod. It has the following content:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`labs`文件夹下`ch15`子文件夹中的`sample-pod.yaml`文件来创建这样的Pod。它的内容如下：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the following steps to run the pod:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下步骤来运行Pod：
- en: 'First, navigate to the correct folder:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到正确的文件夹：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s use the Kubernetes CLI called `kubectl` to deploy this pod:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用名为`kubectl`的Kubernetes命令行工具来部署这个Pod：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we now list all of the pods, we should see the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在列出所有Pod，应该会看到如下内容：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To be able to access this pod, we need to create a service. Let''s use the `sample-service.yaml` file,
    which has the following content:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了能够访问这个Pod，我们需要创建一个服务。我们使用`sample-service.yaml`文件，它的内容如下：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, don''t worry about what exactly a service is at this time. We''ll explain
    this later. Let''s just create this service:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can use `curl` to access the service:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We should receive the Nginx welcome page as an answer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you continue, please remove the two objects you just created:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Kubernetes support in Docker for Desktop
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Starting from version 18.01-ce, Docker for macOS and Docker for Windows have started
    to support Kubernetes out of the box. Developers who want to deploy their containerized
    applications to Kubernetes can use this orchestrator instead of SwarmKit. Kubernetes
    support is turned off by default and has to be enabled in the settings. The first
    time Kubernetes is enabled, Docker for macOS or Windows will need a moment to
    download all the components that are needed to create a single-node Kubernetes
    cluster. Contrary to Minikube, which is also a single-node cluster, the version
    provided by the Docker tools uses containerized versions of all Kubernetes components:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf2e24ff-9110-43ac-8e4a-b30f0c5d2df5.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: Kubernetes support in Docker for macOS and Windows
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram gives us a rough overview of how Kubernetes support has
    been added to Docker for macOS and Windows. Docker for macOS uses hyperkit to
    run a LinuxKit-based VM. Docker for Windows uses Hyper-V to achieve the result.
    Inside the VM, the Docker engine is installed. Part of the engine is SwarmKit,
    which enables **Swarm-Mode**. Docker for macOS or Windows uses the **kubeadm** tool
    to set up and configure Kubernetes in that VM. The following three facts are worth
    mentioning: Kubernetes stores its cluster state in **etcd**, thus we have **etcd**
    running on this VM. Then, we have all the services that make up Kubernetes and,
    finally, some services that support the deployment of Docker stacks from the **Docker
    CLI** into Kubernetes. This service is not part of the official Kubernetes distribution,
    but it is Docker-specific.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: All Kubernetes components run in containers in the **LinuxKit VM**. These containers
    can be hidden through a setting in Docker for macOS or Windows. Later in this
    section, we'll provide a complete list of Kubernetes system containers that will
    be running on your laptop, if you have Kubernetes support enabled. To avoid repetition,
    from now on, I will only talk about Docker for Desktop instead of Docker for macOS
    and Docker for Windows. Everything that I will be saying equally applies to both
    editions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: One big advantage of Docker for Desktop with Kubernetes enabled over Minikube
    is that the former allows developers to use a single tool to build, test, and
    run a containerized application targeted at Kubernetes. It is even possible to
    deploy a multi-service application into Kubernetes using a Docker Compose file.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s get our hands dirty:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to enable Kubernetes. On macOS, click on the Docker icon in
    the menu bar; or, on Windows, go to the command tray and select Preferences. In
    the dialog box that opens, select Kubernetes, as shown in the following screenshot:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/53d9153e-43f2-4b1c-ab52-bf08d8889001.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Enabling Kubernetes in Docker for Desktop
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Then, tick the Enable Kubernetes checkbox. Also, tick the Deploy Docker Stacks
    to Kubernetes by default and Show system containers (advanced) checkboxes. Then,
    click the Apply & Restart button. Installing and configuring of Kubernetes takes
    a few minutes. Now, it's time to take a break and enjoy a nice cup of tea.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the installation is finished (which Docker notifies us of by showing a
    green status icon in the Settings dialog), we can test it. Since we now have two
    Kubernetes clusters running on our laptop, that is, Minikube and Docker for Desktop,
    we need to configure `kubectl` to access the latter.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, let''s list all the contexts that we have:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca14801c-00eb-46cc-8edc-0b3edaba44f4.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: List of contexts for kubectl
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that, on my laptop, I have the two contexts we mentioned previously.
    Currently, the Minikube context is still active, flagged by the asterisk in the
    `CURRENT` column. We can switch to the `docker-for-desktop` context using the
    following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/903685b8-ecdb-455e-85a1-03188389f5c1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: Changing the context for the Kubernetes CLI
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use `kubectl` to access the cluster that Docker for Desktop just
    created. We should see the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3b65b94-4772-4584-9d06-3fa88a05a0eb.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: The single-node Kubernetes cluster created by Docker for Desktop
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: OK, this looks very familiar. It is pretty much the same as what we saw when
    working with Minikube. The version of Kubernetes that my Docker for Desktop is
    using is `1.15.5`. We can also see that the node is a master node.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'If we list all the containers that are currently running on our Docker for
    Desktop, we get the list shown in the following screenshot (note that I use the `--format` argument
    to output the `Container ID` and `Names` of the containers):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6837f756-ac8a-41a7-b8af-6357e140ea1c.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Kubernetes system containers
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding list, we can identify all the now-familiar components that
    make up Kubernetes, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: API server
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etcd
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube proxy
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNS service
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube controller
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube scheduler
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also containers that have the word `compose` in them. These are Docker-specific
    services and allow us to deploy Docker Compose applications onto Kubernetes. Docker
    translates the Docker Compose syntax and implicitly creates the necessary Kubernetes
    objects, such as deployments, pods, and services.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Normally, we don't want to clutter our list of containers with these system
    containers. Therefore, we can uncheck the Show system containers (advanced) checkbox
    in the settings for Kubernetes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to deploy a Docker Compose application to Kubernetes. Navigate
    to the `ch15` subfolder of our `~/fod` folder. We deploy the app as a stack using
    the `docker-compose.yml` file:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将一个 Docker Compose 应用部署到 Kubernetes。进入我们 `~/fod` 文件夹中的 `ch15` 子文件夹。我们使用
    `docker-compose.yml` 文件作为堆栈来部署该应用：
- en: '[PRE9]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We should see the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下内容：
- en: '![](img/c712f4fd-1771-4475-a125-e9286974bdb6.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c712f4fd-1771-4475-a125-e9286974bdb6.png)'
- en: Deploying the stack to Kubernetes
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 部署堆栈到 Kubernetes
- en: 'We can test the application, for example, using `curl`, and we will see that
    it is running as expected:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以测试应用，例如使用 `curl`，并且我们会看到它按照预期运行：
- en: '![](img/86a7e757-6c9f-4706-b38d-1d13096cb908.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86a7e757-6c9f-4706-b38d-1d13096cb908.png)'
- en: Pets application running in Kubernetes on Docker for Desktop
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 宠物应用在 Kubernetes 上运行，使用 Docker for Desktop
- en: 'Now, let''s see exactly what Docker did when we executed the `docker stack
    deploy` command. We can use `kubectl` to find out:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下当我们执行 `docker stack deploy` 命令时，Docker 究竟做了什么。我们可以使用 `kubectl` 来查看：
- en: '![](img/d4a81014-a75b-40c2-8c45-56412e93455d.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4a81014-a75b-40c2-8c45-56412e93455d.png)'
- en: Listing all Kubernetes objects created by docker stack deploy
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列出通过 docker stack deploy 创建的所有 Kubernetes 对象
- en: Docker created a deployment for the `web` service and a stateful set for the `db` service.
    It also automatically created Kubernetes services for `web` and `db` so that they
    can be accessed inside the cluster. It also created the Kubernetes `svc/web-published` service, which
    is used for external access.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 为 `web` 服务创建了一个部署，并为 `db` 服务创建了一个有状态集。它还自动为 `web` 和 `db` 创建了 Kubernetes
    服务，以便它们可以在集群内访问。它还创建了 Kubernetes 的 `svc/web-published` 服务，用于外部访问。
- en: This is pretty cool, to say the least, and tremendously decreases friction in
    the development process for teams targeting Kubernetes as their orchestration
    platform
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这至少可以说是非常酷的，并且大大减少了开发过程中对于 Kubernetes 作为编排平台的团队的摩擦。
- en: 'Before you continue, please remove the stack from the cluster:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请从集群中删除该堆栈：
- en: '[PRE10]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Also, make sure you reset the context for `kubectl` back to Minikube, as we
    will be using Minikube for all our samples in this chapter:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，确保将 `kubectl` 的上下文重置为 Minikube，因为我们将在本章中使用 Minikube 进行所有示例：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have had an introduction to the tools we can use to develop applications
    that will eventually run in a Kubernetes cluster, it is time to learn about all
    the important Kubernetes objects that are used to define and manage such an application.
    We will start with pods.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了可以用来开发最终将在 Kubernetes 集群中运行的应用程序的工具，是时候了解用于定义和管理这类应用程序的所有重要 Kubernetes
    对象了。我们将从 Pod 开始。
- en: Introduction to pods
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 介绍
- en: 'Contrary to what is possible in Docker Swarm, you cannot run containers directly
    in a Kubernetes cluster. In a Kubernetes cluster, you can only run pods. Pods
    are the atomic units of deployment in Kubernetes. A pod is an abstraction of one
    or many co-located containers that share the same Kernel namespaces, such as the
    network namespace. No equivalent exists in Docker SwarmKit. The fact that more
    than one container can be co-located and share the same network namespace is a
    very powerful concept. The following diagram illustrates two pods:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Docker Swarm 中可能做到的不同，你不能直接在 Kubernetes 集群中运行容器。在 Kubernetes 集群中，你只能运行 Pod。Pod
    是 Kubernetes 中部署的基本单元。Pod 是一个抽象，它包含一个或多个共同部署的容器，这些容器共享相同的内核命名空间，例如网络命名空间。在 Docker
    SwarmKit 中没有等效概念。多个容器可以共同部署并共享相同的网络命名空间，这是一个非常强大的概念。下面的图示展示了两个 Pod：
- en: '![](img/81cd2bb8-23c9-4034-9ad3-eb03b5d6a1ba.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81cd2bb8-23c9-4034-9ad3-eb03b5d6a1ba.png)'
- en: Kubernetes pods
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Pod
- en: In the preceding diagram, we have two pods, **Pod 1** and **Pod 2**. The first
    pod contains two containers, while the second one only contains a single container.
    Each pod gets an IP address assigned by Kubernetes that is unique in the whole
    Kubernetes cluster. In our case, these are the following IP addresses: `10.0.12.3` and `10.0.12.5`.
    Both are part of a private subnet managed by the Kubernetes network driver.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，我们有两个 Pod，**Pod 1** 和 **Pod 2**。第一个 Pod 包含两个容器，而第二个 Pod 仅包含一个容器。每个 Pod
    都会获得 Kubernetes 分配的唯一 IP 地址，这些 IP 地址在整个 Kubernetes 集群中都是唯一的。在我们的例子中，这些 IP 地址为：`10.0.12.3`
    和 `10.0.12.5`。它们都属于 Kubernetes 网络驱动程序管理的私有子网。
- en: A pod can contain one to many containers. All those containers share the same
    Linux kernel namespaces, and in particular, they share the network namespace.
    This is indicated by the dashed rectangle surrounding the containers. Since all
    containers running in the same pod share the network namespace, each container
    needs to make sure to use their own port since duplicate ports are not allowed
    in a single network namespace. In this case, in **Pod 1**, the **main container**
    is using port `80` while the **supporting container** is using port `3000`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Requests from other pods or nodes can use the pod's IP address combined with
    the corresponding port number to access the individual containers. For example,
    you could access the application running in the main container of **Pod 1** through `10.0.12.3:80`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Docker container and Kubernetes pod networking
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s compare Docker''s container networking and Kubernetes pod networking.
    In the following diagram, we have the former on the left-hand side and the latter
    on the right-hand side:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/23036596-986c-43b4-83db-2732000cfad9.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Containers in a pod sharing the same network namespace
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: When a Docker container is created and no specific network is specified, then
    the Docker engine creates a **virtual ethernet** (**veth**) endpoint. The first
    container gets **veth0**, the next one gets **veth1**, and so on. These virtual
    ethernet endpoints are connected to the Linux bridge, **docker0**, that Docker
    automatically creates upon installation. Traffic is routed from the **docker0** bridge
    to every connected **veth** endpoint. Every container has its own network namespace.
    No two containers use the same namespace. This is on purpose, to isolate applications
    running inside the containers from each other.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'For a Kubernetes pod, the situation is different. When creating a new pod,
    Kubernetes first creates a so-called **pause **container whose only purpose is
    to create and manage the namespaces that the pod will share with all containers.
    Other than that, it does nothing useful; it is just sleeping. The **pause **container
    is connected to the **docker0 **bridge through **veth0**. Any subsequent container
    that will be part of the pod uses a special feature of the Docker engine that
    allows it to reuse an existing network namespace. The syntax to do so looks like
    this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The important part is the `--net` argument, which uses `container:<container
    name>`as a value. If we create a new container this way, then Docker does not
    create a new veth endpoint; the container uses the same one as the `pause` container.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important consequence of multiple containers sharing the same network
    namespace is the way they communicate with each other. Let''s consider the following
    situation: a pod containing two containers, one listening at port `80` and the
    other at port `3000`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55170356-9c32-42bb-bbcd-a9f51f59017e.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Containers in pods communicating via localhost
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: When two containers use the same Linux kernel network namespace, they can communicate
    with each other through localhost, similarly to how, when two processes are running
    on the same host, they can communicate with each other through localhost too.
    This is illustrated in the preceding diagram. From the main container, the containerized
    application inside it can reach out to the service running inside the supporting
    container through `http://localhost:3000`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Sharing the network namespace
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After all this theory, you might be wondering how a pod is actually created by
    Kubernetes. Kubernetes only uses what Docker provides. So, *how does this network
    namespace share work?* First, Kubernetes creates the so-called `pause` container,
    as mentioned previously. This container has no other function than to reserve
    the kernel namespaces for that pod and keep them alive, even if no other container
    inside the pod is running. Let''s simulate the creation of a pod, then. We start
    by creating the `pause` container and use Nginx for this purpose:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we add a second container called `main`, attaching it to the same network
    namespace as the `pause` container:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Since `pause` and the sample container are both parts of the same network namespace,
    they can reach each other through `localhost`. To show this, we have to `exec` into
    the main container:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can test the connection to Nginx running in the `pause` container and
    listening on port `80`. The following what we get if we use the `wget` utility
    to do so:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccfdb453-ac21-47bd-a2af-716d7c2bcb1e.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Two containers sharing the same network namespace
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'The output shows that we can indeed access Nginx on `localhost`. This is proof
    that the two containers share the same namespace. If that is not enough, we can
    use the `ip` tool to show `eth0` inside both containers and we will get the exact
    same result, specifically, the same IP address, which is one of the characteristics
    of a pod where all its containers share the same IP address:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad510dd1-a0ea-4767-ac14-94eecd16c1a3.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Displaying the properties of eth0 with the ip tool
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'If we inspect the `bridge` network, we can see that only the `pause` container
    is listed. The other container didn''t get an entry in the `Containers` list since
    it is reusing the `pause` container''s endpoint:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e553023b-c1e7-4abf-893c-b8cb75208b13.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Inspecting the Docker default bridge network
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will be looking at the pod life cycle.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Pod life cycle
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in this book, we learned that containers have a life cycle. A container
    is initialized, run, and ultimately exited. When a container exits, it can do
    this gracefully with an exit code zero or it can terminate with an error, which
    is equivalent to a nonzero exit code.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, a pod has a life cycle. Due to the fact that a pod can contain more
    than one container, this life cycle is slightly more complicated than that of
    a single container. The life cycle of a pod can be seen in the following diagram:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f161a33-21c8-4c48-888d-881ef4fbe6a7.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: The life cycle of Kubernetes pods
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: When a **Pod** is created on a cluster node, it first enters into the **pending** status.
    Once all the containers of the pod are up and running, the pod enters into the **running** status.
    The pod only enters into this state if all its containers run successfully. If
    the pod is asked to terminate, it will request all its containers to terminate.
    If all containers terminate with exit code zero, then the pod enters into the **succeeded** status.
    This is the happy path.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at some scenarios that lead to the pod being in the **failed** state.
    There are three possible scenarios:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: If, during the startup of the pod, at least one container is not able to run
    and fails (that is, it exits with a nonzero exit code), the pod goes from the **pending** state
    into the **failed** state.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the pod is in the running status and one of the containers suddenly crashes
    or exits with a nonzero exit code, then the pod transitions from the **running** state
    into the **failed** state.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the pod is asked to terminate and, during the shutdown at least one of the
    containers, exits with a nonzero exit code, then the pod also enters into the **failed** state.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at the specifications for a pod.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Pod specifications
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When creating a pod in a Kubernetes cluster, we can use either an imperative
    or a declarative approach. We discussed the difference between the two approaches
    earlier in this book but, to rephrase the most important aspect, using a declarative
    approach signifies that we write a manifest that describes the end state we want
    to achieve. We'll leave out the details of the orchestrator. The end state that
    we want to achieve is also called the **desired state**. In general, the declarative
    approach is strongly preferred in all established orchestrators, and Kubernetes
    is no exception.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in this chapter, we will exclusively concentrate on the declarative approach.
    Manifests or specifications for a pod can be written using either the YAML or
    JSON formats. In this chapter, we will concentrate on YAML since it is easier
    to read for us humans. Let''s look at a sample specification. Here is the content
    of the `pod.yaml` file, which can be found in the `ch12` subfolder of our `labs` folder:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Each specification in Kubernetes starts with the version information. Pods have
    been around for quite some time and thus the API version is `v1`. The second line
    specifies the type of Kubernetes object or resource we want to define. Obviously,
    in this case, we want to specify a `Pod`. Next follows a block containing metadata.
    At a bare minimum, we need to give the pod a name. Here, we call it `web-pod`.
    The next block that follows is the `spec` block, which contains the specification
    of the pod. The most important part (and the only one in this simple sample) is
    a list of all containers that are part of this pod. We only have one container
    here, but multiple containers are possible. The name we choose for our container
    is `web` and the container image is `nginx:alpine`. Finally, we define a list
    of ports the container is exposing.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have authored such a specification, we can apply it to the cluster
    using the Kubernetes CLI, `kubectl`. In a Terminal, navigate to the `ch15` subfolder
    and execute the following command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will respond with `pod "web-pod" created`. We can then list all the pods
    in the cluster with `kubectl get pods`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As expected, we have one of one pods in the running status. The pod is called `web-pod`,
    as defined. We can get more detailed information about the running pod by using the
    `describe` command:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99fb4f13-8084-40a5-a7c5-773f47e056e2.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: Describing a pod running in the cluster
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Please note the `pod/web-pod` notation in the previous `describe` command. Other
    variants are possible; for example, `pods/web-pod`, `po/web-pod`. `pod` and `po` are
    aliases of `pods`. The `kubectl` tool defines many aliases to make our lives a
    bit easier.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The `describe` command gives us a plethora of valuable information about the
    pod, not the least of which is a list of events that happened and affected this
    pod. The list is shown at the end of the output.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: The information in the `Containers` section is very similar to what we find
    in a `docker container inspect` output.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: We can also see a `Volumes` section with an entry of the `Secret` type. We will
    discuss Kubernetes secrets in the next chapter. Volumes, on the other hand, will
    be discussed next.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Pods and volumes
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 5](f3a48b12-d541-467b-aeb3-df014e60da6b.xhtml), *Data Volumes and
    Configuration*, we learned about volumes and their purpose: accessing and storing
    persistent data. Since containers can mount volumes, pods can do so as well. In
    reality, it is really the containers inside the pod that mount the volumes, but
    that is just a semantic detail. First, let''s see how we can define a volume in
    Kubernetes. Kubernetes supports a plethora of volume types, so we won''t delve
    into too much detail about this. Let''s just create a local volume implicitly
    by defining a `PersistentVolumeClaim` called `my-data-claim`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We have defined a claim that requests 2 GB of data. Let''s create this claim:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can list the claim using `kubectl` (`pvc` is a shortcut for `PersistentVolumeClaim`):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8b60fdf-3b0e-456c-bf0e-5a7a5948ab5c.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: List of PersistentStorageClaim objects in the cluster
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'In the output, we can see that the claim has implicitly created a volume called `pvc-<ID>`.
    We are now ready to use the volume created by the claim in a pod. Let''s use a
    modified version of the pod specification that we used previously. We can find
    this updated specification in the `pod-with-vol.yaml` file in the `ch12` folder.
    Let''s look at this specification in detail:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the last four lines, in the `volumes` block, we define a list of volumes
    we want to use for this pod. The volumes that we list here can be used by any
    of the containers of the pod. In our particular case, we only have one volume.
    We specify that we have a volume called `my-data`, which is a persistent volume
    claim whose claim name is the one we just created. Then, in the container specification,
    we have the `volumeMounts` block, which is where we define the volume we want
    to use and the (absolute) path inside the container where the volume will be mounted.
    In our case, we mount the volume to the `/data` folder of the container filesystem.
    Let''s create this pod:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we can `exec` into the container to double-check that the volume has
    mounted by navigating to the `/data` folder, creating a file there, and exiting
    the container:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we are right, then the data in this container must persist beyond the life
    cycle of the pod. Thus, let''s delete the pod and then recreate it and exec into
    it to make sure the data is still there. This is the result:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b789efc4-6ea6-4338-a236-22aa885902f9.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: Data stored in volume survives pod recreation
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of pods, let's look into how those pods
    are managed with the help of ReplicaSets.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes ReplicaSet
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A single pod in an environment with high availability requirements is insufficient. *What
    if the pod crashes?* *What if we need to update the application running inside
    the pod but cannot afford any service interruption?* These questions and more
    indicate that pods alone are not enough and we need a higher-level concept that
    can manage multiple instances of the same pod. In Kubernetes, the **ReplicaSet **is
    used to define and manage such a collection of identical pods that are running
    on different cluster nodes. Among other things, a ReplicaSet defines which container
    images are used by the containers running inside a pod and how many instances
    of the pod will run in the cluster. These properties and many others are called
    the desired state.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReplicaSet is responsible for reconciling the desired state at all times,
    if the actual state ever deviates from it. Here is a Kubernetes ReplicaSet:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f6cc3c9-2f46-4517-8b87-372a24c0c8bc.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Kubernetes ReplicaSet
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see a **ReplicaSet** called **rs-api,** which
    governs a number of pods. The pods are called **pod-api**. The **ReplicaSet**
    is responsible for making sure that, at any given time, there are always the desired
    number of pods running. If one of the pods crashes for whatever reason, the **ReplicaSet**
    schedules a new pod on a node with free resources instead. If there are more pods
    than the desired number, then the **ReplicaSet** kills superfluous pods. With
    this, we can say that the **ReplicaSet** guarantees a self-healing and scalable
    set of pods. There is no limit to how many pods a **ReplicaSet** can hold.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSet specification
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to what we have learned about pods, Kubernetes also allows us to either
    imperatively or declaratively define and create a `ReplicaSet`. Since the declarative
    approach is by far the most recommended one in most cases, we''re going to concentrate
    on this approach. Here is a sample specification for a Kubernetes `ReplicaSet`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This looks an awful lot like the pod specification we introduced earlier. Let's
    concentrate on the differences, then. First, on line 2, we have the `kind`, which
    was `Pod `and is now `ReplicaSet`. Then, on lines 6–8, we have a selector, which
    determines the pods that will be part of the `ReplicaSet`. In this case, it is
    all the pods that have `app` as a label with the value `web`. Then, on line 9,
    we define how many replicas of the pod we want to run; three, in this case. Finally,
    we have the `template` section, which first defines the `metadata` and then the
    `spec`, which defines the containers that run inside the pod. In our case, we
    have a single container using the `nginx:alpine` image and exporting port `80`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The really important elements are the number of replicas and the selector, which
    specifies the set of pods governed by the `ReplicaSet`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'In our `ch15` folder, we have a file called `replicaset.yaml` that contains
    the preceding specification. Let''s use this file to create the `ReplicaSet`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If we list all the ReplicaSets in the cluster, we get the following (`rs` is
    a shortcut for `replicaset`):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding output, we can see that we have a single ReplicaSet called `rs-web` whose
    desired state is three (pods). The current state also shows three pods and tell
    us that all three pods are ready. We can also list all the pods in the system.
    This results in the following output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Here, we can see our three expected pods. The names of the pods use the name
    of the ReplicaSet with a unique ID appended for each pod. In the `READY` column,
    we can see how many containers have been defined in the pod and how many of them
    are ready. In our case, we only have a single container per pod and, in each case,
    it is ready. Thus, the overall status of the pod is `Running`. We can also see
    how many times each pod had to be restarted. In our case, we don't have any restarts.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Self-healing
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s test the magic powers of the self-healing `ReplicaSet` by randomly
    killing one of its pods and observing what happens. Let''s delete the first pod
    from the previous list:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s list all the pods again. We expect to see only two pods, *right*?
    Wrong:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71366fbf-4638-484c-b2f6-dab56b41f5f4.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: List of pods after killing a pod of the ReplicaSet
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'OK; evidently, the second pod in the list has been recreated, as we can see
    from the `AGE` column. This is auto-healing in action. Let''s see what we discover
    if we describe the ReplicaSet:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f1f58fd-fe0d-46a6-8cdb-df8bf3bfc159.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Describe the ReplicaSet
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: And indeed, we find an entry under `Events` that tells us that the `ReplicaSet` created
    the new pod called `rs-web-q6cr7`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes deployment
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes takes the single-responsibility principle very seriously. All Kubernetes
    objects are designed to do one thing and one thing only, and they are designed
    to do this one thing very well. In this regard, we have to understand Kubernetes
    **ReplicaSets** and **Deployments**. A **ReplicaSet**, as we have learned, is
    responsible for achieving and reconciling the desired state of an application
    service. This means that the **ReplicaSet** manages a set of pods.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '**Deployment** augments a **ReplicaSet** by providing rolling updates and rollback
    functionality on top of it. In Docker Swarm, the Swarm service incorporates the
    functionality of both **ReplicaSet** and **Deployment.** In this regard, SwarmKit
    is much more monolithic than Kubernetes. The following diagram shows the relationship
    of a **Deployment** to a **ReplicaSet**:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6de18ec-b3ad-45c3-bc88-555afcaea7dd.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Kubernetes deployment
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the **ReplicaSet** is defining and governing a set
    of identical pods. The main characteristics of the **ReplicaSet** are that it
    is **self-healing**, **scalable**, and always does its best to reconcile the **desired**
    **state**. Kubernetes Deployment, in turn, adds rolling updates and rollback functionality
    to this. In this regard, a deployment is really a wrapper object to a ReplicaSet.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: We will learn more about rolling updates and rollbacks in the [Chapter 16](cdf765aa-eed9-4d88-a452-4ba817bc81dd.xhtml),
    *Deploying, Updating, and Securing an Application with Kubernetes*.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn more about Kubernetes services and how they
    enable service discovery and routing.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes service
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The moment we start to work with applications consisting of more than one application
    service, we need service discovery. The following diagram illustrates this problem:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/559a71f8-789e-4f46-ac6f-5366093fdbea.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Service discovery
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we have a **Web API** service that needs access to
    three other services: **payments**, **shipping**, and **ordering**. The **Web
    API** should never have to care about how and where to find those three services.
    In the API code, we just want to use the name of the service we want to reach
    and its port number. A sample would be the following URL `http://payments:3000`,
    which is used to access an instance of the payments service.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes, the payments application service is represented by a ReplicaSet
    of pods. Due to the nature of highly distributed systems, we cannot assume that
    pods have stable endpoints. A pod can come and go on a whim. But that's a problem
    if we need to access the corresponding application service from an internal or
    external client. If we cannot rely on pod endpoints being stable, *what else can
    we do?*
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where Kubernetes services come into play. They are meant to provide
    stable endpoints to ReplicaSets or Deployments, as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09cdd19a-e492-480c-9df1-f65ffc600f9d.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Kubernetes service providing stable endpoints to clients
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, in the center, we can see such a Kubernetes **Service**.
    It provides a **reliable** cluster-wide **IP** address, also called a **virtual
    IP** (**VIP**), as well as a **reliable** **Port** that's unique in the whole
    cluster. The pods that the Kubernetes service is proxying are determined by the
    **Selector** defined in the service specification. Selectors are always based
    on labels. Every Kubernetes object can have zero to many labels assigned to it.
    In our case, the **Selector** is **app=web**; that is, all pods that have a label
    called app with a value of web are proxied.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn more about context-based routing and how
    Kubernetes alleviates this task.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Context-based routing
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, we want to configure context-based routing for our Kubernetes cluster.
    Kubernetes offers us various ways to do this. The preferred and most scalable
    way at this time is to use an **IngressController**. The following diagram tries
    to illustrate how this ingress controller works:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc913d65-7d30-4339-b876-5d1c0201e3f5.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: Context-based routing using a Kubernetes ingress controller
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see how context-based (or layer 7) routing
    works when using an **IngressController**, such as Nginx. Here, we have the deployment
    of an application service called **web**. All the pods of this application service
    have the following label: **app=web**. Then, we have a Kubernetes service called
    **web** that provides a stable endpoint to those pods. The service has a (virtual)
    **IP** of `52.14.0.13` and exposes port `30044`. That is, if a request comes to
    any node of the Kubernetes cluster for the name **web** and port `30044`, then
    it is forwarded to this service. The service then load-balances the request to
    one of the pods.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, so good, *but how is an ingress request from a client to the *`http[s]://example.com/web`* URL routed
    to our web service?* First, we have to define routing from a context-based request
    to a corresponding `<service name>/<port> request`. This is done through an **Ingress** object:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: In the **Ingress** object, we define the **Host** and **Path** as the source
    and the (service) name, and the port as the target. When this Ingress object is
    created by the Kubernetes API server, then a process that runs as a sidecar in `IngressController`
    picks this change up.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process modifies the configuration the configuration file of the Nginx reverse
    proxy.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By adding the new route, Nginx is then asked to reload its configuration and
    thus will be able to correctly route any incoming requests to `http[s]://example.com/web`.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we are going to compare Docker SwarmKit with Kubernetes
    by contrasting some of the main resources of each orchestration engine.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Comparing SwarmKit with Kubernetes
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have learned a lot of details about the most important resources
    in Kubernetes, it is helpful to compare the two orchestrators, SwarmKit and Kubernetes,
    by matching important resources. Let''s take a look:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '| **SwarmKit** | **Kubernetes** | **Description** |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| Swarm | Cluster | Set of servers/nodes managed by the respective orchestrator.
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| Node | Cluster member | Single host (physical or virtual) that''s a member
    of the Swarm/cluster. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| Manager node | Master | Node managing the Swarm/cluster. This is the control
    plane. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| Worker node | Node | Member of the Swarm/cluster running application workload.
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| Container | Container** | An instance of a container image running on a node.
    **Note: In a Kubernetes cluster, we cannot run a container directly. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| Task | Pod | An instance of a service (Swarm) or ReplicaSet (Kubernetes)
    running on a node. A task manages a single container while a Pod contains one
    to many containers that all share the same network namespace. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| Service | ReplicaSet | Defines and reconciles the desired state of an application
    service consisting of multiple instances. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| Service | Deployment | A deployment is a ReplicaSet augmented with rolling
    updates and rollback capabilities. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| Routing Mesh | Service | The Swarm Routing Mesh provides L4 routing and load
    balancing using IPVS. A Kubernetes service is an abstraction that defines a logical
    set of pods and a policy that can be used to access them. It is a stable endpoint
    for a set of pods. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| Stack | Stack ** | The definition of an application consisting of multiple
    (Swarm) services.**Note: While stacks are not native to Kubernetes, Docker''s
    tool, Docker for Desktop, will translate them for deployment onto a Kubernetes
    cluster. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| Network | Network policy | Swarm **software-defined networks** (**SDNs**)
    are used to firewall containers. Kubernetes only defines a single flat network.
    Every pod can reach every other pod and/or node, unless network policies are explicitly
    defined to constrain inter-pod communication. |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: Summary
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the basics of Kubernetes. We took an overview
    of its architecture and introduced the main resources that are used to define
    and run applications in a Kubernetes cluster. We also introduced Minikube and
    Kubernetes support in Docker for Desktop.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to deploy an application into a Kubernetes
    cluster. Then, we're going to be updating one of the services of this application
    using a zero downtime strategy. Finally, we're going to instrument application
    services running in Kubernetes with sensitive data using secrets. Stay tuned!
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please answer the following questions to assess your learning progress:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Explain in a few short sentences what the role of a Kubernetes master is.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List the elements that need to be present on each Kubernetes (worker) node.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We cannot run individual containers in a Kubernetes cluster.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Yes
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: B. No
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Explain the reason why the containers in a pod can use `localhost` to communicate
    with each other.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of the so-called pause container in a pod?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bob tells you "Our application consists of three Docker images: `web`, `inventory`,
    and `db`. Since we can run multiple containers in a Kubernetes pod, we are going
    to deploy all the services of our application in a single pod." List three to
    four reasons why this is a bad idea.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain in your own words why we need Kubernetes ReplicaSets.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under which circumstances do we need Kubernetes deployments?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List at least three types of Kubernetes service and explain their purposes and
    their differences.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a list of articles that contain more detailed information about the
    various topics that we discussed in this chapter:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: The Raft Consensus Algorithm: [https://raft.github.io/](https://raft.github.io/)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Compose and Kubernetes with Docker for Desktop: [https://dockr.ly/2G8Iqb9](https://dockr.ly/2G8Iqb9)
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Documentation: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
