<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. Monitoring Docker Hosts and Containers</h1></div></div></div><p class="calibre8">We now know some ways to optimize our Docker deployments. We also know how to scale to improve performance. But how do we know that our tuning assumptions were correct? Being able to monitor our Docker infrastructure and application is important to figure out why and when we need to optimize. Measuring how our system is performing allows us to identify its limits to scale and tune accordingly.</p><p class="calibre8">In addition to monitoring low-level information about Docker, it is also important to measure the business-related performance of our application. By tracing the value stream of our application, we can correlate business-related metrics to system-level ones. With this, our Docker development and operations teams can show their business colleagues how Docker saves their organization's costs and increases business value.</p><p class="calibre8">In this chapter, we will cover the following topics about being able to monitor our Docker infrastructure and applications at scale:</p><div><ul class="itemizedlist"><li class="listitem">The importance of monitoring</li><li class="listitem">Collecting monitored data in Graphite</li><li class="listitem">Monitoring Docker with collectd</li><li class="listitem">Consolidating logs in an ELK stack</li><li class="listitem">Sending logs from Docker</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec22" class="calibre1"/>The importance of monitoring</h1></div></div></div><p class="calibre8">Monitoring is <a id="id138" class="calibre1"/>important as it provides a source of feedback on the Docker deployment that we built. It answers several questions about our application from low-level operating system performance to high-level business targets. Having proper instrumentation inserted in our Docker hosts allows us to identify our system's state. We can use this source of feedback to identify whether our application is behaving as originally planned.</p><p class="calibre8">If our initial hypothesis was incorrect, we can use the feedback data to revise our plan and change our system accordingly by tuning our Docker host and containers or updating our running Docker application. We can also use the same monitoring process to identify errors and bugs after our system is deployed to production.</p><p class="calibre8">Docker has built-in features to log and monitor. By default, a Docker host stores a Docker container's standard output and error streams to JSON files in <code class="literal">/var/lib/docker/&lt;container_id&gt;/&lt;container_id&gt;-json.log</code>. The <code class="literal">docker logs</code> command asks the Docker engine daemon to read the content of the files here.</p><p class="calibre8">Another monitoring <a id="id139" class="calibre1"/>facility is the docker stats command. This queries the Docker engine's remote API's <code class="literal">/containers/&lt;container_id&gt;/stats</code> endpoint to report runtime statistics about the running container's control group regarding its CPU, memory, and network usage. The following is an example output of the <code class="literal">docker stats</code> command reporting the said metrics:</p><div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker run --name running –d busybox \
/bin/sh -c 'while true; do echo hello &amp;&amp; sleep 1; done' dockerhost$ docker stats running</strong>
<strong class="calibre2">CONTAINER     CPU %   MEM USAGE/LIMIT  MEM %     NET I/O</strong>
<strong class="calibre2">running       0.00%   0 B/518.5 MB     0.00%     17.06 MB/119.8 kB</strong>
</pre></div><p class="calibre8">The built-in <code class="literal">docker logs</code> and <code class="literal">docker stats</code> commands work well to monitor our Docker applications for development and small-scale deployments. When we get to a point in our production-grade Docker deployment where we manage tens, hundreds, or even thousands of Docker hosts, this approach will no longer be scalable. It is not feasible to log in to each of our thousand Docker hosts and type <code class="literal">docker logs</code> and <code class="literal">docker stats</code>.</p><p class="calibre8">Doing this one by one also makes it difficult to create a more holistic picture of our entire Docker deployment. Also, not everyone interested in our Docker application's performance can log in to our Docker hosts. Our colleagues dealing with only the business aspect of our application may want to ask certain questions on how our application's deployment in Docker improves what our organization wants. However, they do not necessarily want to learn how to log in and start typing Docker commands in our infrastructure.</p><p class="calibre8">Hence, it is important to be able to consolidate all of the events and metrics from our Docker deployments into a centralized monitoring infrastructure. It allows our operations to scale by having a single point to ask what is happening to our system. A centralized dashboard also enables people outside our development and operations team, such as our business colleagues, to have access to the feedback provided by our monitoring system. The remaining sections will show you how to consolidate messages from <code class="literal">docker logs</code> and collect statistics from data sources such as <code class="literal">docker stats</code>.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec23" class="calibre1"/>Collecting metrics to Graphite</h1></div></div></div><p class="calibre8">To begin monitoring <a id="id140" class="calibre1"/>our Docker deployments, we must first set up an endpoint to send our monitored values to. Graphite is a popular stack for collecting various metrics. Its plaintext protocol is very popular because of its simplicity. Many third-party tools understand this simple protocol. Later, we will show you how easy it is to send data to Graphite after we finish setting it up.</p><p class="calibre8">Another feature of graphite is that it can render the data it gathers into graphs. We can then consolidate these graphs to build a dashboard. The dashboard we crafted in the end will show the various kinds of information that we need to monitor our Docker application.</p><p class="calibre8">In this section, we will set up the following components of Graphite to create a minimal stack:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">carbon-cache</strong>: This is the Graphite component that receives metrics over the network. This <a id="id141" class="calibre1"/>implements the simple plaintext protocol described earlier. It can also listen to a binary-based protocol called the pickle protocol, which is a more advanced but smaller and optimized format to receive metrics.</li><li class="listitem"><strong class="calibre2">whisper</strong>: This is a file-based bounded time series database in which the carbon-cache <a id="id142" class="calibre1"/>persists the metrics it receives. Its bounded or fixed-size nature makes it an ideal solution for monitoring. Over time, the metrics that we monitor will accumulate. Hence, the size of our database will just keep increasing so that you will need to monitor it as well! However, in practice we are mostly interested in monitoring our application until a fixed point. Given this assumption, we can plan the resource requirements of our whisper database beforehand and not think about it this much as the operation of our Docker application continues.</li><li class="listitem"><strong class="calibre2">graphite-web</strong>: This <a id="id143" class="calibre1"/>reads the whisper database to render graphs and dashboards. It is also optimized to create such visualizations in real time by querying carbon-cache endpoints to display data that is yet to be persistent in the whisper database as well.</li></ul></div><div><h3 class="title2"><a id="note19" class="calibre1"/>Note</h3><p class="calibre8">There are other components in carbon, such as carbon-aggregator and carbon-relay. These are the components that are needed in order to scale out Graphite effectively as the <a id="id144" class="calibre1"/>number of metrics you measure grows. More information about these components can be found at <a class="calibre1" href="https://github.com/graphite-project/carbon">https://github.com/graphite-project/carbon</a>. For now, we will focus on deploying just the carbon-cache to create a simple Graphite cluster.</p></div><p class="calibre8">The next <a id="id145" class="calibre1"/>few steps describe how to deploy the <a id="id146" class="calibre1"/>carbon-cache and a whisper database:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, prepare a Docker image for carbon to dogfood our Docker host deployments. Create the following <code class="literal">Dockerfile</code> to prepare this image:<div><pre class="programlisting">FROM debian:jessie

RUN apt-get update &amp;&amp; \
    apt-get --no-install-recommends \
        install -y graphite-carbon

ENV GRAPHITE_ROOT /graphite

ADD carbon.conf /graphite/conf/carbon.conf

RUN mkdir -p $GRAPHITE_ROOT/conf &amp;&amp; \
    mkdir -p $GRAPHITE_ROOT/storage &amp;&amp; \
    touch $GRAPHITE_ROOT/conf/storage-aggregation.conf &amp;&amp; \
    touch $GRAPHITE_ROOT/conf/storage-schemas.conf

VOLUME /whisper
EXPOSE 2003 2004 7002

ENTRYPOINT ["/usr/bin/twistd", "--nodaemon", \
    "--reactor=epoll", "--no_save"]
CMD ["carbon-cache"]</pre></div></li><li class="listitem" value="2">Next, build the <code class="literal">Dockerfile</code> we created earlier as the <code class="literal">hubuser/carbon</code> image, as follows:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker build -t hubuser/carbon .</strong>
</pre></div></li><li class="listitem" value="3">Inside the <code class="literal">carbon.conf</code> configuration file, we will configure the carbon-cache to use the <a id="id147" class="calibre1"/>Docker volume <code class="literal">/whisper</code> as the whisper database. The following is the content describing this setting:<div><pre class="programlisting">[cache]

CARBON_METRIC_INTERVAL = 0
LOCAL_DATA_DIR = /whisper</pre></div></li><li class="listitem" value="4">After building the <code class="literal">hubuser/carbon</code> image, we will prepare a whisper database by creating a data container. Type the following command to accomplish this:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker create --name whisper \--entrypoint='whisper database for graphite' \ hubuser/carbon</strong>
</pre></div></li><li class="listitem" value="5">Finally, run the carbon-cache endpoint attached to the data container we created earlier. We will use custom container names and publicly exposed ports so that we can send and read metrics from it, as follows:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker run --volumes-from whisper -p 2003:2003 \--name=carboncache hubuser/carbon</strong>
</pre></div></li></ol><div></div><p class="calibre8">We now have a place to send all our Docker-related metrics that we will gather later. In order to make use of the metrics we stored, we need a way to read and visualize them. We will now deploy <code class="literal">graphite-web</code> to visualize what is going on with our Docker containers. The <a id="id148" class="calibre1"/>following are the steps to deploy graphite-web in our Docker hosts:</p><div><ol class="orderedlist"><li class="listitem" value="1">Build <code class="literal">Dockerfile</code> as <code class="literal">hubuser/graphite-web</code> to prepare a Docker image to deploy <a id="id149" class="calibre1"/>graphite-web via the following code:<div><pre class="programlisting">FROM debian:jessie

RUN apt-get update &amp;&amp; \
    apt-get --no-install-recommends install -y \
        graphite-web \
        apache2 \
        libapache2-mod-wsgi

ADD local_settings.py /etc/graphite/local_settings.py
RUN ln -sf /usr/share/graphite-web/apache2-graphite.conf \
       /etc/apache2/sites-available/100-graphite.conf &amp;&amp; \
    a2dissite 000-default &amp;&amp; a2ensite 100-graphite &amp;&amp; \
    mkdir -p /graphite/storage &amp;&amp; \
    graphite-manage syncdb --noinput &amp;&amp; \
    chown -R _graphite:_graphite /graphite

EXPOSE 80
ENTRYPOINT ["apachectl", "-DFOREGROUND"]</pre></div></li><li class="listitem" value="2">The preceding Docker image refers to <code class="literal">local_settings.py</code> to configure graphite-web. Place the following annotations to link the carbon-cache container and whisper volume:<div><pre class="programlisting">import os
# --link-from carboncache:carbon
CARBONLINK_HOSTS = ['carbon:7002']
# --volumes-from whisper
WHISPER_DIR = '/whisper'

GRAPHITE_ROOT = '/graphite'
SECRET_KEY = os.environ.get('SECRET_KEY', 'replacekey')
LOG_RENDERING_PERFORMANCE = False
LOG_CACHE_PERFORMANCE = False
LOG_METRIC_ACCESS = False
LOG_DIR = '/var/log/graphite'</pre></div></li><li class="listitem" value="3">After preparing the <code class="literal">Dockerfile</code> and <code class="literal">local_settings.py</code> configuration files, build the <code class="literal">hubuser/graphite-web</code> Docker image with the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker build -t hubuser/graphite-web .</strong>
</pre></div></li><li class="listitem" value="4">Finally, run the <code class="literal">hubuser/graphite-web</code> Docker image linked with the carbon-cache <a id="id150" class="calibre1"/>container and whisper volume by typing the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker run --rm --env SECRET_KEY=somestring \--volumes-from whisper --link carboncache:carbon \-p 80:80 hubuser/graphite-web</strong>
</pre></div><div><h3 class="title2"><a id="note20" class="calibre1"/>Note</h3><p class="calibre8">The <code class="literal">SECRET_KEY</code> environment variable is a necessary component to group together multiple graphite-web instances when you decide to scale out. More <a id="id151" class="calibre1"/>graphite-web settings can be found at <a class="calibre1" href="http://graphite.readthedocs.org/en/latest/config-local-settings.html">http://graphite.readthedocs.org/en/latest/config-local-settings.html</a>.</p></div></li></ol><div></div><p class="calibre8">Now that <a id="id152" class="calibre1"/>we have a complete Graphite deployment, we can run some preliminary tests to see it in action. We will test this by populating the whisper database with random data. Type the following command to send random metrics called <code class="literal">local.random</code> to the carbon-cache endpoint:</p><div><pre class="programlisting">
<strong class="calibre2">dockerhost$ seq `date +%s` -60 $((`date +%s` - 24*60*60)) \| perl -n -e \'print "local.random ".  int(rand(100)) . " " .  $_' \| docker run --link carboncache:carbon -i --rm \busybox nc carbon 2003</strong>
</pre></div><p class="calibre8">Finally, confirm that the <a id="id153" class="calibre1"/>data is persistent by visiting our <code class="literal">hubuser/graphite-web</code>'s composer URL <a class="calibre1" href="http://dockerhost/compose">http://dockerhost/compose</a>. Go to the <strong class="calibre2">Tree</strong> tab and then expand the <code class="literal">Graphite/local</code> folder to get the <code class="literal">random</code> metric. The following is a graph that we will see on our graphite-web deployment:</p><div><img src="img/00015.jpeg" alt="Collecting metrics to Graphite" class="calibre10"/></div><p class="calibre11"> </p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec23" class="calibre1"/>Graphite in production</h2></div></div></div><p class="calibre8">In production, this simple <a id="id154" class="calibre1"/>Graphite setup will reach its limits as we monitor more and more metrics in our Docker deployment. We need to scale this out in order to keep up with the increased number of metrics that we monitor. With this, you need to deploy Graphite with a cluster setup.</p><p class="calibre8">To scale out the metric processing capacity of carbon-cache, we need to augment it with a carbon-relay and carbon-aggregator. For graphite-web to be more responsive, we need to scale it out horizontally along with other caching components, such as memcached. We also need to add another graphite-web instance that connects to other graphite-web instances to create a unified view of all the metrics. The whisper databases will be co-located with a carbon-cache and graphite-web, so it will scale-out naturally along with them.</p><div><h3 class="title2"><a id="note21" class="calibre1"/>Note</h3><p class="calibre8">More information <a id="id155" class="calibre1"/>on how to scale out a Graphite cluster in production is found at <a class="calibre1" href="http://graphite.readthedocs.org">http://graphite.readthedocs.org</a>.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec24" class="calibre1"/>Monitoring with collectd</h1></div></div></div><p class="calibre8">We finished <a id="id156" class="calibre1"/>setting up a place to send all our Docker-related data to. Now, it <a id="id157" class="calibre1"/>is time to actually fetch all the data related to our Docker applications. In this section, we will use <code class="literal">collectd</code>, a popular system statistics collection daemon. It is a very lightweight and high-performance C program. This makes it a noninvasive monitoring software because it doesn't consume many resources from the system it monitors. Being lightweight, it is very simple to deploy as it requires minimum dependencies. It has a wide variety of plugins to monitor almost every component of our system.</p><p class="calibre8">Let's begin monitoring our Docker host. Follow the next few steps to install <code class="literal">collectd</code> and send the metrics to our Graphite deployment:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, install <a id="id158" class="calibre1"/><code class="literal">collectd</code> in our Docker host by typing the <a id="id159" class="calibre1"/>following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ apt-get install collectd-core</strong>
</pre></div></li><li class="listitem" value="2">Next, create a minimum <code class="literal">collectd</code> configuration to send data to our Graphite deployment. You may recall from before that we exposed the carbon-cache's default plaintext protocol port (<code class="literal">2003</code>). Write the following configuration entry in <code class="literal">/etc/collectd/collectd.conf</code> to set this up:<div><pre class="programlisting">LoadPlugin "write_graphite"

&lt;Plugin write_graphite&gt;
  &lt;Node "carboncache"&gt;
    Host "dockerhost"
  &lt;/Node&gt;
&lt;/Plugin&gt;</pre></div></li><li class="listitem" value="3">Now, it is time to measure a few things from our Docker host. Load the following <code class="literal">collectd</code> plugins by placing the next few lines in <code class="literal">/etc/collectd/collectd.conf</code>:<div><pre class="programlisting">LoadPlugin "cpu"
LoadPlugin "memory"
LoadPlugin "disk"
LoadPlugin "interface"</pre></div></li><li class="listitem" value="4">After finishing the configuration, restart <code class="literal">collectd</code> by typing the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ systemctl restart collectd.service</strong>
</pre></div></li><li class="listitem" value="5">Finally, let's create a visualization dashboard in our graphite-web deployment to look at the preceding metrics. Go to <a class="calibre1" href="http://dockerhost/dashboard">http://dockerhost/dashboard</a>, click on <strong class="calibre2">Dashboard</strong>, and then on the <strong class="calibre2">Edit Dashboard</strong> link. It will prompt us with a text area to <a id="id160" class="calibre1"/>place a dashboard definition. Paste the following JSON text in this text area to create our preliminary dashboard:<div><pre class="programlisting">[
  {
    "areaMode": "stacked",
    "yMin": "0",
    "target": [
      "aliasByMetric(dockerhost.memory.*)"
    ],
    "title": "Memory"
  },
  {
    "areaMode": "stacked",
    "yMin": "0",
    "target": [
      "aliasByMetric(dockerhost.cpu-0.*)"
    ],
    "title": "CPU"
  }
]</pre></div></li></ol><div></div><p class="calibre8">We now have a basic <a id="id161" class="calibre1"/>monitoring stack for our Docker host. The last step in the previous section will show a dashboard that looks something similar to the following screenshot:</p><div><img src="img/00016.jpeg" alt="Monitoring with collectd" class="calibre10"/></div><p class="calibre11"> </p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec24" class="calibre1"/>Collecting Docker-related data</h2></div></div></div><p class="calibre8">Now, we will <a id="id162" class="calibre1"/>measure some basic metrics that govern the performance of our application. But how do we drill down to further details on the containers running in our Docker hosts? Inside our Debian Jessie Docker host, our containers run under the <code class="literal">docker-[container_id].scope</code> control group. This information is found at <code class="literal">/sys/fs/cgroup/cpu,cpuacct/system.slice</code> in our Docker host's sysfs. Fortunately, <code class="literal">collectd</code> has a <code class="literal">cgroups</code> plugin that interfaces with the sysfs information exposed earlier. The next few steps will show you how to use this plugin to measure the CPU performance of our running Docker containers:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, insert the following lines in <code class="literal">/etc/collectd/collectd.conf</code>:<div><pre class="programlisting">LoadPlugin "cgroups"

&lt;Plugin cgroups&gt;
  CGroup "/^docker.*.scope/"
&lt;/Plugin&gt;</pre></div></li><li class="listitem" value="2">Next, restart collectd by typing the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ systemctl restart collectd.service</strong>
</pre></div></li><li class="listitem" value="3">Finally, wait for a few minutes for Graphite to receive enough metrics from <code class="literal">collectd</code> so that we can get an initial feel to visualize our Docker container's CPU metrics.</li></ol><div></div><p class="calibre8">We can now <a id="id163" class="calibre1"/>check the CPU metrics of our Docker containers by querying for the <code class="literal">dockerhost.cgroups-docker*.*</code> metrics on our Graphite deployment's render API. The following is the image produced by the render API URL <code class="literal">http://dockerhost/render/?target=dockerhost.cgroups-docker-*.*</code>:</p><div><img src="img/00017.jpeg" alt="Collecting Docker-related data" class="calibre10"/></div><p class="calibre11"> </p><div><h3 class="title2"><a id="note22" class="calibre1"/>Note</h3><p class="calibre8">More information <a id="id164" class="calibre1"/>about the <code class="literal">cgroups</code> plugin can be found in the <code class="literal">collectd</code> documentation page at <a class="calibre1" href="https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups">https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_cgroups</a>.</p></div><p class="calibre8">Currently, the <code class="literal">cgroups</code> plugin only measures the CPU metrics of our running Docker containers. There <a id="id165" class="calibre1"/>is some work in progress, but it is not yet ready at the time of this book's writing. Fortunately, there is a Python-based <code class="literal">collectd</code> plugin that interfaces itself to <code class="literal">docker stats</code>. The following are the steps needed to set up this plugin:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, download the following dependencies to be able to run the plugin:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ apt-get install python-pip libpython2.7</strong>
</pre></div></li><li class="listitem" value="2">Next, download and install the plugin from its GitHub page:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ cd /opt
dockerhost$ git clone https://github.com/lebauce/docker-collectd-plugin.gitdockerhost$ cd docker-collectd-plugindockerhost$ pip install -r requirements.txt</strong>
</pre></div></li><li class="listitem" value="3">Add the following lines to <code class="literal">/etc/collectd/collectd.conf</code> to configure the plugin:<div><pre class="programlisting">TypesDB "/opt/docker-collectd-plugin/dockerplugin.db"
LoadPlugin python

&lt;Plugin python&gt;
  ModulePath "/opt/docker-collectd-plugin"
  Import "dockerplugin"

  &lt;Module dockerplugin&gt;
    BaseURL "unix://var/run/docker.sock"
    Timeout 3
  &lt;/Module&gt;
&lt;/Plugin&gt;</pre></div></li><li class="listitem" value="4">Finally, restart <code class="literal">collectd</code> to reflect the preceding configuration changes via the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ systemctl restart collectd.service</strong>
</pre></div></li></ol><div></div><div><h3 class="title2"><a id="note23" class="calibre1"/>Note</h3><p class="calibre8">There is a case in which we don't want to install a whole stack of Python to just query the Docker container stat's endpoint. In this case, we can use the lower-level <code class="literal">curl_json</code> plugin of <code class="literal">collectd</code> to gather statistics about our container. We can configure it to make a request against the container stat's endpoint and parse the resulting JSON into a set of <code class="literal">collectd</code> metrics. More on how the plugin works can be found at <a class="calibre1" href="https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json">https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_curl_json</a>.</p></div><p class="calibre8">In the <a id="id166" class="calibre1"/>following screenshot, we can explore the metrics given out by the <code class="literal">cgroups</code> plugin from our Graphite deployment at <code class="literal">http://docker/compose</code>:</p><div><img src="img/00018.jpeg" alt="Collecting Docker-related data" class="calibre10"/></div><p class="calibre11"> </p><div><div><div><div><h3 class="title2"><a id="ch04lvl3sec03" class="calibre1"/>Running collectd inside Docker</h3></div></div></div><p class="calibre8">If we want to <a id="id167" class="calibre1"/>deploy our <code class="literal">collectd</code> configuration <a id="id168" class="calibre1"/>similarly to our applications, we can run it inside Docker as well. The following is an initial <code class="literal">Dockerfile</code> that we can use to start with deploying <code class="literal">collectd</code> as a running Docker container:</p><div><pre class="programlisting">FROM debian:jessie

RUN apt-get update &amp;&amp; \
    apt-get --no-install-recommends install -y \
        collectd-core

ADD collectd.conf /etc/collectd/collectd.conf
ENTRYPOINT ["collectd", "-f"]</pre></div><div><h3 class="title2"><a id="note24" class="calibre1"/>Note</h3><p class="calibre8">Most plugins look at the <code class="literal">/proc</code> and <code class="literal">/sys</code> filesystems. In order for <code class="literal">collectd</code> inside a Docker container to access these files, we need to mount them as Docker <a id="id169" class="calibre1"/>volumes, such as <code class="literal">--volume /proc:/host/proc</code>. However, most plugins currently read the hardwired <code class="literal">/proc</code> and <code class="literal">/sys</code> paths. There is ongoing discussion to make this configurable. Refer <a id="id170" class="calibre1"/>to this GitHub page to track its progress: <a class="calibre1" href="https://github.com/collectd/collectd/issues/1169">https://github.com/collectd/collectd/issues/1169</a>.</p></div></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec25" class="calibre1"/>Consolidating logs in an ELK stack</h1></div></div></div><p class="calibre8">Not all statuses <a id="id171" class="calibre1"/>of our Docker hosts and containers are readily available to be queried with our monitoring solution in collectd and Graphite. Some events and metrics are only available as raw lines of text in log files. We need to transform these raw and unstructured logs to meaningful metrics. Similar to raw metrics, we can later ask higher-level questions on what is happening in our Docker-based application through analytics.</p><p class="calibre8">The ELK stack is a popular combination suite from Elastic that addresses these problems. Each letter in the acronym represents each of its components. The following is a description of each of them:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Logstash</strong>: Logstash is the component that is used to collect and manage logs and events. It <a id="id172" class="calibre1"/>is the central point that we use to collect all the logs from different log sources, such as multiple Docker hosts and containers running in our deployment. We can also use Logstash to transform and annotate the logs we receive. This allows us to search and explore the richer features of our logs later.</li><li class="listitem"><strong class="calibre2">Elasticsearch</strong>: Elasticsearch is a distributed search engine that is highly scalable. Its <a id="id173" class="calibre1"/>sharding capabilities allow us to grow and scale our log storage as we continue to receive more and more logs from our Docker containers. Its database engine is document-oriented. This allows us to store and annotate logs as we see fit as we continue to discover more insights about the events we are managing in our large Docker deployments.</li><li class="listitem"><strong class="calibre2">Kibana</strong>: Kibana is an analytics and search dashboard for Elasticsearch. Its simplicity allows <a id="id174" class="calibre1"/>us to create dashboards for our Docker applications. However, Kibana is also very flexible to customize, so we can build dashboards that can provide valuable insights to people who want to understand our Docker-based applications, whether it is a low-level technical detail or higher-level business need.</li></ul></div><p class="calibre8">In the remaining parts of this section, we will set up each of these components and send our Docker host and <a id="id175" class="calibre1"/>container logs to it. The next few steps describe how to build the ELK stack:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, launch the official Elasticsearch image in our Docker host. We will put a container name so that we can link it easily in the later steps, as follows:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker run -d --name=elastic elasticsearch:1.7.1</strong>
</pre></div></li><li class="listitem" value="2">Next, we will run Kibana's official Docker image by linking it against the Elasticsearch container we created in the previous step. Note that we publicly mapped the exposed port <code class="literal">5601</code> to port <code class="literal">80</code> in our Docker host so that the URL for Kibana is prettier, as follows:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker run -d --link elastic:elasticsearch \-p 80:5601 kibana:4.1.1</strong>
</pre></div></li><li class="listitem" value="3">Now, prepare our Logstash Docker image and configuration. Prepare the following <code class="literal">Dockerfile</code> to create the Docker image:<div><pre class="programlisting">FROM logstash:1.5.3

ADD logstash.conf /etc/logstash.conf
EXPOSE 1514/udp</pre></div></li><li class="listitem" value="4">In this Docker image, configure Logstash as a Syslog server. This explains the exposed UDP port in the preceding <code class="literal">Dockerfile</code>. As for the <code class="literal">logstash.conf</code> file, the following is the basic configuration to make it listen as a Syslog server. The latter part of the configuration shows that it sends logs to an Elasticsearch called <code class="literal">elasticsearch</code>. We will use this as the hostname when we link the Elasticsearch container we ran earlier:<div><pre class="programlisting">input {
  syslog {
    port =&gt; 1514
    type =&gt; syslog
  }
}

output {
  elasticsearch {
    host =&gt; "elasticsearch"
  }
}</pre></div><div><h3 class="title2"><a id="tip04" class="calibre1"/>Tip</h3><p class="calibre8">Logstash has a wealth of plugins so that it can read a wide variety of log data sources. In particular, it has a <code class="literal">collectd</code> codec plugin. With this, we can use an ELK stack instead of Graphite to monitor our metrics.</p><p class="calibre8">For more information on how to do this setup, visit <a class="calibre1" href="https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html">https://www.elastic.co/guide/en/logstash/current/plugins-codecs-collectd.html</a>.</p></div></li><li class="listitem" value="5">Now that we have prepared all the files needed, type the following command to create it as the <code class="literal">hubuser/logstash</code> Docker image:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker build -t hubuser/logstash .</strong>
</pre></div></li><li class="listitem" value="6">Run Logstash <a id="id176" class="calibre1"/>with the following command. Note that we are exposing port <code class="literal">1514</code> to the Docker host as the Syslog port. We also linked the Elasticsearch container named <code class="literal">elastic</code> that we created earlier. The target name is set to <code class="literal">elasticsearch</code> as it is the hostname of Elasticsearch that we configured earlier in <code class="literal">logstash.conf</code> to send the logs to:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker run --link elastic:elasticsearch -d \-p 1514:1514/udp hubuser/logstash -f /etc/logstash.conf</strong>
</pre></div></li><li class="listitem" value="7">Next, let's configure our Docker host's Syslog service to forward it to our Logstash container. As a basic configuration, we can set up Rsyslog to forward all the logs. This will include the logs coming from the Docker engine daemon as well. To do this, create the <code class="literal">/etc/rsyslog.d/100-logstash.conf</code> file with the following content:<div><pre class="programlisting">*.* @dockerhost:1514</pre></div></li><li class="listitem" value="8">Finally, restart Syslog to load the changes in the previous step by typing the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ systemctl restart rsyslog.service</strong>
</pre></div></li></ol><div></div><p class="calibre8">We now have a basic functioning ELK stack. Let's now test it by sending a message to Logstash and seeing it appear in our Kibana dashboard:</p><div><ol class="orderedlist"><li class="listitem" value="1">First, type the following command to send a test message:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ logger -t test 'message to elasticsearch'</strong>
</pre></div></li><li class="listitem" value="2">Next, go to our Kibana dashboard by visiting <code class="literal">http://dockerhost</code>. Kibana will now ask us to set the default index. Use the following default values and click on <strong class="calibre2">Create</strong> to start indexing:<div><img src="img/00019.jpeg" alt="Consolidating logs in an ELK stack" class="calibre10"/></div><p class="calibre14"> </p></li><li class="listitem" value="3">Go to <a id="id177" class="calibre1"/><code class="literal">http://dockerhost/#discover</code> and type <code class="literal">elasticsearch</code> in the search. The following screenshot shows the Syslog message we generated earlier:<div><img src="img/00020.jpeg" alt="Consolidating logs in an ELK stack" class="calibre10"/></div><p class="calibre14"> </p><div><h3 class="title2"><a id="note25" class="calibre1"/>Note</h3><p class="calibre8">There are <a id="id178" class="calibre1"/>lot more things we can do on the ELK stack to optimize our logging infrastructure. We can add Logstash plugins and filters to annotate the logs we receive from our Docker hosts and containers. Elasticsearch can be scaled out and tuned to increase its capacity as our logging needs increase. We can create Kibana dashboards to share metrics. To find out more details on how to tune our ELK stack, visit Elastic's guides at <a class="calibre1" href="https://www.elastic.co/guide">https://www.elastic.co/guide</a>.</p></div></li></ol><div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec26" class="calibre1"/>Forwarding Docker container logs</h1></div></div></div><p class="calibre8">Now that we have <a id="id179" class="calibre1"/>a basic functional ELK stack, we can start forwarding our Docker logs to it. From Docker 1.7 onwards, support for custom logging drivers has been available. In this section, we will configure our Docker host to use the syslog driver. By default, Syslog events from Docker will go to the Docker host's Syslog service and since we configured Syslog to forward to our ELK stack, we will see the container logs there. Follow the next few steps to start receiving our container logs in the ELK stack:</p><div><ol class="orderedlist"><li class="listitem" value="1">The Docker engine service is configured via Systemd on our Debian Jessie host. To update how it runs in our Docker host, create a Systemd unit file called<code class="literal"> /etc/systemd/system/docker.service.d/10-syslog.conf</code> with the following content:<div><pre class="programlisting">[Service]
ExecStart=
ExecStart=/usr/bin/docker daemon -H fd:// \
    --log-driver=syslog</pre></div></li><li class="listitem" value="2">Apply the changes on how we will run Docker in our host by reloading the Systemd configuration. The following command will do this:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ systemctl daemon-reload</strong>
</pre></div></li><li class="listitem" value="3">Finally, restart the Docker engine daemon by issuing the following command:<div><pre class="programlisting">
<strong class="calibre2">dockerhost$ systemctl restart docker.service</strong>
</pre></div></li><li class="listitem" value="4">Optionally, apply any Logstash filtering if we want to do custom annotations on our Docker container's logs.</li></ol><div></div><p class="calibre8">Now, any standard output and error streams coming out from our Docker container should be captured to our ELK stack. We can do some preliminary tests to confirm that the setup works. Type the following command to create a test message from Docker:</p><div><pre class="programlisting">
<strong class="calibre2">dockerhost$ docker run --rm busybox echo message to elk</strong>
</pre></div><div><h3 class="title2"><a id="note26" class="calibre1"/>Note</h3><p class="calibre8">The <code class="literal">docker run</code> command also supports the <code class="literal">--log-driver</code> and <code class="literal">--log-opt=[]</code> command-line options to set up the logging driver only for the container we want to run. We can use it to further tune our logging policies for each Docker container running in our Docker host.</p></div><p class="calibre8">After typing the preceding command, our message should now be stored in Elasticsearch. Let's go to our Kibana endpoint in <code class="literal">http://dockerhost</code>, and search for the word <code class="literal">message to elk</code> in the textbox. It should give the Syslog entry for the message we sent earlier. The following screenshot is what the search result should look like in our Kibana results:</p><div><img src="img/00021.jpeg" alt="Forwarding Docker container logs" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the preceding screenshot, we can see the message we sent. There is also other information about Syslog. Docker's Syslog driver sets the default syslog annotations on facility and severity as <strong class="calibre2">system</strong> and <strong class="calibre2">informational</strong>, respectively. In addition, the preceding program is set to <strong class="calibre2">docker/c469a2dfdc9a</strong>.</p><p class="calibre8">The <strong class="calibre2">c469a2dfdc9a</strong> <a id="id180" class="calibre1"/>string is the container ID of the busybox image we ran earlier. The default program label for Docker containers is set in the <code class="literal">docker/&lt;container-id&gt;</code> format. All of the preceding default annotations can be configured by passing arguments to the <code class="literal">--log-opt=[]</code> option.</p><div><h3 class="title2"><a id="note27" class="calibre1"/>Note</h3><p class="calibre8">Aside from the Syslog and JSON file-logging drivers, Docker supports several other endpoints to send logs to. More information about all the logging drivers and their respective usage guides can be found in <a class="calibre1" href="https://docs.docker.com/reference/logging">https://docs.docker.com/reference/logging</a>.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec27" class="calibre1"/>Other monitoring and logging solutions</h1></div></div></div><p class="calibre8">There are <a id="id181" class="calibre1"/>several other solutions for us to deploy to monitor and log <a id="id182" class="calibre1"/>infrastructure to support our Docker-based application. Some of them already have built-in support for monitoring Docker containers. Others should be combined with other solutions, such as the ones we showed previously because they only focus on a specific part of monitoring or logging.</p><p class="calibre8">With others, we may have to do some workarounds. However, their benefits clearly outweigh the compromise we have to make. While the following list is not exhaustive, these are a few stacks we can explore to create our logging and monitoring solutions:</p><div><ul class="itemizedlist"><li class="listitem">cAdvisor <a id="id183" class="calibre1"/>(<a class="calibre1" href="http://github.com/google/cadvisor">http://github.com/google/cadvisor</a>)</li><li class="listitem">InfluxDB <a id="id184" class="calibre1"/>(<a class="calibre1" href="http://influxdb.com">http://influxdb.com</a>)</li><li class="listitem">Sensu <a id="id185" class="calibre1"/>(<a class="calibre1" href="http://sensuapp.org">http://sensuapp.org</a>)</li><li class="listitem">Fluentd <a id="id186" class="calibre1"/>(<a class="calibre1" href="http://www.fluentd.org/">http://www.fluentd.org/</a>)</li><li class="listitem">Graylog <a id="id187" class="calibre1"/>(<a class="calibre1" href="http://www.graylog.org">http://www.graylog.org</a>)</li><li class="listitem">Splunk <a id="id188" class="calibre1"/>(<a class="calibre1" href="http://www.splunk.com">http://www.splunk.com</a>)</li></ul></div><p class="calibre8">Sometimes, our operations staff and developers running and developing our Docker applications are not yet mature enough or do not want to focus on maintaining such monitoring and logging infrastructures. There are several hosted monitoring and logging platforms that we can use so that we can focus on actually writing and improving the performance of our Docker application.</p><p class="calibre8">Some of them work with existing monitoring and logging agents, such as Syslog and collectd. With others, we may have to download and deploy their agents to be able to forward the events <a id="id189" class="calibre1"/>and metrics to their hosted platform. The following is a <a id="id190" class="calibre1"/>nonexhaustive list of some solutions we may want to consider:</p><div><ul class="itemizedlist"><li class="listitem">New Relic <a id="id191" class="calibre1"/>(<a class="calibre1" href="http://www.newrelic.com">http://www.newrelic.com</a>)</li><li class="listitem">Datadog <a id="id192" class="calibre1"/>(<a class="calibre1" href="http://www.datadoghq.com">http://www.datadoghq.com</a>)</li><li class="listitem">Librato <a id="id193" class="calibre1"/>(<a class="calibre1" href="http://www.librato.com">http://www.librato.com</a>)</li><li class="listitem">Elastic's Found <a id="id194" class="calibre1"/>(<a class="calibre1" href="http://www.elastic.co/found">http://www.elastic.co/found</a>)</li><li class="listitem">Treasure <a id="id195" class="calibre1"/>Data (<a class="calibre1" href="http://www.treasuredata.com">http://www.treasuredata.com</a>)</li><li class="listitem">Splunk <a id="id196" class="calibre1"/>Cloud (<a class="calibre1" href="http://www.splunk.com">http://www.splunk.com</a>)</li></ul></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec28" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">We now know why it is important to monitor our Docker deployments in a scalable and accessible manner. We deployed collectd and Graphite to monitor our Docker container's metrics. We rolled out an ELK stack to consolidate the logs coming from various Docker hosts and containers.</p><p class="calibre8">In addition to raw metrics and events, it is also important to know what it means for our application. Graphite-web and Kibana allow us to create custom dashboards and analysis to provide insight in to our Docker applications. With these monitoring tools and skills in our arsenal, we should be able to operate and run our Docker deployments well in production.</p><p class="calibre8">In the next chapter, we will start doing performance tests and benchmark how our Docker applications fare well with a high load. We should be able to use the monitoring systems we deployed to observe and validate our performance testing activities there.</p></div></body></html>