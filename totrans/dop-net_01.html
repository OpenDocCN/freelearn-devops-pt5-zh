<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;The Impact of Cloud on Networking"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. The Impact of Cloud on Networking</h1></div></div></div><p>This chapter will look at ways that networking has changed in the private data centers and evolved in the past few years. It will focus on the emergence of <span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) for public cloud and <span class="strong"><strong>OpenStack</strong></span> for private cloud and ways in which this has changed the way developers want to consume networking. It will look at some of the networking services that AWS and OpenStack provide out of the box and look at some of the features they provide. It will show examples of how these cloud platforms have made networking a commodity much like infrastructure.</p><p>In this chapter, the following topics will be covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An overview of cloud approaches</li><li class="listitem" style="list-style-type: disc">The difference between Spanning Tree networks and Leaf-Spine networking</li><li class="listitem" style="list-style-type: disc">Changes that have occurred in networking with the introduction of public cloud</li><li class="listitem" style="list-style-type: disc">The Amazon Web Services approach to networking</li><li class="listitem" style="list-style-type: disc">The OpenStack approach to networking</li></ul></div><div class="section" title="An overview of cloud approaches"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>An overview of cloud approaches</h1></div></div></div><p>The cloud <a id="id0" class="indexterm"/>provider market is currently saturated with a multitude of different private, public, and hybrid cloud solutions, so choice is not a problem for companies looking to implement public, private, or hybrid cloud solutions.</p><p>Consequently, choosing a cloud solution can sometimes be quite a daunting task, given the array of different options that are available.</p><p>The battle between public and private cloud is still in its infancy, with only around 25 percent of the industry using public cloud, despite its perceived popularity, with solutions such as Amazon Web Services, Microsoft Azure, and Google Cloud taking a large majority of that market share. However, this still means that 75 percent of the cloud market share is available to be captured, so the cloud computing market will likely go through many iterations in the coming years.</p><p>So why are many companies considering public cloud in the first place and why does it differ <a id="id1" class="indexterm"/>from private and hybrid clouds?</p><div class="section" title="Public clouds"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec06"/>Public clouds</h2></div></div></div><p>Public clouds <a id="id2" class="indexterm"/>are essentially a set of data centers and infrastructure that are made publicly available over the Internet to consumers. Despite its <a id="id3" class="indexterm"/>name, it is not magical or fluffy in any way. Amazon Web Services launched their public cloud based on the idea that they could rent out their servers to other companies when they were not using them during busy periods of the year.</p><p>Public cloud <a id="id4" class="indexterm"/>resources can be accessed via a <span class="strong"><strong>Graphical User Interface</strong></span> (<span class="strong"><strong>GUI</strong></span>) or, programmatically, via a set of API endpoints. This allows end users of the public cloud to create infrastructure and networking to host their applications.</p><p>Public clouds are used by businesses for various reasons, such as the speed it takes to configure and using public cloud resources is relatively low. Once credit card details have been provided on a public cloud portal, end users have the freedom to create their own infrastructure and networking, which they can run their applications on.</p><p>This infrastructure can be elastically scaled up and down as required, all at a cost of course to the credit card.</p><p>Public <a id="id5" class="indexterm"/>cloud has become very popular as it removes a set of historical impediments associated with <span class="strong"><strong>shadow IT</strong></span>. Developers are no longer hampered by the restrictions enforced upon them by bureaucratic and slow internal IT processes. Therefore, many businesses are seeing public cloud as a way to skip over these impediments and work in a more agile fashion allowing them to deliver new products to market at a greater frequency.</p><p>When a business moves its operations to a public cloud, they are taking the bold step to stop hosting their own data centers and instead use a publicly available public cloud provider, such as Amazon Web Services, Microsoft Azure, IBM BlueMix, Rackspace, or Google Cloud.</p><p>The reliance <a id="id6" class="indexterm"/>is then put upon the public cloud for uptime and <span class="strong"><strong>Service Level Agreements</strong></span> (<span class="strong"><strong>SLA</strong></span>), which can be a huge cultural shift for an established business.</p><p>Businesses that have moved to public cloud may find they no longer have a need for a large internal infrastructure team or network team, instead all infrastructure and networking is provided by the third-party public cloud, so it can in some quarters be viewed as giving up on internal IT.</p><p>Public cloud has proved a very successful model for many start-ups, given the agility it provides, where start-ups can put out products quickly using software-defined constructs without <a id="id7" class="indexterm"/>having to set up their own data center and remain product focused.</p><p>However, the <span class="strong"><strong>Total Cost of Ownership</strong></span> (<span class="strong"><strong>TCO</strong></span>) to run all of a business's infrastructure in a public cloud is a hotly debated topic, which can be an expensive model if it isn't managed <a id="id8" class="indexterm"/>and maintained correctly. The debate over public versus private cloud TCO rages on as some argue that public cloud is a great short-term fix <a id="id9" class="indexterm"/>but growing costs over a long period of time mean that it may not be a viable long-term solution compared with private cloud.</p></div><div class="section" title="Private cloud"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Private cloud</h2></div></div></div><p>Private <a id="id10" class="indexterm"/>cloud is really just an extension of the initial benefits introduced by virtualization solutions, such as VMware, Hyper-V, and Citrix Xen, which were the cornerstone of the virtualization market. The private cloud world has <a id="id11" class="indexterm"/>moved on from just providing virtual machines, to providing software-defined networking and storage.</p><p>With the launch of public clouds, such as Amazon Web Services, private cloud solutions have sought to provide like-for-like capability by putting a software-defined layer on top of their current infrastructure. This infrastructure can be controlled in the same way as the public cloud via a GUI or programmatically using APIs.</p><p>Private cloud solutions such as Apache CloudStack and open source solutions such as OpenStack have been created to bridge the gap between the private cloud and the public cloud.</p><p>This has allowed vendors the agility of private cloud operations in their own data center by overlaying software-defined constructs on top of their existing hardware and networks.</p><p>However, the major benefit of private cloud is that this can be done within the security of a company's own data centers. Not all businesses can use public cloud for compliance, regularity, or performance reasons, so private cloud is still required for some businesses for particular workloads.</p></div><div class="section" title="Hybrid cloud"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>Hybrid cloud</h2></div></div></div><p>Hybrid cloud <a id="id12" class="indexterm"/>can often be seen as an amalgamation of multiple <a id="id13" class="indexterm"/>clouds. This allows a business to seamlessly run workloads across multiple clouds linked together by a network fabric. The business could select the placement of workloads based on cost or performance metrics.</p><p>A hybrid cloud can often be made up of private and public clouds. So, as an example, a business may have a set of web applications that it wishes to scale up for particular busy periods and are better suited to run on public cloud so they are placed there. However, the business <a id="id14" class="indexterm"/>also needs a highly regulated, PCI-compliant database, which would be better-suited to being deployed in a private on-premises cloud. So a true hybrid <a id="id15" class="indexterm"/>cloud gives a business these kinds of options and flexibility.</p><p>Hybrid cloud really works on the premise of using different clouds for different use cases, where each horse (application workload) needs to run a particular course (cloud). So, sometimes, a vendor-provided <span class="strong"><strong>Platform as a Service</strong></span> (<span class="strong"><strong>PaaS</strong></span>) layer can be used to place <a id="id16" class="indexterm"/>workloads across multiple clouds or alternately different configuration management tools, or container orchestration technologies can be used to orchestrate application workload placement across clouds.</p></div><div class="section" title="Software-defined"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Software-defined</h2></div></div></div><p>The choice <a id="id17" class="indexterm"/>between public, private, or hybrid cloud really depends on the business, so there is no real right or wrong answer. Companies will likely use hybrid cloud models as their culture and processes evolve <a id="id18" class="indexterm"/>over the next few years.</p><p>If a business is using a public, private, or hybrid cloud, the common theme with all implementations is that they are moving towards a software-defined operational model.</p><p>So what does the term <span class="emphasis"><em>software-defined</em></span> really mean? In simple terms, <span class="emphasis"><em>software-defined</em></span> means running a software abstraction layer over hardware. This software abstraction layer allows graphical or programmatic control of the hardware. So, constructs, such as infrastructure, storage, and networking, can be software defined to help simplify operations, manageability as infrastructure and networks scale out.</p><p>When running private clouds, modifications need to be made to incumbent data centers to make them private cloud ready; sometimes, this is important, so the private data center needs to evolve to meet those needs.</p></div></div></div>
<div class="section" title="The difference between Spanning Tree and Leaf-Spine networking"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>The difference between Spanning Tree and Leaf-Spine networking</h1></div></div></div><p>When <a id="id19" class="indexterm"/>considering <a id="id20" class="indexterm"/>the private cloud, traditionally, company's private datacenters have implemented 3-tier <a id="id21" class="indexterm"/>layer 2 networks based on the <span class="strong"><strong>Spanning Tree Protocol</strong></span> (<span class="strong"><strong>STP</strong></span>), which doesn't lend itself <a id="id22" class="indexterm"/>well to <a id="id23" class="indexterm"/>modern software-defined networks. So, we will look at what a STP is in more depth as well as modern Leaf-Spine network architectures.</p><div class="section" title="Spanning Tree Protocol"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Spanning Tree Protocol</h2></div></div></div><p>The implementation <a id="id24" class="indexterm"/>of STP provides a number of options for network architects in terms of implementation, but it also adds a layer of complexity to the network. Implementation of the STP gives network architects the certainty that it will prevent layer 2 loops from occurring in the network.</p><p>A typical representation of a 3-tier layer 2 STP-based network can be shown as follows:</p><div class="mediaobject"><img src="graphics/B05559_01_01.jpg" alt="Spanning Tree Protocol"/></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="strong"><strong>Core</strong></span> layer provides routing services to other parts of the data center and contains the core switches</li><li class="listitem" style="list-style-type: disc">The <span class="strong"><strong>Aggregation</strong></span> layer provides connectivity to adjacent <span class="strong"><strong>Access</strong></span> layer switches and the top of the Spanning Tree core</li></ul></div><p>The bottom of the tree is the <span class="strong"><strong>Access</strong></span> layer; this is where bare metal (physical) or virtual machines connect to the network and are segmented using different VLANs.</p><p>The use of layer 2 networking and STP mean that at the access layer of the network will use VLANs spread throughout the network. The VLANs sit at the access layer, which is where virtual machines or bare metal servers are connected. Typically, these VLANs are grouped by type of application, and firewalls are used to further isolate and secure them.</p><p>Traditional networks are normally segregated into some combination of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Frontend</strong></span>: It typically has web servers that require external access</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Business Logic</strong></span>: This often contains stateful services</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Backend</strong></span>: This typically contains database servers</li></ul></div><p>Applications communicate with each other by tunneling between these firewalls, with specific <span class="strong"><strong>Access Control List</strong></span> (<span class="strong"><strong>ACL</strong></span>) rules that are serviced by network teams and governed by security teams.</p><p>When using STP in a layer 2 network, all switches go through an election process to determine the root switch, which is granted to the switch with the lowest bridge id, with a bridge id encompassing the bridge priority and MAC address of the switch.</p><p>Once elected, the root switch becomes the base of the spanning tree; all other switches in the Spanning Tree are deemed non-root will calculate their shortest path to the root and then block any redundant links, so there is one clear path. The calculation process to work out the shortest path is referred to as network convergence. (For more information refer to the following link: <a class="ulink" href="http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/">http://etutorials.org/Networking/Lan+switching+fundamentals/Chapter+10.+Implementing+and+Tuning+Spanning+Tree/Spanning-Tree+Convergence/</a>)</p><p>Network architects designing the layer 2 Spanning Tree network need to be careful about the placement of the root switch, as all network traffic will need to flow through it, so it should be selected with care and given an appropriate bridge priority as part of the network reference architecture design. If at any point, switches have been given the same bridge priority then the bridge with the lowest MAC address wins.</p><p>Network architects should also design the network for redundancy so that if a root switch fails, there is a nominated backup root switch with a priority of one value less than the nominated root switch, which will take over when a root switch fails. In the scenario, the root switch fails the election process will begin again and the network will converge, which can take some time.</p><p>The use of STP is not without its risks, if it does fail due to user configuration error, data center equipment failure or software failure on a switch or bad design, then the consequences to a network can be huge. The result can be that loops might form within the bridged network, which can result in a flood of broadcast, multicast or unknown-unicast storms that can potentially take down the entire network leading to long network outages. The complexity associated with network architects or engineers troubleshooting STP issues is important, so it is paramount that the network design is sound.</p></div><div class="section" title="Leaf-Spine architecture"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Leaf-Spine architecture</h2></div></div></div><p>In recent <a id="id25" class="indexterm"/>years with the emergence of cloud computing, we have seen data centers move away from a STP in favor of a Leaf-Spine networking architecture. The Leaf-Spine architecture is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B05559_01_02.jpg" alt="Leaf-Spine architecture"/></div><p>In a Leaf-Spine architecture:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Spine switches are connected into a set of core switches</li><li class="listitem" style="list-style-type: disc">Spine switches are then connected with Leaf switches with each Leaf switch deployed at the top of rack, which means that any Leaf switch can connect to any Spine switch in one hop</li></ul></div><p>Leaf-Spine architectures are promoted by companies such as Arista, Juniper, and Cisco. A Leaf-Spine architecture is built on layer 3 routing principle to optimize throughput and reduce latency.</p><p>Both Leaf <a id="id26" class="indexterm"/>and Spine switches communicate with each other via <span class="strong"><strong>external Border Gate Protocol</strong></span> (<span class="strong"><strong>eBGP</strong></span>) as the routing <a id="id27" class="indexterm"/>protocol for the IP fabric. eBGP establishes a <span class="strong"><strong>Transmission Control Protocol</strong></span> (<span class="strong"><strong>TCP</strong></span>) connection to each of its BGP peers before BGP updates can be exchanged between the switches. Leaf switches in the implementation will sit at top of rack and can be configured in <span class="strong"><strong>Multichassis Link Aggregation</strong></span> (<span class="strong"><strong>MLAG</strong></span>) mode using <span class="strong"><strong>Network Interface Controller</strong></span> (<span class="strong"><strong>NIC</strong></span>) bonding.</p><p>MLAG was originally used with <span class="strong"><strong>STP</strong></span> so that two or more switches are bonded to emulate like a single <a id="id28" class="indexterm"/>switch and used for redundancy so they appeared as one switch to STP. In the event of a failure this provided multiple uplinks for redundancy <a id="id29" class="indexterm"/>in the event of a failure as the switches are peered, and it worked around the need to disable redundant paths. Leaf switches <a id="id30" class="indexterm"/>can often have <span class="strong"><strong>internal Border Gate Protocol</strong></span> (<span class="strong"><strong>iBGP</strong></span>) configured between the pairs of switches for resiliency.</p><p>In a Leaf-Spine architecture, Spine switches do not connect to other Spine switches, and Leaf switches do not connect directly to other Leaf switches unless bonded top of rack using MLAG NIC bonding. All links in a Leaf-Spine architecture are set up to forward with no <a id="id31" class="indexterm"/>looping. Leaf-Spine architectures are typically configured to implement <span class="strong"><strong>Equal Cost Multipathing</strong></span> (<span class="strong"><strong>ECMP</strong></span>), which allows all routes to be configured on the switches so that they can access any Spine switch in the layer 3 routing fabric.</p><p>ECMP means that Leaf switches routing table has the next-hop configured to forward to each Spine switch. In an ECMP setup, each leaf node has multiple paths of equal distance to each Spine switch, so if a Spine or Leaf switch fails, there is no impact as long as there are other active paths to another adjacent Spine switches. ECMP is used to load balance flows and <a id="id32" class="indexterm"/>supports the routing of traffic across multiple paths. This is in contrast to the STP, which switches off all but one path to the root when the network converges.</p><p>Normally, Leaf-Spine architectures designed for high performance use 10G access ports at Leaf switches mapping to 40G Spine ports. When device port capacity becomes an issue, new Leaf switches can be added by connecting it to every Spine on the network while pushing the new configuration to every switch. This means that network teams can easily scale out the network horizontally without managing or disrupting the switching protocols or impacting the network performance.</p><p>An illustration of the protocols used in a Leaf-Spine architecture are shown later, with Spine switches connected to Leaf switches using BGP and ECMP and Leaf switches sitting top of rack and configured for redundancy using MLAG and iBGP:</p><div class="mediaobject"><img src="graphics/B05559_01_02A.jpg" alt="Leaf-Spine architecture"/></div><p>The benefits <a id="id33" class="indexterm"/>of a Leaf-Spine architecture are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Consistent latency and throughput in the network</li><li class="listitem" style="list-style-type: disc">Consistent performance for all racks</li><li class="listitem" style="list-style-type: disc">Network once configured becomes less complex</li><li class="listitem" style="list-style-type: disc">Simple scaling of new racks by adding new Leaf switches at top of rack</li><li class="listitem" style="list-style-type: disc">Consistent performance, subscription, and latency between all racks</li><li class="listitem" style="list-style-type: disc">East-west traffic performance is optimized (virtual machine to virtual machine communication) to support microservice applications</li><li class="listitem" style="list-style-type: disc">Removes VLAN scaling issues, controls broadcast and fault domains</li></ul></div><p>The one <a id="id34" class="indexterm"/>drawback of a Leaf-Spine topology is the amount of cables it consumes in the data center.</p></div><div class="section" title="OVSDB"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>OVSDB</h2></div></div></div><p>Modern <a id="id35" class="indexterm"/>switches have now moved towards open source standards, so they can use the same pluggable <a id="id36" class="indexterm"/>framework. The <a id="id37" class="indexterm"/>open standard for virtual switches is <span class="strong"><strong>Open vSwitch</strong></span>, which <a id="id38" class="indexterm"/>was born out of the necessity to come up with an open standard that allowed a virtual switch to forward traffic to different virtual machines on the same physical host and physical network. Open vSwitch uses <span class="strong"><strong>Open vSwitch database</strong></span> (<span class="strong"><strong>OVSDB</strong></span>) that has a standard extensible schema.</p><p>Open vSwitch <a id="id39" class="indexterm"/>was initially deployed <a id="id40" class="indexterm"/>at the hypervisor level but is now being used in container technology too, which has Open vSwitch implementations for networking.</p><p>The <a id="id41" class="indexterm"/>following hypervisors currently implement Open vSwitch as their virtual switching technology:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">KVM</li><li class="listitem" style="list-style-type: disc">Xen</li><li class="listitem" style="list-style-type: disc">Hyper-V</li></ul></div><p>
<span class="strong"><strong>Hyper-V</strong></span> has <a id="id42" class="indexterm"/>recently moved to support Open vSwitch using the implementation created by Cloudbase (<a class="ulink" href="https://cloudbase.it/">https://cloudbase.it/</a>), which is doing some fantastic <a id="id43" class="indexterm"/>work in the open source space and is testament to how Microsoft's business model has evolved and embraced open source technologies and standards in recent years. Who would have thought it? Microsoft technologies now run natively on Linux.</p><p>The Open vSwitch <a id="id44" class="indexterm"/>exchanges <span class="strong"><strong>OpenFlow</strong></span> between virtual switch and physical switches in order to communicate and can be programmatically extended to fit the needs of vendors. In the following diagram, you can see the Open vSwitch architecture. Open vSwitch can run on a server using the KVM, Xen, or Hyper-V virtualization layer:</p><div class="mediaobject"><img src="graphics/B05559_01_03.jpg" alt="OVSDB"/></div><p>The <span class="strong"><strong>ovsdb-server</strong></span> contains <a id="id45" class="indexterm"/>the OVSDB schema that holds all switching <a id="id46" class="indexterm"/>information for the virtual switch. The <span class="strong"><strong>ovs-vswitchd</strong></span> daemon talks <span class="strong"><strong>OpenFlow</strong></span> to any <span class="strong"><strong>Control &amp; Management Cluster,</strong></span> which could be any SDN controller that can communicate using the <span class="strong"><strong>OpenFlow</strong></span> protocol.</p><p>Controllers use OpenFlow to install flow state on the virtual switch, and OpenFlow dictates what actions to take when packets are received by the virtual switch.</p><p>When Open <a id="id47" class="indexterm"/>vSwitch receives a packet it has never seen <a id="id48" class="indexterm"/>before and has no matching flow entries, it sends this packet to the controller. The controller then makes a decision <a id="id49" class="indexterm"/>on how to handle this packet based on the flow rules to either block or forward. The ability to <a id="id50" class="indexterm"/>configure <span class="strong"><strong>Quality of Service</strong></span> (<span class="strong"><strong>QoS</strong></span>) and other statistics is possible on Open vSwitch.</p><p>Open <a id="id51" class="indexterm"/>vSwitch is used to configure security rules and provision ACL rules at the switch level on a hypervisor.</p><p>A Leaf-Spine architecture allows overlay networks to be easily built, meaning that cloud and tenant environments are easily connected to the layer 3 routing fabric. Hardware <span class="strong"><strong>Vxlan Tunnel Endpoints</strong></span> (<span class="strong"><strong>VTEPs</strong></span>) IPs are associated with each Leaf switch or a pair of Leaf switches in MLAG mode and are connected to each physical compute host via <span class="strong"><strong>Virtual Extensible LAN</strong></span> (<span class="strong"><strong>VXLAN</strong></span>) to each Open vSwitch that is installed on a hypervisor.</p><p>This allows <a id="id52" class="indexterm"/>an SDN controller, which is provided by vendors, such as Cisco, Nokia, and Juniper to build an overlay network that creates VXLAN <a id="id53" class="indexterm"/>tunnels to the physical hypervisors using Open vSwitch. New VXLAN tunnels are created automatically if a new compute <a id="id54" class="indexterm"/>is scaled out, then SDN controllers can create new VXLAN tunnels on the Leaf switch as they are peered with the Leaf switch's hardware <span class="strong"><strong>VXLAN Tunnel End Point</strong></span> (<span class="strong"><strong>VTEP</strong></span>).</p><p>Modern <a id="id55" class="indexterm"/>switch vendors, such as Arista, Cisco, Cumulus, and many others, use OVSDB, and this allows <a id="id56" class="indexterm"/>SDN controllers to integrate at the <span class="strong"><strong>Control &amp; Management Cluster</strong></span> level. As long as an SDN controller <a id="id57" class="indexterm"/>uses OVSDB and OpenFlow protocol, they can seamlessly integrate with the switches and are not tied into specific vendors. This gives end users a greater depth of choice when choosing switch vendors and SDN controllers, which can be matched up as they communicate using the same open standard protocol.</p></div></div>
<div class="section" title="Changes that have occurred in networking with the introduction of public cloud"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Changes that have occurred in networking with the introduction of public cloud</h1></div></div></div><p>It is <a id="id58" class="indexterm"/>unquestionable that the emergence of the AWS, which was launched in 2006, changed and shaped the networking landscape forever. AWS has allowed companies to rapidly develop their products on the AWS platform. AWS has created an innovative set of services for end users, so they can manage infrastructure, load balancing, and even databases. These services have led the way in making the DevOps ideology a reality, by allowing users to elastically scale up and down infrastructure. They need to develop products on demand, so infrastructure wait times are no longer an inhibitor to development teams. AWS rich feature set of technology allows users to create infrastructure by clicking on a portal or more advanced users that want to programmatically create infrastructure using configuration management tooling, such as <span class="strong"><strong>Ansible</strong></span>, <span class="strong"><strong>Chef</strong></span>, <span class="strong"><strong>Puppet</strong></span>, <span class="strong"><strong>Salt</strong></span> or <span class="strong"><strong>Platform as a Service</strong></span> (<span class="strong"><strong>PaaS</strong></span>) solutions.</p><div class="section" title="An overview of AWS"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>An overview of AWS</h2></div></div></div><p>In 2016, <a id="id59" class="indexterm"/>the AWS <span class="strong"><strong>Virtual Private Cloud</strong></span> (<span class="strong"><strong>VPC</strong></span>) secures <a id="id60" class="indexterm"/>a set of Amazon <a id="id61" class="indexterm"/>EC2 instances (virtual machines) that can be connected to any existing network using a VPN connection. This simple construct has changed the way that developers want and expect to consume networking.</p><p>In 2016, we live in a consumer-based society with mobile phones allowing us instant access to the Internet, films, games, or an array of different applications to meet our every need, instant gratification if you will, so it is easy to see the appeal of AWS has to end users.</p><p>AWS allows developers to provision instances (virtual machines) in their own personal network, to their desired specification by selecting different flavors (CPU, RAM, and disk) using a few button clicks on the AWS portal's graphical user interface, alternately using a simple call to an API or scripting against the AWS-provided SDKs.</p><p>So now a valid question, why should developers be expected to wait long periods of time for either infrastructure or networking tickets to be serviced in on-premises data centers when AWS is available? It really shouldn't be a hard question to answer. The solution surely has to either be moved to AWS or create a private cloud solution <a id="id62" class="indexterm"/>that enables the same agility. However, the answer isn't always that straightforward, there are following arguments against using AWS and public cloud:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Not knowing where the data is actually stored and in which data center</li><li class="listitem" style="list-style-type: disc">Not being able to hold sensitive data offsite</li><li class="listitem" style="list-style-type: disc">Not being able to assure the necessary performance</li><li class="listitem" style="list-style-type: disc">High running costs</li></ul></div><p>All of <a id="id63" class="indexterm"/>these points are genuine blockers for some businesses that may be highly regulated or need to be PCI compliant or are required to meet specific regularity standards. These points may inhibit some businesses from using public cloud <a id="id64" class="indexterm"/>so as with most solutions it isn't the case of one size fits all.</p><p>In private data centers, there is a cultural issue that teams have been set up to work in silos and are not set up to succeed in an agile business model, so a lot of the time using AWS, Microsoft Azure, or Google Cloud is a quick fix for broken operational models.</p><p>Ticketing systems, a staple of broken internal operational models, are not a concept that aligns itself to speed. An IT ticket raised to an adjacent team can take days or weeks to complete, so requests are queued before virtual or physical servers can be provided to developers. Also, this is prominent for network changes too, with changes such as a simple modification to ACL rules taking an age to be implemented due to ticketing backlogs.</p><p>Developers need to have the ability to scale up servers or prototype new features at will, so long wait times for IT tickets to be processed hinder delivery of new products to market or bug fixes to existing products. It has become common in internal IT that some <span class="strong"><strong>Information Technology Infrastructure Library</strong></span> (<span class="strong"><strong>ITIL</strong></span>) practitioners put a sense of value <a id="id65" class="indexterm"/>on how many tickets that processed over a week as the main metric for success. This shows complete <a id="id66" class="indexterm"/>disregard for customer experience of their developers. There are some operations that need to shift to the developers, which have traditionally lived with internal or shadow IT, but there needs to be a change in operational <a id="id67" class="indexterm"/>processes at a business level to invoke these changes.</p><p>Put simply, AWS has changed the expectations of developers and the expectations placed on infrastructure and networking teams. Developers should be able to service their needs as quickly as making an alteration to an application on their mobile phone, free from slow internal IT operational models associated with companies.</p><p>But for start-ups and businesses that can use AWS, which aren't constrained by regulatory requirements, it skips the need to hire teams to rack servers, configure network devices, and pay for the running costs of data centers. It means they can start viable businesses and run them on AWS by putting in credit card details the same way as you would purchase a new book on Amazon or eBay.</p></div><div class="section" title="OpenStack overview"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>OpenStack overview</h2></div></div></div><p>The reaction <a id="id68" class="indexterm"/>to AWS was met with trepidation from competitors, as it disrupted the cloud computing industry and has led to PaaS solutions <a id="id69" class="indexterm"/>such as <span class="strong"><strong>Cloud Foundry</strong></span> and <span class="strong"><strong>Pivotal</strong></span> coming to fruition <a id="id70" class="indexterm"/>to provide an abstraction layer on top of hybrid clouds.</p><p>When a market is disrupted, it promotes a reaction, from it spawned the idea for a new private cloud. In 2010, a joint venture by Rackspace and NASA, launched an open source cloud-software initiative known as OpenStack, which came about as NASA couldn't put their data in a public cloud.</p><p>The OpenStack <a id="id71" class="indexterm"/>project intended to help organizations offer cloud computing services running on standard hardware and directly set out to mimic the model provided by AWS. The main difference with OpenStack is that it is an open source project that can be used by leading vendors to bring AWS-like ability and agility to the private cloud.</p><p>Since its inception in 2010, OpenStack has grown to have over 500 member companies as part of the OpenStack Foundation, with platinum members and gold members that comprise the biggest IT vendors in the world that are actively driving the community.</p><p>The platinum members of the OpenStack foundation are:</p><div class="mediaobject"><img src="graphics/B05559_01_04.jpg" alt="OpenStack overview"/></div><p>OpenStack is an open source project, which means its source code is publicly available and its underlying architecture is available for analysis, unlike AWS, which acts like a magic box of tricks but it is not really known for how it works underneath its shiny exterior.</p><p>OpenStack is <a id="id72" class="indexterm"/>primarily used to provide an <span class="strong"><strong>Infrastructure as a Service</strong></span> (<span class="strong"><strong>IaaS</strong></span>) function within the private cloud, where it makes commodity x86 compute, centralized storage, and networking features available to end users to self-service their needs, be it via the horizon dashboard or through a set of common API's.</p><p>Many <a id="id73" class="indexterm"/>companies are now implementing OpenStack to build their own data centers. Rather than doing it on their own, some companies are using different vendor hardened distributions of the community upstream project. It has been proven that using a vendor hardened distributions of OpenStack, when starting out, <a id="id74" class="indexterm"/>mean that OpenStack implementation is far likelier to be successful. Initially, for some companies, implementing OpenStack can be seen as complex as it is a completely new set of technology that a company may not be familiar with yet. OpenStack implementations are less likely to fail when using professional service support from known vendors, and it can create a viable alternative to enterprise solutions, such as AWS or Microsoft Azure.</p><p>Vendors, such as Red Hat, HP, Suse, Canonical, Mirantis, and many more, provide different distributions of OpenStack to customers, complete with different methods of installing the platform. Although the source code and features are the same, the business model for these OpenStack vendors is that they harden OpenStack for enterprise use and their differentiator to customers is their professional services.</p><p>There are <a id="id75" class="indexterm"/>many different OpenStack distributions available to customers with the following vendors providing OpenStack distributions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Bright Computing</li><li class="listitem" style="list-style-type: disc">Canonical</li><li class="listitem" style="list-style-type: disc">HPE</li><li class="listitem" style="list-style-type: disc">IBM</li><li class="listitem" style="list-style-type: disc">Mirantis</li><li class="listitem" style="list-style-type: disc">Oracle OpenStack for Oracle Linux, or O3L</li><li class="listitem" style="list-style-type: disc">Oracle OpenStack for Oracle Solaris</li><li class="listitem" style="list-style-type: disc">Red Hat</li><li class="listitem" style="list-style-type: disc">SUSE</li><li class="listitem" style="list-style-type: disc">VMware Integrated OpenStack (VIO)</li></ul></div><p>OpenStack <a id="id76" class="indexterm"/>vendors will support build out, on-going maintenance, upgrades, or any customizations a client needs, all of which are fed back to the community. The <a id="id77" class="indexterm"/>beauty of OpenStack being an open source project is that if vendors customize OpenStack for clients and create a real differentiator <a id="id78" class="indexterm"/>or competitive advantage, they cannot fork OpenStack or uniquely sell this feature. Instead, they have to contribute the source code back to the upstream open source OpenStack project.</p><p>This means that all competing vendors contribute to its success of OpenStack and benefit from each other's innovative work. The OpenStack project is not just for vendors though, and everyone can contribute code and features to push the project forward.</p><p>OpenStack <a id="id79" class="indexterm"/>maintains a release cycle where an upstream release is created every six months and is governed by the OpenStack Foundation. It is important to note that many public clouds, such as at&amp;t, RackSpace, and GoDaddy, are based on OpenStack too, so it is not exclusive to private clouds, but it has undeniably become increasingly popular as a private cloud alternative to AWS public cloud and now widely used for <span class="strong"><strong>Network Function Virtualization</strong></span> (<span class="strong"><strong>NFV</strong></span>).</p><p>So how does AWS and OpenStack work in terms of networking? Both AWS and OpenStack are made up of some mandatory and optional projects that are all integrated to make up its reference architecture. Mandatory projects include compute and networking, which are the staple of any cloud solution, whereas others are optional bolt-ons to enhance or extend capability. This means that end users can cherry-pick the projects they are interested in to make up their own personal portfolio.</p></div></div>
<div class="section" title="The AWS approach to networking"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>The AWS approach to networking</h1></div></div></div><p>Having <a id="id80" class="indexterm"/>discussed both AWS and OpenStack, first, we will explore the AWS approach to networking, before looking at an alternative method using OpenStack and compare the two approaches. When first setting up networking in AWS, a tenant network in AWS is instantiated using VPC, which post 2013 deprecated AWS classic mode; but what is VPC?</p><div class="section" title="Amazon VPC"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>Amazon VPC</h2></div></div></div><p>A VPC is <a id="id81" class="indexterm"/>the new default setting for new customers wishing to access AWS. VPCs can also be connected to customer networks (private data centers) by allowing AWS cloud to extend a private data center for agility. The <a id="id82" class="indexterm"/>concept of connecting a private data center to an AWS VPC is using something AWS refers to as a customer gateway and virtual private gateway. A virtual private gateway in simple terms is just two redundant VPN tunnels, which are instantiated from the customer's private network.</p><p>Customer <a id="id83" class="indexterm"/>gateways expose a set of external static addresses from a customer site, which are typically <span class="strong"><strong>Network Address Translation-Traversal</strong></span> (<span class="strong"><strong>NAT-T</strong></span>) to hide the source address. UDP port <code class="literal">4500</code> should be accessible in the external firewall in the private data center. Multiple VPCs can be supported from one customer gateway device.</p><div class="mediaobject"><img src="graphics/B05559_01_05.jpg" alt="Amazon VPC"/></div><p>A VPC <a id="id84" class="indexterm"/>gives an isolated view of everything an AWS customer has provisioned in AWS public cloud. Different user accounts can then be set up against VPC using the AWS <span class="strong"><strong>Identity and Access Management (IAM)</strong></span> service, which has customizable permissions.</p><p>The following example of a VPC shows instances (virtual machines) mapped with one or more <a id="id85" class="indexterm"/>security groups and connected to different subnets connected to the VPC router:</p><div class="mediaobject"><img src="graphics/B05559_01_06.jpg" alt="Amazon VPC"/></div><p>A <span class="strong"><strong>VPC</strong></span> <a id="id86" class="indexterm"/>simplifies networking greatly by putting the constructs into software and allows users to perform the following network functions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating <a id="id87" class="indexterm"/>instances (virtual machines) mapped to subnets</li><li class="listitem" style="list-style-type: disc">Creating <span class="strong"><strong>Domain Name System</strong></span> (<span class="strong"><strong>DNS</strong></span>) entries that are applied to instances</li><li class="listitem" style="list-style-type: disc">Assigning public and private IP addresses</li><li class="listitem" style="list-style-type: disc">Creating or associating subnets</li><li class="listitem" style="list-style-type: disc">Creating custom routing</li><li class="listitem" style="list-style-type: disc">Applying security groups with associated ACL rules</li></ul></div><p>By default, when an instance (virtual machine) is instantiated in a VPC, it will either be placed on a default subnet or custom subnet if specified.</p><p>All VPCs <a id="id88" class="indexterm"/>come with a default router <a id="id89" class="indexterm"/>when the VPC is created, the router can have additional custom routes added and routing priority can also be set to forward traffic to particular subnets.</p></div><div class="section" title="Amazon IP addressing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Amazon IP addressing</h2></div></div></div><p>When an <a id="id90" class="indexterm"/>instance is spun up in AWS, it will automatically be assigned a mandatory private IP address by <span class="strong"><strong>Dynamic Host Configuration Protocol</strong></span> (<span class="strong"><strong>DHCP</strong></span>) as well as a public IP and DNS entry too unless dictated otherwise. Private IPs are used in AWS to route east-west <a id="id91" class="indexterm"/>traffic between instances when virtual machine needs to communicate with adjacent virtual machines on the same subnet, whereas public IPs are available through the Internet.</p><p>If a <a id="id92" class="indexterm"/>persistent public IP address is required for an instance, AWS offers the elastic IP addresses feature, which is limited to five per VPC account, which any failed instances IP address can be quickly mapped to another instance. It is important to note that it can take up to 24 hours for a public IP address's DNS <span class="strong"><strong>Time To Live</strong></span> (<span class="strong"><strong>TTL</strong></span>) to propagate when using AWS.</p><p>In terms of throughput, AWS instances can support a <span class="strong"><strong>Maximum Transmission Unit</strong></span> (<span class="strong"><strong>MTU</strong></span>) of 1,500 that can be passed to an instance in AWS, so this needs to be considered when considering application performance.</p></div><div class="section" title="Amazon security groups"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/>Amazon security groups</h2></div></div></div><p>Security <a id="id93" class="indexterm"/>groups in AWS are a way of grouping permissive ACL rules, so don't allow explicit denies. AWS security groups act as a virtual firewall for instances, and they can be associated with one or more instances' network interfaces. In a VPC, you can associate a network interface with up to five security groups, adding up to 50 rules to a security group, with a maximum of 500 security groups per VPC. A VPC in an AWS account automatically has a default security group, which will be automatically applied if no other security groups are specified.</p><p>Default security groups allow all outbound traffic and all inbound traffic only from other instances in a VPC that also use the default security group. The default security group cannot be deleted. Custom security groups when first created allow no inbound traffic, but all outbound traffic is allowed.</p><p>Permissive ACL rules associated with security groups govern inbound traffic and are added using the AWS console (GUI) as shown later in the text, or they can be programmatically added <a id="id94" class="indexterm"/>using APIs. Inbound ACL rules <a id="id95" class="indexterm"/>associated with security groups can be added by specifying type, protocol, port range, and the source address. Refer to the following screenshot:</p><div class="mediaobject"><img src="graphics/B05559_01_07.jpg" alt="Amazon security groups"/></div></div><div class="section" title="Amazon regions and availability zones"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec18"/>Amazon regions and availability zones</h2></div></div></div><p>A VPC has <a id="id96" class="indexterm"/>access to different regions and availability zones of shared compute, which dictate the data center that the AWS instances (virtual machines) will be deployed in. Regions in AWS are geographic areas that are <a id="id97" class="indexterm"/>completely isolated by design, where availability zones are isolated locations in that specific region, so an availability zone is a subset of a region.</p><p>AWS gives users the ability to place their resources in different locations for redundancy as sometimes the health of a specific region or availability zone can suffer issues. Therefore, AWS users are encouraged to use more than one availability zones when deploying production workloads on AWS. Users can choose to replicate their instances and data across regions if they choose to.</p><p>Within each isolated AWS region, there are child availability zones. Each availability zone is connected to sibling availability zones using low latency links. All communication from one region to another is across the public Internet, so using geographically distant regions will acquire latency and delay. Encryption of data should also be considered when hosting applications that send data across regions.</p></div><div class="section" title="Amazon Elastic Load Balancing"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec19"/>Amazon Elastic Load Balancing</h2></div></div></div><p>AWS also <a id="id98" class="indexterm"/>allows <span class="strong"><strong>Elastic Load Balancing</strong></span> (<span class="strong"><strong>ELB</strong></span>) to be configured within a VPC as a bolt-on service. ELB can either be internal or external. When ELB <a id="id99" class="indexterm"/>is external, it allows the creation of an Internet-facing entry point into your VPC using an associated DNS entry and balances load between different instances. Security groups are assigned to ELBs to control the access ports that need to be used. </p><p>The following image shows an elastic load balancer, load balancing 3 instances:</p><div class="mediaobject"><img src="graphics/B05559_01_08.jpg" alt="Amazon Elastic Load Balancing"/></div></div></div>
<div class="section" title="The OpenStack approach to networking"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>The OpenStack approach to networking</h1></div></div></div><p>Having <a id="id100" class="indexterm"/>considered AWS networking, we will now explore OpenStack's approach to networking and look at how its services are configured.</p><p>OpenStack is deployed in a data center on multiple controllers. These controllers contain all the OpenStack services, and they can be installed on either virtual machines, bare metal (physical) servers, or containers. The OpenStack controllers should host all the OpenStack services in a highly available and redundant fashion when they are deployed in production.</p><p>Different OpenStack vendors provide different installers to install OpenStack. Some examples <a id="id101" class="indexterm"/>of installers from the most prominent OpenStack distributions are RedHat Director (based on OpenStack TripleO), Mirantis Fuel, HPs HPE installer (based on Ansible), and Juju for Canonical, which all install OpenStack controllers and are used to scale out compute nodes on the OpenStack cloud acting as an OpenStack workflow management tool.</p><div class="section" title="OpenStack services"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec20"/>OpenStack services</h2></div></div></div><p>A breakdown <a id="id102" class="indexterm"/>of the core OpenStack services that are installed on an OpenStack controller are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Keystone</strong></span> is <a id="id103" class="indexterm"/>the identity service for OpenStack that allows user access, which issues tokens, and can be integrated with LDAP or Active directory.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Heat</strong></span> is <a id="id104" class="indexterm"/>the orchestration provisioning tool for OpenStack infrastructure.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Glance</strong></span> is the <a id="id105" class="indexterm"/>image service for OpenStack that stores all image templates for virtual machines or bare metal servers.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cinder</strong></span> is the <a id="id106" class="indexterm"/>block storage service for OpenStack that allows centralized storage volumes to be provisioned and attached to vms or bare metal servers that can then be mounted.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Nova</strong></span> is the <a id="id107" class="indexterm"/>compute service for OpenStack used to provision vms and uses different scheduling algorithms to work out where to place virtual machines on available compute.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Horizon</strong></span> is the <a id="id108" class="indexterm"/>OpenStack dashboard that users connect to view the status of vms or bare metal servers that are running in a tenant network.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Rabbitmq</strong></span> is <a id="id109" class="indexterm"/>the message queue system for OpenStack.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Galera</strong></span> is the <a id="id110" class="indexterm"/>database used to store all OpenStack data in the Nova (compute) and neutron (networking) databases holding vm, port, and subnet information.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Swift</strong></span> is the <a id="id111" class="indexterm"/>object storage service for OpenStack and can be used as a redundant storage backend that stores replicated copies of objects on multiple servers. Swift is not like traditional block or file-based storage; objects can be any unstructured data.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Ironic</strong></span> is the bare metal provisioning service for OpenStack. Originally, a fork of <a id="id112" class="indexterm"/>part of the Nova codebase, it allows provisioning of images on to bare metal servers and uses IPMI and ILO or DRAC interfaces to manage physical hardware.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Neutron</strong></span> is <a id="id113" class="indexterm"/>the networking service for OpenStack and contains ML2 and L3 agents and allows configuration of network subnets and routers.</li></ul></div><p>In terms of <a id="id114" class="indexterm"/>neutron networking services, neutron architecture is very similar in constructs to AWS.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>Useful links covering OpenStack services can be found at:</p><p>
<a class="ulink" href="http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html">http://docs.openstack.org/admin-guide/common/get-started-openstack-services.html</a>.</p><p>
<a class="ulink" href="https://www.youtube.com/watch?v=N90ufYN0B6U">https://www.youtube.com/watch?v=N90ufYN0B6U</a>
</p></div></div></div><div class="section" title="OpenStack tenants"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec21"/>OpenStack tenants</h2></div></div></div><p>A Project, often referred to in OpenStack as a tenant, gives an isolated view of everything that <a id="id115" class="indexterm"/>a team has provisioned in an OpenStack cloud. Different user accounts can then be set up against a Project (tenant) using the keystone identity service, which <a id="id116" class="indexterm"/>can be integrated with <span class="strong"><strong>Lightweight Directory Access Protocol</strong></span> (<span class="strong"><strong>LDAP</strong></span>) or Active Directory to support customizable permission models.</p></div><div class="section" title="OpenStack neutron"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec22"/>OpenStack neutron</h2></div></div></div><p>OpenStack <a id="id117" class="indexterm"/>neutron performs all the networking functions in OpenStack.</p><p>The following network functions are provided by the neutron project in an OpenStack cloud:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating instances (virtual machines) mapped to networks</li><li class="listitem" style="list-style-type: disc">Assigning IP addresses using its in-built DHCP service</li><li class="listitem" style="list-style-type: disc">DNS entries are applied to instances from named servers</li><li class="listitem" style="list-style-type: disc">The assignment of private and Floating IP addresses</li><li class="listitem" style="list-style-type: disc">Creating or associating network subnets</li><li class="listitem" style="list-style-type: disc">Creating routers</li><li class="listitem" style="list-style-type: disc">Applying security groups</li></ul></div><p>OpenStack is set up into its <span class="strong"><strong>Modular Layer 2</strong></span> (<span class="strong"><strong>ML2</strong></span>) and <span class="strong"><strong>Layer 3</strong></span> (<span class="strong"><strong>L3</strong></span>) agents that are configured <a id="id118" class="indexterm"/>on the OpenStack controllers. OpenStack's ML2 plugin allows OpenStack to integrate with switch vendors that use either <a id="id119" class="indexterm"/>Open vSwitch or Linux Bridge and acts as an agnostic plugin to switch vendors, so vendors can create plugins, to make their switches OpenStack <a id="id120" class="indexterm"/>compatible. The ML2 agent runs on the hypervisor communicating over <span class="strong"><strong>Remote Procedure Call</strong></span> (<span class="strong"><strong>RPC</strong></span>) to the compute host server.</p><p>OpenStack compute hosts are typically deployed using a hypervisor that uses Open vSwitch. Most OpenStack vendor distributions use the KVM hypervisor by default in their reference architectures, so this is deployed and configured on each compute host by the chosen OpenStack installer.</p><p>Compute <a id="id121" class="indexterm"/>hosts in OpenStack are connected to the access layer of the STP 3-tier model, or in modern networks connected to the Leaf switches, with VLANs connected to each individual OpenStack compute host. Tenant networks are then used to provide isolation between tenants and use VXLAN and GRE tunneling to connect the layer 2 network.</p><p>Open vSwitch runs in kernel space on the KVM hypervisor and looks after firewall rules by using OpenStack security groups that pushes down flow data via OVSDB from the switches. The neutron L3 agent allows OpenStack to route between tenant networks and uses neutron routers, which are deployed within the tenant network to accomplish this, without a neutron router networks are isolated from each other and everything else.</p></div><div class="section" title="Provisioning OpenStack networks"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec23"/>Provisioning OpenStack networks</h2></div></div></div><p>When <a id="id122" class="indexterm"/>setting up simple networking using neutron in a Project (tenant) network, two different networks, an internal network, and an external network will be configured. The internal network will be used for east-west traffic between instances. This is created as shown in the following horizon dashboard with an appropriate <span class="strong"><strong>Network Name</strong></span>:</p><div class="mediaobject"><img src="graphics/B05559_01_09.jpg" alt="Provisioning OpenStack networks"/></div><p>The <span class="strong"><strong>Subnet Name</strong></span> and <a id="id123" class="indexterm"/>subnet range are then specified in the <span class="strong"><strong>Subnet</strong></span> section, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B05559_01_10.jpg" alt="Provisioning OpenStack networks"/></div><p>Finally, DHCP is enabled on the network, and any named <span class="strong"><strong>Allocation Pools</strong></span> (specifies only a <a id="id124" class="indexterm"/>range of addresses that can be used in a subnet) are optionally configured alongside any named <span class="strong"><strong>DNS Name Servers</strong></span>, as shown below:</p><div class="mediaobject"><img src="graphics/B05559_01_11.jpg" alt="Provisioning OpenStack networks"/></div><p>An external network will also need to be created to make the internal network accessible from <a id="id125" class="indexterm"/>outside of OpenStack, when external networks are created by an administrative user, the set <span class="strong"><strong>External Network</strong></span> checkbox needs to be selected, as shown in the next screenshot:</p><div class="mediaobject"><img src="graphics/B05559_01_12.jpg" alt="Provisioning OpenStack networks"/></div><p>A router <a id="id126" class="indexterm"/>is then created in OpenStack to route packets to the network, as shown below:</p><div class="mediaobject"><img src="graphics/B05559_01_13.jpg" alt="Provisioning OpenStack networks"/></div><p>The created router will then need to be associated with the networks; this is achieved by adding an interface on the router for the private network, as illustrated in the following screenshot:</p><p> </p><div class="mediaobject"><img src="graphics/B05559_01_14.jpg" alt="Provisioning OpenStack networks"/></div><p>
</p><p>The <span class="strong"><strong>External Network</strong></span> that was created then needs to be set as the router's gateway, as <a id="id127" class="indexterm"/>per the following screenshot:</p><div class="mediaobject"><img src="graphics/B05559_01_15.jpg" alt="Provisioning OpenStack networks"/></div><p>This then completes the network setup; the final configuration for the internal and external network is displayed below, which shows one router connected to an internal and external network:</p><div class="mediaobject"><img src="graphics/B05559_01_16.jpg" alt="Provisioning OpenStack networks"/></div><p>In OpenStack, instances <a id="id128" class="indexterm"/>are provisioned onto the internal private network by selecting the private network NIC when deploying instances. OpenStack has the convention of assigning pools of public IPs (floating IP) addresses from an external network for instances that need to be externally routable outside of OpenStack. </p><p>To set up a set of floating IP addresses, an OpenStack administrator will set up an allocation pool using the external network from an external network, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B05559_01_17.jpg" alt="Provisioning OpenStack networks"/></div><p>OpenStack like <a id="id129" class="indexterm"/>AWS, uses security groups to set up firewall rules between instances. Unlike AWS, OpenStack supports both ingress and egress ACL rules, whereas AWS allows all outbound communication, OpenStack can deal with both ingress and egress rules. Bespoke security groups are created to group ACL rules as shown below</p><div class="mediaobject"><img src="graphics/B05559_01_18.jpg" alt="Provisioning OpenStack networks"/></div><p>Ingress and <a id="id130" class="indexterm"/>Rules can then be created against a security group. <span class="strong"><strong>SSH</strong></span> access is configured as an ACL rule against the parent security group, which is pushed down to Open VSwitch into kernel space on each hypervisor, as seen in the next screenshot:</p><div class="mediaobject"><img src="graphics/B05559_01_19.jpg" alt="Provisioning OpenStack networks"/></div><p>Once the <a id="id131" class="indexterm"/>Project (tenant) has two networks, one internal and one external, and an appropriate security group has been configured, instances are ready to be launched on the private network.</p><p>An instance is launched by selecting <span class="strong"><strong>Launch Instance</strong></span> in horizon and setting the following parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Availability Zone</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Instance Name</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Flavor</strong></span> (CPU, RAM, and disk space)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Image Name</strong></span> (base operating system)</li></ul></div><div class="mediaobject"><img src="graphics/B05559_01_20.jpg" alt="Provisioning OpenStack networks"/></div><p>The private <a id="id132" class="indexterm"/>network is then selected as the <span class="strong"><strong>NIC</strong></span> for the instance under the <span class="strong"><strong>Networking</strong></span> tab:</p><div class="mediaobject"><img src="graphics/B05559_01_21.jpg" alt="Provisioning OpenStack networks"/></div><p>This will <a id="id133" class="indexterm"/>mean that when the instance is launched, it will use OpenStack's internal DHCP service to pick an available IP address from the allocated subnet range.</p><p>A security group should also be selected to govern the ACL rules for the instance; in this instance, the <code class="literal">testsg1</code> security group is selected as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B05559_01_22.jpg" alt="Provisioning OpenStack networks"/></div><p>Once the <a id="id134" class="indexterm"/>instance has been provisioned, a floating IP address can be associated from the external network:</p><div class="mediaobject"><img src="graphics/B05559_01_23.jpg" alt="Provisioning OpenStack networks"/></div><p>A floating IP address from the external network floating IP address pool is then selected and associated with the instance:</p><div class="mediaobject"><img src="graphics/B05559_01_24.jpg" alt="Provisioning OpenStack networks"/></div><p>The floating <a id="id135" class="indexterm"/>IP addresses NATs OpenStack instances that are deployed on the internal public IP address to the external network's floating IP address, which will allow the instance to be accessible from outside of OpenStack.</p></div><div class="section" title="OpenStack regions and availability zones"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec24"/>OpenStack regions and availability zones</h2></div></div></div><p>OpenStack <a id="id136" class="indexterm"/>like AWS, as seen on instance creation, also utilizes regions and availability zones. Compute hosts in OpenStack (hypervisors) can be assigned <a id="id137" class="indexterm"/>to different availability zones.</p><p>An availability zone in OpenStack is just a virtual separation of compute resources. In OpenStack, an availability zone can be further segmented into host aggregates. It is important to note that a compute host can be assigned to only one availability zone, but can be a part of multiple host aggregates in that same availability zone.</p><p>Nova uses <a id="id138" class="indexterm"/>a concept named <span class="strong"><strong>nova scheduler rules</strong></span>, which dictates the placement of instances on compute hosts at provisioning time. A simple example of a nova scheduler rule is the <code class="literal">AvailabiltyZoneFilter</code> filter, which means that if a user selects an availability zone at provisioning time, then the instance will land only on any of the compute instances grouped under that availability zone.</p><p>Another example of the <code class="literal">AggregateInstanceExtraSpecsFilter</code> filter that means that if a custom flavor (CPU, RAM, and disk) is tagged with a key value pair and a host aggregate is tagged with the same key value pair, then if a user deploys with that flavor the <code class="literal">AggregateInstanceExtraSpecsFilter</code> filter will place all instances on compute hosts under that host aggregate.</p><p>These host aggregates can be assigned to specific teams, which means that teams can be selective about which applications they share their compute with and can be used to prevent noisy neighbor syndrome. There is a wide array of filters that can be applied in OpenStack in all sorts <a id="id139" class="indexterm"/>of orders to dictate instance scheduling. OpenStack allows cloud operators to create a traditional cloud model with large groups of contended compute <a id="id140" class="indexterm"/>to more bespoke use cases where the isolation of compute resources is required for particular application workloads.</p><p>The following example shows host aggregates with groups and shows a host aggregate named <span class="strong"><strong>1-Host-Aggregate</strong></span>, grouped under an <span class="strong"><strong>Availability Zone</strong></span> named <span class="strong"><strong>DC1</strong></span> containing two compute hosts (hypervisors), which could be allocated to a particular team:</p><div class="mediaobject"><img src="graphics/B05559_01_25.jpg" alt="OpenStack regions and availability zones"/></div></div><div class="section" title="OpenStack instance provisioning workflow"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec25"/>OpenStack instance provisioning workflow</h2></div></div></div><p>When an <a id="id141" class="indexterm"/>instance (virtual machine) is provisioned in OpenStack, the following high-level steps are carried out:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Nova compute service will issue a request for a new instance (virtual machine) using the image selected from the glance images service</li><li class="listitem" style="list-style-type: disc">The nova <a id="id142" class="indexterm"/>request may then be queued by <span class="strong"><strong>RabbitMQ</strong></span> before being processed (RabbitMQ allows OpenStack to deal with multiple simultaneous provisioning requests)</li><li class="listitem" style="list-style-type: disc">Once the request for a new instance is processed, the request will write a new row into the nova Galera database in the nova database</li><li class="listitem" style="list-style-type: disc">Nova will look at the nova scheduler rules defined on the OpenStack controllers and will use those rules to place the instance on an available compute node (KVM hypervisor)</li><li class="listitem" style="list-style-type: disc">If an available hypervisor is found that meets the nova scheduler rules, then the provisioning process will begin</li><li class="listitem" style="list-style-type: disc">Nova will check whether the image already exists on the matched hypervisor. If it doesn't, the image will be transferred from the hypervisor and booted from local disk</li><li class="listitem" style="list-style-type: disc">Nova will issue a neutron request, which will create a new VPort in OpenStack and map it to the neutron network</li><li class="listitem" style="list-style-type: disc">The VPort information will then be written to both the nova and neutron databases in Galera to correlate the instance with the network</li><li class="listitem" style="list-style-type: disc">Neutron <a id="id143" class="indexterm"/>will issue a DHCP request to assign the instance a private IP address from an unallocated IP address from the subnet it has been associated with</li><li class="listitem" style="list-style-type: disc">A private IP address will then be assigned, and the instance will start to start up on the private network</li><li class="listitem" style="list-style-type: disc">The neutron metadata service will then be contacted to retrieve cloud-init information on boot, which will assign a DNS entry to the instance from the named server, if specified</li><li class="listitem" style="list-style-type: disc">Once cloud-init has run, the instance will be ready to use</li><li class="listitem" style="list-style-type: disc">Floating IPs can then be assigned to the instance to NAT to external networks to make the instances publicly accessible</li></ul></div></div><div class="section" title="OpenStack LBaaS"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec26"/>OpenStack LBaaS</h2></div></div></div><p>Like AWS <a id="id144" class="indexterm"/>OpenStack also offers a <span class="strong"><strong>Load-Balancer-as-a-Service</strong></span> (<span class="strong"><strong>LBaaS</strong></span>) option that allows incoming requests to be distributed evenly <a id="id145" class="indexterm"/>among designated instances using a <span class="strong"><strong>Virtual IP</strong></span> (<span class="strong"><strong>VIP</strong></span>). The features and functionality supported by LBaaS are dependent on the <a id="id146" class="indexterm"/>vendor plugin that is used.</p><p>Popular <a id="id147" class="indexterm"/>LBaaS plugins in OpenStack are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Citrix NetScaler</li><li class="listitem" style="list-style-type: disc">F5</li><li class="listitem" style="list-style-type: disc">HaProxy</li><li class="listitem" style="list-style-type: disc">Avi networks</li></ul></div><p>These load balancers all expose varying degrees of features to the OpenStack LBaaS agent. The main driver for utilizing LBaaS on OpenStack is that it allows users to use LBaaS as a broker to the load balancing solution, allowing users to use the OpenStack API or configure the load balancer via the horizon GUI.</p><p>LBaaS allows load balancing to be set up within a tenant network in OpenStack. Using LBaaS means that if for any reason a user wishes to use a new load balancer vendor as opposed <a id="id148" class="indexterm"/>to their incumbent one; as long as they are using OpenStack LBaaS, it is made much easier. As all calls or administration are being done via the LBaaS APIs or Horizon, no changes would be required to the orchestration scripting required to provision and administrate the load balancer, and they wouldn't be tied into each vendor's custom APIs and the load balancing solution becomes a commodity.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Summary</h1></div></div></div><p>In this chapter, we have covered some of the basic networking principles that are used in today's modern data centers, with special focus on the AWS and OpenStack cloud technologies which are two of the most popular solutions.</p><p>Having read this chapter, you should now be familiar with the difference between Leaf-Spine and Spanning Tree network architectures, it should have demystified AWS networking, and you should now have a basic understanding of how private and public networks can be configured in OpenStack.</p><p>In the forthcoming chapters, we will build on these basic networking constructs and look at how they can be programmatically controlled using configuration management tools and used to automate network functions. But first, we will focus on some of the software-defined networking controllers that can be used to extend the capability of OpenStack even further than neutron in the private clouds and some of the feature sets and benefits they bring to ease the pain of managing network operations.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>Useful links for Amazon content are:</p><p>
<a class="ulink" href="https://aws.amazon.com/">https://aws.amazon.com/</a>
</p><p>
<a class="ulink" href="https://www.youtube.com/watch?v=VgzzHCukwpc">https://www.youtube.com/watch?v=VgzzHCukwpc</a>
</p><p>
<a class="ulink" href="https://www.youtube.com/watch?v=jLVPqoV4YjU">https://www.youtube.com/watch?v=jLVPqoV4YjU</a>
</p><p>Useful links for OpenStack content are:</p><p>
<a class="ulink" href="https://wiki.openstack.org/wiki/Main_Page">https://wiki.openstack.org/wiki/Main_Page</a>
</p><p>
<a class="ulink" href="https://www.youtube.com/watch?v=Qz5gyDenqTI">https://www.youtube.com/watch?v=Qz5gyDenqTI</a>
</p><p>
<a class="ulink" href="https://www.youtube.com/watch?v=Li0Ed1VEziQ">https://www.youtube.com/watch?v=Li0Ed1VEziQ</a>
</p></div></div></div></body></html>