- en: Chapter 9. Best Practices for the Real World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have really covered a lot of ground in this book so far. We are now on our
    final chapter. Here, we are going to look at how to combine all the skills you
    have learned and create a production-ready module. Just to go out with a bang,
    we will create a module that configures and deploys Kubernetes as the frontend
    (note that running Kuberbetes as frontend has limitations and would not be the
    best idea for production. The UCP component of the module will be production ready).
    Since there will be a lot of sensitive data, we will take advantage of Hiera.
    We will create a custom fact to automate the retrieval of the UCP fingerprint,
    and we will split out all the kubernetes components and use interlock to proxy
    our API service. We will also take UCP further and look at how to repoint the
    Docker daemon to use the UCP cluster. The server architecture will follow the
    same design as we discussed in the scheduler chapter. We will use three nodes,
    all running Ubuntu 14.04 with an updated kernel to support the native Docker network
    namespace. We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Hiera
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hiera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this topic, we will look at how to make our modules stateless. We will move
    all the data that is specific to the node that is applied to the module in Hiera
    ([https://docs.puppetlabs.com/hiera/3.1/](https://docs.puppetlabs.com/hiera/3.1/)).
    There are two main drivers behind this. The first is to remove any sensitive data
    such as passwords, keys, and so on, out of our modules. The second is if we remove
    node-specific data or state out our modules so that they are generic. We can apply
    them to any number of hosts without changing the logic of the module. This gives
    us the flexibility to publish our modules for other members of the Puppet community.
  prefs: []
  type: TYPE_NORMAL
- en: What data belongs in Hiera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we first sit down and start development on a new module, some of the things
    that we should consider are: whether we can make our module OS agnostic, how we
    can run the module on multiple machines without logic changes or extra development,
    and how we can protect our sensitive data?'
  prefs: []
  type: TYPE_NORMAL
- en: The answer to all these questions is Hiera.
  prefs: []
  type: TYPE_NORMAL
- en: We are able to leverage Hiera by parameterizing our classes. This will allow
    Puppet to automatically look up Hiera at the beginning of the catalogue compilation.
    So, let's explore some examples of the data that you will put in Hiera.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in our chapter about container schedulers, we briefly used Hiera.
    We set the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What data belongs in Hiera](img/B05201_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we are setting parameters such as versions. Why would we want
    to set versions in Hiera and not straight in the module? If we set the version
    of Consul, for example, we might be running version 5.0 in production. Hashicorp
    just released version 6.0 by changing the Hiera value for the environment (for
    more information about Puppet environments, go to [https://docs.puppetlabs.com/puppet/latest/reference/environments.html](https://docs.puppetlabs.com/puppet/latest/reference/environments.html)).
    In Hiera versions from dev to 6.0, we can run multiple versions of the application
    with no module development.
  prefs: []
  type: TYPE_NORMAL
- en: The same can be done with IP addresses or URLs. So in the hieradata for dev,
    your Swarm cluster URL could be `dev.swarm.local` and production could be just
    `swarm.local`.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of data that you will want to separate is passwords/keys. You wouldn't
    want the same password/key in your dev environment as there are in production.
    Again, Hiera will let you obfuscate this data.
  prefs: []
  type: TYPE_NORMAL
- en: We can then take the protection of this data further using eyaml, which Puppet
    supports. This allows you to use keys to encrypt your Hiera `.yaml` files. So,
    when the files are checked into source control, they are encrypted. This helps
    prevent data leakage. For more information on eyaml, visit [https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml](https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml).
  prefs: []
  type: TYPE_NORMAL
- en: As you see, Hiera gives you the flexibility to move your data from the module
    to Hiera to externalize configurations, making the module stateless.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks for Hiera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some really handy tips that you can refer to when using Hiera. Puppet
    allows you to call functions, lookups, and query facts from Hiera. Mastering these
    features will come in very handy. If this is new to you, read the document available
    at [https://docs.puppetlabs.com/hiera/3.1/variables.html](https://docs.puppetlabs.com/hiera/3.1/variables.html)
    before moving on in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s look at an example of looking up facts from Hiera. First, why would
    you want to do this? One really good reason is IP address lookups. If you have
    a class that you are applying to three nodes and you have to advertise an IP address
    like we did in our `consul` module, setting the IP address in Hiera will not work,
    as the IP address will be different for each machine. We create a file called
    `node.yaml` and add the IP address there. The issue is that we will now have multiple
    Hiera files. Every time Puppet loads the catalogue, it looks up all the Hiera
    files to check whether any values have changed. The more files we have, the more
    load it puts on the master and the slower our Puppet runs will become. So we can
    tell Hiera to look up the fact and the interface we want to advertise. Here is
    an example of what the code would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The one call out is that if we called the fact from a module, we would call
    it with the fully qualified name, `$::ipaddress_eth1`. Unfortunately, Hiera does
    not support the use of this. So we can use the short name for the `::ipaddress_eth1`
    fact.
  prefs: []
  type: TYPE_NORMAL
- en: The code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a good understanding of how to make our module stateless, let's
    go ahead and start coding. We will split the coding into two parts. In the first
    part we will write the module to install and configure Docker UCP. The final topic
    will be to run Kubernetes as the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: UCP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing that we need to do is create a new Vagrant repo for this chapter.
    By now, we should be masters at creating a new Vagrant repo. Once we have created
    that, we will create a new module called `<AUTHOR>-ucpconfig` and move it to our
    `modules` directory in the root of our Vagrant repo. We will first set up our
    `servers.yml` file by adding the code shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, we are setting up three servers, where `ucp-01` will be the
    master of the cluster and the other two nodes will join the cluster. We will add
    two more files: `config.json` and `docker_subscription.lic`. Here, `config.json`
    will contain the authorization key to Docker Hub, and `docker_subscription.lic`
    will contain our trial license for UCP. Note that we covered both of these files
    in the container scheduler chapter. If you are having issues setting up these
    files, refer to that chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The file we will now look at is the puppetfile. We need to add the code shown
    in the following screenshot to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have our Vagrant repo set up, we can move on to our module. We
    will need to create four files: `config.pp`, `master.pp`, `node.pp`, and `params.pp`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first file we will look at is `params.pp`. Again, I''d like to code this
    file first as it sets a good foundation for the rest of the module. We do this
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we are setting all our parameters. We will look at each one
    in depth as we apply it to the class. This way, we will have context on the value
    we are setting. You might have noted that we have set a lot of variables to empty
    strings. This is because we will use Hiera to look up the values. I have hardcoded
    some values in as well, such as the UCP version as 1.0.0\. This is just a default
    value if there is no value in Hiera. However, we will be setting the value for
    UCP to 1.0.3\. The expected behavior is that the Hiera value will take precedence.
    You will note that there is a fact that we are referencing, which is `$::ucp_fingerprint`.
    This is a custom fact. This will automate the passing of the UCP fingerprint.
    If you remember, in the container scheduler chapter, we had to build `ucp-01`
    to get the fingerprint and add it to Hiera for the other nodes' benefit. With
    the custom fact, we will automate the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a custom fact, we will first need to create a `lib` folder in the
    root of the module. Under that folder, we will create a folder called `facter`.
    In that folder, we will create a file called `ucp_fingerprint.rb`. When writing
    a custom fact, the filename needs to be the same as the fact''s name. The code
    that we will add to our custom fact is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this code, you can see that we are adding a `bash` command to query our UCP
    master in order to get the fingerprint. I have hardcoded the IP address of the
    master in this fact. In our environment, we would write logic for that to be more
    fluid so that we are allowed to have multiple environments, hostnames, and so
    on. The main part to take away from the custom fact is the command itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move back to our `init.pp` file, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first thing you can see it that we are declaring all our variables at the
    top of our class. This is where Puppet will look up Hiera and match any variable
    that we have declared. The first block of code in the module is going to set up
    our Docker daemon. We are going to add the extra configuration to tell the daemon
    where it can find the backend for the Docker native network. Then, we will declare
    a case statement on the `$::hostname` fact. You will note that we have set a parameter
    for the hostname. This is to make our module more portable. Inside the case statement,
    if the host is a master, you can see that we will use our Consul container as
    the backend for our Docker network. Then, we will order the execution of our classes
    that are applied to the node.
  prefs: []
  type: TYPE_NORMAL
- en: In the next block of code in the case statement, we have declared the `$ucp_deploy_node`
    variable for the `$::hostname` fact. We will use this node to deploy Kubernetes
    from. We will get back to this later in the topic. The final block of code in
    our case statement is the `catch all` or `default`. If Puppet cannot find the
    fact of `$::hostname` in either of our declared variables, it will apply these
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move on to our `master.pp` file, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first thing you will note is that we are declaring our variables again
    at the top of this class. We are doing this as we are declaring many parameters,
    and this makes our module a lot more readable. Doing this in complex modules is
    a must, and I would really recommend you to follow this practice. It will make
    your life a lot easier when it comes to debugging. As you can see in the following
    screenshot, we are tying the parameters back to our `init.pp` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the preceding code, there are very few types which we are
    values that we are setting in the module.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move on to our `node.pp` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we are declaring the parameters at the top of the class, again
    tying them back to our `init.pp` file. We have declared most of our values as
    we will use Hiera.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now move on to our `config.pp` file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this class, we are going to make a few vital configurations for our cluster.
    So, we will walk through each block of code individually. Let''s take a look at
    the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this block, we will declare our variables as we have in all the classes
    in this module. One call out that I have not mentioned yet is that we are only
    declaring the variables that are applied to its class. We are not declaring all
    the parameters. Let''s look at the next set of code now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this block of code, we will pass an array of packages that we need to curl
    our UCP master in order to get the SSL bundle that we will need for TLS coms between
    nodes. Now, let''s see the third block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this code, we are going to create a shell script called `get_ca.sh.erb`
    to get the bundle from the master. Let''s create the file, and the first thing
    that we will need to do is create a `templates` folder in the root of the module.
    Then, we can create our `get_ca.sh.erb` file in the `templates` folder. We will
    add the following code to the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the script, we need to create an `auth` token and pass it
    to the API. Puppet does not natively handle tasks like these well, as we are using
    variables inside the `curl` command. Creating a template file and running an `exec`
    function is fine as long as we make it idempotent. In the next block of code,
    we will do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this block of code, we will run the preceding script. You can see that we
    have set the parameters, `command` `path`, and **cwd** (**current working directory**).
    The next resource is `creates` that tells Puppet that there should be a file called
    `ca.pem` in the current working directory. If Puppet finds that the file does
    not exist, it will execute the exec, and if the file does exist, Puppet will do
    nothing. This will give our exec idempotency.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next block of code, we will create a file in `/etc/profile.d`, which
    will then point the Docker daemon on each node to the master''s IP address, allowing
    us to schedule containers across the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s create a `docker.sh` file in our `templates` directory. In the
    file, we will put the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this file, we are telling the Docker daemon to use TLS, setting the location
    of the key files that we got from the bundle earlier in the class. The last thing
    we are setting is the Docker host that will point to the UCP master.
  prefs: []
  type: TYPE_NORMAL
- en: 'This last block of code in this class should look very familiar, as we are
    setting up a Docker network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now move on to the the file where all our data is, our Hiera file. That
    file is located in the `hieradata` folder, which is present in the root of our
    Vagrant repo. The following screenshot shows the various data present in the Hiera
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, let's list out all the data we have defined here. We are defining the UCP
    master as `ucp-01`, and our deploy node is `ucp-03` (this is the node that we
    are deploying Kubernetes from). The UCP URL of the master is `https://172.17.10.101`.
    This is used when we connect nodes to the master and also when we get our `ca`
    bundle and UCP fingerprint. We keep the username and password as `admin` and `orca`.
    We will use UCP version `1.0.3`. We will then use the Hiera fact lookup that we
    discussed earlier in the book to set the host address and alternate names for
    UCP.
  prefs: []
  type: TYPE_NORMAL
- en: The next parameter will tell UCP that we will use an internal CA. We will then
    set our scheduler to use `spread`, define the ports for both Swarm and the controller,
    tell UCP to persevere the certs, and send the location of the license file for
    UCP. Next, we will set some date for Consul, such as the master IP, the interface
    to advertise, the image to use, and how many nodes to expect at the time of booting
    the Consul cluster. Lastly, we will set the variables we want our Docker daemon
    to use, such as the name of our Docker network, `swarm-private`, the network driver,
    `overlay`, the path to our certs, and the Docker host that we set in our `docker.sh`
    file placed at `/etc/profile.d/`.
  prefs: []
  type: TYPE_NORMAL
- en: So, as you can see, we have a whole lot of data in Hiera. However, as we have
    already discussed, there is data that could be changed for different environments.
    So as you can see, there is a massive benefit in making your module stateless
    and abstracting your data to Hiera, especially if you want to write modules that
    can scale easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now add the following code to our manifest file, `default.pp` located
    in the `manifests` folder in the root of our Vagrant repo. The following code
    defines our node definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then open our terminal and change the directory to the root of our Vagrant
    repo. We will then issue the `vagrant up` command to run Vagrant. Once the three
    boxes are built, you should get the following terminal output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then log in to the web URL at `https://127.0.0.1:8443`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will log in with the `admin` username and `orca` password. In the following
    screenshot, we can see that our cluster is up and healthy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![UCP](img/B05201_09_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since I thought to finish off with a bang, we will do something pretty cool
    now. I wanted to deploy an application that had multiple containers in which we
    could use interlock to showcase application routing/load balancing. What better
    application than Kubernetes. As I mentioned earlier, there are limitations to
    running Kubernetes like this, and it is only for lab purposes. The skills we can
    take away and apply to our Puppet modules is application routing/load balancing.
    In our last topic, we set the parameter for `$ucp_deploy_node` in the case statement
    in our `init.pp` file. In that particular block of code, we had a class called
    `compose.pp`. This is the class that will deploy Kubernetes across our UCP cluster.
    Let''s look at the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first resource just creates a directory called `kubernetes`. This is where
    we will place our Docker Compose file. Now, you will notice something different
    about how we are running Docker Compose. An exec? Why would we use an exec when
    we have a perfectly good provider that is tried and trusted. The reason we are
    using the exec in this case is because we are changing `$PATH` during the last
    Puppet run. What do I mean by that? Remember the file we added to the `/etc/profile.d/`
    directory? It changed the shell settings for where `DOCKER_HOST` is pointing.
    This will only come into effect in the next Puppet run, so the Docker daemon will
    not be pointing to the cluster. This will mean that all the kubernetes containers
    will come up on one host. This will cause a failure in the catalogue, as we will
    get a port collision from two containers using `8080`. Now, this will only come
    into effect when we run the module all at once, as a single catalogue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s have a look at our Docker Compose file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this book, I have been stressing on how I prefer using the Docker Compose
    method to deploy my container apps. This Docker Compose file is a perfect example
    why. We have seven containers in this compose file. I find it easy as we have
    all the logic right there in front of us and we have to write minimal code. Now,
    let's look at the code. The first thing that is different and that we wouldn't
    have seen so far is that we are declaring `version 2`. This is because version
    1.6.2 of Docker Compose has been released ([https://github.com/docker/compose/releases/tag/1.6.2](https://github.com/docker/compose/releases/tag/1.6.2)).
    So, to take advantage of the new features, we need to declare that we want to
    use `version 2`.
  prefs: []
  type: TYPE_NORMAL
- en: The first container we are declaring is interlock. We are going to use interlock
    as our application router that will make server requests to the Kubernetes API.
    For this container, we are going to forward ports, `443`, `8080`, and `8443`,
    to the host. We will then map `/etc/docker` from the host machine to the container.
    The reason for this is we need the keys to connect to the Swarm API. So, we will
    take advantage of the bundle we installed earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the command resource, we will tell interlock where to find the certs, the
    Swarm URL, and lastly, that we want to use `haproxy`. We will then add this container
    to our overlay network, `swarm-private`. The next thing is in the environment
    resource, we will set a constraint and we will tell Compose that we can only run
    interlock on `ucp-03`. We will do this to avoid port collision with the Kubernetes
    API service. The next container is `etcd`. Not much has changed regarding this
    since we configured Kubernetes in the scheduler chapter, so we will move on. The
    next container is the `kubernetes` API service. The one thing we need to call
    out with this is that we are declaring in the environment resource `- INTERLOCK_DATA={"hostname":"kubernetes","domain":"ucp-demo.local"}`.
    This is the URL that interlock will look for when it sends the request to the
    API.
  prefs: []
  type: TYPE_NORMAL
- en: This is the main reason we are running kubernetes to gain the skills of application
    routing. So, I won't go through the rest of the containers. Kubernetes is very
    well documented at [http://kubernetes.io/](http://kubernetes.io/). I would recommend
    that you read up on the features and explore Kubernetes—its a beast, there is
    so much to learn.
  prefs: []
  type: TYPE_NORMAL
- en: So, now we have all our code. Let's run it!
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to see the end-to-end build process, we will open our terminal and change
    the directory to the root of our Vagrant repo. If you have the servers built from
    the earlier topics, issue `vagrant destroy -f && vagrant up`; if not, just a simple
    `vagrant up` command will do. Once the Puppet run is complete, our terminal should
    have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then log in to our web UI at `https://127.0.0.1:8443`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will log in with the `admin` username and the `orca` password. We
    should see the following screenshot once we login successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You will note that we have one application now. If we click on the application
    twice, we can see that Kubernetes is up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You will note that we have our container split across `ucp-02` and `ucp-03`
    due to the environment settings in our Docker Compose file. One thing to take
    note of is that interlock in on `ucp-03` and the Kubernetes API service is on
    `ucp-02`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have built everything successfully, we need to log in to `ucp-03`
    and download the `kubectl` client. We can achieve that by issuing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'So let''s log in to `ucp-03` and issue the `vagrant ssh ucp-03` command from
    the root of our Vagrant repo. We will then change to root (`sudo -i`). Then, we
    will issue the `wget` command, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then make the file executable by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now remember that we set a URL in the environment settings for the `- INTERLOCK_DATA={"hostname":"kubernetes","domain":"ucp-demo.local"}`
    API container. We will need to set that value in our host file. So, use your favorite
    way to edit files on the local machine, such as vim, nano sed, and so on, and
    add `172.17.10.103 kubernetes.ucp-demo.local`. The IP address points to `ucp-03`
    as that is where interlock is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to test our cluster. We will do that by issuing the `./kubectl
    -s kubernetes.ucp-demo.local get nodes` command. We should get the following output
    after this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, everything is up and running. If we go back to our UCP console,
    you can see that the API server is running on `ucp-02` (with the IP address `172.17.10.102`).
    So, how are we doing this? Interlock is processing the HTTP calls on `8080` and
    routing them to our API server. This tells us that our application routing is
    in place. This is a very basic example, but something you should play with as
    you can really design some slick solutions using interlock.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we really focused on how to build a Puppet module that is shippable.
    The use of Hiera and the separation of data from the logic of the module is not
    only applicable to modules that deploy containers, but for any Puppet modules
    you write. At some point in your Puppet career, you will either open source a
    module or contribute to an already open-sourced module. What you have learned
    in this chapter will be invaluable in both of those use cases. Lastly, we finished
    with something fun, deploying Kubernetes as the frontend to UCP. In doing this,
    we also looked at application routing/load balancing. This is obviously a great
    skill to master as your container environment grows, especially to stay away from
    issues such as port collision.
  prefs: []
  type: TYPE_NORMAL
