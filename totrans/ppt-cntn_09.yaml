- en: Chapter 9. Best Practices for the Real World
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章。现实世界的最佳实践
- en: 'We have really covered a lot of ground in this book so far. We are now on our
    final chapter. Here, we are going to look at how to combine all the skills you
    have learned and create a production-ready module. Just to go out with a bang,
    we will create a module that configures and deploys Kubernetes as the frontend
    (note that running Kuberbetes as frontend has limitations and would not be the
    best idea for production. The UCP component of the module will be production ready).
    Since there will be a lot of sensitive data, we will take advantage of Hiera.
    We will create a custom fact to automate the retrieval of the UCP fingerprint,
    and we will split out all the kubernetes components and use interlock to proxy
    our API service. We will also take UCP further and look at how to repoint the
    Docker daemon to use the UCP cluster. The server architecture will follow the
    same design as we discussed in the scheduler chapter. We will use three nodes,
    all running Ubuntu 14.04 with an updated kernel to support the native Docker network
    namespace. We will cover the following topics in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本书中真的涵盖了很多内容。现在我们到了最后一章。在这里，我们将看看如何结合您学到的所有技能并创建一个可投入生产的模块。为了留下深刻印象，我们将创建一个配置和部署Kubernetes作为前端的模块（请注意，将Kubernetes作为前端运行有其限制，并不是生产中的最佳选择。模块的UCP组件将准备就绪）。由于将涉及大量敏感数据，我们将利用Hiera。我们将创建一个自定义事实以自动检索UCP指纹，并将所有Kubernetes组件拆分并使用interlock代理我们的API服务。我们还将深入研究UCP，并查看如何将Docker守护程序重新指向使用UCP集群。服务器架构将与我们在调度器章节中讨论的设计相同。我们将使用三个节点，所有节点都运行Ubuntu
    14.04，并更新内核以支持本地Docker网络命名空间。本章我们将涵盖以下主题：
- en: Hiera
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hiera
- en: The code
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码
- en: Hiera
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hiera
- en: In this topic, we will look at how to make our modules stateless. We will move
    all the data that is specific to the node that is applied to the module in Hiera
    ([https://docs.puppetlabs.com/hiera/3.1/](https://docs.puppetlabs.com/hiera/3.1/)).
    There are two main drivers behind this. The first is to remove any sensitive data
    such as passwords, keys, and so on, out of our modules. The second is if we remove
    node-specific data or state out our modules so that they are generic. We can apply
    them to any number of hosts without changing the logic of the module. This gives
    us the flexibility to publish our modules for other members of the Puppet community.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们将看看如何使我们的模块无状态。我们将移动所有应用于模块的特定于节点的数据到Hiera中（[https://docs.puppetlabs.com/hiera/3.1/](https://docs.puppetlabs.com/hiera/3.1/)）。这背后有两个主要动机。第一个是将任何敏感数据（如密码、密钥等）从我们的模块中移除。第二个是，如果我们将节点特定数据或状态从我们的模块中移除，使它们通用化，我们可以将它们应用于任意数量的主机而不改变模块的逻辑。这为我们提供了将模块发布给Puppet社区其他成员的灵活性。
- en: What data belongs in Hiera
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应该放在Hiera中的数据
- en: 'When we first sit down and start development on a new module, some of the things
    that we should consider are: whether we can make our module OS agnostic, how we
    can run the module on multiple machines without logic changes or extra development,
    and how we can protect our sensitive data?'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们第一次坐下来开始开发一个新模块时，我们应该考虑的一些事情是：我们是否可以使我们的模块与操作系统无关，我们如何在多台机器上运行模块而不需要逻辑更改或额外开发，以及我们如何保护我们的敏感数据？
- en: The answer to all these questions is Hiera.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题的答案都是Hiera。
- en: We are able to leverage Hiera by parameterizing our classes. This will allow
    Puppet to automatically look up Hiera at the beginning of the catalogue compilation.
    So, let's explore some examples of the data that you will put in Hiera.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过参数化我们的类来利用Hiera。这将允许Puppet在目录编译开始时自动查找Hiera。因此，让我们探讨一些您将放入Hiera中的数据示例。
- en: 'Note that in our chapter about container schedulers, we briefly used Hiera.
    We set the following values:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们关于容器调度程序的章节中，我们简要使用了Hiera。我们设置了以下值：
- en: '![What data belongs in Hiera](img/B05201_09_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![应该放在Hiera中的数据](img/B05201_09_01.jpg)'
- en: As you can see, we are setting parameters such as versions. Why would we want
    to set versions in Hiera and not straight in the module? If we set the version
    of Consul, for example, we might be running version 5.0 in production. Hashicorp
    just released version 6.0 by changing the Hiera value for the environment (for
    more information about Puppet environments, go to [https://docs.puppetlabs.com/puppet/latest/reference/environments.html](https://docs.puppetlabs.com/puppet/latest/reference/environments.html)).
    In Hiera versions from dev to 6.0, we can run multiple versions of the application
    with no module development.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们设置了诸如版本之类的参数。为什么我们要在 Hiera 中设置版本，而不是直接在模块中设置？例如，如果我们设置了 Consul 的版本，我们可能在生产环境中运行的是
    5.0 版本，而 Hashicorp 刚发布了 6.0 版本，通过更改 Hiera 中环境的值来进行切换（有关 Puppet 环境的更多信息，请访问 [https://docs.puppetlabs.com/puppet/latest/reference/environments.html](https://docs.puppetlabs.com/puppet/latest/reference/environments.html)）。在从开发环境到
    6.0 的 Hiera 版本中，我们可以运行多个版本的应用程序，而无需进行模块开发。
- en: The same can be done with IP addresses or URLs. So in the hieradata for dev,
    your Swarm cluster URL could be `dev.swarm.local` and production could be just
    `swarm.local`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的操作也适用于 IP 地址或 URL。例如，在开发环境的 hieradata 中，你的 Swarm 集群 URL 可以是 `dev.swarm.local`，而生产环境中则可以是
    `swarm.local`。
- en: Another type of data that you will want to separate is passwords/keys. You wouldn't
    want the same password/key in your dev environment as there are in production.
    Again, Hiera will let you obfuscate this data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种你需要分离的数据类型是密码/密钥。你不希望开发环境中的密码/密钥与生产环境中的相同。同样，Hiera 允许你对这些数据进行模糊处理。
- en: We can then take the protection of this data further using eyaml, which Puppet
    supports. This allows you to use keys to encrypt your Hiera `.yaml` files. So,
    when the files are checked into source control, they are encrypted. This helps
    prevent data leakage. For more information on eyaml, visit [https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml](https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 Puppet 支持的 eyaml 进一步保护这些数据。这使你可以使用密钥加密 Hiera 的 `.yaml` 文件。因此，当文件被提交到源代码管理时，它们会被加密。这有助于防止数据泄漏。有关
    eyaml 的更多信息，请访问 [https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml](https://puppetlabs.com/blog/encrypt-your-data-using-hiera-eyaml)。
- en: As you see, Hiera gives you the flexibility to move your data from the module
    to Hiera to externalize configurations, making the module stateless.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Hiera 为你提供了将数据从模块转移到 Hiera 以外部化配置的灵活性，从而使模块保持无状态。
- en: Tips and tricks for Hiera
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hiera 的小贴士和技巧
- en: There are some really handy tips that you can refer to when using Hiera. Puppet
    allows you to call functions, lookups, and query facts from Hiera. Mastering these
    features will come in very handy. If this is new to you, read the document available
    at [https://docs.puppetlabs.com/hiera/3.1/variables.html](https://docs.puppetlabs.com/hiera/3.1/variables.html)
    before moving on in the chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Hiera 时，有一些非常实用的小贴士可以参考。Puppet 允许你调用函数、查找项和查询事实。掌握这些功能会非常有用。如果这是你第一次接触，建议在继续本章节之前阅读
    [https://docs.puppetlabs.com/hiera/3.1/variables.html](https://docs.puppetlabs.com/hiera/3.1/variables.html)
    中的文档。
- en: 'So, let''s look at an example of looking up facts from Hiera. First, why would
    you want to do this? One really good reason is IP address lookups. If you have
    a class that you are applying to three nodes and you have to advertise an IP address
    like we did in our `consul` module, setting the IP address in Hiera will not work,
    as the IP address will be different for each machine. We create a file called
    `node.yaml` and add the IP address there. The issue is that we will now have multiple
    Hiera files. Every time Puppet loads the catalogue, it looks up all the Hiera
    files to check whether any values have changed. The more files we have, the more
    load it puts on the master and the slower our Puppet runs will become. So we can
    tell Hiera to look up the fact and the interface we want to advertise. Here is
    an example of what the code would look like:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看一个从 Hiera 查找事实的示例。首先，为什么要这么做？一个非常好的理由是 IP 地址查找。如果你有一个类应用到三个节点，并且需要像在
    `consul` 模块中那样宣传一个 IP 地址，将 IP 地址设置在 Hiera 中是行不通的，因为每台机器的 IP 地址会不同。我们创建一个名为 `node.yaml`
    的文件并将 IP 地址添加到那里。问题是，现在我们将有多个 Hiera 文件。每次 Puppet 加载目录时，它都会查找所有 Hiera 文件，检查是否有值发生变化。文件越多，主服务器的负载就越大，Puppet
    执行的速度也会变慢。因此，我们可以告诉 Hiera 查找我们想要宣传的事实和接口。以下是代码的示例：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The one call out is that if we called the fact from a module, we would call
    it with the fully qualified name, `$::ipaddress_eth1`. Unfortunately, Hiera does
    not support the use of this. So we can use the short name for the `::ipaddress_eth1`
    fact.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一需要注意的是，如果我们从模块中调用这个事实，我们将使用完全限定的名称`$::ipaddress_eth1`。不幸的是，Hiera不支持使用这个名称。所以我们可以使用`::ipaddress_eth1`事实的简短名称。
- en: The code
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: Now that we have a good understanding of how to make our module stateless, let's
    go ahead and start coding. We will split the coding into two parts. In the first
    part we will write the module to install and configure Docker UCP. The final topic
    will be to run Kubernetes as the frontend.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地理解了如何使我们的模块无状态，让我们开始编写代码吧。我们将把编码分成两部分。在第一部分中，我们将编写安装和配置Docker UCP的模块。最后的主题将是运行Kubernetes作为前端。
- en: UCP
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UCP
- en: 'The first thing that we need to do is create a new Vagrant repo for this chapter.
    By now, we should be masters at creating a new Vagrant repo. Once we have created
    that, we will create a new module called `<AUTHOR>-ucpconfig` and move it to our
    `modules` directory in the root of our Vagrant repo. We will first set up our
    `servers.yml` file by adding the code shown in the following screenshot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是为本章创建一个新的Vagrant仓库。到现在为止，我们应该已经掌握了如何创建一个新的Vagrant仓库。一旦创建好仓库，我们将创建一个新的模块，名为`<AUTHOR>-ucpconfig`，并将其移动到Vagrant仓库根目录下的`modules`目录中。我们将首先通过添加下图所示的代码来设置`servers.yml`文件：
- en: '![UCP](img/B05201_09_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_02.jpg)'
- en: 'As you can see, we are setting up three servers, where `ucp-01` will be the
    master of the cluster and the other two nodes will join the cluster. We will add
    two more files: `config.json` and `docker_subscription.lic`. Here, `config.json`
    will contain the authorization key to Docker Hub, and `docker_subscription.lic`
    will contain our trial license for UCP. Note that we covered both of these files
    in the container scheduler chapter. If you are having issues setting up these
    files, refer to that chapter.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们正在设置三台服务器，其中`ucp-01`将成为集群的主节点，另外两个节点将加入集群。我们将添加两个文件：`config.json`和`docker_subscription.lic`。其中，`config.json`将包含Docker
    Hub的授权密钥，`docker_subscription.lic`将包含我们UCP的试用许可证。请注意，我们在容器调度章节中已经讲解了这两个文件。如果你在设置这些文件时遇到问题，请参考该章节。
- en: 'The file we will now look at is the puppetfile. We need to add the code shown
    in the following screenshot to it:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要查看的文件是puppetfile。我们需要将下图所示的代码添加到该文件中：
- en: '![UCP](img/B05201_09_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_03.jpg)'
- en: 'Now that we have our Vagrant repo set up, we can move on to our module. We
    will need to create four files: `config.pp`, `master.pp`, `node.pp`, and `params.pp`.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了Vagrant仓库，可以继续进行我们的模块开发了。我们将需要创建四个文件：`config.pp`、`master.pp`、`node.pp`和`params.pp`。
- en: 'The first file we will look at is `params.pp`. Again, I''d like to code this
    file first as it sets a good foundation for the rest of the module. We do this
    as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要查看的文件是`params.pp`。我想先编写这个文件，因为它为模块的其余部分奠定了良好的基础。我们这样做如下：
- en: '![UCP](img/B05201_09_04.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_04.jpg)'
- en: As you can see, we are setting all our parameters. We will look at each one
    in depth as we apply it to the class. This way, we will have context on the value
    we are setting. You might have noted that we have set a lot of variables to empty
    strings. This is because we will use Hiera to look up the values. I have hardcoded
    some values in as well, such as the UCP version as 1.0.0\. This is just a default
    value if there is no value in Hiera. However, we will be setting the value for
    UCP to 1.0.3\. The expected behavior is that the Hiera value will take precedence.
    You will note that there is a fact that we are referencing, which is `$::ucp_fingerprint`.
    This is a custom fact. This will automate the passing of the UCP fingerprint.
    If you remember, in the container scheduler chapter, we had to build `ucp-01`
    to get the fingerprint and add it to Hiera for the other nodes' benefit. With
    the custom fact, we will automate the process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们正在设置所有的参数。我们将深入研究每一个参数，并在将其应用到类时了解它的含义。这样，我们就能理解所设置值的背景。你可能注意到，我们将很多变量设置为空字符串。这是因为我们将使用Hiera来查找这些值。我也硬编码了一些值，比如UCP版本为1.0.0。这只是一个默认值，如果Hiera中没有相应的值，就会使用这个值。然而，我们将把UCP的版本设置为1.0.3。预期的行为是Hiera中的值会优先级更高。你会注意到我们引用了一个事实，即`$::ucp_fingerprint`。这是一个自定义的事实。它将自动传递UCP的指纹。如果你记得，在容器调度章节中，我们必须构建`ucp-01`来获取指纹并将其添加到Hiera，供其他节点使用。通过自定义事实，我们将自动化这个过程。
- en: 'To create a custom fact, we will first need to create a `lib` folder in the
    root of the module. Under that folder, we will create a folder called `facter`.
    In that folder, we will create a file called `ucp_fingerprint.rb`. When writing
    a custom fact, the filename needs to be the same as the fact''s name. The code
    that we will add to our custom fact is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个自定义事实，首先我们需要在模块的根目录下创建一个 `lib` 文件夹。在该文件夹下，我们将创建一个名为 `facter` 的文件夹。在该文件夹中，我们将创建一个名为
    `ucp_fingerprint.rb` 的文件。在编写自定义事实时，文件名需要与事实的名称相同。我们将添加到自定义事实中的代码如下：
- en: '![UCP](img/B05201_09_05.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_05.jpg)'
- en: In this code, you can see that we are adding a `bash` command to query our UCP
    master in order to get the fingerprint. I have hardcoded the IP address of the
    master in this fact. In our environment, we would write logic for that to be more
    fluid so that we are allowed to have multiple environments, hostnames, and so
    on. The main part to take away from the custom fact is the command itself.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，你可以看到我们添加了一个 `bash` 命令来查询我们的 UCP master，以获取指纹。我在这个事实中硬编码了主机的 IP 地址。在我们的环境中，我们会编写逻辑来使其更加灵活，以便允许有多个环境、主机名等。关于自定义事实，最重要的是理解命令本身。
- en: 'We will now move back to our `init.pp` file, which is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将返回到我们的`init.pp`文件，内容如下：
- en: '![UCP](img/B05201_09_06.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_06.jpg)'
- en: The first thing you can see it that we are declaring all our variables at the
    top of our class. This is where Puppet will look up Hiera and match any variable
    that we have declared. The first block of code in the module is going to set up
    our Docker daemon. We are going to add the extra configuration to tell the daemon
    where it can find the backend for the Docker native network. Then, we will declare
    a case statement on the `$::hostname` fact. You will note that we have set a parameter
    for the hostname. This is to make our module more portable. Inside the case statement,
    if the host is a master, you can see that we will use our Consul container as
    the backend for our Docker network. Then, we will order the execution of our classes
    that are applied to the node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先可以看到的是，我们在类的顶部声明了所有的变量。这是 Puppet 查找 Hiera 并匹配我们声明的任何变量的地方。模块中的第一段代码将设置我们的
    Docker 守护进程。我们将添加额外的配置来告诉守护进程它可以在哪里找到 Docker 原生网络的后端。然后，我们将在`$::hostname`事实上声明一个
    case 语句。你会注意到，我们为主机名设置了一个参数。这是为了使我们的模块更加可移植。在 case 语句中，如果主机是 master，你会看到我们将使用我们的
    Consul 容器作为 Docker 网络的后端。接着，我们将按顺序执行应用于节点的类。
- en: In the next block of code in the case statement, we have declared the `$ucp_deploy_node`
    variable for the `$::hostname` fact. We will use this node to deploy Kubernetes
    from. We will get back to this later in the topic. The final block of code in
    our case statement is the `catch all` or `default`. If Puppet cannot find the
    fact of `$::hostname` in either of our declared variables, it will apply these
    classes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 case 语句中的下一个代码块中，我们为 `$::hostname` 事实声明了 `$ucp_deploy_node` 变量。我们将使用此节点来部署
    Kubernetes。稍后我们会回到这个话题。我们 case 语句中的最后一段代码是 `catch all` 或 `default`。如果 Puppet 无法在我们声明的变量中找到
    `$::hostname` 的事实，它将应用这些类。
- en: 'We will now move on to our `master.pp` file, which is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续我们的 `master.pp` 文件，内容如下：
- en: '![UCP](img/B05201_09_07.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_07.jpg)'
- en: 'The first thing you will note is that we are declaring our variables again
    at the top of this class. We are doing this as we are declaring many parameters,
    and this makes our module a lot more readable. Doing this in complex modules is
    a must, and I would really recommend you to follow this practice. It will make
    your life a lot easier when it comes to debugging. As you can see in the following
    screenshot, we are tying the parameters back to our `init.pp` file:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到的是，我们再次在这个类的顶部声明了变量。这样做是因为我们声明了很多参数，这使得我们的模块更具可读性。在复杂的模块中这样做是必须的，我真的推荐你遵循这种做法。当涉及到调试时，这会让你的工作轻松很多。正如你在下面的截图中看到的，我们将参数绑定回我们的`init.pp`文件：
- en: '![UCP](img/B05201_09_08.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_08.jpg)'
- en: As you can see from the preceding code, there are very few types which we are
    values that we are setting in the module.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码可以看出，我们在模块中设置的值的类型非常少。
- en: 'We will now move on to our `node.pp` file, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续处理我们的 `node.pp` 文件，内容如下：
- en: '![UCP](img/B05201_09_09.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_09.jpg)'
- en: As you can see, we are declaring the parameters at the top of the class, again
    tying them back to our `init.pp` file. We have declared most of our values as
    we will use Hiera.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们在类的顶部声明了参数，再次将它们与我们的`init.pp`文件关联。我们已经声明了大部分的值，因为我们将使用Hiera。
- en: 'We will now move on to our `config.pp` file, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续我们的`config.pp`文件，如下所示：
- en: '![UCP](img/B05201_09_10.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_10.jpg)'
- en: 'In this class, we are going to make a few vital configurations for our cluster.
    So, we will walk through each block of code individually. Let''s take a look at
    the first one:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们将进行一些对集群至关重要的配置。所以，我们将逐个代码块地讲解。让我们先看看第一个：
- en: '![UCP](img/B05201_09_11.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_11.jpg)'
- en: 'In this block, we will declare our variables as we have in all the classes
    in this module. One call out that I have not mentioned yet is that we are only
    declaring the variables that are applied to its class. We are not declaring all
    the parameters. Let''s look at the next set of code now:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码块中，我们将声明我们的变量，就像在本模块的所有类中一样。有一点我还没有提到，那就是我们仅声明应用于其类的变量，并不是声明所有的参数。现在让我们看看下一个代码块：
- en: '![UCP](img/B05201_09_12.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_12.jpg)'
- en: 'In this block of code, we will pass an array of packages that we need to curl
    our UCP master in order to get the SSL bundle that we will need for TLS coms between
    nodes. Now, let''s see the third block:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们将传递一个包数组，这些包是我们需要用来通过`curl`连接到UCP主节点并获取SSL证书包的，以便节点之间的TLS通信。现在，让我们看看第三个代码块：
- en: '![UCP](img/B05201_09_13.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_13.jpg)'
- en: 'In this code, we are going to create a shell script called `get_ca.sh.erb`
    to get the bundle from the master. Let''s create the file, and the first thing
    that we will need to do is create a `templates` folder in the root of the module.
    Then, we can create our `get_ca.sh.erb` file in the `templates` folder. We will
    add the following code to the file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们将创建一个名为`get_ca.sh.erb`的脚本，从主节点获取证书包。首先，我们需要在模块的根目录下创建一个`templates`文件夹，然后在该文件夹中创建我们的`get_ca.sh.erb`文件。我们将向该文件中添加以下代码：
- en: '![UCP](img/B05201_09_14.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_14.jpg)'
- en: 'As you can see in the script, we need to create an `auth` token and pass it
    to the API. Puppet does not natively handle tasks like these well, as we are using
    variables inside the `curl` command. Creating a template file and running an `exec`
    function is fine as long as we make it idempotent. In the next block of code,
    we will do this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在脚本中所看到的，我们需要创建一个`auth`令牌并将其传递给API。Puppet本身并不擅长处理这些任务，因为我们在`curl`命令中使用了变量。只要我们确保其幂等性，创建一个模板文件并运行`exec`函数是可以的。在下一个代码块中，我们将这样做：
- en: '![UCP](img/B05201_09_15.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_15.jpg)'
- en: In this block of code, we will run the preceding script. You can see that we
    have set the parameters, `command` `path`, and **cwd** (**current working directory**).
    The next resource is `creates` that tells Puppet that there should be a file called
    `ca.pem` in the current working directory. If Puppet finds that the file does
    not exist, it will execute the exec, and if the file does exist, Puppet will do
    nothing. This will give our exec idempotency.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们将运行之前的脚本。你可以看到，我们设置了`command`、`path`和**cwd**（**当前工作目录**）这几个参数。下一个资源是`creates`，它告诉Puppet当前工作目录中应该有一个名为`ca.pem`的文件。如果Puppet发现该文件不存在，它将执行`exec`，如果文件存在，Puppet将什么也不做。这将使我们的`exec`具备幂等性。
- en: 'In the next block of code, we will create a file in `/etc/profile.d`, which
    will then point the Docker daemon on each node to the master''s IP address, allowing
    us to schedule containers across the cluster:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码块中，我们将在`/etc/profile.d`中创建一个文件，该文件将把每个节点上的Docker守护进程指向主节点的IP地址，从而使我们能够在集群中调度容器：
- en: '![UCP](img/B05201_09_16.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_16.jpg)'
- en: 'Now, let''s create a `docker.sh` file in our `templates` directory. In the
    file, we will put the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在`templates`目录中创建一个`docker.sh`文件。在该文件中，我们将放入以下代码：
- en: '![UCP](img/B05201_09_17.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_17.jpg)'
- en: In this file, we are telling the Docker daemon to use TLS, setting the location
    of the key files that we got from the bundle earlier in the class. The last thing
    we are setting is the Docker host that will point to the UCP master.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，我们告诉Docker守护进程使用TLS，设置我们之前在类中获取的密钥文件的位置。我们设置的最后一项是Docker主机，它将指向UCP主节点。
- en: 'This last block of code in this class should look very familiar, as we are
    setting up a Docker network:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类中的最后一段代码应该非常熟悉，因为我们正在设置一个Docker网络：
- en: '![UCP](img/B05201_09_18.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_18.jpg)'
- en: 'We can now move on to the the file where all our data is, our Hiera file. That
    file is located in the `hieradata` folder, which is present in the root of our
    Vagrant repo. The following screenshot shows the various data present in the Hiera
    file:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续处理存放所有数据的文件——我们的 Hiera 文件。该文件位于 `hieradata` 文件夹中，该文件夹位于 Vagrant 仓库的根目录。以下截图展示了
    Hiera 文件中的各种数据：
- en: '![UCP](img/B05201_09_19.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_19.jpg)'
- en: So, let's list out all the data we have defined here. We are defining the UCP
    master as `ucp-01`, and our deploy node is `ucp-03` (this is the node that we
    are deploying Kubernetes from). The UCP URL of the master is `https://172.17.10.101`.
    This is used when we connect nodes to the master and also when we get our `ca`
    bundle and UCP fingerprint. We keep the username and password as `admin` and `orca`.
    We will use UCP version `1.0.3`. We will then use the Hiera fact lookup that we
    discussed earlier in the book to set the host address and alternate names for
    UCP.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们列出我们在这里定义的所有数据。我们将 UCP 主节点定义为 `ucp-01`，我们的部署节点为 `ucp-03`（这是我们从中部署 Kubernetes
    的节点）。主节点的 UCP URL 为 `https://172.17.10.101`。当我们将节点连接到主节点时，或者获取我们的 `ca` 包和 UCP
    指纹时，都将使用该 URL。我们将用户名和密码保持为 `admin` 和 `orca`。我们将使用 UCP 版本 `1.0.3`。然后，我们将使用之前在本书中讨论过的
    Hiera fact 查找来设置 UCP 的主机地址和备用名称。
- en: The next parameter will tell UCP that we will use an internal CA. We will then
    set our scheduler to use `spread`, define the ports for both Swarm and the controller,
    tell UCP to persevere the certs, and send the location of the license file for
    UCP. Next, we will set some date for Consul, such as the master IP, the interface
    to advertise, the image to use, and how many nodes to expect at the time of booting
    the Consul cluster. Lastly, we will set the variables we want our Docker daemon
    to use, such as the name of our Docker network, `swarm-private`, the network driver,
    `overlay`, the path to our certs, and the Docker host that we set in our `docker.sh`
    file placed at `/etc/profile.d/`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个参数将告诉 UCP 我们将使用内部 CA。接下来，我们将设置调度器使用 `spread`，定义 Swarm 和控制器的端口，告诉 UCP 保留证书，并发送
    UCP 许可证文件的位置。接下来，我们将设置一些 Consul 的日期信息，例如主节点 IP、要广告的接口、要使用的镜像，以及在启动 Consul 集群时预期的节点数量。最后，我们将设置
    Docker 守护进程使用的变量，例如我们的 Docker 网络名称 `swarm-private`、网络驱动程序 `overlay`、证书路径以及我们在
    `docker.sh` 文件中设置的 Docker 主机，该文件位于 `/etc/profile.d/`。
- en: So, as you can see, we have a whole lot of data in Hiera. However, as we have
    already discussed, there is data that could be changed for different environments.
    So as you can see, there is a massive benefit in making your module stateless
    and abstracting your data to Hiera, especially if you want to write modules that
    can scale easily.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在 Hiera 中有大量数据。然而，正如我们之前讨论的，某些数据可能会根据不同的环境发生变化。因此，正如你所看到的，将模块设为无状态并将数据抽象到
    Hiera 中，尤其是当你希望编写易于扩展的模块时，是非常有益的。
- en: 'We will now add the following code to our manifest file, `default.pp` located
    in the `manifests` folder in the root of our Vagrant repo. The following code
    defines our node definition:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将以下代码添加到我们的清单文件 `default.pp` 中，该文件位于 Vagrant 仓库根目录的 `manifests` 文件夹中。以下代码定义了我们的节点定义：
- en: '![UCP](img/B05201_09_20.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_20.jpg)'
- en: 'We can then open our terminal and change the directory to the root of our Vagrant
    repo. We will then issue the `vagrant up` command to run Vagrant. Once the three
    boxes are built, you should get the following terminal output:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以打开终端并将目录切换到 Vagrant 仓库的根目录。接下来，我们将发出 `vagrant up` 命令来运行 Vagrant。当三个虚拟机构建完成后，你应该会看到以下终端输出：
- en: '![UCP](img/B05201_09_21.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_21.jpg)'
- en: 'We can then log in to the web URL at `https://127.0.0.1:8443`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以登录到网址 `https://127.0.0.1:8443`：
- en: '![UCP](img/B05201_09_22.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_22.jpg)'
- en: 'We will log in with the `admin` username and `orca` password. In the following
    screenshot, we can see that our cluster is up and healthy:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `admin` 用户名和 `orca` 密码进行登录。在以下截图中，我们可以看到我们的集群已启动并处于健康状态：
- en: '![UCP](img/B05201_09_23.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![UCP](img/B05201_09_23.jpg)'
- en: Kubernetes
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes
- en: 'Since I thought to finish off with a bang, we will do something pretty cool
    now. I wanted to deploy an application that had multiple containers in which we
    could use interlock to showcase application routing/load balancing. What better
    application than Kubernetes. As I mentioned earlier, there are limitations to
    running Kubernetes like this, and it is only for lab purposes. The skills we can
    take away and apply to our Puppet modules is application routing/load balancing.
    In our last topic, we set the parameter for `$ucp_deploy_node` in the case statement
    in our `init.pp` file. In that particular block of code, we had a class called
    `compose.pp`. This is the class that will deploy Kubernetes across our UCP cluster.
    Let''s look at the file:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我想以一个精彩的方式结束，我们现在来做一些很酷的事情。我想部署一个有多个容器的应用程序，我们可以使用 interlock 来展示应用路由/负载均衡。有什么比
    Kubernetes 更好的应用程序呢？正如我之前提到的，像这样运行 Kubernetes 有一些局限性，它仅限于实验室使用。我们可以从中学到的技能并将其应用到我们的
    Puppet 模块中，就是应用路由/负载均衡。在我们上一节中，我们在 `init.pp` 文件的 case 语句中设置了 `$ucp_deploy_node`
    参数。在那个特定的代码块中，我们有一个叫做 `compose.pp` 的类。这个类将会在我们的 UCP 集群中部署 Kubernetes。让我们看一下这个文件：
- en: '![Kubernetes](img/B05201_09_24.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes](img/B05201_09_24.jpg)'
- en: The first resource just creates a directory called `kubernetes`. This is where
    we will place our Docker Compose file. Now, you will notice something different
    about how we are running Docker Compose. An exec? Why would we use an exec when
    we have a perfectly good provider that is tried and trusted. The reason we are
    using the exec in this case is because we are changing `$PATH` during the last
    Puppet run. What do I mean by that? Remember the file we added to the `/etc/profile.d/`
    directory? It changed the shell settings for where `DOCKER_HOST` is pointing.
    This will only come into effect in the next Puppet run, so the Docker daemon will
    not be pointing to the cluster. This will mean that all the kubernetes containers
    will come up on one host. This will cause a failure in the catalogue, as we will
    get a port collision from two containers using `8080`. Now, this will only come
    into effect when we run the module all at once, as a single catalogue.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个资源只是创建一个名为 `kubernetes` 的目录。我们将在这里放置我们的 Docker Compose 文件。现在，你会注意到我们运行 Docker
    Compose 的方式有些不同。使用 exec？为什么我们要用 exec，而不是使用一个已经验证过的、完全可靠的 provider 呢？我们在这种情况下使用
    exec 的原因是，我们在上次 Puppet 运行时更改了 `$PATH`。我是什么意思呢？还记得我们添加到 `/etc/profile.d/` 目录的文件吗？它更改了
    `DOCKER_HOST` 指向的 shell 设置。这只会在下一次 Puppet 运行时生效，所以 Docker 守护进程不会指向集群。这就意味着所有的
    Kubernetes 容器都会在同一台主机上启动。这将导致清单失败，因为我们会遇到两个容器使用 `8080` 端口时的冲突。现在，这只会在我们一次性运行整个模块时生效，也就是作为一个单独的清单。
- en: 'Now, let''s have a look at our Docker Compose file:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下我们的 Docker Compose 文件：
- en: '![Kubernetes](img/B05201_09_25.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes](img/B05201_09_25.jpg)'
- en: In this book, I have been stressing on how I prefer using the Docker Compose
    method to deploy my container apps. This Docker Compose file is a perfect example
    why. We have seven containers in this compose file. I find it easy as we have
    all the logic right there in front of us and we have to write minimal code. Now,
    let's look at the code. The first thing that is different and that we wouldn't
    have seen so far is that we are declaring `version 2`. This is because version
    1.6.2 of Docker Compose has been released ([https://github.com/docker/compose/releases/tag/1.6.2](https://github.com/docker/compose/releases/tag/1.6.2)).
    So, to take advantage of the new features, we need to declare that we want to
    use `version 2`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我一直强调我更喜欢使用 Docker Compose 方法来部署我的容器应用程序。这份 Docker Compose 文件正是这一方法的完美示例。我们在这个
    Compose 文件中有七个容器。我觉得这样很简单，因为所有的逻辑都摆在我们面前，我们只需要写最少的代码。现在，让我们看看代码。首先不同的是，我们声明了 `version
    2`。这是因为 Docker Compose 的 1.6.2 版本已发布（[https://github.com/docker/compose/releases/tag/1.6.2](https://github.com/docker/compose/releases/tag/1.6.2)）。因此，为了利用新特性，我们需要声明我们想使用
    `version 2`。
- en: The first container we are declaring is interlock. We are going to use interlock
    as our application router that will make server requests to the Kubernetes API.
    For this container, we are going to forward ports, `443`, `8080`, and `8443`,
    to the host. We will then map `/etc/docker` from the host machine to the container.
    The reason for this is we need the keys to connect to the Swarm API. So, we will
    take advantage of the bundle we installed earlier in the chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明的第一个容器是 Interlock。我们将使用 Interlock 作为应用程序路由器，它会向 Kubernetes API 发起服务器请求。对于这个容器，我们将端口
    `443`、`8080` 和 `8443` 转发到主机。然后，我们会将主机的 `/etc/docker` 映射到容器中。这样做的原因是我们需要密钥来连接到
    Swarm API。所以，我们将利用本章早些时候安装的 bundle。
- en: In the command resource, we will tell interlock where to find the certs, the
    Swarm URL, and lastly, that we want to use `haproxy`. We will then add this container
    to our overlay network, `swarm-private`. The next thing is in the environment
    resource, we will set a constraint and we will tell Compose that we can only run
    interlock on `ucp-03`. We will do this to avoid port collision with the Kubernetes
    API service. The next container is `etcd`. Not much has changed regarding this
    since we configured Kubernetes in the scheduler chapter, so we will move on. The
    next container is the `kubernetes` API service. The one thing we need to call
    out with this is that we are declaring in the environment resource `- INTERLOCK_DATA={"hostname":"kubernetes","domain":"ucp-demo.local"}`.
    This is the URL that interlock will look for when it sends the request to the
    API.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令资源中，我们将告诉 Interlock 哪里可以找到证书、Swarm URL，并且最后，告诉它我们要使用 `haproxy`。接下来，我们将把这个容器添加到我们的覆盖网络
    `swarm-private`。接下来，在环境资源中，我们将设置一个约束，并告诉 Compose 只在 `ucp-03` 上运行 Interlock。我们这样做是为了避免与
    Kubernetes API 服务的端口冲突。下一个容器是 `etcd`。自从我们在调度器章节中配置 Kubernetes 后，关于这一部分没有太大变化，因此我们将跳过。下一个容器是
    `kubernetes` API 服务。需要特别注意的是，在环境资源中我们声明了 `- INTERLOCK_DATA={"hostname":"kubernetes","domain":"ucp-demo.local"}`。这是
    Interlock 在发送请求到 API 时将查找的 URL。
- en: This is the main reason we are running kubernetes to gain the skills of application
    routing. So, I won't go through the rest of the containers. Kubernetes is very
    well documented at [http://kubernetes.io/](http://kubernetes.io/). I would recommend
    that you read up on the features and explore Kubernetes—its a beast, there is
    so much to learn.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们运行 Kubernetes 的主要原因，目的是掌握应用程序路由的技能。因此，我不会继续讲解其余的容器。Kubernetes 在[http://kubernetes.io/](http://kubernetes.io/)上有详细的文档。我建议你阅读一下相关特性，并探索
    Kubernetes——它非常强大，有很多东西可以学习。
- en: So, now we have all our code. Let's run it!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有的代码。让我们运行它吧！
- en: 'Just to see the end-to-end build process, we will open our terminal and change
    the directory to the root of our Vagrant repo. If you have the servers built from
    the earlier topics, issue `vagrant destroy -f && vagrant up`; if not, just a simple
    `vagrant up` command will do. Once the Puppet run is complete, our terminal should
    have the following output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到完整的构建过程，我们将打开终端并将目录切换到我们 Vagrant 仓库的根目录。如果你已经根据之前的章节构建了服务器，可以运行 `vagrant
    destroy -f && vagrant up`；如果没有，直接运行 `vagrant up` 命令即可。一旦 Puppet 执行完毕，我们的终端应该会显示以下输出：
- en: '![Kubernetes](img/B05201_09_26.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes](img/B05201_09_26.jpg)'
- en: 'We can then log in to our web UI at `https://127.0.0.1:8443`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以登录到我们的 Web UI，地址是 `https://127.0.0.1:8443`：
- en: '![Kubernetes](img/B05201_09_27.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes](img/B05201_09_27.jpg)'
- en: 'Then, we will log in with the `admin` username and the `orca` password. We
    should see the following screenshot once we login successfully:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用 `admin` 用户名和 `orca` 密码登录。登录成功后，我们应该能看到如下截图：
- en: '![Kubernetes](img/B05201_09_28.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes](img/B05201_09_28.jpg)'
- en: 'You will note that we have one application now. If we click on the application
    twice, we can see that Kubernetes is up and running:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到现在我们有了一个应用程序。如果我们双击该应用程序，就可以看到 Kubernetes 已经启动并运行：
- en: '![Kubernetes](img/B05201_09_29.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes](img/B05201_09_29.jpg)'
- en: You will note that we have our container split across `ucp-02` and `ucp-03`
    due to the environment settings in our Docker Compose file. One thing to take
    note of is that interlock in on `ucp-03` and the Kubernetes API service is on
    `ucp-02`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，由于我们在 Docker Compose 文件中的环境设置，我们的容器被分布在 `ucp-02` 和 `ucp-03` 上。需要注意的一点是，Interlock
    在 `ucp-03` 上，Kubernetes API 服务则在 `ucp-02` 上。
- en: 'Now that we have built everything successfully, we need to log in to `ucp-03`
    and download the `kubectl` client. We can achieve that by issuing the following
    command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功构建了一切，我们需要登录到 `ucp-03` 并下载 `kubectl` 客户端。我们可以通过发出以下命令来实现：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'So let''s log in to `ucp-03` and issue the `vagrant ssh ucp-03` command from
    the root of our Vagrant repo. We will then change to root (`sudo -i`). Then, we
    will issue the `wget` command, as shown in the following screenshot:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_30.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: 'We will then make the file executable by issuing the following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now remember that we set a URL in the environment settings for the `- INTERLOCK_DATA={"hostname":"kubernetes","domain":"ucp-demo.local"}`
    API container. We will need to set that value in our host file. So, use your favorite
    way to edit files on the local machine, such as vim, nano sed, and so on, and
    add `172.17.10.103 kubernetes.ucp-demo.local`. The IP address points to `ucp-03`
    as that is where interlock is running.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to test our cluster. We will do that by issuing the `./kubectl
    -s kubernetes.ucp-demo.local get nodes` command. We should get the following output
    after this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes](img/B05201_09_31.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: As you can see, everything is up and running. If we go back to our UCP console,
    you can see that the API server is running on `ucp-02` (with the IP address `172.17.10.102`).
    So, how are we doing this? Interlock is processing the HTTP calls on `8080` and
    routing them to our API server. This tells us that our application routing is
    in place. This is a very basic example, but something you should play with as
    you can really design some slick solutions using interlock.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we really focused on how to build a Puppet module that is shippable.
    The use of Hiera and the separation of data from the logic of the module is not
    only applicable to modules that deploy containers, but for any Puppet modules
    you write. At some point in your Puppet career, you will either open source a
    module or contribute to an already open-sourced module. What you have learned
    in this chapter will be invaluable in both of those use cases. Lastly, we finished
    with something fun, deploying Kubernetes as the frontend to UCP. In doing this,
    we also looked at application routing/load balancing. This is obviously a great
    skill to master as your container environment grows, especially to stay away from
    issues such as port collision.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
