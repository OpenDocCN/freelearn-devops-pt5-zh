<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-self-adaptation-applied-to-instrumented-services" class="calibre15">Self-Adaptation Applied to Instrumented Services</h2>

<p class="calibre3">An <a href="https://prometheus.io/docs/practices/instrumentation/">instrumented service</a> provides more detailed metrics then what we can scrape from <a href="https://prometheus.io/docs/instrumenting/exporters/">exporters</a>. The ability to add all the metrics we might need opens the doors that are often closed with exporters. That does not mean that they are any less useful but that we need to think of the nature of the resource we are observing.</p>

<p class="calibre3">Hardware metrics should be scraped from exporters. After all, we cannot instrument CPU. Third-party services are another good example of a use-case where exporters are often a better option. If we use a database, we should look for an exporter that fetches metrics from it and transforms them into the Prometheus-friendly format. The same goes for proxies, gateways, and just about almost any other service that we did not develop.</p>

<p class="calibre3">We might choose to <a href="https://prometheus.io/docs/instrumenting/writing_exporters/">write an exporter</a> even for services we are in control of if we already invested a lot of time implementing metrics that are not in Prometheus format.</p>

<p class="calibre3">Exporters can get us only half-way through. We can instruct the system to scale based on, for example, memory utilization. <a href="https://github.com/google/cadvisor">cAdvisor</a> provides information about the containers running inside the cluster, but the metrics it provides are too generic. We cannot get service-specific data. Inability to fine-tune metrics on per-service basis leaves us with insufficient information that can be used only for basic alerts. Instrumentation provides the missing piece of the puzzle.</p>

<p class="calibre3">In cases we are willing to invest time to instrument our services, the results are impressive. We can get everything we need without compromises. We can accomplish almost any level of details and instrument services in a way that we can write reliable alerts that will notify the system with all the information it needs. The result is a step closer towards self-healing and, more importantly, self-adaptation. The reason I’m putting self-adaptation into the “more important” group lies in the fact that self-healing is already mostly solved by other tools. Schedulers (e.g. Docker Swarm) already do a decent job at self-healing services. If we exclude hardware from the scope, we are left with self-adaptation of services as the major obstacle left to solve.</p>

<h3 id="leanpub-auto-setting-up-the-objectives" class="calibre20">Setting Up The Objectives</h3>

<p class="calibre3">We need to define the scope of what we want to accomplish through instrumentation. We’ll keep it small by limiting ourselves to a single goal. We’ll scale services if their response times are over an upper limit and de-scale them if they’re below a lower limit. Any other alert will lead to a notification to Slack. That does not mean that Slack notifications should exist forever. Instead, they should be treated as a temporary solution until we find a way to translate manual corrective actions into automated responses performed by the system.</p>

<p class="calibre3">A good example of alerts that are often treated manually are responses with errors (status codes 500 and above). We’ll send alerts whenever they reach a threshold over a specified period. They will result in Slack notifications that will become pending tasks for humans. An internal rule should be to fix the problem first, evaluate why it happened, and write a script that will repeat the same set of steps. With such a script, we’ll be able to instruct the system to do the same if the same alert is fired again. With such an approach, we (humans) can spend our time solving unexpected problems and leave machines to remedy those that are reoccurring.</p>

<p class="calibre3">The summary of the objectives we’ll try to accomplish is as follows.</p>

<ul class="calibre26">
  <li class="calibre14">Define maximum response time of a service and create the flow that will scale it.</li>
  <li class="calibre14">Define minimum response time of a service and create the flow that will de-scale it.</li>
  <li class="calibre14">Define thresholds based on responses with status codes 500 and above and send Slack notifications.</li>
</ul>

<p class="calibre3">Please note that response time thresholds cannot rely only on milliseconds. We must define quantiles, rates, and a few other things. Also, we need to set the minimum and the maximum number of replicas of a service. Otherwise, we risk scaling to infinity or de-scaling to zero replicas. Once we start implementing the system, we’ll see whether those additional requirements are enough or we’ll need to extend the scope further.</p>

<p class="calibre3">That was more than enough talk. Let’s move to the practical exploration of the subject.</p>

<p class="calibre3">As always, the first step is to create a cluster and deploy a few services.</p>

<h3 id="leanpub-auto-creating-the-cluster-and-deploying-services-6" class="calibre20">Creating The Cluster And Deploying Services</h3>

<p class="calibre3">You know the drill. We’ll create a Swarm cluster and deploy a few stacks we are already familiar with. Once we’re done, we’ll have the base required for the exploration of the tasks at hand.</p>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/vfarcic/8bafbe912f277491eb2ce6f9d29039f9">12-alert-instrumentation.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chmod +x scripts/dm-swarm-12.sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>./scripts/dm-swarm-12.sh
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">eval</code> <code class="k">$(</code>docker-machine env swarm-1<code class="k">)</code>
<code class="lineno">6 </code>
<code class="lineno">7 </code>docker stack ls
</pre></div>

</figure>

<p class="calibre3">We created the cluster and deployed three stacks. The output of the last command gives us the list of those stacks.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>NAME                SERVICES
<code class="lineno">2 </code>monitor             3
<code class="lineno">3 </code>proxy               2
<code class="lineno">4 </code>jenkins             2
</pre></div>

</figure>

<p class="calibre3">Now we’re ready to explore how to scrape metrics from instrumented services.</p>

<h3 id="leanpub-auto-scraping-metrics-from-instrumented-services" class="calibre20">Scraping Metrics From Instrumented Services</h3>

<p class="calibre3">The <a href="https://github.com/vfarcic/go-demo">go-demo</a> service is already instrumented with a few metrics. However, they do not mean much by themselves. Their usage starts only once Prometheus scrapes them. Even then, they provide only a visual representation and the ability to query them after we find a problem. The major role of graphs and the capacity to query metrics comes after we detect an issue and we want to drill deeper into it. But, before we get there, we need to set up alerts that will notify us that there is a problem.</p>

<p class="calibre3">We cannot think about metrics before we have some data those metrics will evaluate. So, we’ll start from the beginning and explore how to let Prometheus know that metrics are coming from services we instrumented. That should be a relatively easy thing to accomplish since we already have all the tools and processes we need.</p>

<p class="calibre3">Let’s start by deploying the <code class="calibre19">go-demo</code> stack. The <code class="calibre19">main</code> service inside it is already instrumented and provides the <code class="calibre19">/metrics</code> endpoint Prometheus can query.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/go-demo-instrument.yml <code class="se">\</code>
<code class="lineno">3 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">We can use the time Swarm needs to initialize the replicas of the stack and explore the YAML file. The definition of the <a href="https://github.com/vfarcic/docker-flow-monitor/blob/master/stacks/go-demo-instrument.yml">go-demo-instrument.yml</a> stack is as follows (limited to relevant parts).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>  main:
<code class="lineno"> 3 </code>    ...
<code class="lineno"> 4 </code>    networks:
<code class="lineno"> 5 </code>      ...
<code class="lineno"> 6 </code>      - monitor
<code class="lineno"> 7 </code>    deploy:
<code class="lineno"> 8 </code>      ...
<code class="lineno"> 9 </code>      labels:
<code class="lineno">10 </code>        - com.df.notify=true
<code class="lineno">11 </code>        ...
<code class="lineno">12 </code>        - com.df.scrapePort=8080
<code class="lineno">13 </code>      ...
</pre></div>

</figure>

<p class="calibre3">We used <code class="calibre19">com.df.notify=true</code> label to let <em class="calibre21">Docker Flow Swarm Listener</em> know that it should notify <em class="calibre21">Docker Flow Monitor</em>. The <code class="calibre19">com.df.scrapePort</code> is set to <code class="calibre19">8080</code> thus letting Prometheus know the port it should use to scrape metrics. The <code class="calibre19">monitor</code> network is added to the stack. Since <em class="calibre21">Docker Flow Monitor</em> is attached to the same network, they will be able to communicate internally by using service names.</p>

<p class="calibre3">Let’s confirm that <em class="calibre21">Docker Flow Monitor</em> configured Prometheus correctly.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/config"</code>
</pre></div>

</figure>

<p class="calibre3">As you can see, <code class="calibre19">job_name</code> is set to the name of the service (<code class="calibre19">go-demo_main</code>). The <code class="calibre19">names</code> argument is set to <code class="calibre19">tasks.go-demo_main</code>. When service DNS is prefixed with <code class="calibre19">tasks.</code>, Overlay network returns IPs of all the replicas. That way, Prometheus will be able to scrape metrics from all those that form the <code class="calibre19">go-demo_main</code> service.</p>


<figure class="image1">
  <img src="../images/00062.jpeg" alt="Figure 12-1: Prometheus configuration with the go-demo_main job" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-1: Prometheus configuration with the go-demo_main job</figcaption>
</figure>


<p class="calibre3">Before we proceed, please confirm that all the replicas of the services that form the <code class="calibre19">go-demo</code> stack are up-and-running.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>Running go-demo
</pre></div>

</figure>

<p class="calibre3">The output should show three replicas of <code class="calibre19">go-demo_main</code> and one replica of <code class="calibre19">go-demo_db</code> services with the current state <code class="calibre19">running</code>. If that’s not the case, please wait a while longer.</p>

<p class="calibre3">The output should be similar to the one that follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \
<code class="lineno"> 2 </code>       ERROR PORTS
<code class="lineno"> 3 </code>go-demo_main.1 vfarcic/go-demo:latest swarm-2 Running       Running about a minu\
<code class="lineno"> 4 </code>te ago
<code class="lineno"> 5 </code>go-demo_db.1   mongo:latest           swarm-1 Running       Running about a minu\
<code class="lineno"> 6 </code>te ago
<code class="lineno"> 7 </code>go-demo_main.2 vfarcic/go-demo:latest swarm-2 Running       Running about a minu\
<code class="lineno"> 8 </code>te ago
<code class="lineno"> 9 </code>go-demo_main.3 vfarcic/go-demo:latest swarm-3 Running       Running about a minu\
<code class="lineno">10 </code>te ago
</pre></div>

</figure>

<p class="calibre3">Now we can verify that all the targets (replicas) are indeed registered.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/targets"</code>
</pre></div>

</figure>

<p class="calibre3">As you can see, Prometheus registered three targets that correspond to three replicas of the service. Now we know that it scrapes metrics from all of them and can explore different ways to query data.</p>


<figure class="image">
  <img src="../images/00063.jpeg" alt="Figure 12-2: Prometheus targets that correspond with the replicas of the go-demo_main service" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-2: Prometheus targets that correspond with the replicas of the go-demo_main service</figcaption>
</figure>


<p class="calibre3">If we focus only on scraping instrumented services, the process can be described with a simple diagram from the figure 12-3.</p>


<figure class="image">
  <img src="../images/00064.jpeg" alt="Figure 12-3: Prometheus scrapes metrics from services" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-3: Prometheus scrapes metrics from services</figcaption>
</figure>


<h3 id="leanpub-auto-querying-metrics-from-instrumented-services" class="calibre20">Querying Metrics From Instrumented Services</h3>

<p class="calibre3">Let’s open the Prometheus <em class="calibre21">graph</em> screen and explore different ways to query metrics scraped from the <code class="calibre19">go-demo_main</code> service.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/graph"</code>
</pre></div>

</figure>

<p class="calibre3">Please click the <em class="calibre21">Graph</em> tab, enter the query that follows, and click the <em class="calibre21">Execute</em> button</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>http_server_resp_time_sum / http_server_resp_time_count
</pre></div>

</figure>

<p class="calibre3">We divided the summary of response times with the count of the requests. You’ll notice that the output graph shows that the average value is close to zero. Feel free to hover over one of the lines and observe that the values are only a few milliseconds. The <code class="calibre19">go-demo_main</code> service pings itself periodically and the responses are very fast.</p>

<p class="calibre3">We should generate some slower responses since the current result does not show metrics in their true glory.</p>

<p class="calibre3">The <code class="calibre19">/demo/hello</code> endpoint of the service can be supplemented with the <code class="calibre19">delay</code> parameter. When set to a value in milliseconds, the service will wait for the given period before responding to the request. Since we want to demonstrate a variety of response times, we should send a few requests with different <code class="calibre19">delay</code> values.</p>

<p class="calibre3">The commands that will send thirty requests with random delays are as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..30<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    <code class="nv">DELAY</code><code class="o">=</code>$<code class="o">[</code> <code class="nv">$RANDOM</code> % <code class="o">6000</code> <code class="o">]</code>
<code class="lineno">3 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/hello?delay=</code><code class="nv">$DELAY</code><code class="s">"</code>
<code class="lineno">4 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">The delay of each of the thirty requests was set to a random value between zero and six thousand milliseconds. Now we should have more variable metrics we can explore.</p>

<p class="calibre3">Please write the query that follows in the <em class="calibre21">Expression</em> field and click the <em class="calibre21">Execute</em> button.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>http_server_resp_time_sum / http_server_resp_time_count
</pre></div>

</figure>

<p class="calibre3">This time you should see a graph with more differentiating response times. You should note that the response times you’re seeing are a combination of those we sent (with up to six seconds delay) and fast pings that the service executes periodically.</p>

<p class="calibre3">The previous query is not enough, and we should add a few functions into the mix.</p>

<p class="calibre3">We’ll add <code class="calibre19">rate</code>. It calculates per-second average rate of increase of the time series in the range vector.</p>

<p class="calibre3">We’ll also limit the metrics to the last five minutes. While that does not make much of a difference when presenting data in a graph, it is crucial for defining alerts. Since they are our ultimate goal while we’re working with Prometheus, we should get used to such limits from the start.</p>

<p class="calibre3">Please write the query that follows in the <em class="calibre21">Expression</em> field and click the <em class="calibre21">Execute</em> button.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>rate(http_server_resp_time_sum[5m]) / rate(http_server_resp_time_count[5m])
</pre></div>

</figure>

<p class="calibre3">Since we are trying to define queries we’ll use in alerts, we might want to limit the results only to a single service.</p>

<p class="calibre3">The expression limited to <code class="calibre19">go-demo_main</code> service is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>rate(http_server_resp_time_sum{service="go-demo"}[5m]) / rate(http_server_resp_t\
<code class="lineno">2 </code>ime_count{service="go-demo"}[5m])
</pre></div>

</figure>

<p class="calibre3">The graph output of the last query should show a spike in response times of each of the three replicas. That spike corresponds to the thirty requests we created with the <code class="calibre19">delay</code> parameter.</p>


<figure class="image">
  <img src="../images/00065.jpeg" alt="Figure 12-4: Prometheus graph with response duration spike" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-4: Prometheus graph with response duration spike</figcaption>
</figure>


<p class="calibre3">The <code class="calibre19">service</code> label comes from instrumentation. It is hard-coded to <code class="calibre19">go-demo</code> and, therefore, not very reliable. If we would be confident that there will be only one instance of that service, we could continue using that label as one of the filters. However, that might not be the case. Even though it is likely we’ll use only one instance of the <code class="calibre19">go-demo</code> service in production, we might still run the same service as part of testing or some other processes. That would produce incorrect query results since we would be combining different instances of the service and potentially get unreliable data. Instead, it might be a better idea to use the <code class="calibre19">job</code> label.</p>

<p class="calibre3">The <code class="calibre19">job</code> label comes out-of-the-box with all metrics scraped by Prometheus. It corresponds to the <code class="calibre19">job_name</code> specified in the <code class="calibre19">scrape</code> section of the configuration. Since <em class="calibre21">Docker Flow Monitor</em> uses the “real” name of the service to register scraping target, it is always unique. That fixes one of our problems since we cannot have two services with the same name inside a single cluster. In our case, the full name of the service is the combination of the name of the stack and the name of the service defined in that YAML file. Since we deployed <code class="calibre19">go-demo</code> stack with the service <code class="calibre19">main</code>, the full name of the service is <code class="calibre19">go-demo_main</code>.</p>

<p class="calibre3">If we’d like to see metrics of all the services that provide instrumentation with the metric name <code class="calibre19">http_server_resp_time</code>, the query would be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_sum[5m])) by (job) / sum(rate(http_server_resp_ti\
<code class="lineno">2 </code>me_count[5m])) by (job)
</pre></div>

</figure>

<p class="calibre3">Since we used <code class="calibre19">sum</code> to summarize data <code class="calibre19">by</code> <code class="calibre19">job</code>, each line represents a different service. That is not so obvious from the current graph since we are scraping metrics from only one service, so you’ll need to trust me on this one. If we’d have metrics from multiple services, each would get its line in the graph.</p>

<p class="calibre3">That must be it. Doesn’t it? We have an average response time for each job (service) as measured over last five minutes. Unfortunately, even though such expressions might be useful when watching graphs, they have little value when used for alerts. It might be even dangerous to instruct the system to do some corrective actions based on such an alert.</p>

<p class="calibre3">Let’s say that we create an alert defined as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_sum[5m])) by (job) / sum(rate(http_server_resp_ti\
<code class="lineno">2 </code>me_count[5m])) by (job) &gt; 0.1
</pre></div>

</figure>

<p class="calibre3">It will fire if average response time is over one hundred milliseconds (0.1 seconds). Now let us imagine that nine out of ten responses are around ten milliseconds while one out of ten lasts for five hundred milliseconds (half a second). The above alert would not fire in such a scenario since the average is 59 milliseconds, which is still way below the 100 milliseconds alert threshold. As a result, we would never know that there is a problem experienced by ten percent of those who invoke this service.</p>

<p class="calibre3">Rejection of the above mentioned alert definition might lead you to write something simpler. The new alert could be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>http_server_resp_time_sum &gt; 0.25
</pre></div>

</figure>

<p class="calibre3">If there is a request that lasted longer than the threshold, fire an alert. We even increased the threshold from <code class="calibre19">0.1</code> to <code class="calibre19">0.25</code> seconds. While I do like the simplicity of that alert, it is even worse than the one with average response time. It would be enough to have one request that passed the threshold to fire an alert and, potentially, initiate the process that would scale the number of replicas of that service. What if there were a million other responses that were way below that threshold. The alert would still fire and probably produce undesirable consequences. Do we really care that one out of million responses is slow?</p>

<p class="calibre3">The problem is that we were focused on averages. While there is value in them, they derailed us from creating a query that we could use to create a useful alert. Instead, we should focus on percentages. A better goal would be to construct an expression that would give us the percentage of requests that are above the certain threshold.</p>

<p class="calibre3">The new query is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_bucket{le="0.1"}[5m])) by (job) / sum(rate(http_s\
<code class="lineno">2 </code>erver_resp_time_count[5m])) by (job)
</pre></div>

</figure>

<p class="calibre3">The first part of the expression returns summary of the number of requests that are in the <code class="calibre19">0.1</code> bucket. In other words, it retrieves all the requests that are equal to or faster than <code class="calibre19">0.1</code> second. Further on, we are dividing that result with the summary of all the requests. The result is the percentage of requests that are below the <code class="calibre19">0.1</code> seconds threshold.</p>


<figure class="image">
  <img src="../images/00066.jpeg" alt="Figure 12-5: Prometheus graph with percentage of response times below 0.1 second threshold" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-5: Prometheus graph with percentage of response times below 0.1 second threshold</figcaption>
</figure>


<p class="calibre3">If you do not see a drop in the percentage of requests, the likely cause is that more than one hour passed since you executed thirty requests with the delay. If that’s the case, please rerun the commands that follow and, after that, re-execute the expression.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..30<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    <code class="nv">DELAY</code><code class="o">=</code>$<code class="o">[</code> <code class="nv">$RANDOM</code> % <code class="o">6000</code> <code class="o">]</code>
<code class="lineno">3 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/hello?delay=</code><code class="nv">$DELAY</code><code class="s">"</code>
<code class="lineno">4 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">That was an expression worthy of an alert. We could use it to create something similar to a <em class="calibre21">service license agreement</em>. The alert would fire if, for example, less than 0.999 (99.9%) responses are below the defined time-based threshold. The only thing missing is to limit the output of the expression to the <code class="calibre19">go-demo_main</code> service.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_bucket{job="go-demo_main",le="0.1"}[5m])) / sum(r\
<code class="lineno">2 </code>ate(http_server_resp_time_count{job="go-demo_main"}[5m]))
</pre></div>

</figure>

<p class="calibre3">Let’s try to explore at least one more example.</p>

<p class="calibre3">Among others, the <code class="calibre19">http_server_resp_time</code> metric has the <code class="calibre19">code</code> label that contains status codes of the responses. We can use that information to define an expression that will retrieve the number of requests that produced an error. Since we are returning standard <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">HTTP response codes</a>, we can filter metrics so that only those with the <code class="calibre19">code</code> label that starts with <code class="calibre19">5</code> are retrieved.</p>

<p class="calibre3">Before we start filtering metrics in search for errors, we should generate some requests that do result in error responses.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..100<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/random-error"</code>
<code class="lineno">3 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">We sent a hundred requests to the <code class="calibre19">/demo/random-error</code> endpoint. Approximately one out of ten requests resulted in error responses.</p>

<p class="calibre3">The expression that follows will retrieve the rate of error responses over the period of five minutes.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_count{code=~"^5..$"}[5m])) by (job)
</pre></div>

</figure>

<p class="calibre3">The total number does not mean much unless you plan on sending an alert every time an error occurs. Such an action would likely result in too many alerts, and you’d run a risk of developing high-tolerance and start ignoring them. That’s not the way to go. Instead, we should use a similar approach as with response times. We’ll calculate the rate by dividing the number of errors with the total number of responses.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_count{code=~"^5..$"}[5m])) by (job) / sum(rate(ht\
<code class="lineno">2 </code>tp_server_resp_time_count[5m])) by (job)
</pre></div>

</figure>

<p class="calibre3">That would be an useful alert that could be fired if the number is higher than some threshold.</p>


<figure class="image">
  <img src="../images/00067.jpeg" alt="Figure 12-6: Prometheus graph with error rate percentage" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-6: Prometheus graph with error rate percentage</figcaption>
</figure>


<p class="calibre3">Now that we defined two sets of expressions, we can take a step further and convert them into alerts.</p>

<h3 id="leanpub-auto-firing-alerts-based-on-instrumented-metrics" class="calibre20">Firing Alerts Based On Instrumented Metrics</h3>

<p class="calibre3">Now that we have a solid understanding of some of the expressions based on instrumented metrics, we can proceed and apply that knowledge to create a few alerts.</p>

<p class="calibre3">Let us deploy updated version of the <code class="calibre19">go-demo</code> stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/go-demo-instrument-alert.yml <code class="se">\</code>
<code class="lineno">3 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">We’ll take a couple of moments to discuss the changes to the updated stack while waiting for the services to become operational. The stack definition, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>  main:
<code class="lineno"> 3 </code>    ...
<code class="lineno"> 4 </code>    deploy:
<code class="lineno"> 5 </code>      ...
<code class="lineno"> 6 </code>      labels:
<code class="lineno"> 7 </code>        ...
<code class="lineno"> 8 </code>        - com.df.alertName.1=mem_limit
<code class="lineno"> 9 </code>        - com.df.alertIf.1=@service_mem_limit:0.8
<code class="lineno">10 </code>        - com.df.alertFor.1=5m
<code class="lineno">11 </code>        - com.df.alertName.2=resp_time
<code class="lineno">12 </code>        - com.df.alertIf.2=sum(rate(http_server_resp_time_bucket{job="go-demo_ma\
<code class="lineno">13 </code>in", le="0.1"}[5m])) / sum(rate(http_server_resp_time_count{job="go-demo_main"}[\
<code class="lineno">14 </code>5m])) &lt; 0.99
<code class="lineno">15 </code>        - com.df.alertLabels.2=scale=up,service=go-demo_main
<code class="lineno">16 </code>        - com.df.scrapePort=8080
<code class="lineno">17 </code>        - com.df.scaleMin=2
<code class="lineno">18 </code>        - com.df.scaleMax=4
<code class="lineno">19 </code>      ...
</pre></div>

</figure>

<p class="calibre3">The <code class="calibre19">com.df.alertName</code> label was present in the previous stack. However, since specifying memory limit is not enough anymore, we added an index suffix (<code class="calibre19">.1</code>) that allows us to specify multiple alerts. The same <code class="calibre19">.1</code> suffix was added to the rest of labels that form that alert.</p>

<p class="calibre3">The second alert will fire if the number of the responses in the <code class="calibre19">0.1</code> bucket (equal to or below 100 milliseconds) is smaller than 99% of all the requests. The rate is measured over the period of five minutes and the results are restricted to the job <code class="calibre19">go-demo_main</code>. The <code class="calibre19">if</code> statement we used is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_bucket{job="go-demo_main", le="0.1"}[5m])) / sum(\
<code class="lineno">2 </code>rate(http_server_resp_time_count{job="go-demo_main"}[5m])) &lt; 0.99
</pre></div>

</figure>

<p class="calibre3">Since we are measuring the percentage of requests, there’s no real need to set <code class="calibre19">for</code> statement. As soon as more than one percent of requests result in response times over 100 milliseconds, the alert will be fired. Later on we’ll discuss what should be done with such an alert. For the moment, we’ll limit the scope and let Alertmanager forward all alerts to Slack.</p>

<p class="calibre3">We also added <code class="calibre19">scale=up</code> and <code class="calibre19">service=go-demo_main</code> alert labels. Later on, the <code class="calibre19">scale</code> label will help the system know whether it should scale up or down.</p>

<p class="calibre3">Finally, we used <code class="calibre19">com.df.scaleMin</code> and <code class="calibre19">com.df.scaleMax</code> labels to specify the minimum and the maximum number of replicas allowed for this service. We won’t use those labels just yet. Just remember that they are defined.</p>

<p class="calibre3">Next, we’ll repeat the commands that will create slow responses and verify that the alerts are indeed fired. But, before we do that, we’ll open the Prometheus’ alert screen and confirm that the new alert is indeed registered.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">The <em class="calibre21">godemo_main_resp_time</em> row should be green meaning that the alert is registered but that the condition is not met. In other words, at least 99% of responses were generated in 100 milliseconds or less.</p>

<p class="calibre3">Now we can truly test the alert. Let’s generate some slow responses.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..30<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    <code class="nv">DELAY</code><code class="o">=</code>$<code class="o">[</code> <code class="nv">$RANDOM</code> % <code class="o">6000</code> <code class="o">]</code>
<code class="lineno">3 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/hello?delay=</code><code class="nv">$DELAY</code><code class="s">"</code>
<code class="lineno">4 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">You already executed those commands at least once so there should be no reason to explain what happened. Instead, we’ll go back to the <em class="calibre21">alerts</em> screen and confirm that the alert is indeed firing.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">The <em class="calibre21">godemo_main_resp_time</em> should be red. If it isn’t, please wait a few moments and refresh the screen. Feel free to click it if you’d like to see the definition of the alert.</p>


<figure class="image">
  <img src="../images/00068.jpeg" alt="Figure 12-7: Prometheus alert in firing state" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-7: Prometheus alert in firing state</figcaption>
</figure>


<p class="calibre3">Please visit the <em class="calibre21">#df-monitor-tests</em> channel inside <a href="https://devops20.slack.com/">devops20.slack.com</a>. You should see the <em class="calibre21">[FIRING] go-demo_main service is in danger!</em> message.</p>

<p class="calibre3">The process, so far, can be described through the diagram in figure 12-8.</p>


<figure class="image">
  <img src="../images/00069.jpeg" alt="Figure 12-8: Alerts that result in Slack notifications" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-8: Alerts that result in Slack notifications</figcaption>
</figure>


<p class="calibre3">That worked out quite well. When slow responses start piling up, we’ll get a Slack notification letting us know that we should enter the cluster and scale the service. The only problem is that we should not waste our time with such operations. We should let the system scale up automatically.</p>

<h3 id="leanpub-auto-scaling-services-automatically" class="calibre20">Scaling Services Automatically</h3>

<p class="calibre3">With the alerts firing from Prometheus into Alertmanager, the only thing left to do is to send requests to Jenkins to scale the service. We already created a similar Alertmanager config in one of the previous chapters, so we’ll comment only on a few minor differences. The configuration is injected into the <code class="calibre19">alert-manager</code> service as a Docker secret.</p>

<p class="calibre3">Since secrets are immutable, we cannot update the one that is currently used. Instead, we’ll have to remove the stack and the secret and create them again.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack rm monitor
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker secret rm alert_manager_config
</pre></div>

</figure>

<p class="calibre3">Now we can create a new secret with the updated Alertmanager configuration.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">echo</code> <code class="s">"route:</code>
<code class="lineno"> 2 </code><code class="s">  group_by: [service,scale]</code>
<code class="lineno"> 3 </code><code class="s">  repeat_interval: 5m</code>
<code class="lineno"> 4 </code><code class="s">  group_interval: 5m</code>
<code class="lineno"> 5 </code><code class="s">  receiver: 'slack'</code>
<code class="lineno"> 6 </code><code class="s">  routes:</code>
<code class="lineno"> 7 </code><code class="s">  - match:</code>
<code class="lineno"> 8 </code><code class="s">      service: 'go-demo_main'</code>
<code class="lineno"> 9 </code><code class="s">      scale: 'up'</code>
<code class="lineno">10 </code><code class="s">    receiver: 'jenkins-go-demo_main-up'</code>
<code class="lineno">11 </code>
<code class="lineno">12 </code><code class="s">receivers:</code>
<code class="lineno">13 </code><code class="s">  - name: 'slack'</code>
<code class="lineno">14 </code><code class="s">    slack_configs:</code>
<code class="lineno">15 </code><code class="s">      - send_resolved: true</code>
<code class="lineno">16 </code><code class="s">        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is \</code>
<code class="lineno">17 </code><code class="s">in danger!'</code>
<code class="lineno">18 </code><code class="s">        title_link: 'http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts'</code>
<code class="lineno">19 </code><code class="s">        text: '{{ .CommonAnnotations.summary}}'</code>
<code class="lineno">20 </code><code class="s">        api_url: 'https://hooks.slack.com/services/T308SC7HD/B59ER97SS/S0KvvyStV\</code>
<code class="lineno">21 </code><code class="s">nIt3ZWpIaLnqLCu'</code>
<code class="lineno">22 </code><code class="s">  - name: 'jenkins-go-demo_main-up'</code>
<code class="lineno">23 </code><code class="s">    webhook_configs:</code>
<code class="lineno">24 </code><code class="s">      - send_resolved: false</code>
<code class="lineno">25 </code><code class="s">        url: 'http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/jenkins/job/service-scale/buil\</code>
<code class="lineno">26 </code><code class="s">dWithParameters?token=DevOps22&amp;service=go-demo_main&amp;scale=1'</code>
<code class="lineno">27 </code><code class="s">"</code> <code class="calibre19">|</code> docker secret create alert_manager_config -
</pre></div>

</figure>

<p class="calibre3">Remember that the gist with all the commands from this chapter is available from <a href="https://gist.github.com/vfarcic/8bafbe912f277491eb2ce6f9d29039f9">12-alert-instrumentation.sh</a>. Use it to copy and paste the command if you got tired of typing.</p>

<p class="calibre3">The difference, when compared with the similar configuration we used before, is the <code class="calibre19">scale</code> label and a subtle change in the Jenkins receiver name. This time we are not grouping routes based only on <code class="calibre19">service</code> but with the combination of the labels <code class="calibre19">service</code> and <code class="calibre19">scale</code>. Even though we are, at the moment, focused only on scaling up, soon we’ll try to add another alert that will de-scale the number of replicas. While we would accomplish the current objective without the <code class="calibre19">scale</code> label, it might be a good idea to be prepared for what’s coming next.</p>

<p class="calibre3">This time, the <code class="calibre19">match</code> section uses a combination of both <code class="calibre19">service</code> and <code class="calibre19">scale</code> labels. If they are set to <code class="calibre19">go-demo_main</code> and <code class="calibre19">up</code>, the alert will be forwarded to the <code class="calibre19">jenkins-go-demo_main-up</code> receiver. Any other combination will be sent to Slack. The <code class="calibre19">jenkins-go-demo_main-up</code> receiver is triggering a build of the Jenkins job <code class="calibre19">service-scale</code> with a few parameters. It contains the authentication token, the name of the service that should be scaled, and the increment in the number replicas.</p>

<p class="calibre3">The <code class="calibre19">repeat_interval</code> is set to five minutes. Alertmanager will send a new notification every five minutes (plus the <code class="calibre19">group_interval</code>) unless the problem is fixed and Prometheus stops firing alerts. That is almost certainly not the value you should use in production. One hour (<code class="calibre19">1h</code>) is a much more reasonable period. However, I’d like to avoid making you wait for too long so, in this case, it’s set to five minutes (<code class="calibre19">5m</code>).</p>

<p class="calibre3">Let us deploy the stack with the new secret.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">DOMAIN</code><code class="o">=</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code> <code class="se">\</code>
<code class="lineno">2 </code>    docker stack deploy <code class="se">\</code>
<code class="lineno">3 </code>    -c stacks/docker-flow-monitor-slack.yml <code class="se">\</code>
<code class="lineno">4 </code>    monitor
</pre></div>

</figure>

<p class="calibre3">There’s only one thing missing before we see the alert in its full glory. We need to run the Jenkins job manually. The first build will fail due to a  bug we already experienced in one of the previous chapters.</p>

<p class="calibre3">Please open the <code class="calibre19">service-scale</code> activity screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/jenkins/blue/organizations/jenkins/ser\</code>
<code class="lineno">2 </code><code class="s">vice-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">You’ll have to login with <code class="calibre19">admin</code> as both username and password. Afterward, click the <em class="calibre21">Run</em> button and observe the failure. The issue is that Jenkins was not aware that the job uses a few parameters. After the first run, it’ll get that information, and the job should not fail again. If it does, it’ll be for a different reason.</p>

<p class="calibre3">The <code class="calibre19">go-demo_main</code> service should have three replicas. Let’s double-check that.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>Running go-demo
</pre></div>

</figure>

<p class="calibre3">The output should be similar to the one that follows (ID are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \
<code class="lineno"> 2 </code>   ERROR PORTS
<code class="lineno"> 3 </code>go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running 42 minutes a\
<code class="lineno"> 4 </code>go
<code class="lineno"> 5 </code>go-demo_db.1   mongo:latest           swarm-3 Running       Running 42 minutes a\
<code class="lineno"> 6 </code>go
<code class="lineno"> 7 </code>go-demo_main.2 vfarcic/go-demo:latest swarm-3 Running       Running 42 minutes a\
<code class="lineno"> 8 </code>go
<code class="lineno"> 9 </code>go-demo_main.3 vfarcic/go-demo:latest swarm-1 Running       Running 42 minutes a\
<code class="lineno">10 </code>go
</pre></div>

</figure>

<p class="calibre3">Before we proceed, please make sure that all replicas of the <code class="calibre19">monitor</code> stack are up and running. You can use <code class="calibre19">docker stack ps monitor</code> command to check the status.</p>

<p class="calibre3">Now we can send requests that will produce delayed responses and open the Prometheus <em class="calibre21">alerts</em> screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..30<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    <code class="nv">DELAY</code><code class="o">=</code>$<code class="o">[</code> <code class="nv">$RANDOM</code> % <code class="o">6000</code> <code class="o">]</code>
<code class="lineno">3 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/hello?delay=</code><code class="nv">$DELAY</code><code class="s">"</code>
<code class="lineno">4 </code><code class="k">done</code>
<code class="lineno">5 </code>
<code class="lineno">6 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">The <em class="calibre21">godemo_main_resp_time</em> alert should be red. If it is not, please wait a few moments and refresh the screen.</p>

<p class="calibre3">Prometheus fired the alert to Alertmanager which, in turn, notified Jenkins. As a result, we should see a new build of the <code class="calibre19">service-scale</code> job.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/jenkins/blue/organizations/jenkins/ser\</code>
<code class="lineno">2 </code><code class="s">vice-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">Please click on the latest build. It should be green with the output of the last task set to <code class="calibre19">go-demo_main</code> was scaled from 3 to 4 replicas.</p>


<figure class="image">
  <img src="../images/00070.jpeg" alt="Figure 12-9: A build of a Jenkins job that scales Docker services" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-9: A build of a Jenkins job that scales Docker services</figcaption>
</figure>


<p class="calibre3">We should confirm that Jenkins indeed did the work it was supposed to do. The number of replicas of the <code class="calibre19">go-demo_main</code> service should be four.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack ps <code class="se">\</code>
<code class="lineno">2 </code>    -f desired-state<code class="o">=</code>Running go-demo
</pre></div>

</figure>

<p class="calibre3">The output of the <code class="calibre19">stack ps</code> command is as follows (IDs are removed for brevity).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>NAME           IMAGE                  NODE    DESIRED STATE CURRENT STATE       \
<code class="lineno"> 2 </code>      ERROR PORTS
<code class="lineno"> 3 </code>go-demo_main.1 vfarcic/go-demo:latest swarm-1 Running       Running about an hou\
<code class="lineno"> 4 </code>r ago
<code class="lineno"> 5 </code>go-demo_db.1   mongo:latest           swarm-2 Running       Running about an hou\
<code class="lineno"> 6 </code>r ago
<code class="lineno"> 7 </code>go-demo_main.2 vfarcic/go-demo:latest swarm-2 Running       Running about an hou\
<code class="lineno"> 8 </code>r ago
<code class="lineno"> 9 </code>go-demo_main.3 vfarcic/go-demo:latest swarm-3 Running       Running about an hou\
<code class="lineno">10 </code>r ago
<code class="lineno">11 </code>go-demo_main.4 vfarcic/go-demo:latest swarm-3 Running       Running 2 minutes ago
</pre></div>

</figure>

<p class="calibre3">Since we stopped simulating slow responses, the alert in Prometheus should turn into green. Otherwise, if Prometheus would continue firing the alert, Alertmanager would send another notification to Jenkins ten minutes later. Since the service has the <code class="calibre19">com.df.scaleMax</code> label set to four, Jenkins job would not scale the service. Instead, it would send a notification to Slack so that we (humans) can deal with the problem.</p>

<p class="calibre3">Let’s remove the stack and the secret and work on Alertmanager configuration that will also de-scale services.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack rm monitor
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker secret rm alert_manager_config
</pre></div>

</figure>

<p class="calibre3">The command that creates a new secret is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">echo</code> <code class="s">"route:</code>
<code class="lineno"> 2 </code><code class="s">  group_by: [service,scale]</code>
<code class="lineno"> 3 </code><code class="s">  repeat_interval: 5m</code>
<code class="lineno"> 4 </code><code class="s">  group_interval: 5m</code>
<code class="lineno"> 5 </code><code class="s">  receiver: 'slack'</code>
<code class="lineno"> 6 </code><code class="s">  routes:</code>
<code class="lineno"> 7 </code><code class="s">  - match:</code>
<code class="lineno"> 8 </code><code class="s">      service: 'go-demo_main'</code>
<code class="lineno"> 9 </code><code class="s">      scale: 'up'</code>
<code class="lineno">10 </code><code class="s">    receiver: 'jenkins-go-demo_main-up'</code>
<code class="lineno">11 </code><code class="s">  - match:</code>
<code class="lineno">12 </code><code class="s">      service: 'go-demo_main'</code>
<code class="lineno">13 </code><code class="s">      scale: 'down'</code>
<code class="lineno">14 </code><code class="s">    receiver: 'jenkins-go-demo_main-down'</code>
<code class="lineno">15 </code>
<code class="lineno">16 </code><code class="s">receivers:</code>
<code class="lineno">17 </code><code class="s">  - name: 'slack'</code>
<code class="lineno">18 </code><code class="s">    slack_configs:</code>
<code class="lineno">19 </code><code class="s">      - send_resolved: true</code>
<code class="lineno">20 </code><code class="s">        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is \</code>
<code class="lineno">21 </code><code class="s">in danger!'</code>
<code class="lineno">22 </code><code class="s">        title_link: 'http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts'</code>
<code class="lineno">23 </code><code class="s">        text: '{{ .CommonAnnotations.summary}}'</code>
<code class="lineno">24 </code><code class="s">        api_url: 'https://hooks.slack.com/services/T308SC7HD/B59ER97SS/S0KvvyStV\</code>
<code class="lineno">25 </code><code class="s">nIt3ZWpIaLnqLCu'</code>
<code class="lineno">26 </code><code class="s">  - name: 'jenkins-go-demo_main-up'</code>
<code class="lineno">27 </code><code class="s">    webhook_configs:</code>
<code class="lineno">28 </code><code class="s">      - send_resolved: false</code>
<code class="lineno">29 </code><code class="s">        url: 'http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/jenkins/job/service-scale/buil\</code>
<code class="lineno">30 </code><code class="s">dWithParameters?token=DevOps22&amp;service=go-demo_main&amp;scale=1'</code>
<code class="lineno">31 </code><code class="s">  - name: 'jenkins-go-demo_main-down'</code>
<code class="lineno">32 </code><code class="s">    webhook_configs:</code>
<code class="lineno">33 </code><code class="s">      - send_resolved: false</code>
<code class="lineno">34 </code><code class="s">        url: 'http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/jenkins/job/service-scale/buil\</code>
<code class="lineno">35 </code><code class="s">dWithParameters?token=DevOps22&amp;service=go-demo_main&amp;scale=-1'</code>
<code class="lineno">36 </code><code class="s">"</code> <code class="calibre19">|</code> docker secret create alert_manager_config -
</pre></div>

</figure>

<p class="calibre3">We added an additional route and a receiver. Both are very similar to their counterparts in charge of scaling up. The only substantial difference is that the route match now looks for <code class="calibre19">scale</code> label with the value <code class="calibre19">down</code> and that a Jenkins build is invoked with <code class="calibre19">scale</code> parameter set to <code class="calibre19">-1</code>. As I mentioned earlier in one of the previous chapters, it is unfortunate that we need to produce so much duplication. But, since webhook <code class="calibre19">url</code> cannot be parametrized, we need to hard-code each combination. I would encourage you, dear reader, to contribute to Alertmanager project by adding Jenkins receiver. Until then, repetition of similar configuration entries is unavoidable.</p>

<p class="calibre3">Let us deploy the <code class="calibre19">monitor</code> stack with the new configuration injected as a Docker secret.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">DOMAIN</code><code class="o">=</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code> <code class="se">\</code>
<code class="lineno">2 </code>    docker stack deploy <code class="se">\</code>
<code class="lineno">3 </code>    -c stacks/docker-flow-monitor-slack.yml <code class="se">\</code>
<code class="lineno">4 </code>    monitor
</pre></div>

</figure>

<p class="calibre3">Please wait until the <code class="calibre19">monitor</code> stack is up-and-running. You can check the status of its services with <code class="calibre19">docker stack ps monitor</code> command.</p>

<p class="calibre3">While we’re into creating services, we’ll deploy a new definition of the <code class="calibre19">go-demo</code> stack as well.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/go-demo-instrument-alert-2.yml <code class="se">\</code>
<code class="lineno">3 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">The new definition of the stack, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>  main:
<code class="lineno"> 3 </code>    ...
<code class="lineno"> 4 </code>    deploy:
<code class="lineno"> 5 </code>      ...
<code class="lineno"> 6 </code>      labels:
<code class="lineno"> 7 </code>        ...
<code class="lineno"> 8 </code>        - com.df.alertName.3=resp_time_below
<code class="lineno"> 9 </code>        - com.df.alertIf.3=sum(rate(http_server_resp_time_bucket{job="my-service\
<code class="lineno">10 </code>", le="0.025"}[5m])) / sum(rate(http_server_resp_time_count{job="my-service"}[5m\
<code class="lineno">11 </code>])) &gt; 0.75
<code class="lineno">12 </code>        - com.df.alertLabels.3=scale=down,service=go-demo_main
<code class="lineno">13 </code>        ...
</pre></div>

</figure>

<p class="calibre3">We added a new set of labels that define the alert that will send a notification that the service should be scaled down. The expression of the alert uses a similar logic as the one we’re using to scale up. It calculates the percentage of responses that were created in twenty-five milliseconds or less. If the result is over 75 percent, the system has more replicas than it needs so it should be scaled down.</p>

<p class="calibre3">Since <code class="calibre19">go-demo</code> produces internal pings that are very fast, there’s no need to create fake responses. The alert will fire soon.</p>

<p class="calibre3">If you doubt the new alert, we can visit the Prometheus <em class="calibre21">alerts</em> screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">The <em class="calibre21">godemo_main_resp_time_below</em> alert should be red.</p>

<p class="calibre3">Similarly, we can visit Jenkins <em class="calibre21">service-scale</em> job and confirm that a new build was executed.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/jenkins/blue/organizations/jenkins/ser\</code>
<code class="lineno">2 </code><code class="s">vice-scale/activity"</code>
</pre></div>

</figure>

<p class="calibre3">The output of the last step says that <em class="calibre21">go-demo_main was scaled from 3 to 2 replicas</em>. That might sound confusing since the previous build scaled it to four replicas. However, we re-deployed the <code class="calibre19">go-demo</code> stack which, among other things, specifies that the number of replicas should be three.</p>

<p class="calibre3">That leads us to an important note.</p>

<aside class="tip">
    <p class="calibre3">Once you adopt auto-scaling of services, you will not be able to use <code class="calibre19">docker stack deploy</code> commands to update services with new releases. Such actions would undo scaling and de-scaling performed by the system. Instead, you should use <code class="calibre19">docker service update --image</code> command to deploy new releases. Unlike <code class="calibre19">stack deploy</code>, <code class="calibre19">service update</code> changes only the specified aspect of a service, not the whole service definition.</p>

</aside>


<figure class="image">
  <img src="../images/00071.jpeg" alt="Figure 12-10: A build of a Jenkins job that scales Docker services" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-10: A build of a Jenkins job that scales Docker services</figcaption>
</figure>


<p class="calibre3">Prometheus will continue firing alerts because the service is still responding faster than the defined lower limit. Since Alertmanager has both the <code class="calibre19">repeat_interval</code> and the <code class="calibre19">group_interval</code> set to five minutes, it will ignore the alerts until ten minutes expire. For more information about <code class="calibre19">repeat_interval</code> and <code class="calibre19">group_interval</code> options, please visit <a href="https://prometheus.io/docs/alerting/configuration/#route">route</a> section of Alertmanager configuration.</p>

<p class="calibre3">Once more than ten minutes pass, it will send a build request to Jenkins. This time, since the service is already using the minimum number of replicas, Jenkins will decide not to continue de-scaling and will send a notification message to Slack.</p>

<p class="calibre3">Please visit the <em class="calibre21">#df-monitor-tests</em> channel inside <a href="https://devops20.slack.com/">devops20.slack.com</a>. Wait for a few minutes, and you should see a Slack notification stating that <em class="calibre21">go-demo_main could not be scaled</em>.</p>

<p class="calibre3">Specifying long <code class="calibre19">alertIf</code> labels can be daunting and error prone. Fortunately, <em class="calibre21">Docker Flow Monitor</em> provides shortcuts for the expressions we used.</p>

<p class="calibre3">Let’s deploy the <code class="calibre19">go-demo</code> stack one last time.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/go-demo-instrument-alert-short.yml <code class="se">\</code>
<code class="lineno">3 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">The definition of the stack, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>  main:
<code class="lineno"> 3 </code>    ...
<code class="lineno"> 4 </code>    deploy:
<code class="lineno"> 5 </code>      ...
<code class="lineno"> 6 </code>      labels:
<code class="lineno"> 7 </code>        ...
<code class="lineno"> 8 </code>        - com.df.alertIf.1=@service_mem_limit:0.8
<code class="lineno"> 9 </code>        ...
<code class="lineno">10 </code>        - com.df.alertIf.2=@resp_time_above:0.1,5m,0.99
<code class="lineno">11 </code>        ...
<code class="lineno">12 </code>        - com.df.alertIf.3=@resp_time_below:0.025,5m,0.75
<code class="lineno">13 </code>      ...
</pre></div>

</figure>

<p class="calibre3">This time we used shortcuts for all three alerts. <code class="calibre19">@resp_time_above:0.1,5m,0.99</code> was expanded into the expression that follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_bucket{job="my-service", le="0.1"}[5m])) / sum(ra\
<code class="lineno">2 </code>te(http_server_resp_time_count{job="my-service"}[5m])) &lt; 0.99```
<code class="lineno">3 </code>
<code class="lineno">4 </code>Similarly, `@resp_time_below:0.025,5m,0.75` became the following expression.
</pre></div>

</figure>
<p class="calibre3">sum(rate(http_server_resp_time_bucket{job=”my-service”, le=”0.025”}[5m])) / sum(rate(http_server_resp_time_count{job=”my-service”}[5m])) &gt; 0.75
```</p>

<aside class="tip">
    <p class="calibre3">For more info, please visit <a href="http://monitor.dockerflow.com/usage/#alertif-parameter-shortcuts">AlertIf Parameter Shortcuts</a> section of the <em class="calibre21">Docker Flow Monitor</em> documentation.</p>

</aside>

<p class="calibre3">Feel free to confirm that the alerts were correctly configured in Prometheus. They should be the same as they were before since the shortcuts expand to the same full expressions we deployed previously.</p>

<p class="calibre3">We managed to create a system that scales services depending on thresholds based on response times. It is entirely automated except if the service is already running the minimum or the maximum number of replicas. In those cases scaling probably does not help and humans need to find out what is the unexpected circumstance that generated the alerts.</p>

<p class="calibre3">We started with expected and created a fallback when unexpected happens. Next, we’ll explore the situation when we start from unexpected.</p>

<h3 id="leanpub-auto-sending-error-notifications-to-slack" class="calibre20">Sending Error Notifications To Slack</h3>

<p class="calibre3">Errors inside our code usually fall into two groups.</p>

<p class="calibre3">There are those we are throwing to the caller function because we do not yet know how to handle it properly, or we are too lazy to implement proper recuperation from such a failure. For example, we might implement a function that reads files from a directory and returns an error if that fails. In such a case we might want to get a notification when the error occurs and do some actions to fix it. After evaluating the problem, we might find out that the directory we’re reading does not exist. Apart from the obvious fix to create the missing directory (immediate response), we should probably modify our code so that the directory is created as a result of receiving such an error. Even better, we should probably extend our code to check whether the directory exists before reading the files from it. Errors such as those fall into “I did not expect it the first time it happened, but it will not happen again” type of situations. There’s nothing we would need to do outside the service. The solution depends entirely on code modifications.</p>

<p class="calibre3">Another common type of errors is related to problems with communication with other services. For example, we might get a notification that there was an error establishing communication with a database. The first action should be to fix the problem (restore DB connection). After that, we should create another set of alerts that will monitor the database itself and execute some corrective steps to fix the issue. Those metrics should not be inside the service that communicates with the database but probably coming from an exporter that is specialized with that particular database. If such an exporter does not exist, database probably has some metrics that could be transformed into Prometheus format. The alternative approach would be to ping the database periodically. We might be able to use <a href="https://github.com/prometheus/blackbox_exporter">Blackbox exporter</a> for that. If none of those options is applicable to your database, you might need to evaluate whether it is worth using something that does not have exporter, not it has metrics, nor it can be pinged.</p>

<p class="calibre3">The examples we explored do not warrant the effort of making anything more complicated than a simple notification to Slack. Self-adaptation does not apply, and self-healing depends on your code more than anything else.</p>

<p class="calibre3">Let us take a look at a few examples.</p>

<p class="calibre3">We’ll continue using metrics coming from instrumentation added to the <code class="calibre19">go-demo</code> service. In the previous chapter you already saw the metrics with errors based on response codes, so let’s deploy the updated stack with the labels that will add a new alert to <em class="calibre21">Docker Flow Monitor</em>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/go-demo-instrument-alert-error.yml <code class="se">\</code>
<code class="lineno">3 </code>    go-demo
</pre></div>

</figure>

<p class="calibre3">The definition of the stack, limited to relevant parts, is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>  main:
<code class="lineno"> 3 </code>    ...
<code class="lineno"> 4 </code>    deploy:
<code class="lineno"> 5 </code>      ...
<code class="lineno"> 6 </code>      labels:
<code class="lineno"> 7 </code>        ...
<code class="lineno"> 8 </code>#        - com.df.alertName.3=resp_time_below
<code class="lineno"> 9 </code>#        - com.df.alertIf.3=@resp_time_below:0.025,5m,0.75
<code class="lineno">10 </code>        - com.df.alertName.3=erro_rrate
<code class="lineno">11 </code>        - com.df.alertIf.3=sum(rate(http_server_resp_time_count{job="go-demo_mai\
<code class="lineno">12 </code>n", code=~"^5..$$"}[5m])) / sum(rate(http_server_resp_time_count{job="go-demo_ma\
<code class="lineno">13 </code>in"}[5m])) &gt; 0.001
<code class="lineno">14 </code>        - com.df.alertLabels.3=service=go-demo_main,type=errors
<code class="lineno">15 </code>        - com.df.alertAnnotations.3=summary=Error rate is too high,description=D\
<code class="lineno">16 </code>o something or start panicking
<code class="lineno">17 </code>      ...
</pre></div>

</figure>

<p class="calibre3">We commented the <code class="calibre19">resp_time_below</code> alert because it would only create noise. Without “real” traffic, responses are too fast, and Prometheus would continuously fire alerts that would only derail us from the task at hand.</p>

<p class="calibre3">The new alert is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_count{job="go-demo_main", code=~"^5..$$"}[5m])) /\
<code class="lineno">2 </code> sum(rate(http_server_resp_time_count{job="go-demo_main"}[5m])) &gt; 0.001
</pre></div>

</figure>

<p class="calibre3">It takes the summary of all response time counts filtered by response codes that start with <code class="calibre19">5</code>. That envelops all service side errors. That number is divided with the count of all responses thus giving us the percentage of requests that failed due to server errors. The alert will fire if the result is greater than <code class="calibre19">0.001</code> (0.1%). In other words, Prometheus will fire the alert if more than 0.1% of responses result in server side errors.</p>

<p class="calibre3">If you tried to write a similar alert, your first thought might have been to send an alert whenever there is an error. Don’t do that! It would probably result in spam since errors are an unavoidable part of what we do. The real goal is not to be notified the system produces an error but when the rate of errors surpasses some threshold. Those thresholds will differ from one service to another. In out case, it is set to 0.1%. That does not mean that alerts should never fire after a single error. In some cases, they should. But this is not one of those. The service will produce errors, and we want to know when there are too many of them and thus discard a temporary problem or something that happened only once, but it will not repeat.</p>

<p class="calibre3">One thing you should note is that inside stack YML definition, dollar signs (<code class="calibre19">$</code>) need to be escaped with another dollar. For that reason, the part of the regular expression that should be <code class="calibre19">^5..$</code> is defined as <code class="calibre19">^5..$$</code>.</p>

<p class="calibre3">Let us open the Prometheus’ alert screen and confirm that the new alert is indeed registered.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">Among others, there should be the <em class="calibre21">godemo_main_error_rate</em> alert marked as green thus indicating that the percentage of server error rate is below 0.1%.</p>

<p class="calibre3">We do not need to change Alertmanager configuration. It is already configured to send all alerts to Slack unless they match one of the routes.</p>

<p class="calibre3">Let us generate a few responses with fake errors and see whether the system works.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="k">for</code> i in <code class="o">{</code><code class="o">1</code>..100<code class="o">}</code><code class="calibre19">;</code> <code class="k">do</code>
<code class="lineno">2 </code>    curl <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/demo/random-error"</code>
<code class="lineno">3 </code><code class="k">done</code>
</pre></div>

</figure>

<p class="calibre3">Around ten out of those hundred requests should result in errorred responses. That’s just enough to confirm that the alerts work as expected.</p>

<p class="calibre3">Let’s go back to the alerts screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">This time <em class="calibre21">godemo_main_error_rate</em> should be red indicating that Prometheus fired it to Alertmanger which, in turn, sent a Slack notification. Please visit the <em class="calibre21">#df-monitor-tests</em> channel inside <a href="https://devops20.slack.com/">devops20.slack.com</a> and confirm that the notification was indeed sent. The message should say <em class="calibre21">[FIRING] go-demo_main service is in danger!</em>.</p>

<p class="calibre3">If this would not be a simulation, your immediate action should be to go back to Prometheus and query metrics in search of the cause of the problem. The expression could be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>sum(rate(http_server_resp_time_count{job="go-demo_main", code=~"^5..$$"}[5m])) b\
<code class="lineno">2 </code>y (path)
</pre></div>

</figure>

<p class="calibre3">By grouping data by path, we can discover which one generated those errors and relate it to the code that is in charge of that path. That would get us a step closer to the discovery of the cause of he problem. The rest would greatly differ from one case to another. Maybe we’d need to consult logs, make more queries in Prometheus, find out which node is causing the problem, correlate data with network information, and so on and so forth. No one can give you exact set of steps that should be followed in all cases. The most important part is that you know that there is an error, which service generated it, and what is the path of the requests behind it. You’re on your own for the rest.</p>

<p class="calibre3">All the metrics we used so far have their shortcuts that let us write a more concise stack definition. The error rate is no exception. An example can be found in the stack <code class="calibre19">stacks/go-demo-instrument-alert-short-2.yml</code>. The definition, limited to relevant parts, is as follows</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>...
<code class="lineno"> 2 </code>  main:
<code class="lineno"> 3 </code>    ...
<code class="lineno"> 4 </code>    deploy:
<code class="lineno"> 5 </code>      ...
<code class="lineno"> 6 </code>      labels:
<code class="lineno"> 7 </code>        ...
<code class="lineno"> 8 </code>        - com.df.alertName.3=errorate
<code class="lineno"> 9 </code>        - com.df.alertIf.3=@resp_time_server_error:5m,0.001
<code class="lineno">10 </code>      ...
</pre></div>

</figure>

<p class="calibre3">The system we built in this chapter can be described through the diagram in the figure 12-11.</p>


<figure class="image1">
  <img src="../images/00072.jpeg" alt="Figure 12-11: Alerts fired to Jenkins and Slack" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-11: Alerts fired to Jenkins and Slack</figcaption>
</figure>


<h3 id="leanpub-auto-what-now-10" class="calibre20">What Now?</h3>

<p class="calibre3">Did we finish with self-adaptation applied to services? We’re not even close. The exercises we passed through should give you only the base you’ll need to extend and adapt to your specific use-cases. I hope that now you know what to do. What you need now, more than anything else, is time. Instrument your services, start scraping metrics, create alerts based both on exporters and instrumentation, start receiving notifications, and observe the patterns. The system should start small and grow organically.</p>

<p class="calibre3">Do not create a project out of the lessons you learned but adopt continuous improvement. Every alert is not only a potential problem but also an opportunity to make the system better and more robust.</p>

<p class="calibre3">The figure 12-12 provides a high-level overview of the system we built so far. Use it as a way to refresh your memory with everything we learned by now.</p>


<figure class="image">
  <img src="../images/00073.jpeg" alt="Figure 12-12: Self-healing and self-adapting system (so far)" class="calibre17"/>
  <figcaption class="calibre18">Figure 12-12: Self-healing and self-adapting system (so far)</figcaption>
</figure>


<p class="calibre3">The time has come to take another rest. Destroy the cluster and recharge your batteries. There’s still a lot left to do.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker-machine rm <code class="se">\</code>
<code class="lineno">2 </code>    -f swarm-1 swarm-2 swarm-3
</pre></div>

</figure>



</div>
</body></html>