<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Testing Network Changes"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Testing Network Changes</h1></div></div></div><p>This chapter will focus on an important part of the software development lifecycle as well as DevOps, testing, and quality assurance. This chapter will describe why it is essential to incorporate network changes as part of the continuous integration process and test them thoroughly. It will then go on to look at open source test tooling that is available to facilitate the creation of tests suites for network operations.</p><p>This chapter will focus on the overall quality assurance process, outlining some of the best-practice approaches that can be adopted by network teams or teams implementing network operations.</p><p>We will also look at the benefits of implementing feedback loops, quality reporting, and what checks can be implemented to make sure that the network is functioning as expected. These are all essential topics as network teams move toward code-driven network operations.</p><p>In this chapter, the following topics will be covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Testing overview</li><li class="listitem" style="list-style-type: disc">Quality assurance best practices</li><li class="listitem" style="list-style-type: disc">Available test tools</li></ul></div><div class="section" title="Testing overview"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec41"/>Testing overview</h1></div></div></div><p>There are many ways to ensure quality when making operational or development changes.</p><p>When combining quality checks and ordering them, they can be used to form a set of quality gates that development, infrastructure, or even network changes should flow through before they reach production. We will briefly touch upon some of the more popular testing strategies that are used to ensure that any changes to a system or application are operating effectively, comprised of the following phases of testing:</p><div class="mediaobject"><img src="graphics/B05559_08_01.jpg" alt="Testing overview"/></div><div class="section" title="Unit testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec89"/>Unit testing</h2></div></div></div><p>One of the most <a id="id659" class="indexterm"/>popular types of quality assurance is the unit test. A <span class="strong"><strong>unit test</strong></span> will <a id="id660" class="indexterm"/>test each isolated code operation and make sure that each method or function exhibits the desired behavior with different inputs.</p><p>One or more unit tests will be required to make sure that a method or function works as desired. So multiple unit tests may need to be written to test any basic operation asserting either a pass or failure based on one isolated operation.</p><p>Unit tests can normally be carried out against compiled binaries, as opposed to requiring a fully-fledged test environment. Utilizing popular test frameworks, unit tests can be used to assert a pass or failure based on input.</p><p>For example, a unit test for an Apache Tomcat web server could involve making sure that the code can serve traffic on HTTP port 8080:</p><div class="mediaobject"><img src="graphics/B05559_08_02.jpg" alt="Unit testing"/></div></div><div class="section" title="Component testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec90"/>Component testing</h2></div></div></div><p>
<span class="strong"><strong>Component testing</strong></span> involves <a id="id661" class="indexterm"/>testing a single component in isolation and <a id="id662" class="indexterm"/>making sure that it behaves the way it should as a self-contained entity.</p><p>Component testing normally involves deploying an application to a test environment and executing a suite of tests against the component that tests all its features and functionality. Microservice applications are small components which need to be tested each time they are released.</p><p>This may involve making sure a banking application can process transactions correctly based on a specific type of account.</p></div><div class="section" title="Integration testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec91"/>Integration testing</h2></div></div></div><p>
<span class="strong"><strong>Integration testing</strong></span> involves <a id="id663" class="indexterm"/>more than one microservice component, so if <a id="id664" class="indexterm"/>two different components are integrated, a set of integration tests needs to be written to make sure they both integrate and exhibit the desired behavior.</p><p>Integration testing normally requires a simulation of a database schema or multiple components to be deployed in an environment and tested together. While a unit test can assert the behavior of the build binaries, an integration test is slightly more complex.</p><p>Mocking or stubbing can be carried out in order to simulate another application's endpoint behavior and assert if it is operating as expected.</p><p>Integration testing could test that two different microservice endpoints can be connected and that a transaction such as a TCP handshake can be completed correctly between the initiator service and the receiver service, with the reception of ACK making sure that the two-way TCP handshake is working correctly between the two microservice applications:</p><div class="mediaobject"><img src="graphics/B05559_08_03.jpg" alt="Integration testing"/></div></div><div class="section" title="System testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec92"/>System testing</h2></div></div></div><p>
<span class="strong"><strong>System testing</strong></span> is normally <a id="id665" class="indexterm"/>carried out on a full-blown environment with a set of <a id="id666" class="indexterm"/>fully deployed components. System testing will test the whole system and is normally utilized as a final step before production. Some of the tests that can be carried out are user journey tests such as setting up a full transaction. This tests that the fully integrated system can pass all the end-to-end testing as a customer would use it in production.</p><p>This may result in integrating multiple microservice applications together such as microservice <span class="strong"><strong>A</strong></span>, <span class="strong"><strong>B</strong></span>, <span class="strong"><strong>C</strong></span>, and <span class="strong"><strong>D</strong></span> and making sure they all integrate functionally and work as a single entity:</p><div class="mediaobject"><img src="graphics/B05559_08_04.jpg" alt="System testing"/></div></div><div class="section" title="Performance testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec93"/>Performance testing</h2></div></div></div><p>
<span class="strong"><strong>Performance testing</strong></span> is fairly <a id="id667" class="indexterm"/>self-explanatory, it will baseline the application's <a id="id668" class="indexterm"/>performance on the first execution. It will then use that baseline to check for any performance degradations in the application every time a new release takes place.</p><p>Performance tests will be used to check performance metrics, this is useful to see if a code commit causes performance issues in the overall system. Performance testing can be incorporated into the system test phase.</p><p>Alternatively, performance <a id="id669" class="indexterm"/>testing can also mean <span class="strong"><strong>Stress Testing or Load Testing</strong></span> the application, network, or <a id="id670" class="indexterm"/>infrastructure to its absolute limit and by writing tests to see to find if the system can cope with the desired traffic patterns.</p><p>
<span class="strong"><strong>Endurance Testing</strong></span> means <a id="id671" class="indexterm"/>setting a time period for testing and see how long the infrastructure, network, or application can cope with stress for a fixed period of time.</p><p>
<span class="strong"><strong>Spike Testing</strong></span> is making <a id="id672" class="indexterm"/>sure a system can cope with a sudden spike in traffic from a dormant traffic pattern, which tests if the system can cope with a high degree of variance.</p><p>
<span class="strong"><strong>Scalability Testing</strong></span> on the other hand <a id="id673" class="indexterm"/>can mean horizontally scaling out infrastructure or scaling up more applications to the point it makes no performance benefit. This identifies the scaling limits a system has.</p><p>
<span class="strong"><strong>Volume Testing</strong></span> can be used <a id="id674" class="indexterm"/>to see the volume of transactions or data a system can process over a given period of time.</p><p>The following diagram shows the different types of testing that fall under the performance testing umbrella:</p><div class="mediaobject"><img src="graphics/B05559_08_05.jpg" alt="Performance testing"/></div></div><div class="section" title="User acceptance testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec94"/>User acceptance testing</h2></div></div></div><p>
<span class="strong"><strong>User acceptance testing</strong></span> involves <a id="id675" class="indexterm"/>having end users test new features or <a id="id676" class="indexterm"/>functionality. User acceptance testing is normally utilized to make sure customers or product managers are happy with the development changes that have been made. This type of testing is normally exploratory and fairly manual. It is often used to test the look and feel of a website or graphical user interface.</p></div><div class="section" title="Why is testing relevant to network teams?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec95"/>Why is testing relevant to network teams?</h2></div></div></div><p>Quality assurance is a huge part <a id="id677" class="indexterm"/>of network or infrastructure changes, it is not just solely a software development concern. If the network or infrastructure, which software is installed upon, is not operating as desired, then this will have the same customer impact as a software bug.</p><p>The customer doesn't differentiate between software bugs, infrastructure, or networking issues. All a customer knows is that they can't utilize products and as far as they are concerned the business is not meeting their needs or providing a good and reliable service.</p><p>Not having adequate testing can be very harmful to a business, as its very reputation can be damaged, and the rise of social media means that if websites are down or not operational, within a blink of the eye an outage can be all over social media channels.</p><p>If one user notices an issue they can send a tweet, which alerts other customers to the issue, one tweet becomes many and before the company knows it, the outage is trending on Twitter or other social media and now everyone across the world is aware that the business is having problems.</p><p>This situation is the worst fear for many online businesses, if the site is not up, operational, and providing a good user experience, then the business is no longer making money and customers may go to a competitor.</p><p>One of the key objectives for any development, infrastructure, or networking team is to provide a good service to end users and prevent downtime or outages. Typically, a set of <span class="strong"><strong>Key Performance Indicators</strong></span> (<span class="strong"><strong>KPIs</strong></span>) are used <a id="id678" class="indexterm"/>to quantify performance and set targets to decipher if a business is meeting customer needs.</p><p>So making the delivery of network changes less prone to error should be the aim of any network team. In <a class="link" href="ch04.html" title="Chapter 4. Configuring Network Devices Using Ansible">Chapters 4</a>, <span class="emphasis"><em>Configuring Network Devices Using Ansible</em></span>, <a class="link" href="ch05.html" title="Chapter 5. Orchestrating Load Balancers Using Ansible">Chapter 5</a>, <span class="emphasis"><em>Orchestrating Load Balancers Using Ansible</em></span>, and <a class="link" href="ch06.html" title="Chapter 6. Orchestrating SDN Controllers Using Ansible">Chapter 6</a>, <span class="emphasis"><em>Orchestrating SDN Controllers Using Ansible,</em></span> we looked at ways to automate network devices, load balancers, and SDN controllers using configuration management tooling. At the same time, having a set of repeatable tests for any network change should also be something that network teams are striving for. The ideal scenario being that a network team knows that a change is going to fail, before it has a customer impact. This means testing changes sufficiently before giving them a seal of approval and failing as fast as possible in test environments so breaking changes are not pushed directly to production environments.</p><p>One of the common concerns from network engineers when initially moving to an automated process is a lack of trust in the automation. Network engineers are used to going through due diligence and a subset of checks prior to releasing network changes. Just because automation is in place doesn't mean that the manual check-list network engineers used to validate network changes goes away.</p><p>However, when considering software delivery, the overall process is only as fast as the slowest component, so if those network validation checks remain manual, then the whole process will be slowed down. This will result in manual stops being placed in the automation process, which will inevitably slow down the delivery of a new product to market.</p><p>The simple solution is to automate each of the networking check-lists, so any validation that was carried out manually <a id="id679" class="indexterm"/>by a network engineer instead becomes an automated check or test as part of an automated test suite which is run alongside the automation.</p><p>These checks or tests are then written and built up over a period of time. So if a situation occurs when an edge case is found and it doesn't have test coverage that causes a failure. Rather than using the argument that the automation doesn't work and making a case to revert to tried and tested manual approaches, network engineers need to instead create a new test or check and add it to the automated validation pack which will catch the issue and fail in a test environment before it reaches the end user.</p></div><div class="section" title="Network changes and testing today"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec96"/>Network changes and testing today</h2></div></div></div><p>Network teams still remain in the main work with a waterfall methodology, so they need to align and adopt a more agile approach. This will allow network teams to better integrate with the rest of the IT team and become a participant in the Continuous Delivery processes rather an observer.</p><p>When the waterfall methodology was the de-facto way of delivering software development projects to market, then a very rigid process lifecycle would be followed.</p><p>Waterfall processes stipulated <a id="id680" class="indexterm"/>every new feature would traverse the following phases:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Analysis</li><li class="listitem" style="list-style-type: disc">Design</li><li class="listitem" style="list-style-type: disc">Implementation</li><li class="listitem" style="list-style-type: disc">Test</li></ul></div><p>One of the main implementations of the waterfall <a id="id681" class="indexterm"/>method was known as the <span class="strong"><strong>V-Model</strong></span>, which was initially used to simplify projects into deliverable chunks. This meant that all stakeholders could identify progress and look for potential delays.</p><p>This simplification made project managers and senior management happy as they had an easy way of tracking projects and whether they were on time or going over budget.</p><p>The structure of the V-Model is shown as follows:</p><div class="mediaobject"><img src="graphics/B05559_08_06.jpg" alt="Network changes and testing today"/></div><p>In waterfall terms, the analysis and design phases took place on the left-hand side of the V-Model and would happen at the start of the process. The left-hand side of the V-Model in simplistic terms is used to <a id="id682" class="indexterm"/>interact with stakeholders, do necessary research, and gather all necessary high-level and low-level  requirements to work out what is required to implement a new product or change. The left side of the V-Model was also about documenting the overall process at an architectural level.</p><p>After the initial requirement gathering as part of the analysis phase, the analysis phase was signed off, which meant that the architectural design phase could begin and some meat could be put around the requirements. As part of the design phase, high-level and low-level design documents would be created to document the proposed changes which would have an associated review, sign off, and approval process.</p><p>Once the design was completed, then the left side of the V-Model was complete and implementation of the change or product would take place. The implementation phase could span numerous weeks, or even months, to deliver the desired result and this lifecycle phase sits at the bottom of the V-Model.</p><p>Once all the requirements were implemented, the implementation phase would be signed off and the project would move to the right-hand side of the V-Model where the Test phase would commence.</p><p>A test team would then carry out unit, integration, and then finally system testing on any change or new product feature. Any issues found with the implementation phase would result in a change request. This would mean that the high- or low-level design would need to be updated, re-work on the implementation would need to be done, and then tests would need to be repeated or re-written in order for the product to be refined.</p><p>With the move to agile development covered in <a class="link" href="ch03.html" title="Chapter 3. Bringing DevOps to Network Operations">Chapter 3</a>, <span class="emphasis"><em>Bringing DevOps to Network Operations,</em></span> the V-Model has been seen to be a sub-optimal delivery mechanism. For reporting purposes, the V-Model is ideal and transparent, but it means that the implementation process suffers from the rigid restrictions enforced on engineers.</p><p>The V-Model doesn't take into account that any engineer likes to iterate processes and the actual implementation they write down at the start of a process may not be the final design they implement. The V-Model doesn't align well to prototyping as engineers typically like to spend time with the system and try, fail, iterate, and then improve the implementation.</p><p>Not accounting for prototyping leads to multiple change requests which have cost implications to businesses, so using two week sprints in an agile methodology to plan in iterative development has proved <a id="id683" class="indexterm"/>much more realistic. Although it is still something that senior managers struggle with as they are indoctrinated with having the need to report due dates and milestones, the due date is, by all intents and purposes, a made-up date.</p><p>An engineer in the waterfall process will still do the same amount of prototyping to deliver implementations, and work takes <span class="emphasis"><em>x</em></span> amount of time regardless of how a plan is structured. Agile development is just structured to accept prototyping and time-boxed spikes.</p><p>So the age-old question from a project manager to an engineer is always; <span class="emphasis"><em>when will this be done by?</em></span> The engineer that replies; <span class="emphasis"><em>I don't know</em></span> isn't an acceptable answer in a waterfall methodology. What is expected is an estimate, or in engineering circles a made-up date, which the project manager will likely change later when, inevitably, said date isn't met.</p><p>So how does any of this have any relevance to network changes and testing overall? Well, network teams today typically implement a mini V-Model when they think about making network changes. Network managers will act as a project manager that will plan out a design, implementation, and test phase cycle and report this back to senior management teams as network changes are seen as long pieces of work that need massive planning and testing before implementation.</p><p>Network managers may not split out testing into test, integration, and system testing as traditionally, network testing is not as sophisticated as this, but it doesn't allow network engineers the freedom to prototype.</p><p>Instead network engineers, like infrastructure engineers or any operational team before them, will be pressured into making changes to a rigid plan. The plan will be indicative of the following criteria being met:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Does the implemented change work as desired?</li><li class="listitem" style="list-style-type: disc">Did the change break anything?</li><li class="listitem" style="list-style-type: disc">Is the documentation updated to reflect change?</li></ul></div><p>If all these points are met, they would have deemed a successful change by a network team.</p><p>However, this doesn't tell <a id="id684" class="indexterm"/>the complete and whole story as other points have to be considered when making network changes within the remit of a Continuous Delivery model:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Will the change break anything that isn't immediately visible?</li><li class="listitem" style="list-style-type: disc">Was the same change implemented to pre-production environments?</li><li class="listitem" style="list-style-type: disc">Was the same change tested and validated on pre-production environments?</li></ul></div><p>The initial three points are mandatory requirements when doing any change, but the remaining three points should be considered mandatory too, in order to maintain a successful Continuous Delivery model. This is something of a mind-set change for network engineers, that they need to take this into consideration when making changes.</p><p>All network changes from a network engineer need to be committed to the source control management system and propagated through all the necessary environments before being pushed to production, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05559_08_07.jpg" alt="Network changes and testing today"/></div><p>If a change is made directly to production manually by network engineers and not implemented first on test, pre-production and production environments via an automated, then the network configuration will be forever misaligned on test environments. This will have dire consequences as the configuration of test, pre-production environments, and production will have drifted apart.</p><p>This means that any developer, infrastructure, or network engineer using test and pre-production environments expecting a copy of production for mission-critical changes will be sadly disappointed. This can, in turn, compromise any tests run on those environments as they are no longer a proper reflection of the production estate.</p><p>So what does this mean in practice? The system test box on the V-Model may pass in pre-production environments but fail in production. This does not build confidence in the Continuous Delivery process which is now a mission critical part of the business.</p><p>It cannot be highlighted how important it is to make sure all network, code, or infrastructure changes are pushed to pre-production environments prior to production to maintain the validity of all <a id="id685" class="indexterm"/>environments. Any team deviating from this process can compromise the whole system and negate the testing.</p><p>This is not only used to test the changes, but also to keep the pre-production environments as a scaled-down mirror image of production that avoids the scenario of all tests pass in test environments but when they are deployed to production they cause outages to customers. So it means it is important that all validation tests are completed in associated test environments before a change is released to production.</p><p>If manual changes are pushed directly into production, even in the event of emergencies, then the changes need to be<a id="id686" class="indexterm"/> immediately put back into the <span class="strong"><strong>Source Control Management</strong></span> (<span class="strong"><strong>SCM</strong></span>) system, the SCM system should be the single source of truth for all configuration at all times.</p><p>If any manual changes are applied, snowflake environments will become common, which are shown below. This is where an engineer has made a manual change to production outside the process and not pushed the change to any of the other environments using the deployment pipeline:</p><div class="mediaobject"><img src="graphics/B05559_08_08.jpg" alt="Network changes and testing today"/></div><p>In order for network changes to be delivered at the desired rate, network changes and testing cannot continue to be done in a mini V-Model strategy. If network teams and managers are serious about being collaborators in Continuous Delivery models and DevOps models, they need to<a id="id687" class="indexterm"/> keep pace with the rest of the agile changes being made in development and infrastructure teams.</p><p>The solution though is not to stop validating changes and paying due diligence, lessons can be learned from formulaic quality assurance processes that have been successfully applied on development and infrastructure changes for years, these processes can also help test network changes.</p></div></div></div>
<div class="section" title="Quality assurance best practices"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec42"/>Quality assurance best practices</h1></div></div></div><p>Quality assurance teams, when utilizing a waterfall V-Model structure of delivery, worked in silos that <a id="id688" class="indexterm"/>retrospectively tested development changes once they were completed by a development team.</p><p>This led to quality assurance teams having to react to every development change, as it was an impossible task having to write tests for a feature they had not yet seen, or understand how it fully operated. Situations would often arise where developers without warning would commit features into source control management systems and then quality assurance teams would have to react to them:</p><div class="mediaobject"><img src="graphics/B05559_08_09.jpg" alt="Quality assurance best practices"/></div><p>This method of working <a id="id689" class="indexterm"/>provided lots of challenges such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Developers changing user interfaces, so the quality assurance team's automated tests broke as test engineers were not aware of the user interface changes</li><li class="listitem" style="list-style-type: disc">Test engineers not understanding new features meaning appropriate tests weren't written to test functionality properly</li><li class="listitem" style="list-style-type: disc">Developers having to spend lots of time explaining how features worked to quality assurance testers so they could write tests post-commit</li><li class="listitem" style="list-style-type: disc">Delays associated with fixing broken regression tests that were not down to bugs but test issues</li><li class="listitem" style="list-style-type: disc">Quality assurance teams were acting in a completely reactive fashion as they could not see what new developer changes were coming</li><li class="listitem" style="list-style-type: disc">Quality assurance packs never passing, or being green, meant that actual software issues slipped through the process</li></ul></div><p>So, when considering network testing, the solution to this problem is not to hire a separate test team. Instead, it is about incorporating and integrating network testing into a Continuous Delivery model.</p><p>Agile development has shown that as code changes were being written, embedding quality assurance test engineers in the development team meant that tests could be written pre-commit. It is a far more productive method of working.</p><p>Moving quality assurance engineers out of the siloed quality assurance team and allowing them to work together in the same scrum team means that individuals that work together can collaborate and make sure that the submitted commit will work at every phase.</p><p>The associated regression, integration, or system tests then form a set of automated quality gates that the change will propagate through:</p><div class="mediaobject"><img src="graphics/B05559_08_10.jpg" alt="Quality assurance best practices"/></div><p>The main benefits of <a id="id690" class="indexterm"/>the agile testing approach over using a siloed waterfall approach is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Quality assurance testers were no longer working reactively and have complete visibility of what developers are creating</li><li class="listitem" style="list-style-type: disc">Each agile user story could have proper acceptance criteria written that included automated testing, allowing quality assurance engineers to work on test tasks as developers code new features</li><li class="listitem" style="list-style-type: disc">When new features were coded, relevant tests are written for a new feature</li><li class="listitem" style="list-style-type: disc">New feature tests can be added to the regression pack, so that every time a code commit is made the feature is tested</li></ul></div><p>This process change removes the inhibitor, which is simply the team structure, and joins two teams together so they become more productive, which is in essence the DevOps way.</p><div class="section" title="Creating testing feedback loops"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec97"/>Creating testing feedback loops</h2></div></div></div><p>If we think back <a id="id691" class="indexterm"/>to the continuous integration process in <a class="link" href="ch07.html" title="Chapter 7. Using Continuous Integration Builds for Network Configuration">Chapter 7</a>, <span class="emphasis"><em>Using Continuous Integration Builds for Network Configuration</em></span>, then we had the <a id="id692" class="indexterm"/>commit process. The commit essentially starting the whole Continuous Delivery process. Once a commit has taken place, the change is already on the road to production. Any commit to the trunk/mainline/master branch is a final change, so if a network commit is made, it is already on its way to production.</p><p>If no validation engine or tests exist post check-in, then changes will flow all the way through test environments reaching production environments.</p><div class="mediaobject"><img src="graphics/B05559_08_11.jpg" alt="Creating testing feedback loops"/></div><p>This means that utilizing feedback loops with proper test gates is essential, so once a code commit has taken <a id="id693" class="indexterm"/>place, it will be adequately tested and <a id="id694" class="indexterm"/>provide an immediate indicator that a change has failed. Once all the quality gates have completed successfully only then should the change be promoted to production, this model promotes continuous improvement and failing fast. The further to the left a change fails, the less cost it incurs to a business.</p></div><div class="section" title="Continuous integration testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec98"/>Continuous integration testing</h2></div></div></div><p>In <a class="link" href="ch07.html" title="Chapter 7. Using Continuous Integration Builds for Network Configuration">Chapter 7, </a>
<span class="emphasis"><em>Using Continuous Integration Builds for Network Configuration</em></span>, we focused on the <a id="id695" class="indexterm"/>process of continuous <a id="id696" class="indexterm"/>integration and how multiple different checks can be applied as part of the validation engine for a user commit. This makes sure that the user commit is always properly validated.</p><p>Continuous integration provides a set of feedback loops where a code commit is submitted to the SCM and the validation engine will return either a pass or a failure. All testing can form the validation engine for changes:</p><div class="mediaobject"><img src="graphics/B05559_08_12.jpg" alt="Continuous integration testing"/></div><p>The continuous integration process when applied to development, takes the approach that all changes <a id="id697" class="indexterm"/>are committed to the <span class="strong"><strong>Trunk/Mainline/Master</strong></span> branch:</p><div class="mediaobject"><img src="graphics/B05559_08_13.jpg" alt="Continuous integration testing"/></div><p>The new development <a id="id698" class="indexterm"/>feature will be committed <a id="id699" class="indexterm"/>to the <span class="strong"><strong>Trunk/Mainline/Master</strong></span> branch. This new commit will be compiled and be immediately integrated with the rest of the code base, and then subsequent unit tests will then be executed to determine a pass or failure against the build binaries, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/B05559_08_14.jpg" alt="Continuous integration testing"/></div><p>Using the commit to <span class="strong"><strong>Trunk/Mainline/Master</strong></span> continuous integration approach relies on a degree of discipline from teams. If a commit fails <a id="id700" class="indexterm"/>and the <span class="strong"><strong>CI Build Server</strong></span> returns <a id="id701" class="indexterm"/>a failed build, then the team member that <a id="id702" class="indexterm"/>made the failed commit has a duty to fix the build immediately, by either reverting or fixing the broken commit.</p><p>Continuous integration builds should under no circumstances ever be left in a failed state as it means the <span class="strong"><strong>Trunk/Mainline/Master</strong></span> is not in a clean state and all subsequent code commits will not have valid continuous integration performed until the build is fixed. This slows down a team's productivity so continuous integration is a collaborative process and failure should be seen as a learning opportunity.</p></div><div class="section" title="Gated builds on branches"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec99"/>Gated builds on branches</h2></div></div></div><p>Another popular method <a id="id703" class="indexterm"/>is using <span class="strong"><strong>feature branches</strong></span> and <span class="strong"><strong>gated builds</strong></span>. Every <a id="id704" class="indexterm"/>time a developer makes a change, they will raise a merge <a id="id705" class="indexterm"/>request which will be peer-reviewed by other members of the team and then subsequently merged.</p><p>Each merge request, when accepted, will start the merge process, but as part of the merge process something known as a gated build will execute.</p><div class="mediaobject"><img src="graphics/B05559_08_15.jpg" alt="Gated builds on branches"/></div><p>A gated build will be invoked when a merge occurs prior to integration with the <span class="strong"><strong>Trunk/Mainline/Master</strong></span>. It will run the equivalent of a continuous integration build as a pre-commit, but only if the build and unit testing associated with the pre-commit build passes, will the contents of the merge request be merged to the <span class="strong"><strong>Trunk/Mainline/Master</strong></span> branch.</p><p>The gated build process means that the <span class="strong"><strong>Trunk/Mainline/Master</strong></span> branch is always kept completely clean and functional. Where pure continuous integration can have developers break the continuous integration build, gated builds prevent this from happening, as long as the tests are good.</p></div><div class="section" title="Applying quality assurance best practices to networking"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec100"/>Applying quality assurance best practices to networking</h2></div></div></div><p>Network teams can greatly <a id="id706" class="indexterm"/>benefit from adopting some of the best practices and tried and tested methodologies that have been implemented to test development or infrastructure changes.</p><p>Quality assurance is all about principles and processes, so test methodologies are fairly agnostic and the tools used to implement the process come secondary.</p><p>When teams are working within a Continuous Delivery model, any changes to network devices, load balancers, or even SDN controllers should be defined in source control using orchestration and configuration management tools such as Ansible.</p><p>In a Continuous Delivery model, network changes need to propagate through environments and at all times be governed by source control management systems, with the state of the SCM system being the state of the network:</p><div class="mediaobject"><img src="graphics/B05559_08_16.jpg" alt="Applying quality assurance best practices to networking"/></div><p>Network changes can be treated much like code changes and adequate testing could be created as a network team initiative or by collaborating with the quality assurance team. The type of testing at each phase may vary slightly from the set of tests that a development or infrastructure team would run as part of their deployment pipeline.</p><p>However, equivalent network-specific testing can be derived to create a set of robust tests that network changes have to <a id="id707" class="indexterm"/>traverse before being deployed to production by associating particular network tests with each quality gate on the deployment pipeline.</p><p>Network teams, such as development and infrastructure teams need to create a set of feedback loops to govern network changes, so that different test categories can be executed in the deployment pipeline.</p><p>All testing should ideally be automated as part of each network change, in a proactive manner and written at the same time as the network change is being lined up. This then allows network changes to be tested in an automated manner at the point of inception:</p><div class="mediaobject"><img src="graphics/B05559_08_17.jpg" alt="Applying quality assurance best practices to networking"/></div><p>When setting up continuous integration, selecting either continuous integration or a gated build strategy is down to the preference of the network team or engineers that commit changes.</p><p>Unit testing should be integrated <a id="id708" class="indexterm"/>with the network continuous integration process. A network operator will first check in a code change or change the state of the network.</p><p>The <span class="strong"><strong>CI Build Server</strong></span> will <a id="id709" class="indexterm"/>check the Ansible <code class="literal">var</code> YAML files using Lint, which will make sure that the YAML files are valid syntax.</p><p>If valid, the same playbook that would be executed on any downstream environment will be executed against a CI test environment to make sure the playbook is successful in terms of syntax and execution.</p><p>Finally, a set of unit tests will be executed against the environment to validate its functional and desired state of the environment after the playbook has been executed:</p><div class="mediaobject"><img src="graphics/B05559_08_18.jpg" alt="Applying quality assurance best practices to networking"/></div><p>The important thing to note is that unit tests are executed as part of the continuous integration process and <a id="id710" class="indexterm"/>that these tests can either be part of the merge request validation <a id="id711" class="indexterm"/>or executed on commit to the <span class="strong"><strong>Trunk/Mainline/Master</strong></span> branch.</p></div><div class="section" title="Assigning network testing to quality gates"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec101"/>Assigning network testing to quality gates</h2></div></div></div><p>When looking at what type of <a id="id712" class="indexterm"/>testing network teams can carry out to <a id="id713" class="indexterm"/>validate network changes, they can be broken into different test categories and assigned to different quality gates.</p><p>Some of the main test environments covered in this chapter are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Unit test</li><li class="listitem" style="list-style-type: disc">Integration test</li><li class="listitem" style="list-style-type: disc">System test</li></ul></div><p>Before considering where to put tests, we should first look at the network team's needs. With a blank canvas what would be a beneficial set of tests that could help with network operations?</p><p>Some of the following tests spring to mind, but any check or validation that is valid to a particular team is applicable and should be included:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Network checklist that network engineers carry out manually when making changes</li><li class="listitem" style="list-style-type: disc">Unit testing against network automation to make sure the network device is in the desired state</li><li class="listitem" style="list-style-type: disc">Testing performance of the network to see what the desired throughput is and test when parts of the network become oversubscribed and need to be scaled out</li><li class="listitem" style="list-style-type: disc">Testing failover of network devices</li><li class="listitem" style="list-style-type: disc">Testing network code quality</li><li class="listitem" style="list-style-type: disc">Testing different user journeys through the network</li><li class="listitem" style="list-style-type: disc">Testing Quality of Services</li></ul></div><p>All of these types of tests can then be assigned to particular test environments and quality gates created:</p><div class="mediaobject"><img src="graphics/B05559_08_19.jpg" alt="Assigning network testing to quality gates"/></div></div></div>
<div class="section" title="Available test tools"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec43"/>Available test tools</h1></div></div></div><p>Test tools, like all <a id="id714" class="indexterm"/>tools, should be used to facilitate test processes and outcomes. So. for every single test quality gate, tools are required to wrap processes, schedule, and execute tests.</p><p>There are various test tools available on the market today that network engineers could greatly benefit from using.</p><div class="section" title="Unit testing tools"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec102"/>Unit testing tools</h2></div></div></div><p>Network unit testing <a id="id715" class="indexterm"/>as said many times before will form part of the <a id="id716" class="indexterm"/>continuous integration build process and scheduled by a continuous integration build server.</p><p>One open source tool that can help with unit testing network changes is Test Kitchen. <span class="strong"><strong>Test Kitchen</strong></span> is a unit <a id="id717" class="indexterm"/>testing tool which utilizes the Busser framework and can be used to carry out infrastructure testing. Test Kitchen supports many <a id="id718" class="indexterm"/>test frameworks <a id="id719" class="indexterm"/>such as <span class="strong"><strong>Bats</strong></span> and <span class="strong"><strong>RSpec</strong></span>.</p><p>The <span class="strong"><strong>Test Kitchens Busser</strong></span> framework <a id="id720" class="indexterm"/>is comprised of the following architectural components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Driver</li><li class="listitem" style="list-style-type: disc">Provisioner</li><li class="listitem" style="list-style-type: disc">Platform</li><li class="listitem" style="list-style-type: disc">Suites</li></ul></div><p>Test Kitchen defines all its plugins using a <code class="literal">kitchen.yml</code> file, which outlines the Driver, Provisioner, Platform, and Suites to use for the testing.</p><p>A <span class="strong"><strong>Driver</strong></span> can be <a id="id721" class="indexterm"/>any platform that can be used to provision a virtual machine or container. Test Kitchen has support for Vagrant, Amazon, OpenStack, and Docker, and so can be used to test infrastructure changes.</p><p>A <span class="strong"><strong>Provisioner</strong></span> is a <a id="id722" class="indexterm"/>configuration management tool such as Ansible, Chef, Puppet, or Salt and is used to configure the server into the state that needs to be tested.</p><p>The <span class="strong"><strong>Platform</strong></span> is the <a id="id723" class="indexterm"/>operating system that the Provisioner will execute on. Multiple Platforms can be specified for cross-operating system testing. This could be very useful when testing new versions of network operating systems operate in the same way as their predecessors when doing software upgrades.</p><p>
<span class="strong"><strong>Suites</strong></span> are used to <a id="id724" class="indexterm"/>create a test suite in combination with the Platform definition, so if two different Platforms are defined, then unit tests will be executed against each different platform in a consistent manner.</p></div><div class="section" title="Test Kitchen example using OpenStack"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec103"/>Test Kitchen example using OpenStack</h2></div></div></div><p>The <code class="literal">test kitchen</code> gem will need to be pre-installed on the Ansible <a id="id725" class="indexterm"/>controller host. Then perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">From an Ansible controller node, in the folder containing the top player file structure, as shown in the following screenshot:<div class="mediaobject"><img src="graphics/B05559_08_20.jpg" alt="Test Kitchen example using OpenStack"/></div><p>Here, execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>kitchen init –provisioner=ansible –driver=openstack</strong></span>
</pre></div><p>This creates a <code class="literal">kitchen.yml</code> file and a test subdirectory.</p></li><li class="listitem">Next, the <code class="literal">folder test</code> folder needs to be created which will store the unit tests:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir ./tests/integration/default/bats</strong></span>
</pre></div></li><li class="listitem">The <code class="literal">test kitchen</code> file <a id="id726" class="indexterm"/>will then need to be populated <a id="id727" class="indexterm"/>with the Driver, Platform, Provisioner, and Suites.<p>In the following example:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The Driver is specified as OpenStack, with a <code class="literal">cumulus-vx</code> image and Platform being created.</li><li class="listitem" style="list-style-type: disc">The size of the image is <code class="literal">m1.large</code> which specifies the CPU, RAM, and disk for the server.</li><li class="listitem" style="list-style-type: disc">The instance will be created within the <code class="literal">network_team</code> tenant and <code class="literal">qa</code> availability zone.</li><li class="listitem" style="list-style-type: disc">Once spun up, the <code class="literal">configure_device.yml</code> playbook will be executed to configure the network device before the default folder under <code class="literal">test/integration</code> which was defined in step 2. This tells Test Kitchen the location of the Bats tests that will be executed to test the state of the device:</li></ul></div><div class="mediaobject"><img src="graphics/B05559_08_21.jpg" alt="Test Kitchen example using OpenStack"/></div></li><li class="listitem">Each test can be <a id="id728" class="indexterm"/>given a unique name <a id="id729" class="indexterm"/>and the <code class="literal">.bats</code> file extension to define each unit test under the <code class="literal">bats</code> directory that was created in step 2:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Test/integration/default/bats/unit_test.bats</strong></span>
</pre></div></li><li class="listitem">An example of a test that can be written using Bats is as follows:<div class="mediaobject"><img src="graphics/B05559_08_22.jpg" alt="Test Kitchen example using OpenStack"/></div><p>This checks that the <code class="literal">eth0</code> interface is in a good working state when executed.</p></li><li class="listitem">Finally, to execute <code class="literal">test kitchen</code>, execute <a id="id730" class="indexterm"/>the command <a id="id731" class="indexterm"/>shown in the following screenshot:<div class="mediaobject"><img src="graphics/B05559_08_23.jpg" alt="Test Kitchen example using OpenStack"/></div><p>Test Kitchen will then carry out the following workflow:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create instance in OpenStack.</li><li class="listitem">Run playbook.</li><li class="listitem">Install Busser plugin.</li><li class="listitem">Run unit tests.</li><li class="listitem">Destroy instance if all tests passed.</li></ol></div></li></ol></div></div><div class="section" title="Network checklist"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec104"/>Network checklist</h2></div></div></div><p>Network engineers, as <a id="id732" class="indexterm"/>discussed, often have a set of manual checklists that <a id="id733" class="indexterm"/>they use to validate if a network change has been successful or not.</p><p>Sometimes, this could involve validating whether a user interface has the desired configuration that checks if the automation has worked as desired.</p><p>Instead of doing these checks manually, Selenium can be used to carry out graphical user interface checks.</p><p>Selenium's workflow can be summarized as test scripts invoking the Selenium web driver which then creates a browser session to test a website or web page:</p><div class="mediaobject"><img src="graphics/B05559_08_24.jpg" alt="Network checklist"/></div><p>Test scripts can be written in <a id="id734" class="indexterm"/>multiple languages such as Java, Python, or Ruby.</p><p>Selenium can be installed in <a id="id735" class="indexterm"/>Python form by doing a pip install when <a id="id736" class="indexterm"/>using Python for authoring scripts.</p><p>As Selenium is browser-based, it works with multiple browsers such as Internet Explorer, Firefox, Chrome, and Safari and tests cross-browser support.</p><p>A Selenium test sample is <a id="id737" class="indexterm"/>shown in the following screenshot; this script will launch <a class="ulink" href="http://google.co.uk">google.co.uk</a> in Chrome, type <code class="literal">DevOps For Networking</code> and finally click the <span class="strong"><strong>Search</strong></span> button on Google:</p><div class="mediaobject"><img src="graphics/B05559_08_25.jpg" alt="Network checklist"/></div><p>So, any graphical interface can be screen scraped such as a load balancer or network device interface to assert that the correct information has been entered and returned. This can also be useful if older network devices don't have an API.</p></div><div class="section" title="Network user journey"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec105"/>Network user journey</h2></div></div></div><p>A good test methodology is <a id="id738" class="indexterm"/>to test user journeys throughout the network. This <a id="id739" class="indexterm"/>can be done by doing point-to-point testing in the network.</p><p>A good example of network user journeys may <a id="id740" class="indexterm"/>be testing <span class="strong"><strong>Equal Cost Multipath</strong></span> (<span class="strong"><strong>ECMP</strong></span>) on Leaf-Spine architecture to make sure it is performing as desired.</p><p>Another test may be setting up point-to-point tests across data centers to make sure links are performing as desired and do not suddenly depreciate.</p><p>Setting up user journey <a id="id741" class="indexterm"/>testing means that if a baseline performance <a id="id742" class="indexterm"/>drops, then it can be tracked back to specific network changes as part of the network deployment pipeline. This is done in much the same way as baselining application performance and making sure a new release doesn't cause a drop in performance that will impact end users.</p><p>Network user journey testing mean that if an ill-performing path through the network is found, then it can be localized and fixed quickly so it improves mean time to resolution when issues occur. Network engineers can use a tool <a id="id743" class="indexterm"/>such as <span class="strong"><strong>iPerf</strong></span> to send large amounts of packets through points in the network. This can be useful to see where the bottlenecks are in the network and make sure the performance is as desired.</p></div><div class="section" title="Quality of Service"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec106"/>Quality of Service</h2></div></div></div><p>A lot of network<a id="id744" class="indexterm"/> tools now offer <span class="strong"><strong>Quality of Service</strong></span> (<span class="strong"><strong>QoS</strong></span>), which <a id="id745" class="indexterm"/>allows network operators to limit the amount of network bandwidth that particular tenants utilize in a network.</p><p>This prevents noisy test environments from impacting a production environment. This is possible as network devices can set guarantees on performance on particular tenant networks. This means that certain application workloads are always guaranteed a certain network throughput, while other less crucial tenant networks can be capped at peak times.</p><p>Different thresholds and alerting can be set up on network devices and faults in network hardware can be detected if the QoS drops at a random time. It also guards network engineers against the age-old: <span class="emphasis"><em>I think we have a network problem</em></span>. Instead they can prove it is an application issue, as the network service is stable and performing as desired and can be easily displayed.</p><p>It is good to simulate and test QoS away from production environments and have network teams come up with different scenarios to design the best fit for the network, based on the applications that they are hosting.</p></div><div class="section" title="Failover testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec107"/>Failover testing</h2></div></div></div><p>
<span class="strong"><strong>Failover testing</strong></span> should ideally be <a id="id746" class="indexterm"/>tested regularly by network teams, as modern <a id="id747" class="indexterm"/>networks should be disaster recovery-aware and designed for failure.</p><p>Network failover tests can be simulated by writing an Ansible playbook or role that disables a service or reboots a switch to make sure that the system adequately fails over.</p><p>Utilizing <code class="literal">delegate_to: localhost</code>, API commands can be issued to network devices such as switches to disable <a id="id748" class="indexterm"/>them programmatically using the API. Alternatively, <a id="id749" class="indexterm"/>Ansible can SSH onto a network device's operating system and issue an impromptu hard reboot.</p><p>Supplementary monitoring should be set up while doing failover testing to make sure the network does not drop packets and test the speed at which the network device fails over after the initial primary device is disabled.</p></div><div class="section" title="Network code quality tooling"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec108"/>Network code quality tooling</h2></div></div></div><p>When defining the desired state of the network  as code, make sure the Python code that is written to create Ansible modules, as <a id="id750" class="indexterm"/>well as any other code that is used is of a high standard and good quality.</p><p>
<span class="strong"><strong>SonarQube</strong></span> is an open<a id="id751" class="indexterm"/> source code quality tool which allows teams to analyze their codes quality. Its architecture is comprised of three main components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SonarQube Runner</li><li class="listitem" style="list-style-type: disc">SonarQube database</li><li class="listitem" style="list-style-type: disc">SonarQube web interface</li></ul></div><p>Sonar has a range of plug-ins that can be configured to provide unit test reporting, code coverage, or code quality rules and can be set-up for any language be it Python, Java, or C#.</p><p>SonarQube will snapshot a code repository every time it is run and store the history of a project in terms of code quality. This can be trended over time showing quality improvements or drops in the code quality. Sonar can be used to define specific best practice or rules, which show up as violations when broken by commits.</p><p>The <span class="strong"><strong>SonarQube Runner</strong></span> uses <a id="id752" class="indexterm"/>a <code class="literal">sonar.properties</code> file at runtime that can be included as part of the source control management system. This can be pulled down as part of the continuous integration process. This means that after a new code commit on a custom Ansible module the SonarQube Runner can be executed against the code to test the new commit and see the impact.</p><p>The SonarQube Runner will execute a code quality check using one of the plug-ins stipulated in the <code class="literal">sonar.properties</code> file. In the case of a new or changed Ansible module that will invoke the Python-specific group of code quality tests. Information will subsequently be displayed on the Sonar web-interface once the analysis is complete.</p><p>The workflow for this process is shown in the following screenshot with the SonarQube Runner triggering the whole process:</p><div class="mediaobject"><img src="graphics/B05559_08_26.jpg" alt="Network code quality tooling"/></div><p>An example of the sonar <a id="id753" class="indexterm"/>Python SonarQube project <a id="id754" class="indexterm"/>dashboard is shown in the following screenshot, outlining the bugs, vulnerabilities, and tech debt to fix all the issues in the code:</p><div class="mediaobject"><img src="graphics/B05559_08_27.jpg" alt="Network code quality tooling"/></div><p>Tracking code quality <a id="id755" class="indexterm"/>and metrics is very important when <a id="id756" class="indexterm"/>implementing a continuous improvement model in any company. So adequately measuring and analyzing where improvements can be made in the code that drives all processes is important in order to have engineers engage and write tests.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec44"/>Summary</h1></div></div></div><p>In this chapter we have looked at why testing network changes are necessary. We focused on the benefits of utilizing feedback loops to continuously improve network operations. We then explored some of the challenges associated with the way network teams approach network changes and testing and how they will need to adapt and adopt quality assurance best practices to keep up when companies are running a Continuous Delivery model supplemented by a DevOps methodology.</p><p>We then looked at how network teams could set up quality gates for testing and looked at some of the tests that could be mapped at each stage of testing. Finally we looked at some available tools that could be used to carry out network testing to implement unit testing, check-lists, and code quality checks.</p><p>In this chapter you have learned about different types of test strategies such as unit, component, integration, performance, system, and user acceptance testing. Key takeaways also include quality assurance best practices, and why they are applicable to networking and different types of network validations that could help assert automated network changes.</p><p>This chapter has also delved into test tools that can be used to help test networking such as Test Kitchen (<a class="ulink" href="http://kitchen.ci/">http://kitchen.ci/</a>), SonarQube (<a class="ulink" href="http://www.sonarqube.org/">http://www.sonarqube.org/</a>), and iPerf (<a class="ulink" href="https://iperf.fr/">https://iperf.fr/</a>).</p><p>In the next chapter we will focus on deployment pipelines, look at the tooling that can be used to automatically deploy network changes. We will also look at the difference between Continuous Delivery and deployment and when each approach should be implemented.</p><p>The following blogs and presentations may be useful for further understanding microservice test strategies in more detail:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://martinfowler.com/articles/microservice-testing/">http://martinfowler.com/articles/microservice-testing/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://www.youtube.com/watch?v=FotoHYyY8Bo">https://www.youtube.com/watch?v=FotoHYyY8Bo</a></li></ul></div></div></body></html>