<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Service Discovery &#x2013; The Key to Distributed Services"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Service Discovery – The Key to Distributed Services</h1></div></div></div><div class="blockquote"><table border="0" cellpadding="0" cellspacing="0" class="blockquote" summary="Block quote" width="100%"><tr><td valign="top"> </td><td valign="top"><p><span class="emphasis"><em>It does not take much strength to do things, but it requires a great deal of strength to decide what to do.</em></span></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td align="right" colspan="2" style="text-align: center" valign="top">--<span class="attribution"><span class="emphasis"><em>Elbert Hubbard</em></span></span></td></tr></table></div><p>The more services we have, the bigger the chance for a conflict to occur if we are using predefined ports. After all, there can be no two services listening on the same port. Managing an accurate list of all the ports used by, let's say, a hundred services is a challenge in itself. Add to that list the databases those services need and the number grows even more. For that reason, we should deploy services without specifying ports and letting Docker assign random ones for us. The only problem is that we need to discover the port number and let others know about it:</p><div class="mediaobject"><img alt="Service Discovery – The Key to Distributed Services" src="graphics/B05848_08_01.jpg"/><div class="caption"><p>Figure 8-1 – Single node with services deployed as Docker containers</p></div></div><p>Things will get even more complicated later on when we start working on a distributed system with services deployed into<a class="indexterm" id="id296"/> one of the multiple servers. We can choose to define in advance which service goes to which server, but that would cause a lot of problems. We should try to utilize server resources as best we can, and that is hardly possible if we define in advance where to deploy each service. Another problem is that automatic scaling of services would be difficult at best, and not to mention automatic recuperation from, let's say, server failure. On the other hand, if we deploy services to the server that has, for example, least number of containers running, we need to add the IP to the list of data needed to be discovered and stored somewhere:</p><div class="mediaobject"><img alt="Service Discovery – The Key to Distributed Services" src="graphics/B05848_08_02.jpg"/><div class="caption"><p>Figure 8-2 – Multiple nodes with services deployed as Docker containers</p></div></div><p>There are many other examples of cases when we need to store and retrieve (discover) some information related to the services we are working with.</p><p>To be able to locate our services, we need at least the following two processes to be available for us:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Service registration</strong></span> process that <a class="indexterm" id="id297"/>will store, as a minimum, the host and the port service is running on.</li><li class="listitem"><span class="strong"><strong>Service discovery</strong></span> process that will <a class="indexterm" id="id298"/>allow others to be able to discover the information we stored during the registration process:<div class="mediaobject"><img alt="Service Discovery – The Key to Distributed Services" src="graphics/B05848_08_03.jpg"/><div class="caption"><p>Figure 8-3 – Service registration and discovery</p></div></div></li></ol></div><p>Besides those processes, we need to consider several other aspects. Should we unregister the service if it stops working and deploy/register a new instance? What happens when there are multiple copies of the same service? How do we balance the load among them? What happens if a server goes down? Those and many other questions are tightly related to the registration and discovery processes and will be the subject of the next chapters. For now, we'll limit the scope only to the <span class="emphasis"><em>service discovery</em></span> (the common name that envelops both processes mentioned above) and the tools we might use for such a task. Most of them feature highly available distributed key/value storage.</p><div class="section" title="Service Registry"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec19"/>Service Registry</h1></div></div></div><p>The goal of the service registry is simple. Provide capabilities to store service information, be fast, persistent, fault-tolerant, and so on. In its essence, service registry is a database with a very limited <a class="indexterm" id="id299"/>scope. While other databases might need to deal with a vast amount of data, service registry expects a relatively small data load. Due to the nature of the task, it should expose some API so that those in need of it's data can access it easily.</p><p>There's not much more to be said (until we start evaluating different tools) so we'll move on to service registration.</p><div class="section" title="Service Registration"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec43"/>Service Registration</h2></div></div></div><p>Microservices tend to be very<a class="indexterm" id="id300"/> dynamic. They are created and destroyed, deployed to one server and then moved to another. They are always changing and evolving. Whenever there is any change in service properties, information about those changes needs to be stored in some database (we'll call it <span class="emphasis"><em>service registry</em></span> or simply <span class="emphasis"><em>registry</em></span>). The logic behind service registration is simple even though the implementation of that logic might become complicated. Whenever a service is deployed, its data (IP and port as a minimum) should be stored in the service registry. Things are a bit more complicated when a service is destroyed or stopped. If that is a result of a purposeful action, service data should be removed from the registry. However, there are cases when service is stopped due to a failure and in such a situation we might choose to do additional actions meant to restore the correct functioning of that service. We'll speak about such a situation in more details when we reach the self-healing chapter.</p><p>There are quite a few ways service registration can be performed.</p></div><div class="section" title="Self-Registration"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec44"/>Self-Registration</h2></div></div></div><p>
<span class="emphasis"><em>Self-registration</em></span> is a common way to register service information. When a service is deployed it notifies the registry about its existence and sends its data. Since each service needs to be capable of sending its <a class="indexterm" id="id301"/>data to the registry, this can be considered an anti-pattern. By using this approach, we are breaking <span class="emphasis"><em>single concern</em></span> and <span class="emphasis"><em>bounded context</em></span> principles that we are trying to enforce inside our microservices. We'd<a class="indexterm" id="id302"/> need to add the registration code to each service and, therefore, increase<a class="indexterm" id="id303"/> the development complexity. More importantly, that would couple services to a specific registry service. Once their number increases, modifying all of them to, for example, change the registry would be a very cumbersome work. Besides, that was one of the reasons we moved away from monolithic applications; freedom to modify any service without affecting the whole system. The alternative would be to create a library that would do that for us and include it in each service. However, this approach would severally limit our ability to create entirely self-sufficient microservices. We'd increase their dependency on external resources (in this case the registration library).</p><p>De-registration is, even more, problematic and can quickly become quite complicated with the self-registration concept. When a service is stopped purposely, it should be relatively easy to remove<a class="indexterm" id="id304"/> its data from the registry. However, services are not always stopped on purpose. They might fail in unexpected ways and the process they're running in might stop. In such a case it might be difficult (if not impossible) to always be able to de-register the service from itself:</p><div class="mediaobject"><img alt="Self-Registration" src="graphics/B05848_08_04.jpg"/><div class="caption"><p>Figure 8-4 – Self-registration</p></div></div><p>While self-registration might be common, it is not an optimum nor productive way to perform this type of operations. We should look at alternative approaches.</p></div><div class="section" title="Registration Service"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec45"/>Registration Service</h2></div></div></div><p>Registration service or third party registration is a process that manages registration and de-registration of all services. The service is in charge of checking which microservices are running and should update<a class="indexterm" id="id305"/> the registry accordingly. A similar process is applied when services are stopped. The registration service should detect the absence of a microservice and remove its data from the registry. As an additional function, it can notify some other process of the absence of the microservice that would, in turn, perform some corrective actions like re-deployment of the absent microservice, email notifications, and so on. We'll call this registration and de-registration process <span class="emphasis"><em>service registrator</em></span> or simply <span class="emphasis"><em>registrator</em></span> (actually, as you'll soon see, there is a product with the same name):</p><div class="mediaobject"><img alt="Registration Service" src="graphics/B05848_08_05.jpg"/><div class="caption"><p>Figure 8-5 – Registration service</p></div></div><p>A separate registration service is a much better option than self-registration. It tends to be more reliable and, at the<a class="indexterm" id="id306"/> same time, does not introduce unnecessary coupling inside our microservices code.</p><p>Since we established what will be the underlying logic behind the services registration process, it is time to discuss the discovery.</p></div><div class="section" title="Service Discovery"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec46"/>Service Discovery</h2></div></div></div><p>Service discovery is the <a class="indexterm" id="id307"/>opposite of service registration. When a client<a class="indexterm" id="id308"/> wants to access a service (the client might also be another service), it must know, as a minimum, where that service is. One approach we can take is self-discovery.</p><div class="section" title="Self-Discovery"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec10"/>Self-Discovery</h3></div></div></div><p>Self-discovery uses the same principles as self-registration. Every client or a service that wants to access other services <a class="indexterm" id="id309"/>would need to consult the registry. Unlike self-registration that posed problems mostly related to our internal ways to connect services, self-discovery might be used by clients and services outside our control. One example would be a front-end running in user browsers. That front-end might need to send requests to many separate back-end services running on different ports or even different IPs. The fact that we do have the information stored in the registry does not mean that others can, should, or know how to use it. Self-discovery can be effectively used only for the communication between internal services. Even such a limited scope poses a lot of additional problems many of which are the same as those created by self-registration. Due to what we know by now, this option should be discarded.</p></div><div class="section" title="Proxy Service"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec11"/>Proxy Service</h3></div></div></div><p>Proxy services have been <a class="indexterm" id="id310"/>around for a while and proved their worth many times over. The next chapter will explore them in more depth so we'll go through them only briefly. The idea is that each service should be accessible through one or more fixed addresses. For example, the list of books from our <code class="literal">books-ms</code> service should be available only through the <code class="literal">[DOMAIN]/api/v1/books</code> address. Notice that there is no IP, port nor any other deployment-specific detail. Since there will be no service with that exact address, something will have to detect such a request and redirect it to the IP and port of the actual service. Proxy services tend to be the best type of tools that can fulfill this task.</p><p>Now that we have a general, and hopefully clear, idea of what we're trying to accomplish, let's take a look at some of the tools that can help us out.</p></div></div><div class="section" title="Service Discovery Tools"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec47"/>Service Discovery Tools</h2></div></div></div><p>The primary objective of <span class="emphasis"><em>service discovery tools</em></span> is to help services find and talk to one another. To perform their duty, they need to know where each service is. The concept is not new, and many tools<a class="indexterm" id="id311"/> existed long before Docker was born. However, containers brought the need for such tools to a whole new level.</p><p>The basic idea behind <span class="emphasis"><em>service discovery</em></span> is for each new instance of a service (or application) to be able to identify its current environment and store that information. Storage itself is performed in a registry usually in key/value format. Since the discovery is often used in distributed system, registry needs to be scalable, fault-tolerant and distributed among all nodes in the cluster. The primary usage of such a storage is to provide, as a minimum, IP and port of a service to all interested parties that might need to communicate with it. This data is often extended with other types of information.</p><p>Discovery tools tend to provide some API that can be used by a service to register itself as well as by others to find the information about that service.</p><p>Let's say that we have two services. One is a provider, and the other one is its consumer. Once we deploy the provider, we need to store its information in the <span class="emphasis"><em>service registry</em></span> of choice. Later on, when the consumer tries to access the provider, it would first query the registry and call the provider using the IP and port obtained from the registry. To decouple the consumer from a particular implementation of the registry, we often employ some <span class="emphasis"><em>proxy service</em></span>. That way the consumer would always request information from the fixed address that would reside inside the proxy that, in turn, would use the discovery service to find out the provider information and redirect the request. Actually, in many cases, there is no need for the proxy to query the service registry if there is a process that updates its configuration every time data in the registry changes. We'll go through <span class="emphasis"><em>reverse proxy</em></span> later on in<a class="indexterm" id="id312"/> the book. For now, it is important to understand that the flow that is based on three actors; consumer, proxy, and provider.</p><p>What we are looking for in the service discovery tools is data. As a minimum, we should be able to find out where the service is, whether it is healthy and available, and what is its configuration. Since <a class="indexterm" id="id313"/>we are building a distributed system with multiple servers, the tool needs to be robust, and failure of one node should not jeopardize data. Also, each of the nodes should have the same data replica. Further on, we want to be able to start services in any order, be able to destroy them, or to replace them with newer versions. We should also be able to reconfigure our services and see the data change accordingly.</p><p>Let's take a look at a few of the tools we can use to accomplish the goals we set.</p></div><div class="section" title="Manual Configuration"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec48"/>Manual Configuration</h2></div></div></div><p>Most of the services are still managed manually. We decide in advance where to deploy the service, what is its configuration and hope beyond reason that it will continue working properly until the end<a class="indexterm" id="id314"/> of days. Such approach is not easily scalable. Deploying a second instance of the service means that we need to start the manual process all over. We have to bring up a new server or find out which one has low utilization of resources, create a new set of configurations and deploy it. The situation is even more complicated in the case of, let's say, a hardware failure since the reaction time is usually slow when things are managed manually. Visibility is another sore point. We know what the static configuration is. After all, we prepared it in advance. However, most of the services have a lot of information generated dynamically. That information is not easily visible. There is no single location we can consult when we are in need of that data.</p><p>Reaction time is inevitably slow, failure resilience questionable at best and monitoring difficult to manage due to a lot of manually handled moving parts.</p><p>While there was an excuse to do this job manually in the past or when the number of services and/or servers is small, with the emergence of service discovery tools, this excuse quickly evaporated.</p></div><div class="section" title="Zookeeper"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec49"/>Zookeeper</h2></div></div></div><p>Zookeeper is one of the <a class="indexterm" id="id315"/>oldest projects of this type. It originated out of the Hadoop world, where it was built to help the maintenance of various components in a Hadoop cluster. It is mature, reliable and used by <a class="indexterm" id="id316"/>many big companies (YouTube, eBay, Yahoo, and so on). The format of the data it stores is similar to the organization of the file system. If run on a server cluster, Zookeeper will share the state of the configuration across all of the nodes. Each cluster elects a leader and clients can connect to any of the servers to retrieve data.</p><p>The main advantages Zookeeper <a class="indexterm" id="id317"/>brings to the table is its maturity, robustness, and feature richness. However, it comes with its set of disadvantages, with Java and complexity being main culprits. While Java is great for many use cases, it is massive for this type of work. Zookeeper's usage<a class="indexterm" id="id318"/> of Java, together with a considerable number of dependencies, makes Zookeeper much more resource hungry that its competition. On top of those problems, Zookeeper is complex. Maintaining it requires considerably more knowledge than we should expect from an application of this type. That is the part where feature richness converts itself from an advantage to a liability. The more features an application has, the bigger the chances that we won't need all of them. Thus, we end up paying the price in the form of complexity for something we do not fully need.</p><p>Zookeeper paved the way that others followed with considerable improvements. "Big players" are using it because there were no better alternatives at the time. Today, Zookeeper shows its age, and we are better off with alternatives.</p><p>We'll skip Zookeeper examples and skip straight into better options.</p></div><div class="section" title="etcd"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec50"/>etcd</h2></div></div></div><p>etcd is a key/value store accessible through HTTP. It is distributed and features hierarchical configuration system<a class="indexterm" id="id319"/> that can be used to build service discovery. It is very easy to deploy, setup and use, provides reliable data persistence, it's secure <a class="indexterm" id="id320"/>and with excellent documentation.</p><p>etcd is a better option than Zookeeper due to its simplicity. However, it needs to be combined with a few third-party tools before it can serve service discovery objectives.</p><div class="section" title="Setting Up etcd"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec12"/>Setting Up etcd</h3></div></div></div><p>Let us set up the <span class="emphasis"><em>etcd</em></span>. First, we<a class="indexterm" id="id321"/> should create the first node in the cluster (<span class="emphasis"><em>serv-disc-01</em></span>) together with the, already familiar, <span class="emphasis"><em>cd</em></span> VM.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>vagrant up cd serv-disc-01 --provision</strong></span>
<span class="strong"><strong>vagrant ssh serv-disc-01</strong></span>
</pre></div><p>With the cluster node <span class="emphasis"><em>serv-disc-01</em></span> up and running, we can install <code class="literal">etcd</code> and <code class="literal">etcdctl</code> (etcd command line client).</p><div class="informalexample"><pre class="programlisting">curl -L https://github.com/coreos/etcd/releases/\
download/v2.1.2/etcd-v2.1.2-linux-amd64.tar.gz \
    -o etcd-v2.1.2-linux-amd64.tar.gz
tar xzf etcd-v2.1.2-linux-amd64.tar.gz
sudo mv etcd-v2.1.2-linux-amd64/etcd* /usr/local/bin
rm -rf etcd-v2.1.2-linux-amd64*
etcd &gt;/tmp/etcd.log 2&gt;&amp;1 &amp;</pre></div><p>We downloaded, uncompressed <a class="indexterm" id="id322"/>and moved the executables to <code class="literal">/usr/local/bin</code> so that they are easily accessible. Then, we removed unneeded files and, finally, run the <code class="literal">etcd</code> with output redirected to <code class="literal">/tmp/etcd.log</code>.</p><p>Let's see what we can do with etcd.</p><p>Basic operations are <code class="literal">set</code> and <code class="literal">get</code>. Please note that we can set a key/value inside a directory:</p><div class="informalexample"><pre class="programlisting">etcdctl set myService/port "1234"
etcdctl set myService/ip "1.2.3.4"
etcdctl get myService/port # Outputs: 1234
etcdctl get myService/ip # Outputs: 1.2.3.4</pre></div><p>The first command put the key <code class="literal">port</code> with the value <code class="literal">1234</code> into the directory <code class="literal">myService</code>. The second did the same with the key <code class="literal">ip</code>, and the last two commands were used to output values of those two keys.</p><p>We can also list all the keys in the specified directory or delete a key with its value:</p><div class="informalexample"><pre class="programlisting">etcdctl ls myService
etcdctl rm myService/port
etcdctl ls myService</pre></div><p>The last command output only the <code class="literal">/myService/ip</code> value since previous command removed the port.</p><p>Besides <code class="literal">etcdctl</code>, we can also run all commands through HTTP API. Before we try it out, let's install <code class="literal">jq</code> so that we can see the formatted output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install -y jq</strong></span>
</pre></div><p>We can, for example, put a value into <code class="literal">etcd</code> through its HTTP API and retrieve it through a <code class="literal">GET</code> request.</p><div class="informalexample"><pre class="programlisting">curl http://localhost:2379/v2/keys/myService/newPort \
  -X PUT \
  -d value="4321" | jq '.'
curl http://localhost:2379/v2/keys/myService/newPort \
  | jq '.'</pre></div><p>The <code class="literal">jq '.'</code> is not required, but I tend to use it often to format JSON. The output should be similar to the following:</p><div class="informalexample"><pre class="programlisting">{
    "action": "set",
    "node": {
        "createdIndex": 16,
        "key": "/myService/newPort",
        "modifiedIndex": 16,
        "value": "4321"
    }
}
{
    "action": "get",
    "node": {
        "createdIndex": 16,
        "key": "/myService/newPort",
        "modifiedIndex": 16,
        "value": "4321"
    }
}</pre></div><p>HTTP API is especially useful when we need to query etcd remotely. In most, I prefer the <code class="literal">etcdctl</code>, when running ad-hoc commands while HTTP is a preferred way to interact with <code class="literal">etcd</code> through some code.</p><p>Now that we've seen (briefly) how etcd works on a single server, let us try it inside a cluster. The cluster setup<a class="indexterm" id="id323"/> requires a few additional arguments to be passed to <code class="literal">etcd</code>. Let's say that we'll have a cluster of three nodes with IPs <code class="literal">10.100.197.201</code> (<code class="literal">serv-disc-01</code>), <code class="literal">10.100.197.202</code> (<code class="literal">serv-disc-02</code>) and <code class="literal">10.100.197.203</code> (<code class="literal">serv-disc-03</code>). The etcd command that should be run on the first server would be the following (please don't run it yet):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NODE_NAME=serv-disc-0$NODE_NUMBER</strong></span>
<span class="strong"><strong>NODE_IP=10.100.197.20$NODE_NUMBER</strong></span>
<span class="strong"><strong>NODE_01_ADDRESS=http://10.100.197.201:2380</strong></span>
<span class="strong"><strong>NODE_01_NAME=serv-disc-01</strong></span>
<span class="strong"><strong>NODE_01="$NODE_01_NAME=$NODE_01_ADDRESS"</strong></span>
<span class="strong"><strong>NODE_02_ADDRESS=http://10.100.197.202:2380</strong></span>
<span class="strong"><strong>NODE_02_NAME=serv-disc-02</strong></span>
<span class="strong"><strong>NODE_02="$NODE_02_NAME=$NODE_02_ADDRESS"</strong></span>
<span class="strong"><strong>NODE_03_ADDRESS=http://10.100.197.203:2380</strong></span>
<span class="strong"><strong>NODE_03_NAME=serv-disc-03</strong></span>
<span class="strong"><strong>NODE_03="$NODE_03_NAME=$NODE_03_ADDRESS"</strong></span>
<span class="strong"><strong>CLUSTER_TOKEN=serv-disc-cluster</strong></span>
<span class="strong"><strong>etcd -name serv-disc-1 \</strong></span>
<span class="strong"><strong>    -initial-advertise-peer-urls http://$NODE_IP:2380 \</strong></span>
<span class="strong"><strong>    -listen-peer-urls http://$NODE_IP:2380 \</strong></span>
<span class="strong"><strong>    -listen-client-urls \</strong></span>
<span class="strong"><strong>    http://$NODE_IP:2379,http://127.0.0.1:2379 \</strong></span>
<span class="strong"><strong>    -advertise-client-urls http://$NODE_IP:2379 \</strong></span>
<span class="strong"><strong>    -initial-cluster-token $CLUSTER_TOKEN \</strong></span>
<span class="strong"><strong>    -initial-cluster \</strong></span>
<span class="strong"><strong>    $NODE_01,$NODE_02,$NODE_03 \</strong></span>
<span class="strong"><strong>    -initial-cluster-state new</strong></span>
</pre></div><p>I extracted parts that would change from one server (or a cluster) to another into variables so that you can see them clearly. We won't go into details of what each argument means. You can find more<a class="indexterm" id="id324"/> information in the <a class="ulink" href="https://coreos.com/etcd/docs/latest/clustering.html">https://coreos.com/etcd/docs/latest/clustering.html</a>. Suffice to say that we specified the IP and the name<a class="indexterm" id="id325"/> of the server where this command should run as well as the list of all the servers in the cluster.</p><p>Before we start working on the <code class="literal">etcd</code> deployment to the cluster, let us kill the currently running instance and create the rest of servers (there should be three in total):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pkill etcd</strong></span>
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant up serv-disc-02 serv-disc-03</strong></span>
</pre></div><p>Doing the same set of tasks manually across multiple servers is tedious and error prone. Since we already worked with Ansible, we can use it to set up etcd across the cluster. This should be a fairly easy task since we already have all the commands, and all we have to do is translate those we already run into the Ansible format. We can create the <code class="literal">etcd</code> role and add it to the playbook with the same name. The role is fairly simple. It copies the executables to the <code class="literal">/usr/local/bin</code> directory and runs etcd with the cluster arguments (the very long command we examined above). Let us take a look at it before running the playbook.</p><p>The first task in the <code class="literal">roles/etcd/tasks/main.yml</code> is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Files are copied</strong></span>
<span class="strong"><strong>  copy:</strong></span>
<span class="strong"><strong>    src: "{{ item.src }}"</strong></span>
<span class="strong"><strong>    dest: "{{ item.dest }}"</strong></span>
<span class="strong"><strong>    mode: 0755</strong></span>
<span class="strong"><strong>  with_items: files</strong></span>
<span class="strong"><strong>  tags: [etcd]</strong></span>
</pre></div><p>The name is purely descriptive and followed with the copy module. Then, we are specifying few of the module options. The copy option <code class="literal">src</code> indicates the name of the local file we want to copy and is relative to the <code class="literal">files</code> directory inside the role. The second copy option (<code class="literal">dest</code>) is the destination path on the remote server. Finally, we are setting the mode to be <code class="literal">755</code>. The user that runs with roles will have <code class="literal">read/write/execute</code> permissions, and those belonging to the same group and everyone else will be assigned <code class="literal">read/execute</code> permissions. Next is the <code class="literal">with_items</code> declaration that allows us to use a list of values. In this case, the values are specified in the <code class="literal">roles/etcd/defaults/main.yml</code> file and are as follows:</p><div class="informalexample"><pre class="programlisting">files: [
  {src: 'etcd', dest: '/usr/local/bin/etcd'},
  {src: 'etcdctl', dest: '/usr/local/bin/etcdctl'}
]</pre></div><p>Externalizing variables is a good way to keep things that might change in the future separated from the tasks. If, for example, we are to copy another file through this role, we'd add it here and avoid even opening the tasks file. The task that uses the <code class="literal">files</code> variable will iterate for each <a class="indexterm" id="id326"/>value in the list and, in this case, run twice; once for <code class="literal">etcd</code> and the second time for <code class="literal">etcdctl</code>. Values from variables are represented with the variable key surrounded with <code class="literal">{{</code> and <code class="literal">}}</code> and use the Jinja2 format. Finally, we set <code class="literal">etcd</code> to be the tag associated with this task. Tags can be used to filter tasks when running playbooks and are very handy when we want to run only a subset of them or when we want to exclude something.</p><p>The second task is as follows:</p><div class="informalexample"><pre class="programlisting">- name: Is running
  shell: "nohup etcd -name {{ ansible_hostname }} \
    -initial-advertise-peer-urls \
    http://{{ ip }}:2380 \
    -listen-peer-urls \
    http://{{ ip }}:2380 \
    -listen-client-urls \
    http://{{ ip }}:2379,http://127.0.0.1:2379 \
    -advertise-client-urls \
    http://{{ ip }}:2379 \
    -initial-cluster-token {{ cl_token }} \
    -initial-cluster \
    {{ cl_node_01 }},{{ cl_node_02 }},{{ cl_node_03 }} \
    -initial-cluster-state new \
    &gt;/var/log/etcd.log 2&gt;&amp;1 &amp;"
  tags: [etcd]</pre></div><p>Shell module is often the last resort since it does not work with states. In most cases, commands run as through shell will not check whether something is in the correct state or not and run every time we execute Ansible playbook. However, etcd always runs only a single instance and there is no risk that multiple executions of this command will produce multiple instances. we have a lot of arguments and all those that might change are put as variables. Some of them, like ansible_hostname, are discovered by Ansible. Others were defined by us and placed in the <code class="literal">roles/etcd/defaults/main.yml</code>. With all the tasks defined, we can take a look at the playbook <code class="literal">etcd.yml</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: etcd</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - etcd</strong></span>
</pre></div><p>When this playbook is run, Ansible will configure all the servers defined in an inventory, use <code class="literal">vagrant</code> as the remote user, run commands as <code class="literal">sudo</code> and execute the <code class="literal">common</code> and <code class="literal">etcd</code> roles.</p><p>Let us take a look at the <code class="literal">hosts/serv-disc</code> file. It is our inventory that contains the list of all hosts we're using:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[etcd]</strong></span>
<span class="strong"><strong>10.100.194.20[1:3]</strong></span>
</pre></div><p>In this example, you can a different way to define hosts. The second line is Ansible's way of saying that all addresses <a class="indexterm" id="id327"/>between <code class="literal">10.100.194.201</code> and <code class="literal">10.100.194.203</code> should be used. In total, we have three IPs specified for this purpose.</p><p>Let's run the <code class="literal">etcd</code> playbook and see it in action:</p><div class="informalexample"><pre class="programlisting">vagrant ssh cd
ansible-playbook \/vagrant/ansible/etcd.yml \
    -i /vagrant/ansible/hosts/serv-disc</pre></div><p>We can check whether etcd cluster was correctly set by putting a value through one server and getting it from the another:</p><div class="informalexample"><pre class="programlisting">curl http://serv-disc-01:2379/v2/keys/test \
  -X PUT \
  -d value="works" | jq '.'
curl http://serv-disc-03:2379/v2/keys/test \
  | jq '.'</pre></div><p>The output of those commands should be similar to the following:</p><div class="informalexample"><pre class="programlisting">{
    "action": "set",
    "node": {
        "createdIndex": 8,
        "key": "/test",
        "modifiedIndex": 8,
        "value": "works"
    }
}
{
    "action": "get",
    "node": {
        "createdIndex": 8,
        "key": "/test",
        "modifiedIndex": 8,
        "value": "works"
    }
}</pre></div><p>We sent the HTTP PUT request to the <code class="literal">serv-disc-01</code> server (<code class="literal">10.100.197.201</code>) and retrieved the stored value<a class="indexterm" id="id328"/> through the HTTP GET request from the <code class="literal">serv-disc-03</code> (<code class="literal">10.100.197.203</code>) node. In other words, data set through any of the servers in the cluster is available in all of them. Isn't that neat?</p><p>Our cluster (after we deploy few containers), would look as presented in the Figure 8-6.</p><div class="mediaobject"><img alt="Setting Up etcd" src="graphics/B05848_08_06.jpg"/><div class="caption"><p>Figure 8-6 – Multiple nodes with Docker containers and etcd</p></div></div><p>Now that we have a place to store the information related to our services, we need a tool that will send that information to etcd automatically. After all, why would we put data to etcd manually if that can be done automatically? Even if we would want to put the information manually to etcd, we often don't know what that information is. Remember, services might be deployed to a server with least containers running and it might have a random port assigned. Ideally, that tool should monitor Docker on all nodes and update etcd whenever a new container is run, or an existing one is stopped. One of the tools that can help us with this goal is <span class="emphasis"><em>Registrator</em></span>.</p></div><div class="section" title="Setting Up Registrator"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec13"/>Setting Up Registrator</h3></div></div></div><p>Registrator automatically registers and <a class="indexterm" id="id329"/>deregisters services by inspecting<a class="indexterm" id="id330"/> containers as they are brought online or stopped. It currently supports <span class="strong"><strong>etcd</strong></span>, <span class="strong"><strong>Consul</strong></span> and <a class="indexterm" id="id331"/>
<span class="strong"><strong>SkyDNS 2</strong></span>.</p><p>Setting up Registrator with etcd<a class="indexterm" id="id332"/> registry is easy. We can simply run the Docker container as follows (please do not run it yourself):</p><div class="informalexample"><pre class="programlisting">docker run -d --name registrator \
    -v /var/run/docker.sock:/tmp/docker.sock \
    -h serv-disc-01 \
    gliderlabs/registrator \
    -ip 10.100.194.201 etcd://10.100.194.201:2379</pre></div><p>With this command we are sharing <code class="literal">/var/run/docker.sock</code> as Docker volume. Registrator will monitor and intercept Docker events and, depending on the event type, put or remove service information to/from etcd. With the <code class="literal">-h</code> argument we are specifying the hostname. Finally, we are passing two arguments to Registrator. The first one is the <code class="literal">-ip</code> and represents the IP of the host and the second one is the protocol (<code class="literal">etcd</code>), the IP (<code class="literal">serv-disc-01</code>) and the port (<code class="literal">2379</code>) of the registration service.</p><p>Before we proceed, let's create a new Ansible role called <span class="emphasis"><em>registrator</em></span> and deploy it to all nodes inside the cluster. The <code class="literal">roles/registrator/tasks/main.yml</code> file is as follows.</p><div class="informalexample"><pre class="programlisting">- name: Container is running
  docker:
    name: "{{ registrator_name }}"
    image: gliderlabs/registrator
    volumes:
      - /var/run/docker.sock:/tmp/docker.sock
    hostname: "{{ ansible_hostname }}"
    command: -ip {{ facter_ipaddress_eth1 }} {{ registrator_protocol }}://{{ facter_ipaddress_eth1 }}:2379
  tags: [etcd]</pre></div><p>This Ansible role is equivalent<a class="indexterm" id="id333"/> to the manual command we saw earlier. Please note that we changed the hard-coded <code class="literal">etcd</code> protocol with a variable. That way we can reuse this role with other registries as well. Keep in mind that having quotes is not mandatory in Ansible except when value starts with <code class="literal">{{</code> as in the case of the <code class="literal">hos</code>
<code class="literal">tname</code> value.</p><p>Let's take a look at the <code class="literal">registrator-etcd.yml</code> playbook.</p><div class="informalexample"><pre class="programlisting">- hosts: all
  remote_user: vagrant
  serial: 1
  sudo: yes
  vars:
    - registrator_protocol: etcd
    - registrator_port: 2379
  roles:
    - common
    - docker
    - etcd
    - registrator</pre></div><p>Most of the playbook is similar to those we used before except the <code class="literal">vars</code> key. In this case, we're using it to define the Registrator protocol as <code class="literal">etcd</code> and port of the registry as <code class="literal">2379</code>.</p><p>With everything in place, we can run the playbook.</p><div class="informalexample"><pre class="programlisting">ansible-playbook \
    /vagrant/ansible/registrator-etcd.yml \
    -i /vagrant/ansible/hosts/serv-disc</pre></div><p>Once the playbook is finished executing, Registrator will be running on all three nodes of our cluster.</p><p>Let's give Registrator a spin and run one container inside one of the three cluster nodes:</p><div class="informalexample"><pre class="programlisting">export DOCKER_HOST=tcp://serv-disc-02:2375
docker run -d --name nginx \
    --env SERVICE_NAME=nginx \
    --env SERVICE_ID=nginx \
    -p 1234:80 \
    nginx</pre></div><p>We exported the <code class="literal">DOCKER_HOST</code> variable so that Docker commands are sent to the cluster node 2 (<code class="literal">serv-disc-02</code>) and<a class="indexterm" id="id334"/> run the <code class="literal">nginx</code> container exposing port <code class="literal">1234</code>. We'll use <code class="literal">nginx</code> later on, and there will be plenty of opportunities to get familiar with it. For now, we are not interested in what nginx does, but that Registrator detected it and stored the information in etcd. In this case, we put a few environment variables (<code class="literal">SERVICE_NAME</code> and <code class="literal">SERVICE_ID</code>) that Registrator can use to identify better the service.</p><p>Let us take a look at Registrator's log.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker logs registrator</strong></span>
</pre></div><p>The output should be similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>2015/08/30 19:18:12 added: 5cf7dd974939 nginx</strong></span>
<span class="strong"><strong>2015/08/30 19:18:12 ignored: 5cf7dd974939 port 443 not published on host</strong></span>
</pre></div><p>We can see that Registrator detected nginx container with the ID <code class="literal">5cf7dd974939</code>. We can also see that it ignored the port <code class="literal">443</code>. The <code class="literal">nginx</code> container internally exposes ports 80 and <code class="literal">443</code>. However, we exposed only <code class="literal">80</code> to the outside world, so Registrator decided to ignore the port <code class="literal">443</code>. After all, why would we store the information about the port not accessible to anyone?</p><p>Now, let us take a look at data stored in etcd:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://serv-disc-01:2379/v2/keys/ | jq '.'</strong></span>
<span class="strong"><strong>curl http://serv-disc-01:2379/v2/keys/nginx-80/ | jq '.'</strong></span>
<span class="strong"><strong>curl http://serv-disc-01:2379/v2/keys/nginx-80/nginx | jq '.'</strong></span>
</pre></div><p>The output of the last command is as follows:</p><div class="informalexample"><pre class="programlisting">{
  "node": {
    "createdIndex": 13,
    "modifiedIndex": 13,
    "value": "10.100.194.202:1234",
    "key": "/nginx-80/nginx"
  },
  "action": "get"
}</pre></div><p>The first command listed all keys at the root, the second listed all those inside <code class="literal">nginx-80</code> and the last one retrieved the final value. Registrator stored values in the format <code class="literal">/</code> that matches environment variables we used when running the container. Please note that in case more that one port is defined for a service, Registrator adds it as a suffix (e.g. nginx-<code class="literal">80</code>). The value that Registrator <a class="indexterm" id="id335"/>put corresponds with the IP of the host where the container is running and the port that we exposed.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>Please note that even though the container is run on the node 2, we queried etcd running on the node 1. It was yet another demonstration that data is replicated across all nodes etcd is running on.</p></div></div><p>What happens when we remove the container?</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker rm -f nginx</strong></span>
<span class="strong"><strong>docker logs registrator</strong></span>
</pre></div><p>The output of Registrator logs should be similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>2015/08/30 19:32:31 removed: 5cf7dd974939 nginx</strong></span>
</pre></div><p>Registrator detected that we removed the container and sent a request to etcd to remove corresponding values. We can confirm that with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://serv-disc-01:2379/v2/keys/nginx-80/nginx | jq '.'</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">{
  "index": 14,
  "cause": "/nginx-80/nginx",
  "message": "Key not found",
  "errorCode": 100
}</pre></div><p>The service with the ID <code class="literal">nginx/nginx</code> disappeared.</p><p>Registrator combined with <code class="literal">etcd</code> is a powerful, yet simple, combination that will allow us to practice many advanced techniques. Whenever we bring up a container, data will be stored in etcd and propagated to all nodes in the cluster. What we'll do with that information will be the subject of the next chapter.</p><div class="mediaobject"><img alt="Setting Up Registrator" src="graphics/B05848_08_07.jpg"/><div class="caption"><p>Figure 8-7 – Multiple nodes with Docker containers, etcd and Registrator</p></div></div><p>There is one more piece of the<a class="indexterm" id="id336"/> puzzle missing. We need a way to create configuration files with data stored in <code class="literal">etcd</code> as well as run some commands when those files are created.</p></div><div class="section" title="Setting Up confd"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec14"/>Setting Up confd</h3></div></div></div><p>The confd is a lightweight tool that can be used to maintain configuration files. The most common usage of the tool is<a class="indexterm" id="id337"/> keeping configuration files up-to-date using data stored in <code class="literal">etcd</code>, <code class="literal">consul</code>, and few other data registries. It can also be used to reload applications when configuration files change. In other words, we can use it as a way to reconfigure services with the information stored in etcd (or few other registries).</p><p>Installing <code class="literal">confd</code> is straightforward. The commands are as follows (please don't run them yet):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://github.com/kelseyhightower/confd/releases\/download/v0.10.0/confd-0.10.0-linux-amd64</strong></span>
<span class="strong"><strong>sudo mv confd-0.10.0-linux-amd64 /usr/local/bin/confd</strong></span>
<span class="strong"><strong>sudo chmod 755 /usr/local/bin/confd</strong></span>
<span class="strong"><strong>sudo mkdir -p /etc/confd/{conf.d,templates}</strong></span>
</pre></div><p>In order for <code class="literal">confd</code> to work, we need a configuration file located in the <code class="literal">/etc/confd/conf.d/</code> directory and a template in the <code class="literal">/etc/confd/templates</code>.</p><p>Example configuration file is as follows:</p><div class="informalexample"><pre class="programlisting">[template]
src = "nginx.conf.tmpl"
dest = "/tmp/nginx.conf"
keys = [
    "/nginx/nginx"
]</pre></div><p>As a minimum, we need to specify template source, destination file, and keys that will be fetched from the registry.</p><p>Templates use GoLang text templates format. An example template is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>The address is {{getv "/nginx/nginx"}};</strong></span>
</pre></div><p>When this template is<a class="indexterm" id="id338"/> processed, it will substitute <code class="literal">{{getv "/nginx/nginx"}}</code> with the value from the registry.</p><p>Finally, <code class="literal">confd</code> can be run in two modes. In the Daemon mode, it polls a registry and updates destination configuration whenever relevant values change. The <code class="literal">onetime</code> mode is run once. An example of the <code class="literal">onetime</code> mode is as follows (please do not run it yet):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>confd -onetime -backend etcd -node 10.100.197.202:2379</strong></span>
</pre></div><p>This command would run in the <code class="literal">onetime</code> mode, would use <code class="literal">etcd</code> as the backend running on the specified node. When executed, destination configuration would be updated with values from the <code class="literal">etcd</code> registry.</p><p>Now that we know basics of how confd works, let's take a look at the Ansible role <code class="literal">confd</code> that will make sure that it is installed on all servers in the cluster.</p><p>The <code class="literal">roles/confd/tasks/main.yml</code> file is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Directories are created</strong></span>
<span class="strong"><strong>  file:</strong></span>
<span class="strong"><strong>    path: "{{ item }}"</strong></span>
<span class="strong"><strong>    state: directory</strong></span>
<span class="strong"><strong>  with_items: directories</strong></span>
<span class="strong"><strong>  tags: [confd]</strong></span>
<span class="strong"><strong>- name: Files are copied</strong></span>
<span class="strong"><strong>  copy:</strong></span>
<span class="strong"><strong>    src: "{{ item.src }}"</strong></span>
<span class="strong"><strong>    dest: "{{ item.dest }}"</strong></span>
<span class="strong"><strong>    mode: "{{ item.mode }}"</strong></span>
<span class="strong"><strong>  with_items: files</strong></span>
<span class="strong"><strong>  tags: [confd]</strong></span>
</pre></div><p>This Ansible role is even simpler than the one we created for <code class="literal">etcd</code> since we are not even running the binary. It makes sure that directories are created and that files are copied to the destination servers. Since there are multiple directories and files involved, we defined them as variables in the <code class="literal">roles/confd/defaults/main.yml</code> file:</p><div class="informalexample"><pre class="programlisting">directories:
  - /etc/confd/conf.d
  - /etc/confd/templates

files: [
  { src: 'example.toml', dest: '/etc/confd/conf.d/example.toml', mode: '0644' },
  { src: 'example.conf.tmpl', dest: '/etc/confd/templates/example.conf.tmpl', mode: '0644' },
  { src: 'confd', dest: '/usr/local/bin/confd', mode: '0755' }
]</pre></div><p>We defined directories where <a class="indexterm" id="id339"/>we'll put configurations and templates. We also defined files that need to be copied; one binary, one configuration, and one template file that we'll use to try out confd.</p><p>Finally, we need <code class="literal">confd.yml</code> file that will act as the Ansible playbook:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: confd</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - confd</strong></span>
</pre></div><p>There's nothing new to discuss since this file is almost the same the other playbooks we worked with.</p><p>With everything set up, we can deploy confd to all the cluster servers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook \</strong></span>
<span class="strong"><strong>    /vagrant/ansible/confd.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/serv-disc</strong></span>
</pre></div><p>With <code class="literal">confd</code> installed on all nodes in the cluster, we can try it out.</p><p>Let's run the nginx container again so that Registrator can put some data to etcd:</p><div class="informalexample"><pre class="programlisting">export DOCKER_HOST=tcp://serv-disc-01:2375

docker run -d --name nginx \
    --env SERVICE_NAME=nginx \
    --env SERVICE_ID=nginx \
    -p 4321:80 \
    Nginx
confd -onetime -backend etcd -node 10.100.194.203:2379</pre></div><p>We run the nginx container on the <code class="literal">serv-disc-01</code> node and exposed the port <code class="literal">4321</code>. Since <span class="emphasis"><em>Registrator</em></span> is already running on that server, it put data to <code class="literal">etcd</code>. Finally, we run the local instance of <code class="literal">confd</code> that checked all its configuration files and compared keys with those stored in etcd. Since <code class="literal">nginx/nginx</code> key has been changed in etcd, it processed the template and updated the destination config. That can be seen from the output that should be similar to the following (timestamp has been removed for brevity):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd confd[15241]: INFO Backend set to etcd</strong></span>
<span class="strong"><strong>cd confd[15241]: INFO Starting confd</strong></span>
<span class="strong"><strong>cd confd[15241]: INFO Backend nodes set to 10.100.194.203:2379</strong></span>
<span class="strong"><strong>cd confd[15241]: INFO Target config /tmp/example.conf out of sync</strong></span>
<span class="strong"><strong>cd confd[15241]: INFO Target config /tmp/example.conf has been updated</strong></span>
</pre></div><p>It found that the <code class="literal">/tmp/example.conf</code> is out of sync and updated it. Let us confirm that:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat /tmp/example.conf</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>The address is 10.100.194.201:4321</strong></span>
</pre></div><p>If any of the changes in<a class="indexterm" id="id340"/> templates or <code class="literal">etcd</code> data is updated, running <code class="literal">confd</code> will make sure that all destination configurations are updated accordingly:</p><div class="mediaobject"><img alt="Setting Up confd" src="graphics/B05848_08_08.jpg"/><div class="caption"><p>Figure 8-8 – Multiple nodes with Docker containers, etcd, Registrator and confd</p></div></div></div><div class="section" title="Combining etcd, Registrator, and confd"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec15"/>Combining etcd, Registrator, and confd</h3></div></div></div><p>When etcd, Registrator, and confd are combined, we get a simple yet powerful way to automate all our service<a class="indexterm" id="id341"/> discovery and configuration needs. That will come in handy when we start working on more advanced deployment strategies. The combination also demonstrates the effectiveness of having the right mix of small tools. Those three do what we need them to do. Less than this and we would not be able to accomplish the goals set in front of us. If, on the other hand, they were designed with bigger scope in mind, we would introduce unnecessary complexity and overhead on server resources and maintenance.</p><p>Before we make the final verdict, let's take a look at another combination of tools with similar goals. After all, we should never settle for some solution without investigating alternatives.</p></div></div><div class="section" title="Consul"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec51"/>Consul</h2></div></div></div><p>Consul is strongly consistent datastore that uses gossip to form dynamic clusters. It features hierarchical <a class="indexterm" id="id342"/>key/value store that can be used not only to store data but also to register watches that can be used for a variety of tasks, from sending notifications about data changes, to running health checks and custom commands depending on their output.</p><p>Unlike Zookeeper <a class="indexterm" id="id343"/>and etcd, Consul implements service discovery system embedded, so there is no need to build your own or use a third-party one. This discovery includes, among other things, health checks of nodes and services running on top of them.</p><p>ZooKeeper and etcd provide only a primitive K/V store and require that application developers build their own system to provide service discovery. Consul, on the other hand, provides a built-in framework for service discovery. Clients only need to register services and perform discovery using the DNS or HTTP interface. The other two tools require either a hand-made solution or the usage of third-party tools.</p><p>Consul offers out of the box native support for multiple data centers and the gossip system that works not <a class="indexterm" id="id344"/>only with nodes in the same cluster but across data centers as well.</p><p>Consul has another nice feature that distinguishes it from the others. Not only that it can be used to discover information about deployed services and nodes they reside on, but it also provides easy to extend health checks through HTTP and TCP requests, TTLs (time-to-live), custom scripts and even Docker commands.</p><div class="section" title="Setting Up Consul"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec16"/>Setting Up Consul</h3></div></div></div><p>As before, we'll start by<a class="indexterm" id="id345"/> exploring manual installation commands and, later on, automate them with Ansible. We'll configure it on the <code class="literal">cd</code> node as an exercise:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install -y unzip</strong></span>
<span class="strong"><strong>wget https://releases.hashicorp.com/consul/0.6.4/consul_0.6.4_linux_amd64.zip</strong></span>
<span class="strong"><strong>unzip consul_0.6.4_linux_amd64.zip</strong></span>
<span class="strong"><strong>sudo mv consul /usr/local/bin/consul</strong></span>
<span class="strong"><strong>rm -f consul_0.6.4_linux_amd64.zip</strong></span>
<span class="strong"><strong>sudo mkdir -p /data/consul/{data,config,ui}</strong></span>
</pre></div><p>We started by installing <code class="literal">unzip</code> since it is not included in default Ubuntu distribution. Then we downloaded the Consul ZIP, unpacked it, moved it to the <code class="literal">/usr/local/bin</code> directory, removed the ZIP file since we won't need it anymore and, finally, created few directories. Consul will place its information to the <code class="literal">data</code> directory and configuration files into <code class="literal">config</code>.</p><p>Next we can run <code class="literal">consul</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo consul agent \</strong></span>
<span class="strong"><strong>    -server \</strong></span>
<span class="strong"><strong>    -bootstrap-expect 1 \</strong></span>
<span class="strong"><strong>    -data-dir /data/consul/data \</strong></span>
<span class="strong"><strong>    -config-dir /data/consul/config \</strong></span>
<span class="strong"><strong>    -node=cd \</strong></span>
<span class="strong"><strong>    -bind=10.100.198.200 \</strong></span>
<span class="strong"><strong>    -client=0.0.0.0 \</strong></span>
<span class="strong"><strong>    -ui \</strong></span>
<span class="strong"><strong>    &gt;/tmp/consul.log &amp;</strong></span>
</pre></div><p>Running Consul was very straight forward. We specified that it should run the <code class="literal">agent</code> as a <code class="literal">server</code> and that there<a class="indexterm" id="id346"/> will be only one server instance <code class="literal">(-bootstrap-expect 1</code>). That is followed by locations of key directories; <code class="literal">ui</code>, <code class="literal">data</code> and <code class="literal">config</code>. Then we specified the name of the <code class="literal">node</code>, address it will <code class="literal">bind</code> to and which <code class="literal">client</code> can connect to it (<code class="literal">0.0.0.0</code> refers to all). Finally, we redirected the output and made sure that it's running in the background (<code class="literal">&amp;</code>).</p><p>Let's verify that Consul started correctly.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cat /tmp/consul.log</strong></span>
</pre></div><p>The output of the log file should be similar to the following (timestamps are removed for brevity).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>==&gt; Starting Consul agent...</strong></span>
<span class="strong"><strong>==&gt; Starting Consul agent RPC...</strong></span>
<span class="strong"><strong>==&gt; Consul agent running!</strong></span>
<span class="strong"><strong>         Node name: 'cd'</strong></span>
<span class="strong"><strong>        Datacenter: 'dc1'</strong></span>
<span class="strong"><strong>            Server: true (bootstrap: true)</strong></span>
<span class="strong"><strong>       Client Addr: 0.0.0.0 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)</strong></span>
<span class="strong"><strong>      Cluster Addr: 10.100.198.200 (LAN: 8301, WAN: 8302)</strong></span>
<span class="strong"><strong>    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false</strong></span>
<span class="strong"><strong>             Atlas: &lt;disabled&gt;</strong></span>
<span class="strong"><strong>==&gt; Log data will now stream in as it occurs:</strong></span>
<span class="strong"><strong>[INFO] serf: EventMemberJoin: cd 10.100.198.200</strong></span>
<span class="strong"><strong>[INFO] serf: EventMemberJoin: cd.dc1 10.100.198.200</strong></span>
<span class="strong"><strong>[INFO] raft: Node at 10.100.198.200:8300 [Follower] entering Follower state</strong></span>
<span class="strong"><strong>[WARN] serf: Failed to re-join any previously known node</strong></span>
<span class="strong"><strong>[INFO] consul: adding LAN server cd (Addr: 10.100.198.200:8300) (DC: dc1)</strong></span>
<span class="strong"><strong>[WARN] serf: Failed to re-join any previously known node</strong></span>
<span class="strong"><strong>[INFO] consul: adding WAN server cd.dc1 (Addr: 10.100.198.200:8300) (DC: dc1)</strong></span>
<span class="strong"><strong>[ERR] agent: failed to sync remote state: No cluster leader</strong></span>
<span class="strong"><strong>[WARN] raft: Heartbeat timeout reached, starting election</strong></span>
<span class="strong"><strong>[INFO] raft: Node at 10.100.198.200:8300 [Candidate] entering Candidate state</strong></span>
<span class="strong"><strong>[INFO] raft: Election won. Tally: 1</strong></span>
<span class="strong"><strong>[INFO] raft: Node at 10.100.198.200:8300 [Leader] entering Leader state</strong></span>
<span class="strong"><strong>[INFO] consul: cluster leadership acquired</strong></span>
<span class="strong"><strong>[INFO] consul: New leader elected: cd</strong></span>
<span class="strong"><strong>[INFO] raft: Disabling EnableSingleNode (bootstrap)</strong></span>
</pre></div><p>We can see that the <a class="indexterm" id="id347"/>Consul agent we run in server mode elected itself as the leader (which is to be expected since it's the only one).</p><p>With Consul up and running, let's see how we can put some data into it.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -X PUT -d 'this is a test' \</strong></span>
<span class="strong"><strong>    http://localhost:8500/v1/kv/msg1</strong></span>
<span class="strong"><strong>curl -X PUT -d 'this is another test' \</strong></span>
<span class="strong"><strong>    http://localhost:8500/v1/kv/messages/msg2</strong></span>
<span class="strong"><strong>curl -X PUT -d 'this is a test with flags' \</strong></span>
<span class="strong"><strong>    http://localhost:8500/v1/kv/messages/msg3?flags=1234</strong></span>
</pre></div><p>The first command created the <code class="literal">msg1</code> key with the value <code class="literal">this is a test</code>. The second had nested the key <code class="literal">msg2</code> into a parent key <code class="literal">messages</code>. Finally, the last command added the <code class="literal">flag</code> with the value <code class="literal">1234</code>. Flags can be used to store version number or any other information that can be expressed as an integer.</p><p>Let's take a look how to retrieve the information we just stored:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://localhost:8500/v1/kv/?recurse \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>
</pre></div><p>The output of the command is as follows (order is not guaranteed):</p><div class="informalexample"><pre class="programlisting">[
    {
        "CreateIndex": 141,
        "Flags": 0,
        "Key": "messages/msg2",
        "LockIndex": 0,
        "ModifyIndex": 141,
        "Value": "dGhpcyBpcyBhbm90aGVyIHRlc3Q="
    },
    {
        "CreateIndex": 142,
        "Flags": 1234,
        "Key": "messages/msg3",
        "LockIndex": 0,
        "ModifyIndex": 147,
        "Value": "dGhpcyBpcyBhIHRlc3Qgd2l0aCBmbGFncw=="
    },
    {
        "CreateIndex": 140,
        "Flags": 0,
        "Key": "msg1",
        "LockIndex": 0,
        "ModifyIndex": 140,
        "Value": "dGhpcyBpcyBhIHRlc3Q="
    }
]</pre></div><p>Since we used the <code class="literal">recurse</code> query, keys were returned from the root recursively.</p><p>Here we can see all the <a class="indexterm" id="id348"/>keys we inserted. However, the value is base64 encoded. Consul can store more than text and, in fact, it stores everything as binary under the hood. Since not everything can be represented as text, you can store anything in Consul's K/V, but there are size limitations.</p><p>We can also retrieve a single key:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://localhost:8500/v1/kv/msg1 \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>
</pre></div><p>The output is the same as before but limited to the key <code class="literal">msg1</code>:</p><div class="informalexample"><pre class="programlisting">[
    {
        "CreateIndex": 140,
        "Flags": 0,
        "Key": "msg1",
        "LockIndex": 0,
        "ModifyIndex": 140,
        "Value": "dGhpcyBpcyBhIHRlc3Q="
    }
]</pre></div><p>Finally, we can request only the value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://localhost:8500/v1/kv/msg1?raw</strong></span>
</pre></div><p>This time, we put the <code class="literal">raw</code> query parameter and the result is only the value of the requested key:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>this is a test</strong></span>
</pre></div><p>As you might have guessed, Consul keys can easily be deleted. The command to, for example, delete the <code class="literal">messages/msg2</code> key is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -X DELETE http://localhost:8500/v1/kv/messages/msg2</strong></span>
</pre></div><p>We can also delete recursively:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl -X DELETE http://localhost:8500/v1/kv/?recurse</strong></span>
</pre></div><p>The Consul agent we<a class="indexterm" id="id349"/> deployed was set up to be the server. However, most agents do not need to run in the server mode. Depending on the number of nodes, we might opt for three Consul agents running in the server mode and many non-server agents joining it. If, on the other hand, the number of nodes is indeed big, we might increase the number of agents running in the server mode to five. If only one server is running, there will be data loss in case of its failure. In our case, since the cluster consists of only three nodes and this is a demo environment, one Consul agent running in the server mode is more than enough.</p><p>The command to run an agent on the <code class="literal">serv-disc-02</code> node and make it join the cluster is as follows (please don't run it yet):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo consul agent \</strong></span>
<span class="strong"><strong>    -data-dir /data/consul/data \</strong></span>
<span class="strong"><strong>    -config-dir /data/consul/config \</strong></span>
<span class="strong"><strong>    -node=serv-disc-02 \</strong></span>
<span class="strong"><strong>    -bind=10.100.197.202 \</strong></span>
<span class="strong"><strong>    -client=0.0.0.0 \</strong></span>
<span class="strong"><strong>    &gt;/tmp/consul.log &amp;</strong></span>
</pre></div><p>The only difference we did when compared with the previous execution is the removal of arguments <code class="literal">-server</code> and <code class="literal">-bootstrap-expect 1</code>. However, running Consul in one of the cluster servers is not enough. We need to join it with the Consul agent running on the other server. The command to accomplish that is as follows (please don't run it yet).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>consul join 10.100.198.200</strong></span>
</pre></div><p>The effect of running this command is that agents of both servers would be clustered and data synchronized between them. If we continued adding Consul agents to other servers and joining them, the effect would be an increased number of cluster nodes registered in Consul. There is no need to join more than one agent since Consul uses a gossip protocol to manage membership and broadcast messages to the cluster. That is one of the useful improvements when compared to <code class="literal">etcd</code> that requires us to specify the list of all servers in the cluster. Managing such a list tends to be more complicated when the number of servers increases. With the gossip protocol, Consul is capable of discovering nodes in the cluster without us telling it where they are.</p><p>With Consul basics covered, let's see how we can automate its configuration across all servers in the cluster. Since we are already committed to Ansible, we'll create a new role for Consul. While the <a class="indexterm" id="id350"/>configuration we're about to explore is very similar to those we did by now, there are few new details we have not yet seen.tasks from the Ansible role <code class="literal">roles/consul/tasks/main.yml</code> are as follows:</p><div class="informalexample"><pre class="programlisting">- name: Directories are created
  file:
    path: "{{ item }}"
    state: directory
  with_items: directories
  tags: [consul]
- name: Files are copied
  copy:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    mode: "{{ item.mode }}"
  with_items: files
  tags: [consul]</pre></div><p>We started by creating directories and copying files. Both tasks use variables array specified in the <code class="literal">with_items</code> tag.</p><p>Let's take a look at those variables. They are defined in the <code class="literal">roles/consul/defaults/main.yml</code>:</p><div class="informalexample"><pre class="programlisting">logs_dir: /data/consul/logs
directories:
  - /data/consul/data
  - /data/consul/config
  - "{{ logs_dir }}"
files: [
  { src: 'consul', dest: '/usr/local/bin/consul', mode: '0755' },
  { src: 'ui', dest: '/data/consul', mode: '0644' }
]</pre></div><p>Even though we could specify all those variables inside the <code class="literal">roles/consul/tasks/main.yml</code> file, having them separated allows us to change their values more easily. In this case, have a simple list of directories and the list of files in JSON format with source, destination and mode.tinue with the tasks in the <code class="literal">roles/consul/tasks/main.yml</code>. The third one is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Is running</strong></span>
<span class="strong"><strong>  shell: "nohup consul agent {{ consul_extra }} \</strong></span>
<span class="strong"><strong>    -data-dir /data/consul/data \</strong></span>
<span class="strong"><strong>    -config-dir /data/consul/config \</strong></span>
<span class="strong"><strong>    -node={{ ansible_hostname }} \</strong></span>
<span class="strong"><strong>    -bind={{ ip }} \</strong></span>
<span class="strong"><strong>    -client=0.0.0.0 \</strong></span>
<span class="strong"><strong>    &gt;{{ logs_dir }}/consul.log 2&gt;&amp;1 &amp;"</strong></span>
<span class="strong"><strong>  tags: [consul]</strong></span>
</pre></div><p>Since Consul makes <a class="indexterm" id="id351"/>sure that there is only one process running at the time, there is no danger running this task multiple times. It is equivalent to the command we run manually with an addition of a few variables.</p><p>If you remember the manual execution of Consul, one node should run Consul in the server node and the rest should join at least one node so that Consul can gossip that information to the whole cluster. We defined those differences as the (<code class="literal">consul_extra</code>) variable. Unlike those we used before that are defined in <code class="literal">roles/consul/defaults/main.y</code>
<code class="literal">ml</code> file inside the role, <code class="literal">consul_extra</code> is defined in the <code class="literal">hosts/serv-disc</code> inventory file. Let's take a look at it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[consul]</strong></span>
<span class="strong"><strong>10.100.194.201 consul_extra="-server -bootstrap"</strong></span>
<span class="strong"><strong>10.100.194.20[2:3] consul_server_ip="10.100.194.201"</strong></span>
</pre></div><p>We defined variables to the right of the server IPs. In this case, the <code class="literal">.201</code> is acting as a server. The rest is defining the <code class="literal">consul_server_ip</code> variables that we'll discuss very soon.</p><p>Let's jump into the fourth (and last) task defined in the <code class="literal">roles/consul/tasks/main.yml</code> file:</p><div class="informalexample"><pre class="programlisting">- name: Has joined
  shell: consul join {{ consul_server_ip }}
  when: consul_server_ip is defined
  tags: [consul]</pre></div><p>This task makes sure that every Consul agent, except the one running in the server mode, joins the cluster. The task runs the same command like the one we executed manually, with the addition of the <code class="literal">consul_server_ip</code> variable that has a double usage. The first usage is to provide value for the <code class="literal">shell</code> command. The second usage is to decide whether this task is run at all. We accomplished that using the <code class="literal">when: consu</code>
<code class="literal">l_server_i</code>
<code class="literal">p is defined</code> definition:</p><p>Finally, we have the <code class="literal">consul.yml</code> playbook, that is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: consul</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - consul</strong></span>
</pre></div><p>There's not much to say about it since it follows the same structure as the playbooks we used before.</p><p>Now that we have the playbook, let us execute it and take a look at Consul nodes.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook \</strong></span>
<span class="strong"><strong>    /vagrant/ansible/consul.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/serv-disc</strong></span>
</pre></div><p>We can confirm whether Consul is indeed running on all nodes by sending the <span class="emphasis"><em>nodes</em></span> request to one of its agents:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl serv-disc-01:8500/v1/catalog/nodes \</strong></span>
<span class="strong"><strong>    | jq '.'</strong></span>
</pre></div><p>The output of the <a class="indexterm" id="id352"/>command is as follows.</p><div class="informalexample"><pre class="programlisting">[
    {
        "Address": "10.100.194.201",
        "Node": "serv-disc-01"
    },
    {
        "Address": "10.100.194.202",
        "Node": "serv-disc-02"
    },
    {
        "Address": "10.100.194.203",
        "Node": "serv-disc-03"
    }
]</pre></div><p>All three nodes in the cluster are now running Consul. With that out of the way, we can move back to Registrator and see how it behaves when combined with Consul.</p><div class="mediaobject"><img alt="Setting Up Consul" src="graphics/B05848_08_09.jpg"/><div class="caption"><p>Figure 8-9 – Multiple nodes with Docker containers and Consul</p></div></div></div></div><div class="section" title="Setting Up Registrator"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec52"/>Setting Up Registrator</h2></div></div></div><p>Registrator has two Consul <a class="indexterm" id="id353"/>protocols. We'll take a look at <code class="literal">consulkv</code> first<a class="indexterm" id="id354"/> since its results should be very similar to those obtained with the etcd protocol:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export DOCKER_HOST=tcp://serv-disc-01:2375</strong></span>
<span class="strong"><strong>docker run -d --name registrator-consul-kv \</strong></span>
<span class="strong"><strong>    -v /var/run/docker.sock:/tmp/docker.sock \</strong></span>
<span class="strong"><strong>    -h serv-disc-01 \</strong></span>
<span class="strong"><strong>    gliderlabs/registrator \</strong></span>
<span class="strong"><strong>    -ip 10.100.194.201 consulkv://10.100.194.201:8500/services</strong></span>
</pre></div><p>Let's take a look at the Registrator log and check whether everything seems to be working correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker logs registrator-consul-kv</strong></span>
</pre></div><p>The output should be similar to the following (timestamps were removed for brevity):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Starting registrator v6 ...</strong></span>
<span class="strong"><strong>Forcing host IP to 10.100.194.201</strong></span>
<span class="strong"><strong>consulkv: current leader  10.100.194.201:8300</strong></span>
<span class="strong"><strong>Using consulkv adapter: consulkv://10.100.194.201:8500/services</strong></span>
<span class="strong"><strong>Listening for Docker events ...</strong></span>
<span class="strong"><strong>Syncing services on 1 containers</strong></span>
<span class="strong"><strong>ignored: 19c952849ac2 no published ports</strong></span>
<span class="strong"><strong>ignored: 46267b399098 port 443 not published on host</strong></span>
<span class="strong"><strong>added: 46267b399098 nginx</strong></span>
</pre></div><p>The result is the same as when we run Registrator with the etcd protocol. It found the nginx container running (the one that we started previously while practicing <code class="literal">etcd</code>) and published the exposed port <code class="literal">4321</code> to Consul. We can confirm that by querying Consul:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://serv-disc-01:8500/v1/kv/services/nginx/nginx?raw</strong></span>
</pre></div><p>As expected, the output is the IP and the port exposed through the nginx container:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>10.100.194.201:4321</strong></span>
</pre></div><p>However, Registrator has another protocol called <code class="literal">consul</code> (the one we just used is <code class="literal">consulkv</code>) that utilizes Consul's format for storing service information.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d --name registrator-consul \</strong></span>
<span class="strong"><strong>    -v /var/run/docker.sock:/tmp/docker.sock \</strong></span>
<span class="strong"><strong>    -h serv-disc-01 \</strong></span>
<span class="strong"><strong>    gliderlabs/registrator \</strong></span>
<span class="strong"><strong>    -ip 10.100.194.201 consul://10.100.194.201:8500</strong></span>
</pre></div><p>Let's see what information<a class="indexterm" id="id355"/> Registrator sent to Consul this time:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://serv-disc-01:8500/v1/catalog/service/nginx-80 | jq '.'</strong></span>
</pre></div><p>This time, the data is a bit more complete yet still in a very simple format:</p><div class="informalexample"><pre class="programlisting">[
  {
    "ModifyIndex": 185,
    "CreateIndex": 185,
    "Node": "serv-disc-01",
    "Address": "10.100.194.201",
    "ServiceID": "nginx",
    "ServiceName": "nginx-80",
    "ServiceTags": [],
    "ServiceAddress": "10.100.194.201",
    "ServicePort": 4321,
    "ServiceEnableTagOverride": false
  }
]</pre></div><p>Besides the IP and the port that is normally stored with <code class="literal">etcd</code> or <code class="literal">consulkv</code> protocols, this time, we got more<a class="indexterm" id="id356"/> information. We know the node the service is running on, service ID and the name. We can do even better than that with few additional environment variables. Let's bring up another nginx container and see the data stored in Consul:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>docker run -d --name nginx2 \</strong></span>
<span class="strong"><strong>    --env "SERVICE_ID=nginx2" \</strong></span>
<span class="strong"><strong>    --env "SERVICE_NAME=nginx" \</strong></span>
<span class="strong"><strong>    --env "SERVICE_TAGS=balancer,proxy,www" \</strong></span>
<span class="strong"><strong>    -p 1111:80 \</strong></span>
<span class="strong"><strong>    nginx</strong></span>
<span class="strong"><strong>curl http://serv-disc-01:8500/v1/catalog/service/nginx-80 | jq '.'</strong></span>
</pre></div><p>The output of the last command is as follows.</p><div class="informalexample"><pre class="programlisting">[
  {
    "ModifyIndex": 185,
    "CreateIndex": 185,
    "Node": "serv-disc-01",
    "Address": "10.100.194.201",
    "ServiceID": "nginx",
    "ServiceName": "nginx",
    "ServiceTags": [],
    "ServiceAddress": "10.100.194.201",
    "ServicePort": 4321,
    "ServiceEnableTagOverride": false
  },
  {
    "ModifyIndex": 202,
    "CreateIndex": 202,
    "Node": "serv-disc-01",
    "Address": "10.100.194.201",
    "ServiceID": "nginx2",
    "ServiceName": "nginx",
    "ServiceTags": [
      "balancer",
      "proxy",
      "www"
    ],
    "ServiceAddress": "10.100.194.201",
    "ServicePort": 1111,
    "ServiceEnableTagOverride": false
  }
]</pre></div><p>The second container (<code class="literal">nginx2</code>) was registered and, this time, Consul got tags that we might find useful later<a class="indexterm" id="id357"/> on. Since both containers are listed under the same name Consul considers them to be two instances of the same service.</p><p>Now that we know<a class="indexterm" id="id358"/> how Registrator works in conjunction with Consul, let's configure it in all nodes of the cluster. The good news is that the role is already created, and we set the protocol to be defined with the variable <code class="literal">protocol</code>. We also put the name of the container as the <code class="literal">registrator_name</code> variable so that we can bring the Registrator container with the consul protocol without getting in conflict with the etcd one we configured earlier:</p><p>The playbook <code class="literal">registrator.yml</code> is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: registrator</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  vars:</strong></span>
<span class="strong"><strong>    - registrator_name: registrator-c</strong></span>
<span class="strong"><strong>onsul</strong></span>
<span class="strong"><strong> docker</strong></span>
<span class="strong"><strong>    - consul</strong></span>
<span class="strong"><strong>    - registrator</strong></span>
</pre></div><p>The <code class="literal">registrator-etcd.yml</code> has the <code class="literal">registrator_protocol</code> variable set to <code class="literal">etcd</code> and <code class="literal">registrator_port</code> to <code class="literal">2379</code>. We didn't need it in this case since we already had default values set to <code class="literal">consul</code> and <code class="literal">8500</code> in the <code class="literal">roles/registrator/defaults/main.yml</code> file. On the other hand, we<a class="indexterm" id="id359"/> did overwrite the default value of the <code class="literal">registrator_name</code>:</p><p>With everything ready, we can run the playbook:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook \</strong></span>
<span class="strong"><strong>    /vagrant/ansible/registrator.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/serv-disc</strong></span>
</pre></div><p>Once the execution<a class="indexterm" id="id360"/> of this playbook is finished, Registrator with the consul protocol will be configured on all nodes in the cluster:</p><div class="mediaobject"><img alt="Setting Up Registrator" src="graphics/B05848_08_10.jpg"/><div class="caption"><p>Figure 8-10 – Multiple nodes with Docker containers, Consul and Registrator</p></div></div><p>How about templating? Should we use confd or something else?</p><div class="section" title="Setting Up Consul Template"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec17"/>Setting Up Consul Template</h3></div></div></div><p>We can use confd with <a class="indexterm" id="id361"/>Consul in the same way as we used it with etcd. However, Consul has its own templating service with features more in line with what Consul offers.</p><p>Consul Template is a very convenient way to create files with values obtained from Consul. As a bonus, it can also run arbitrary commands after the files have been updated. Just as confd, Consul Template also uses Go Template format.</p><p>By now, you're probably <a class="indexterm" id="id362"/>accustomed to the routine. First we'll try Consul Template manually. As with all other tools, we set up in this chapter, installation consists of downloading the release, unpacking it and making sure that the executable is in the system path.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://releases.hashicorp.com/consul-template/0.12.0/\</strong></span>
<span class="strong"><strong>consul-template_0.12.0_linux_amd64.zip</strong></span>
<span class="strong"><strong>sudo apt-get install -y unzip</strong></span>
<span class="strong"><strong>unzip consul-template_0.12.0_linux_amd64.zip</strong></span>
<span class="strong"><strong>sudo mv consul-template /usr/local/bin</strong></span>
<span class="strong"><strong>rm -rf consul-template_0.12.0_linux_amd64*</strong></span>
</pre></div><p>With Consul Template available on the node, we should create one template:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo '</strong></span>
<span class="strong"><strong>{{range service "nginx-80"}}</strong></span>
<span class="strong"><strong>The address is {{.Address}}:{{.Port}}</strong></span>
<span class="strong"><strong>{{end}}</strong></span>
<span class="strong"><strong>' &gt;/tmp/nginx.ctmpl</strong></span>
</pre></div><p>When this template is processed, it will iterate (<code class="literal">range</code>) over all services with the name <code class="literal">nginx-80</code>. Each iteration will produce the text with service <code class="literal">Address</code> and <code class="literal">Port</code>. Template has been created as <code class="literal">/tmp/nginx.ctmpl</code>.</p><p>Before we run the Consul Template, let's take another look at what we have stored in Consul for the nginx services:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl http://serv-disc-01:8500/v1/catalog/service/nginx-80 | jq '.'</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">[
  {
    "ModifyIndex": 185,
    "CreateIndex": 185,
    "Node": "serv-disc-01",
    "Address": "10.100.194.201",
    "ServiceID": "nginx",
    "ServiceName": "nginx-80",
    "ServiceTags": [],
    "ServiceAddress": "10.100.194.201",
    "ServicePort": 4321,
    "ServiceEnableTagOverride": false
  },
  {
    "ModifyIndex": 202,
    "CreateIndex": 202,
    "Node": "serv-disc-01",
    "Address": "10.100.194.201",
    "ServiceID": "nginx2",
    "ServiceName": "nginx-80",
    "ServiceTags": [
      "balancer",
      "proxy",
      "www"
    ],
    "ServiceAddress": "10.100.194.201",
    "ServicePort": 1111,
    "ServiceEnableTagOverride": false
  }
]</pre></div><p>We have two nginx<a class="indexterm" id="id363"/> services up and running and registered in Consul. Let's see the result of applying the template we created:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>consul-template \</strong></span>
<span class="strong"><strong>    -consul serv-disc-01:8500 \</strong></span>
<span class="strong"><strong>    -template "/tmp/nginx.ctmpl:/tmp/nginx.conf" \</strong></span>
<span class="strong"><strong>    -once</strong></span>
<span class="strong"><strong>cat /tmp/nginx.conf</strong></span>
</pre></div><p>The result of the second command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>The address is 10.100.194.201:4321</strong></span>
<span class="strong"><strong>The address is 10.100.194.201:1111</strong></span>
</pre></div><p>The Consul Template command we executed found both services and generated the output in the format we specified. We specified that it should run only once. The alternative is to run it in daemon mode. In such a case, it would monitor the registry for changes and apply them to specified configuration files.</p><p>We will go into details of how Consul Template works later on when we start using it in our deployment pipeline. Until then, please <a class="indexterm" id="id364"/>consult <a class="ulink" href="https://www.consul.io/docs/">https://www.consul.io/docs/</a> yourself. For now, it is important to understand that it can obtain any information stored in Consul and apply it to the template we specify. Besides creating the file, it can also run custom commands. That will come in handy with <code class="literal">reverse proxy</code>, that is the subject of our next chapter.</p><p>We didn't try Consul Template applied to Consul's key/value format. In that combination, there is no significant difference when compared to confd.</p><p>The major downside Consul Template has is its tight coupling with Consul. Unlike confd that can be used with many different registries, Consul Template is created as a templating engine tightly integrated with Consul. That is, at the same time, an advantage, since it understands Consul's service format. If you choose to use Consul, Consul Template is a great fit.</p><p>Before we move on to the next subject, let's create Consul Template role and configure it on all nodes. The <code class="literal">roles/consul-template/tasks/main.yml</code> file is as follows.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- name: Directory is created</strong></span>
<span class="strong"><strong>  file:</strong></span>
<span class="strong"><strong>    path: /data/consul-template</strong></span>
<span class="strong"><strong>    state: directory</strong></span>
<span class="strong"><strong>  tags: [consul-template]</strong></span>
<span class="strong"><strong>- name: File is copied</strong></span>
<span class="strong"><strong>  copy:</strong></span>
<span class="strong"><strong>    src: consul-template</strong></span>
<span class="strong"><strong>    dest: /usr/local/bin/consul-template</strong></span>
<span class="strong"><strong>    mode: 0755</strong></span>
<span class="strong"><strong>  tags: [consul-template]</strong></span>
</pre></div><p>There's nothing<a class="indexterm" id="id365"/> exciting with this role. It's probably the simplest one we did by now. The same holds true for the <code class="literal">consul-template.yml</code> playbook:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>- hosts: consul-template</strong></span>
<span class="strong"><strong>  remote_user: vagrant</strong></span>
<span class="strong"><strong>  serial: 1</strong></span>
<span class="strong"><strong>  sudo: yes</strong></span>
<span class="strong"><strong>  roles:</strong></span>
<span class="strong"><strong>    - common</strong></span>
<span class="strong"><strong>    - consul-template</strong></span>
</pre></div><p>And, finally, we can configure it on all nodes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ansible-playbook \</strong></span>
<span class="strong"><strong>    /vagrant/ansible/consul-template.yml \</strong></span>
<span class="strong"><strong>    -i /vagrant/ansible/hosts/serv-disc</strong></span>
</pre></div><p>The end result is very similar to the etcd/Registrator combination with the difference in data format sent to Consul:</p><div class="mediaobject"><img alt="Setting Up Consul Template" src="graphics/B05848_08_11.jpg"/><div class="caption"><p>Figure 8-11 – Multiple nodes with Docker containers, Consul, Registrator and Consul Template</p></div></div><p>Up to this point, we <a class="indexterm" id="id366"/>covered Consul's features that are, somewhat, similar to the etcd/registrator/confd combination. It's time to take a look at the characteristics that make Consul indeed stand up from the crowd.</p></div></div><div class="section" title="Consul Health Checks, Web UI, and Data Centers"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec53"/>Consul Health Checks, Web UI, and Data Centers</h2></div></div></div><p>Monitoring health of cluster nodes <a class="indexterm" id="id367"/>and services is as important as testing and deployment<a class="indexterm" id="id368"/> itself. While we should <a class="indexterm" id="id369"/>aim towards having stable environments that never fail, we should also acknowledge that unexpected failures happen and be prepared to act accordingly. We can, for example, monitor memory usage and, if it reaches a certain threshold, move some services to a different node in the cluster. That would be an example of preventive actions performed before the "disaster" would happen. On the other hand, not all potential failures can be detected in time for us to act on time. A single service<a class="indexterm" id="id370"/> can fail. A whole node can stop working due to a hardware failure. In such cases, we should be prepared to act as fast as possible by, for example, replacing a node with a new one and moving failed services. We won't go into details how Consul can help us in this task since there is a whole chapter dedicated to <code class="literal">self-healing systems</code> and Consul will play a major role in it. For now, suffice to say that Consul has a simple, elegant and, yet, powerful way to perform health checks that can help us define what actions should be performed when health thresholds are reached.</p><p>If you googled <code class="literal">etcd ui</code> or <code class="literal">etcd dashboard</code> you probably saw that there are a few solutions available, and you might be asking why we haven't presented them. The reason is simple; etcd is a key/value<a class="indexterm" id="id371"/> store and not much more. Having a UI to present data is not of much use since we can easily obtain it through the etcdctl. That does not mean that etcd UI is of no use but that it does not make much difference due to its limited scope.</p><p>Consul is much more than<a class="indexterm" id="id372"/> a simple key/value store. As we've already seen, besides storing key/value pairs, it has a notion of a service together with data that belong to it. It can also perform health checks, thus becoming a good candidate for a dashboard that can be used to see the status of our nodes and services running on top of them. Finally, it understands the concept of multiple data centers. All those features combined, let us see the need for a dashboard in a different light.</p><p>With the Consul Web UI, we can view all services and nodes, monitor health checks and their statuses, read and set key/value data as well as switch from one data center to another. To see it in action, please open <code class="literal">http://10.100.194.201:8500/ui</code> in your favorite browser. You'll see items in the top menu that correspond to the steps we performed earlier through the API.</p><p>The <code class="literal">Services</code> menu item lists all the services we registered. There's not much at the moment since only Consul server, Docker UI and two instances of the nginx service are up and running. We can filter them by name or status and see details by clicking on one of the registered services:</p><div class="mediaobject"><img alt="Consul Health Checks, Web UI, and Data Centers" src="graphics/B05848_08_12.jpg"/><div class="caption"><p>Figure 8-12 – Consul Web UI services</p></div></div><p>Nodes <a class="indexterm" id="id373"/>show us the list<a class="indexterm" id="id374"/> of all nodes belonging to the selected <a class="indexterm" id="id375"/>data center. In our case, we have three nodes. The first one has three registered services:</p><div class="mediaobject"><img alt="Consul Health Checks, Web UI, and Data Centers" src="graphics/B05848_08_13.jpg"/><div class="caption"><p>Figure 8-13 – Consul Web UI nodes</p></div></div><p>The <code class="literal">Key/Value</code> screen can<a class="indexterm" id="id376"/> be used to both display and modify data. In it, you can see data put to Consul by the Registrator instance set to use <code class="literal">consulkv</code> as the<a class="indexterm" id="id377"/> protocol. Please feel free to add data yourself and <a class="indexterm" id="id378"/>see how they are visualized in the UI. Besides working with Consul key/value data with the API we used before, you can also manage them through the UI.</p><div class="mediaobject"><img alt="Consul Health Checks, Web UI, and Data Centers" src="graphics/B05848_08_14.jpg"/><div class="caption"><p>Figure 8-14 – Consul Web UI key/value</p></div></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>Please note that Consul allows us to group nodes into data centers. We haven't used this feature since<a class="indexterm" id="id379"/> we are running only three nodes. When nodes in the cluster start increasing, splitting them into data<a class="indexterm" id="id380"/> centers is often a good idea and Consul helps us to visualize them through its UI.</p></div></div><div class="section" title="Combining Consul, Registrator, Template, Health Checks and WEB UI"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec18"/>Combining Consul, Registrator, Template, Health Checks and WEB UI</h3></div></div></div><p>Consul, together with the tools we <a class="indexterm" id="id381"/>explored, is in many cases a better solution<a class="indexterm" id="id382"/> than what etcd offers. It was designed with<a class="indexterm" id="id383"/> services architecture and discovery in mind. It is simple, yet powerful. It provides a complete solution without sacrificing<a class="indexterm" id="id384"/> simplicity and, in many cases, it is the best tool for service discovery and health checking needs (at least among those we evaluated).</p></div></div></div></div>
<div class="section" title="Service Discovery Tools Compared"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec20"/>Service Discovery Tools Compared</h1></div></div></div><p>All of the tools are <a class="indexterm" id="id385"/>based on similar principles and architecture. They run on nodes, require a quorum to operate and are strongly consistent. They all provide some form of key/value storage.</p><p>Zookeeper is the oldest of the three, and the age shows in its complexity, utilization of resources and goals it's trying to accomplish. It was designed in a different age than the rest of the tools we evaluated (even though it's not much older).</p><p>
<code class="literal">etcd</code> with Registrator and <code class="literal">confd</code> is a very simple, yet very powerful combination that can solve most, if not all, of our service discovery needs. It showcases the power we can obtain when we combine simple and very specific tools. Each of them performs a very specific task, communicates through well-established API and is capable of working with relative autonomy. They are <code class="literal">microservices</code> both in their architectural as well as their functional approach.</p><p>What distinguishes <code class="literal">Consul</code> is the support for multiple data centers and health checking without the usage of third-party tools. That does not mean that the usage of third-party tools is wrong. Actually, throughout this book we are trying to combine different tools by choosing those that are <a class="indexterm" id="id386"/>performing better than others without introducing unnecessary features overhead. The best results are obtained when we use right tools for the job. If the tool does more than the job we require, its efficiency drops. On the other hand, a tool that doesn't do what we need it to do is useless. Consul strikes the right balance. It does very few things, and it does them well.</p><p>The way Consul uses the gossip protocol to propagate knowledge about the cluster makes it easier to set up than etcd, especially in the case of a big data center. The ability to store data as a service makes it more complete and useful than key/value storage used in etcd (even though Consul has that option as well). While we could accomplish the same by inserting multiple keys in etcd, Consul's service achieves a more compact result that often requires a single query to retrieve all the data related to the service. On top of that, Registrator has quite a good implementation of the Consul protocol making the two an excellent combination, especially when Consul Template is added to this mixture. Consul's Web UI is like a cherry on top of a cake and provides a good way to visualize your services and their health.</p><p>I can't say that Consul is a clear winner. Instead, it has a slight edge when compared with etcd. Service discovery as a concept, as well as the tools we can use, is so new that we can expect a lot of changes in this field. By the time you read this book, it's likely that new tools will come, or those we evaluated will change enough that some of the exercises we did will become obsolete. Have an open mind and try to take bits of advice from this chapter with a grain of salt. The logic we employed is solid and is not likely to change anytime soon. The same can not be said for tools. They are bound to evolve rapidly soon.</p><p>We are left with one more subject before we can get back to our deployment procedure. The integration step will require that we go through <code class="literal">reverse proxy</code>.</p><p>Before we move on, let's destroy the virtual machines we created for the purpose of service discovery practice and free some resources for the next chapter.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>exit</strong></span>
<span class="strong"><strong>vagrant destroy -f</strong></span>
</pre></div></div></body></html>