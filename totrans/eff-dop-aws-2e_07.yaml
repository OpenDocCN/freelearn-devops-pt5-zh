- en: Running Containers in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml), *Scaling Your Infrastructure*,
    our architecture changed quite a bit. We explored different ways to scale our
    applications in AWS, but one of the major technologies that we left out was containers.
    Containers are at the heart of the **software development life cycle** (**SDLC**)
    of many major technology companies.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have used our personal computers to develop our applications. This
    works well for simple projects, such as our Hello World application. However,
    when it comes to more complex projects with many dependencies, it's a different
    story. Have you ever heard of situations in which a certain feature works on a
    developer's laptop but does not work for the rest of the organization-or-even
    worse, *does not work in production?* A lot of these issues stem from the differences
    between environments. When we build our staging and production environments, we
    rely on CloudFormation, Terraform, and Ansible, to keep those environments consistent.
    Unfortunately, we can't easily replicate that to our local development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Containers address this issue. With them, we can package an application and
    include the operating system, the application code, and everything in between.
    Containers can also help at a later stage, when it's time to break out the monolithic
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at **Docker**, the most popular container technology.
    After a brief explanation of what Docker is and how to use its basic functionalities,
    we will Dockerize our application. This will help us to understand the value of
    using Docker as a developer. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Dockerizing our Hello World application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the EC2 container service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating our CI/CD pipeline to utilize ECS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book covers ECS, but also offers further options for using Docker in AWS.
    You can also take a look at CoreOS Tectonic ([https://tectonic.com/](https://tectonic.com/)),
    Mesosphere DC/OS ([https://mesosphere.com](https://mesosphere.com/)), or Docker
    Datacenter ([https://www.docker.com/products/docker-datacenter](https://www.docker.com/products/docker-datacenter)).
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The technical requirements for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dockerfile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EC2 Container Registry** (**ECR**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic Container Service** (**ECS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application Load Balancer** (**ALB**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodeBuild
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CodePipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GitHub links for the code used in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/helloworld/blob/master/helloworld.js](https://github.com/yogeshraheja/helloworld/blob/master/helloworld.js)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/helloworld/blob/master/package.json](https://github.com/yogeshraheja/helloworld/blob/master/package.json)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/helloworld/blob/master/Dockerfile](https://github.com/yogeshraheja/helloworld/blob/master/Dockerfile)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecr-repository-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecr-repository-cf-template.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecs-cluster-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecs-cluster-cf-template.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-alb-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-alb-cf-template.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-service-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-service-cf-template.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codebuild-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codebuild-cf-template.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://raw.githubusercontent.com/yogeshraheja/EffectiveDevOpsTemplates/master/helloworld-ecs-service-cf-template.py](https://raw.githubusercontent.com/yogeshraheja/EffectiveDevOpsTemplates/master/helloworld-ecs-service-cf-template.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codepipeline-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codepipeline-cf-template.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dockerizing our Hello World application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker, and containers in general, are very powerful tools, worth exploring.
    By combining resource isolation features, including **union capable filesystem**
    (**UCF**), Docker allows for the creation of packages called **containers**, which
    include everything that is needed to run an application. Containers, like virtual
    machines, are self-contained, but they virtualize the OS itself, instead of virtualizing
    the hardware. In practice, this makes a huge difference. As you have probably
    noticed by now, starting a virtual machine, such as an EC2 instance, takes time.
    This comes from the fact that in order to start a virtual machine, the hypervisor
    (that's the name of the technology that creates and runs virtual machines) has
    to simulate all of the motions involved in starting a physical server, loading
    an operating system, and going through the different run-levels. In addition,
    virtual machines have a much larger footprint on the disk and in the memory. With
    Docker, the added layer is hardly noticeable, and the size of the containers can
    stay very small. In order to better illustrate this, we will first install Docker
    and explore its basic usage a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start to use Docker, it might be useful to better understand Docker's
    concept and architecture. First, we will discuss Docker's fundamental changes
    with regards to the SDLC. Following that introduction, we will install Docker
    on our computers and look at some of the most common commands needed to use Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Docker fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The best way to understand how Docker works is to compare how using Docker
    differs from what we''ve done so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c68877c6-26c3-4a77-a641-937b0f365231.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first stack on the left represents what we did so far. Using the EC2 service,
    we picked an AMI providing AWS Linux, and, with the help of the user data field,
    we installed Ansible to configure our system. When Ansible kicks in, it installs
    and configures the system, so that later, CodeDeploy can deploy and run our application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The middle stack represents what it means to use Docker on top of EC2\. The
    process starts the same way with an AMI running AWS Linux. However, this time,
    instead of relying on Ansible and CodeDeploy, we will simply install the Docker
    server application. After that, we will deploy Docker containers, which will have
    everything that was previously provided by Ansible and CodeDeploy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the big win of that architecture is what we see on the last stack on
    the right. No matter what the underlying technology is, as long as we can run
    a Docker server, we can run the exact same container. This means that we can easily
    test what we will deploy on EC2\. Similarly, if an issue happens in a container
    running on an EC2 instance, we can pull the exact same container and run it locally
    to possibly troubleshoot the issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to make that happen, Docker relies on a couple of key concepts, as
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87dc7058-dd1d-4f80-a35d-4ac2568dce89.png)'
  prefs: []
  type: TYPE_IMG
- en: At its core, Docker runs a daemon that loads images (templates describing the
    stack of the application, including the operating system, application code, and
    everything in between) and runs them in self-contained directories called containers.
    When working in Docker, as a developer, your work mostly consists of building
    new images by layering new commands on top of pre-existing images. Images are
    stored in external registries. Those registries can be public or private. Finally,
    all the interaction is done through a RESTful API, usually using the command-line
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: Docker in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To see Docker in action, we will start by installing it on our computer. The
    installation of Docker is very straightforward; you can follow the instructions
    found at [http://dockr.ly/2iVx6yG](http://dockr.ly/2iVx6yG) to install and start
    Docker on Mac, Linux, and Windows. Docker provides two offerings: Docker **Community
    Edition** (**CE**) and Docker **Enterprise Edition** (**EE**). Throughout this
    book, we are going to focus on open source tools, as well as using Docker CE,
    which is free of cost. Again, we will be demonstrating the following examples
    on a Linux based Centos 7.x distribution. If you are also following the same operating
    system then follow the instructions available at [https://docs.docker.com/install/linux/docker-ce/centos/](https://docs.docker.com/install/linux/docker-ce/centos/)
    to set up Docker locally on your system. When you are done with the installation
    of Docker CE, verify the installed Docker version using the `docker` utility.
    At the time of writing this book, `18.06` is the latest version of Docker, although
    you might see a newer version on your system now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once Docker is up and running, we can start using it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we will do is pull an image from a registry. By default,
    Docker points to Docker Hub ([https://hub.docker.com](https://hub.docker.com/)),
    which is the official Docker registry from the company Docker Inc. In order to
    pull an image, we will run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `latest` default tag, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In a matter of seconds, Docker will download the image called `alpine` from
    the registry, which is a minimal Docker image based on Alpine Linux with a complete
    package index. This is only `4.41 MB` in size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: When working with Docker, the size of a container matters. Consequently, working
    with smaller base images, such as Alpine Linux, is highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run our container. In order to do this, we will start with the following
    simple command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: On the surface, not a lot seems to have happened here, and we were left with
    the same output as we had when running echo `Hello World` without Docker. What
    really happened behind the scenes is a lot more interesting; Docker loaded the
    `alpine` Linux image that we previously pulled, and used the Alpine operating
    system `echo` command to print `Hello World`. Finally, because the `echo` command
    completed, the container was terminated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Containers can also be used in a more interactive way, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can, for example, start a shell and interact with it by using the following
    command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `-i` option means interactive; this allows us to type commands in our container
    while the `-t` option allocates a pseudo TTY to see what we are typing as well
    as the output of our commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers can also be run in the background by using the `-d` option, which
    will detach our container from the Terminal:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This command returns a 64-bit long ID of the container running the `alpine`
    image and the sleep `1000` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can keep track of the different running containers running by using the
    following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of running the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bd7bd33-00e4-4266-aae0-fe91c4a52faf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Running containers can be stopped using the `stop` option followed by the container
    name or ID (adapt the ID and name based on the output of your `docker ps` command):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Stopped containers can be started again with the `start` option, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, containers can be removed by using the the `rm` command, but always
    stop the container before removing them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/110220a2-1d44-4f16-b631-f7ad7c814dfa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This brief overview should provide us with the knowledge we need when reading
    this chapter. We will discover a few more commands along the way, but for a complete
    list of options, you can use the `docker help` command or consult the Docker CLI
    documentation at [http://dockr.ly/2jEF8hj](http://dockr.ly/2jEF8hj). Running simple
    commands through containers is sometimes useful but, as we know, the real strength
    of Docker is its ability to handle any code, including our web application. In
    order to make that happen, we will use another key concept of Docker: a Dockerfile.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating our Dockerfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dockerfiles are text files that are usually collocated with applications that
    instruct Docker on how to build a new Docker image. Through the creation of those
    files, you have the ability to tell Docker which Docker image to start from, what
    to copy on the container filesystem, what network port to expose, and so on. You
    can find the full documentation of the Dockerfile at [http://dockr.ly/2jmoZMw](http://dockr.ly/2jmoZMw).
    We are going to create a Dockerfile for our Hello World application, at the root
    of the `helloworld` project that we created in our GitHub repository, using the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The first instruction of a Dockerfile is always a `FROM` instruction. This
    tells Docker which Docker image to start from. We could use the Alpine image,
    as we did, but we can also save some time by using an image that has more than
    just an operating system. Through Docker Hub, the official Docker registry, Docker
    provides a number of curated sets of Docker repositories called **official**.
    We know that in order to run our application, we need Node.js and `npm`. We can
    use the Docker CLI to look for an official `node` image. To do that, we will use
    the `docker search` command and filter only on official images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can also search for this using our browser. As a result,
    we would end up with that same image, [https://hub.docker.com/_/node/](https://hub.docker.com/_/node/).
    As we can see, the following screenshot comes in a variety of versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea4202d1-ce5c-474e-a493-1bc16c06733a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Docker images are always made up of a name and a tag, using the syntax `name:tag`.
    If the tag is omitted, Docker will default to `latest`. From the preceding `docker
    pull` command, we can see how the output says `Using default tag: latest`. When
    creating a Dockerfile, it is best practice to use an explicit tag that doesn''t
    change over time (unlike the `latest` tag).'
  prefs: []
  type: TYPE_NORMAL
- en: If you are trying to migrate an application currently running on AWS Linux and
    make a certain number of assumptions based on that OS, you may want to look into
    using the official AWS Docker image. You can read more about this at [http://amzn.to/2jnmklF](http://amzn.to/2jnmklF).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the first line of our file, we will add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will tell Docker that we want to use that specific version of the `node`
    image. This means that we won''t have to install `node` or `npm`. Since we have
    the OS and runtime binaries needed by our application, we can start looking into
    adding our application to this image. First, we will want to create a directory
    on top of the `node:carbon` image''s filesystem, to hold our code. We can do that
    using the `RUN` instruction, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We now want to copy our application files onto the image. We will use the `COPY`
    directive to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that you copy the `helloworld.js` and `package.json` files inside
    the `/helloworld` project directory where you are locally developing Dockerfile.
    The files are placed at [https://github.com/yogeshraheja/helloworld/blob/master/helloworld.js](https://github.com/yogeshraheja/helloworld/blob/master/helloworld.js)
    and [https://github.com/yogeshraheja/helloworld/blob/master/package.json](https://github.com/yogeshraheja/helloworld/blob/master/package.json).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use the `WORKDIR` instruction to set our new working directory
    to be that `helloworld` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the `npm install` command to download and install our dependencies.
    Because we won''t use that container to test our code, we can just install the
    `npm` packages needed for production, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our application uses port `3000`. We need to make this port accessible to our
    host. In order to do that, we will use the `EXPOSE` instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can start our application. For that, we will use the `ENTRYPOINT`
    instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can now save the file. It should look like the template at [https://github.com/yogeshraheja/helloworld/blob/master/Dockerfile](https://github.com/yogeshraheja/helloworld/blob/master/Dockerfile).
    We can now build our new image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in the Terminal, we will again use the `docker` command, but this time
    with the `build` argument. We will also use the `-t` option to provide the name
    `helloworld` to our image, followed by a (`.`) dot that indicates the location
    of our Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each command produces a new intermediary container with the
    changes triggered by that step.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now run our newly created image to create a container with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are adding the`-p` option to our command to map the exposed port of
    our container to a port on our host. There are a few ways to validate that our
    container is working correctly. We can start by looking at the logs produced by
    our container (replace the container ID with the output of the previous command):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use the `docker ps` command to see the status of our container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2440413-c06d-48aa-a81d-6b7da5fd9448.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, of course, we can simply test the application with the `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, if your host has a public IP then you can even verify the outputs on
    the browser with `<ip:exposedport>`, which in my case is `54.205.200.149:3000`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3adc093-30ac-418d-ba0a-0e7e057a5f76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, kill the container using the `docker kill` command and container ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our image is working correctly, we can commit the code to GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In addition, you can now create an account (for free) on Docker Hub and upload
    that new image. If you want to give it a try, you can follow the instructions
    at [http://dockr.ly/2ki6DQV](http://dockr.ly/2ki6DQV).
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the ability to easily share containers makes a big difference when collaborating
    on projects. Instead of sharing code and asking people to compile or build packages,
    you can actually share a Docker image. For instance, this can be done by running
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of running the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bdb0d8a-8b89-48e9-8e3c-e0ae88bb3309.png)'
  prefs: []
  type: TYPE_IMG
- en: You can experience the Hello World application, the exact way I see it, no matter
    what your underlying architecture is. This new way of running applications makes
    Docker a very strong solution for sharing work or collaborating on projects. Docker's
    strengths do not end with work collaboration, however. As we are about to see,
    using containers in production is also a very interesting option. In order to
    easily implement such solutions, AWS created the EC2 container service. We are
    going to use it to deploy our newly created `helloworld` image.
  prefs: []
  type: TYPE_NORMAL
- en: Using the EC2 container service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We just went over creating a Docker image for our application. Here, we saw
    how easy and fast it is to start a container using Docker. This is a very transformative
    experience compared to using only virtual machine technologies such as EC2\. One
    possibility that we haven''t explicitly mentioned so far is that you can start
    multiple containers with the same image. We can, for example, start our `helloworld`
    container five times, binding five different ports using the following command
    (adapt the ID based on the image ID you built. If needed, run Docker images to
    find its ID):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can validate that everything is working using the `ps` and `curl` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of running the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bc2a770-1f61-41fa-b8aa-be32f8b55a01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Cleaning up containers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can clean up everything by stopping and removing all containers with these
    two handy one-line commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`$ docker stop $(docker ps -a -q)`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**`$ docker system prune`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of running the preceding commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f26a7e76-daad-43cd-bac0-4b92cec1cb45.png)'
  prefs: []
  type: TYPE_IMG
- en: This ability to start multiple containers on a single host with almost no overhead
    or latency makes Docker an ideal candidate for production. In addition, more and
    more companies are deciding to take the service-oriented architecture approach
    to an all-new level by breaking out each business function into a separate service.
    This is often called a **microservices** approach. Docker is a natural fit for
    microservices and for managing microservice architecture. This is because it provides
    a platform that is language agnostic (you can start any type of application written
    in any language inside your container), able to scale horizontally and vertically
    with ease, and a common story around deployment as we deploy containers instead
    of a variety of services. We will implement our container architecture using the
    **Infrastructure as Code** (**IaC**) best practices and use CloudFormation through
    the intermediary of Troposphere. The first service we are going to look at is
    AWS's ECR.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an ECR repository to manage our Docker image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first part of this chapter, we used the Docker Hub public registry. AWS
    provides a similar service to this called ECR. This allows you to keep your images
    in a private registry called a **repository**. ECR is fully compatible with the
    Docker CLI but also integrates deeply with the remaining ECS services. We are
    going to use this to store our `helloworld` images.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, we will rely heavily on CloudFormation to make our changes. Unlike
    what we saw previously, because of its nature, the ECS infrastructure we are going
    to build needs to be very modular. This is because, in practice, we will want
    to share some of those components with other services. Consequently, we will create
    a number of templates and link them to one another. One good way to do that is
    to rely on CloudFormation's export ability, which allows us to do cross-stack
    referencing.
  prefs: []
  type: TYPE_NORMAL
- en: One of the added bonuses that export provides is a fail-safe mechanism. You
    can't delete or edit a stack if another stack references an exported output.
  prefs: []
  type: TYPE_NORMAL
- en: To generate our template, we will create a new Troposphere script. To do this,
    go to the `EffectiveDevOpsTemplates` repository and create a new script named
    `ecr-repository-cf- template.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing a number of modules, including the `Export` mentioned
    earlier and the `ecr` module, in order to create our repository. We will also
    create our template variable, `t`, as we did in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are going to create a number of CloudFormation templates in this chapter,
    we will add a description so that it''s easier to understand which template does
    what when looking at them in the AWS console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create a parameter for the name of the repository so that we will be
    able to reuse that CloudFormation template for every repository we create:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our repository as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We are keeping the code very simple here and not enforcing any particular permissions.
    If you need to restrict who can access your repository and see more complex configurations,
    you can refer to the AWS documentation and, in particular, [http://amzn.to/2j7hA2P](http://amzn.to/2j7hA2P).Â Lastly,
    we will output the name of the repository we created and export its value through
    a template variable `t`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can save our script now. It should look like this: [https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecr-repository-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecr-repository-cf-template.py).
    We will now generate the CloudFormation template and create our stack as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few minutes, our stack will be created. We can validate that the repository
    was correctly created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see our exported output with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Our repository can now be used to store our `helloworld` image. We will use
    the Docker CLI to do that. The first step of that process is to log in to the
    `ecr` service. You can do this with the following handy one-line command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of running the preceding command can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/805706d0-cfab-4e33-a0ae-de6edbd3a93c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Back in our `helloworld` directory where the Dockerfile is, we will tag our
    image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a common practice to use the `latest` tag to designate the most recent
    version of an image. In addition, you need to adapt the following command based
    on the output of the `aws ecr describe-repositories` output (we assume here that
    you have already built your image):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now push that image to our registry as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how each layer of our image is pushed in parallel to our registry.
    Once the operation completes, we can validate that the new image is present in
    our registry as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: At this point, our image is now available to the rest of our infrastructure.
    We are going to move on to the next step of our process, which is the creation
    of the ECS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an ECS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating an ECS cluster is a very similar process to the one in [Chapter 6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml),
    *Scaling Your Infrastructure*, when we created an Auto Scaling Group to run our
    Hello World application. The main difference is that there is one more level of
    abstraction. ECS will run a number of services called **task**s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of those tasks may exist multiple times in order to handle the traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/328a29f3-4714-484d-b04b-b22d0d86e9a3.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to do that, the ECS service provides an orchestration layer. That orchestration
    layer is in charge of managing the life cycle of containers, including upgrading
    or downgrading and scaling your containers up or down. The orchestration layer
    also distributes all containers for every service across all instances of the
    cluster optimally. Finally, it also exposes a discovery mechanism that interacts
    with other services such as ALB and ELB to register and deregister containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Task placement strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: By default, the entire orchestration system is managed by AWS. However, you
    also have the ability to customize it through the creation of a task placement
    strategy. This will let you configure the orchestration to optimize for instance
    count, for load distribution, to add constraints, and make sure that certain tasks
    are launched on the same instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a new script to generate our ECS cluster. The filename will
    be `ecs-cluster-cf-template.py`. This template starts almost exactly like the
    template we created in [Chapter 6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml),
    *Scaling Your Infrastructure*, for the Auto Scaling Group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The only new import is the cluster one from the ECS module. Just like we did
    in [Chapter 6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml), *Scaling Your Infrastructure*,
    we will extract our IP address in order to use it later for the SSH security group,
    create our template variable, and add a description to the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now proceed with adding our parameters, which are the the same parameters
    as used in [Chapter 6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml), *Scaling Your
    Infrastructure*. This includes the SSH key-pair, the VPC ID, and its subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will look at creating our security group resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: There is one important difference here. In [Chapter 6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml),
    *Scaling Your Infrastructure*, we opened up port `3000` since that's what our
    application is using. Here, we are opening every port to the CIDR 1 `72.16.0.0/12`,
    which is the private IP space of our internal network. This will give our ECS
    cluster the ability to run multiple `helloworld` containers on the same hosts,
    binding different ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create our cluster resource. This can simply be done with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will focus on configuring instances of the cluster, starting with
    their IAM role. Overall, this is one of the more complex resources to create in
    ECS as the cluster will need to perform a number of interactions with other AWS
    services. We can create a complete custom policy for it or import the policies
    AWS created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now tie our role with the instance profile as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to create our launch configuration. The following code snippet
    shows what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we don't install Ansible like we did before. Instead, we are
    using an ECS- optimized AMI (you can read more about this at [http://amzn.to/2jX0xVu](http://amzn.to/2jX0xVu))
    that lets us use the `UserData` field to configure the ECS service, and then starting
    it. Now that we have our launch configuration, we can create our Auto Scaling
    Group resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with ECS, scaling is needed at two levels:'
  prefs: []
  type: TYPE_NORMAL
- en: The containers level, as we will need to run more containers of a given service
    if the traffic spikes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The underlying infrastructure level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Containers, through the intermediary of their task definitions, set a requirement
    for CPU and memory. They will require, for example, 1024 CPU units, which represents
    one core, and 256 memory units, which means 256 MB of RAM. If the ECS instances
    are close to being filled up on one of those two constraints, the ECS Auto Scaling
    Group needs to add more instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a4a4d06-7d7b-46d5-88f7-dfe4d3e19d6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In terms of implementation, the process is very similar to what we did in [Chapter
    6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml), *Scaling Your Infrastructure*.
    Here, we first create the Auto Scaling Group resource, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create scaling policies and alarms to monitor the CPU and memory
    reservation metrics. In order to accomplish that, we will take advantage of Python
    to generate our stack and create for loops as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will provide a small amount of resource information, namely the
    stack ID, the VPC ID, and the public subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: CloudFormation provides a number of pseudo-parameters, such as `AWS::StackName`.
    Throughout the chapter, we will rely on it to make our template generic enough
    to be used across different environments and services. In the preceding code,
    we created an ECR repository for our `helloworld` container. The name was generated
    by the stack creation command. If required, we can reuse that exact same template
    to create another repository for another container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script is now complete, and should look like the script at: [https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecs-cluster-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/ecs-cluster-cf-template.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, we can now commit our script and create our stack by first generating
    our template, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: To create our stack, we need three parameters; the key-pair, the VPC ID, and
    the subnets. In the previous chapters, we used the web interface to create those
    stacks. Here, we will look at how to get that information using the CLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the VPC ID and the subnet IDs, we can use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our stack by combining the preceding outputs. Since ECS clusters
    can run a variety of containers and a number of applications and services, we
    will aim for one ECS cluster per environment, starting with staging. In order
    to differentiate each environment, we will rely on the stack names. Consequently,
    it is important to call your `staging-cluster` stack, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We will now add a load balancer. In the previous chapter, we used an ELB for
    our Auto Scaling Group. Later, we also mentioned the existence of the ALB service.
    This time, we will create an ALB instance to proxy our application traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an ALB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, ECS provides an orchestrator that takes care of allocating
    the containers across our Auto Scaling Group. It also keeps track of which port
    each container uses and integrates with ALB so that our load balancer can correctly
    route the incoming traffic to all containers running a given service. ECS supports
    both the ELB and ALB services but the ALB gives more flexibility when working
    with containers. We will demonstrate how to create an ALB using CloudFormation
    through Troposphere.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by creating a new file and calling it `helloworld-ecs-alb-cf-template.py`.
    We will then put our usual import, and will create our template variable and add
    a description, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now going to create our security group. No surprises here; we are opening
    `TCP/3000` to the world, as we did in [Chapter 6](c54f64c9-e8a3-4eed-b68d-087ff40f8b1d.xhtml),
    *Scaling Your Infrastructure*, with the ELB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The main difference from what we did previously is that instead of starting
    with a parameter section and requesting, yet again, to provide the VPC ID and
    public subnets, we are taking advantage of the value that we exported before.
    When we launch this stack, we will call it `staging-alb`. The block of code inside
    the `ImportValue` parameter does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we get the name of our stack. We will launch that stack under the name
    `staging-alb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `Split` function breaks the stack name on the character `-`, meaning that
    we end up with [`staging`, `alb`].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `Select` function takes the first element of the list: staging.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `Join` function concatenates that element with the string `cluster-vpc-id`.
    In the end, we get `Import("staging-cluster-vpc-id")`, which is the name of the
    key we defined to export the VPC ID when we created our ECS cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c1f4d0bf-28f4-4c35-8f74-9a0faf3f1924.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now create our ALB. ALB, being more flexible and feature-rich than
    ELB, requires a bit more effort when it comes to configuration. ALB works through
    the intermediary of three different resources. The first one is the ALB resource,
    which handles incoming connections. On the opposite side, we can find the target
    groups, which are the resources used by the ECS clusters registered to those ALBs.
    Finally, in order to tie the two, we find the listener''s resources. We will first
    define our load balancer resource, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We use a very similar series of calls to the function to import our subnet as
    we did just before for the VPC ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create our target group and configure our health check, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will add the listener to connect our target group to our load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we will want to create two outputs. The first output is the target
    group. We will export its value so that our application can register to the group.
    The second output is the DNS record of the ALB. This will be the entry point to
    our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The file is now ready, and should look like the file at: [https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-alb-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-alb-cf-template.py).
    We can now generate our template and create our stack, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned, it is important to call the stack `staging-alb`, and that first
    word is used to import the VPC ID and subnets. The last stack we need is the creation
    of our container service.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our ECS hello world service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have an ECS cluster and a load balancer ready to take on traffic on one
    side and an ECR repository containing the image of our application on the other
    side. We now need to tie the two together. This is done by creating an ECS service
    resource. We will create a new file called `helloworld-ecs-service-cf-template.py`
    and start as usual with its imports, template variable creation, and template
    description:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Our template will take one argument, which is the tag of the image we want
    to deploy. Our repository currently only has one image tagged as the latest, but
    in the next section we will update our deployment pipeline and automatize the
    deployment of our service to ECS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In ECS, applications are defined by their task definitions. This is where we
    declare which repository to use to get our image, how much CPU and memory the
    application needs, and all other system properties such as port mapping, environment
    variables, mount points, and so on. We will keep our task definition minimal;
    in order to select the proper image, we will utilize the `ImportValue` function
    (we previously exported the repository name) combined with a `Join` function to
    craft the repository URL. We will require 32 MB of RAM and one-quarter of a core
    to run our application. Finally, we will specify that port `3000` needs to be
    mapped onto the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'As for most of the AWS managed services, the ECS service needs a certain set
    of permissions provided by the intermediary of a role. We will create that role
    and use the vanilla policy for the ECS service role, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We will complete the creation of our template with the addition of the ECS
    service resource, which ties the task definition, the ECS cluster, and the ALB
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, as always, we will output the template generated by our code using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The script is now ready and should look like the script at: [https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-service-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-ecs-service-cf-template.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now generate the template and create our stack, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few minutes, the stack should be created. We can circle back to the
    output of the ALB stack to get the URL of our newly deployed application and test
    its output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'This can also be confirmed from the browser, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3386d1b4-0477-4392-a767-430badf63775.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have completed the creation of our staging ECS environment. At this point,
    we can easily manually deploy new code to our staging, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the changes in the `helloworld` code, locally. For example, change `Hello
    World` to `Hello From Yogesh Raheja`, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/32772faa-db7f-42ee-ad93-f0b38472839f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Log in to the `ecr` registry, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Build your Docker container, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Pick a new unique tag, and use it to tag your image. For example, let''s suppose
    that your new tag is `foobar`, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Push the image to the `ecr` repository, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the ECS service CloudFormation stack, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the outputs after it updates, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The browser output also reflects the updated image response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f8c34e6-dd67-438f-a599-6502944443a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this sequence of events, we are going to automate the deployment process
    and create a new continuous integration/continuous deployment pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a CI/CD pipeline to deploy to ECS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we know, having the ability to continuously deploy code across our environments
    is a very powerful tool as it helps to break out those traditional Dev versus
    Ops silos and improve the velocity at which new code is being released. We created
    a pipeline that allows us to automatically deploy new changes from our Hello World
    application to our Auto Scaling Groups for staging and production. We will create
    a similar pipeline but, this time, it will deploy changes to ECS. Our ECS infrastructure
    will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf0104de-f636-4398-a66e-f602ed354865.png)'
  prefs: []
  type: TYPE_IMG
- en: Reusing the CloudFormation templates produced in the previous section will create
    a production environment identical to the staging one. Note that the `ecr` repository
    is meant to be unique for a given application, and therefore will share it across
    our environments. In addition, we will follow the best practices learned in [Chapter
    3](8a74da7b-0748-4b90-a3bc-58e853e820ec.xhtml), *Treating Your Infrastructure
    As Code*, and create our pipeline through a CloudFormation stack. Our first step
    will be to create an ECS cluster for production.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our production ECS cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thanks to the upfront work we did with our CloudFormation templates, adding
    a new environment will be trivial. We will start by launching a production ECS
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to wait for the creation of the stack to complete as we need to get
    some of the exported values from the cluster creation. We can run the following
    command to get our Terminal to hang until we can create our next stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'In the meantime, we create our ALB and wait for the creation process to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can create our service with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, our production environment should be working. We can get its
    URL by looking at the output of the ALB stack creation, and we can CURL the endpoint
    to ensure that the application is up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b95d0e1-3bb8-4ee5-a297-99d55463d50e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that our production environment is ready, we will look into automating the
    creation of containers. In order to accomplish that, we will rely on the CodeBuild
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Automating the creation of containers with CodeBuild
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS CodeBuild is a managed service geared toward compiling source code. It is
    comparable to Jenkins but since it's a managed service that conforms to AWS standards,
    it presents a different set of features and benefits. In our case, using CodeBuild
    over Jenkins will allow us to create containers without needing to spin up and
    manage an extra EC2 instance. The service also integrates well with CodePipeline,
    which, as before, will drive our process.
  prefs: []
  type: TYPE_NORMAL
- en: We will use CloudFormation through the intermediary of Troposphere to create
    our CodeBuild project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also create a new script and call it `helloworld-codebuild-cf-template.py`.
    We will start with our usual import, template variable creation, and description,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define a new role to grant the proper permissions to our CodeBuild
    project. The CodeBuild project will interact with a number of AWS services such
    as ECR, CodePipeline, S3, and CloudWatch logs. To speed up the process, we will
    rely on the AWS vanilla policies to configure the permissions. This gives us the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'CodeBuild projects require defining a number of elements. The first one we
    will define is the environment. This tells CodeBuild what type of hardware and
    OS we need to build our project, and what needs to be preinstalled. It will also
    let us define extra environment variables. We will use a Docker image provided
    by AWS, which will give us everything we need to get our work done. The Docker
    image comes with the AWS and Docker CLI preinstalled and configured. We will also
    define an environment variable to find our `ecr` repository endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: In CodeBuild, most of the logic is defined in a resource called a `buildspec`.
    The `buildspec` section defines the different phases of the build and what to
    run during those phases. It is very similar to the Jenkins file we created in
    Chapter 5, *Adding Continuous Integration and Continuous Deployment*. The `buildspec`
    section can be created as part of the CodeBuild project or added as a YAML file
    to the root directory of the projects that are being built. We will opt for the
    first option and define `buildspec` inside our CloudFormation template. We will
    create a variable and store a YAML string into it. Since it's going to be a multiline
    variable, we will use the Python triple quote syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first key-pair we need to specify is the version of the template. The current
    version of CodeBuild templates is `0.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal of our build process is to generate a new container image, tag it,
    and push it to the `ecr` repository. This will be done in three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-build**: This will generate the container image tag and log in to ECR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build**: This will build the new container image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-build**: This will push the new container image to ECR and update the
    `latest` tag to point to the new container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to easily understand what is in each container, we will tag them with
    the SHA of the most recent Git commit in the `helloworld` project. This will help
    in understanding what is in each container, as we will be able to run commands
    such as `git checkout <container tag>` or `git log <container tag>`. Due to how
    CodeBuild and CodePipeline are architected, getting this tag in CodeBuild requires
    a bit of work. We will need to run two complex commands as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one will extract the execution ID of the current code pipeline execution.
    This is achieved by combining the AWS CLI and the environment variables `CODEBUILD_BUILD_ID`
    and `CODEBUILD_INITIATOR`, which are defined by the CodeBuild service when a build
    starts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will use that execution ID to extract the artifact revision ID, which
    happens to be the commit SHA we are looking for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These commands use some of the most advanced features of the `--query` filter
    option. You can read more about this at the following link: [http://amzn.to/2k7SoLE](http://amzn.to/2k7SoLE).'
  prefs: []
  type: TYPE_NORMAL
- en: In CodeBuild, each command runs in its own environment, and therefore the easiest
    way to share data across steps is to use temporary files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right after the `buildspec` version definition, add the following to generate
    the first part of our pre-build phase and extract the tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Our tag is now present in the `/tmp/tag.txt` file. We now need to generate
    two files as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one will contain the argument for the `docker tag` command (this will
    be something like `<AWS::AccountId>.dkr.ecr.us-east-1.amazonaws.com/helloworld:<tag>`).
    To do that, we will take advantage of the environment variable defined earlier
    in our template.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second file will be a JSON file that will define a key-value pair with the
    tag. We will use that file a bit later when we work on deploying our new containers
    to ECS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After the previous commands, add the following commands to generate those files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'To conclude the `pre_build` section, we will log in to our `ecr` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define our build phase. Thanks to the `build_tag` file created
    earlier, the build phase will be straightforward. We will call the `docker build`
    command in a similar way to how we did in the first section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now add the `post_build` phase to complete the build. In this section,
    we will push the newly built container to our `ecr` repository as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the phases, one of the sections that is also defined in a `buildspec`
    is the `artifacts` section. This section is used to define what needs to be uploaded
    to S3 after the build succeeds, as well as how to prepare it. We will export the
    `build.json` file and set the `discard-paths` variable to true so we don''t preserve
    the `/tmp/` directory information. Finally, we will close our triple quote string
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our `buildspec` variable is defined, we can add our CodeBuild project
    resource. Through the instantiation of the project, we will set a name for our
    project, set its environment by calling the variable previously defined, set the
    service role, and configure the source and artifact resources, which define how
    to handle the build process and its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, we will conclude the creation of the script with the following `print`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Our script is now complete and should look like this: [https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codebuild-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codebuild-cf-template.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can save the file, add it to git, generate the CloudFormation template,
    and create our stack as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: In a matter of minutes, our stack will be created. We will now want to take
    advantage of it. To do so, we will turn to CodePipeline once again and create
    a brand new, container-aware pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our deployment pipeline with CodePipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use AWS CodePipeline to build a pipeline very similar to the one we
    created in Chapter 5, *Adding Continuous Integration and Continuous Deployment*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22a0888a-7630-48da-9601-20035de62882.png)'
  prefs: []
  type: TYPE_IMG
- en: We will start with a Source step where we will connect to GitHub and trigger
    new pipelines that run automatically when the code changes. After this, we will
    build a new container and push it to our `ecr` repository rely upon the CodeBuild
    project we just created. We will then deploy the new container to staging. To
    do that, we will use the CloudFormation integration provided by CodePipeline,
    combined with the `build.json` file produced in the `buildspec` section of our
    CodeBuild project. You may recall that our `helloworld` service templates take
    the tag to deploy as an argument. We will trigger a stack update action and override
    the default value for that parameter with what's defined in the `build.json` file.
    After that, we will add a manual approval step before triggering the same deployment
    again, but this time for production.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and updating CloudFormation templates through CodePipeline will require
    specifying the location of the template within the inputs. In order to easily
    provide it, we will first start by adding the CloudFormation template to our source.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the CloudFormation template to our code base
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ECS changes are driven by the task definition present in our `helloworld-ecs-service-
    cf.template` file. So far we have only stored our Python script in GitHub. We
    will have to make a special case for that template and store the JSON output of
    it so that CodePipeline can interact with our stack. We will add this file to
    our Git repository in a new directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Now that our template is present in our source, we can create our CloudFormation
    template for our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a CloudFormation template for CodePipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by creating a file called `helloworld-codepipeline-cf- template.py`
    inside EffectiveDevOpsTemplates locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start the script with our boilerplates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The first resource we will create is the S3 bucket that the pipeline will use
    to store all the artifacts produced by each stage. We will also turn on versioning
    on that bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now create the IAM roles needed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first role we are going to define will be for the CodePipeline service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'The second role will be used by the deploy stages to perform CloudFormation
    changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create our pipeline resource. We will first configure its name and
    specify the role **Amazon Resource Name** (**ARN**) of the role we just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we will reference the S3 bucket created earlier so that we have
    a place to store the different artifacts produced through the pipeline execution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define each stage of the pipeline. The CloudFormation structure
    reflects what we did previously using the web interface. Each stage has a unique
    name and is composed of actions. Each action is defined by a name, a category,
    a configuration, and, optionally, input and output artifacts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our first stage will be the GitHub stage, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: We will create a first artifact called `App` with the content of the repository.
    In order to avoid hardcoding any `OAuthToken`, we will configure the GitHub integration
    after creating the CloudFormation stack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Our next step will be to configure our build. As mentioned, we will simply
    call out to the CodeBuild stack we spawned up in the last section. We will store
    the output artifact under the name `BuildOutput`, meaning that we now have two
    artifacts: the `App` artifact and `BuildOutput`, which contains the `tag.json`
    file produced by CodeBuild:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: We will now create our staging deployment. Unlike before, we won't use CodeDeploy
    but will directly update our CloudFormation template. In order to accomplish that,
    we will need to provide the location of the template to the configuration of our
    action. Since we added it to our `helloworld` GitHub repository, we can reference
    it with the help of the `App` artifact. Our template is present under `<directory
    root>/templates/helloworld-ecs-service- cf.template`, which in turn means for
    CodePipeline `App::templates/helloworld-ecs-service-cf.template`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next trick in configuring our CloudFormation action relies on the fact
    that we can override the parameters provided for the stack. CloudFormation provides
    a couple of functions to help with dynamic parameters. You can read more about
    those at [http://amzn.to/2kTgIUJ](http://amzn.to/2kTgIUJ). We will focus on a
    particular one here: `Fn::GetParam`. This function returns a value from a key-value
    pair file present in an artifact. This is where we take advantage of the file
    we created in CodeBuild, as it will contain a JSON string in the format `{ "tag":
    "<latest git commit sha>" }`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'After the staging deployment completes, we will request a manual approval,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will create a last stage to run the production deployment. The
    code is exactly the same here as it is for staging, except for the name of the
    stage and the stack targeted by our configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Our pipeline resource has now been created. We can conclude the creation of
    our script by printing out our template:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The script is now ready to be used. It should look like the script at: [https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codepipeline-cf-template.py](https://github.com/yogeshraheja/EffectiveDevOpsTemplates/blob/master/helloworld-codepipeline-cf-template.py).'
  prefs: []
  type: TYPE_NORMAL
- en: We can now create our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Starting and configuring our CloudFormation stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will proceed as usual for the first part of our pipeline''s creation, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: We are using the `CAPABILITY_NAMED_IAM` capability in this case, as we are defining
    custom names at the IAM level.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will create our pipeline. However, a small catch is that we didn''t specify
    the GitHub credentials in the pipeline. This is because we don''t want to store
    it in clear text in GitHub. AWS offers a service within IAM to do encryption,
    but we won''t cover that in this book. Consequently, we will simply edit the pipeline
    the first time around, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open [https://console.aws.amazon.com/codepipeline](https://console.aws.amazon.com/codepipeline)
    in your browser
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your newly created pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Edit at the top
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the pen icon on the GitHub action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/506252fb-0441-4a3c-9b01-ee48aac15212.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Connect to GitHub on the right-hand-side menu and follow the steps
    to authorize AWS CodePipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your `helloworld` project in the repository step and the master branch
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Update, save the pipeline changes, and finally, Save and Continue
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a few seconds, your pipeline will trigger, and you should see your first
    deployment going through. This concludes the creation of our CI/CD pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5852e5b1-5dc4-4346-814f-96a5c982093c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will also be able to see all of the CloudFormation stack details on the
    AWS console with the `CREATE_COMPLETE` status, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fcd718ba-f6e2-4acc-90ec-f5d648d0aa71.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the concept of containers, using Docker and ECS.
    After exploring the basics of how Docker works, we created a container for our
    application. After running it locally, we created a new set of resources to run
    Docker containers on AWS. We did that using the DevOps best practices and used
    CloudFormation to generate our resources, treating our infrastructure as code.
    This allows us to keep those changes under source control. Resource-wise, we created
    an ECR repository to manage the different revisions of our containers. We also
    created two ECS clusters with auto scaling capabilities for staging and production,
    two ALBs to proxy the traffic to our containers, a set of tasks, and an ECS service,
    to configure and deploy our application.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we re-implemented a CI/CD pipeline. We did that by using CodeBuild,
    CodePipeline, and their integrations with CloudFormation.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue improving our systems and we will implement one of the last
    key characteristics of DevOps; measuring everything. By taking advantage of a
    number of features that are present in the different services that we use, and
    by coupling them with other AWS services (such as CloudWatch), we will be able
    to implement a monitoring strategy for our infrastructure and services.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Docker? List the important components of Docker Engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you install and configure the latest Docker CE on any supported platform/OS
    of your choice?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you create a Docker image and use the same image to create a web server
    container?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you create ECR and ECS using AWS webconsole to get familiar with ECS terminologies?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following links for further information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker Documentation**: [https://docs.docker.com](https://docs.docker.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Hub**: [https://hub.docker.com](https://hub.docker.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS CodeBuild**: [https://aws.amazon.com/codebuild/](https://aws.amazon.com/codebuild/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS CodePipeline**: [https://aws.amazon.com/codepipeline/](https://aws.amazon.com/codepipeline/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Elastic Container Service**: [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
