- en: Release Management – Continuous Delivery
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发布管理 – 持续交付
- en: Release management has been always the boring part of software development.
    It is the discussion where people from different teams (operations, management,
    development, and so on) put all the details together to plan how to deploy a new
    version of one of the apps from the company (or various others).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 发布管理一直是软件开发中最无聊的部分。这是一个讨论的过程，来自不同团队的人（运维、管理、开发等）会把所有细节整理在一起，计划如何部署公司某个应用（或多个应用）的新版本。
- en: 'This is usually a big event that happens at 4 a.m. in the morning, and it is
    a binary event: we either succeed in releasing the new version or we fail and
    have to roll back.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是一个发生在早上4点的大事件，而且是一个二元事件：我们要么成功发布新版本，要么失败并需要回滚。
- en: Stress and tension are the common denominators in these type of deployments,
    and above everything else, we are playing against the statistics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的部署中，压力和紧张是共同的因素，最重要的是，我们在与统计数据作斗争。
- en: In this chapter, you are going to learn how to create a continuous delivery
    pipeline and deploy a microservices-based system to update it, keeping all the
    lights on.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何创建一个持续交付管道，并部署基于微服务的系统来更新它，确保所有服务持续可用。
- en: 'We will specifically cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专门讨论以下主题：
- en: Playing against the statistics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与统计数据作斗争
- en: The test system
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试系统
- en: Setting up a continuous delivery pipeline for images
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为镜像设置持续交付管道
- en: Setting up Jenkins
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Jenkins
- en: Continuous delivery for your application
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的应用程序设置持续交付
- en: Playing against the statistics
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与统计数据作斗争
- en: 'We have spoken several times about the Big Bang event that a deployment is.
    This is something that I always try to avoid when I set up a new system: releases
    should be smooth events that can be done at any time without effort and you should
    be able to roll back within minutes with little to no effort.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们曾多次提到部署就像“大爆炸”事件。这是我在设置新系统时总是尽量避免的事情：发布应该是平稳的事件，可以随时轻松完成，而且应该能够在几分钟内轻松回滚。
- en: 'This might be a gigantic task, but once you provide a solid foundation to your
    engineers, marvelous things happen: they start being more efficient. If you provide
    a solid base that will give them confidence that within a few clicks (or commands)
    that they can return the system to a stable state, you will have removed a big
    part of the complexity of any software system.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个庞大的任务，但一旦你为工程师提供了坚实的基础，奇妙的事情就会发生：他们开始变得更加高效。如果你提供一个稳固的基础，使他们确信只需几次点击（或命令）就能将系统恢复到稳定状态，你就解决了任何软件系统中的大部分复杂性。
- en: 'Let''s talk about statistics. When we are creating a deployment plan, we are
    creating a system configured in series: it is a finite list of steps that will
    result in our system being updated:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈统计数据。当我们创建部署计划时，我们是在创建一个配置为串联的系统：这是一个有限的步骤列表，将导致我们的系统更新：
- en: Copy a JAR file into a server
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将JAR文件复制到服务器
- en: Stop the old Spring Boot app
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停止旧的Spring Boot应用
- en: Copy the properties files
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制属性文件
- en: Start the new Spring Boot app
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动新的Spring Boot应用
- en: 'If any of the steps fail, the whole system fails. This is what we call a series
    system: the failure of any of the components (steps) compromises the full system.
    Let''s assume that every step has a 1% failure ratio. 1% seems quite an acceptable
    number...until we connect them in a series. From the preceding example, let''s
    assume that we have a deployment with 10 steps. These steps have a 1% of failure
    rate which is equals to 99% of success rate or 0.99 reliability. Connecting them
    in a series means that the whole reliability of our system can be expressed as
    follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何步骤失败，整个系统都会失败。这就是我们所说的串联系统：任何一个组件（步骤）的失败都会影响整个系统。假设每个步骤的失败率为1%。1%的失败率似乎是一个可以接受的数字……直到我们将它们串联起来。从前面的例子来看，假设我们有一个包含10个步骤的部署。这些步骤的失败率为1%，即成功率为99%，或者说是0.99的可靠性。将它们串联起来意味着整个系统的可靠性可以表示如下：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This means that our system has a 90.43 percent success rate or, in other words,
    9.57 percent failure rate. Things have changed dramatically: nearly 1 in every
    10 deployments is going to fail, which, by any means, is quite far from the 1
    percent of the individual steps that we quoted earlier.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的系统成功率为90.43%，换句话说，失败率为9.57%。情况发生了巨大变化：每10次部署就有1次会失败，这与我们之前提到的每个单独步骤的1%失败率相差甚远。
- en: Nearly 10 percent is quite a lot for depending on the systems, and it might
    be a risk that we are not willing to take, so why don't we work to reduce this
    risk to an acceptable level? Why don't we shift this risk to a previous step that
    does not compromise production and we reduce our deployment to a simple switch
    (on/off) that we can disconnect at any time?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎 10% 的占比对于依赖系统来说已经相当多，这可能是我们不愿承担的风险，那么为什么不努力将这个风险降到一个可接受的水平呢？为什么不把这个风险转移到一个不会影响生产的前置步骤中，并且将部署简化为一个可以随时断开的简单开关（开/关）呢？
- en: These are two concepts called canary and blue green deployments, and we are
    going to study how to use them in Kubernetes so that we reduce the risk of our
    deployments failing, as well as the stress of the **big bang event** that a deployment
    means in traditional software development.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个概念叫做金丝雀发布（canary）和蓝绿部署（blue green deployments），我们将研究如何在 Kubernetes 中使用它们，从而降低部署失败的风险，减少传统软件开发中部署时发生的
    **大爆炸事件** 带来的压力。
- en: The test system
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试系统
- en: In order to articulate a continuous delivery pipeline, we need a system to play
    with, and after some talks and demos, I have developed one that I tend to use,
    as it has pretty much no business logic and leaves a lot of space to think about
    the underlying infrastructure.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现持续交付流水线，我们需要一个可以操作的系统，经过一些讨论和演示，我开发了一个我倾向于使用的系统，因为它几乎没有业务逻辑，并且留给我们很多思考底层基础设施的空间。
- en: 'I call the system **Chronos**, and as you can guess, its purpose is related
    to the management of time zones and formats of dates. The system is very simple:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我称这个系统为 **Chronos**，正如你所猜测的，它的目的是与时区和日期格式的管理有关。这个系统非常简单：
- en: '![](img/351f1bf2-923e-416b-8cba-ccdea3091760.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/351f1bf2-923e-416b-8cba-ccdea3091760.png)'
- en: 'We have three services:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有三个服务：
- en: An API aggregator
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 聚合器
- en: A service that translates a timestamp into a date in ISO format
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将时间戳转换为 ISO 格式日期的服务
- en: A service that translates a timestamp into a date in UTC format
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将时间戳转换为 UTC 格式日期的服务
- en: These services work in coordination to translate a timestamp into a date in
    different formats, but it is also open to extensions as we can aggregate more
    services to add more capabilities and expose them through the API Aggregator.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些服务协调工作，将时间戳转换为不同格式的日期，但它们也可以扩展，我们可以聚合更多服务以添加更多功能，并通过 API 聚合器暴露它们。
- en: Every service will be packed into a different Docker image, deployed as a Deployment
    in Kubernetes and exposed via Services (externals and internals) to the cluster
    and to the outer world in the case of the API Aggregator.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务都会打包成一个不同的 Docker 镜像，作为 Kubernetes 中的部署（Deployment）进行部署，并通过服务（包括外部和内部服务）暴露到集群和外部世界（如
    API 聚合器）。
- en: ISO date and UTC date services
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ISO 日期和 UTC 日期服务
- en: 'The ISO date service simply takes a timestamp and returns the equivalent date
    using the ISO format. Let''s take a look at its code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ISO 日期服务仅仅接收一个时间戳，并返回使用 ISO 格式表示的等效日期。让我们看看它的代码：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This service itself is very simple: it uses a library called moment and a framework
    called **hapi** to provide the ISO Date equivalent to a timestamp passed as a
    URL parameter. The language used to write the service is Node.js, but you don''t
    need to be an expert in the language; you should just be able to read JavaScript.
    As with every Node.js application, it comes with a `package.json` that is used
    to describe the project and its dependencies:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务本身非常简单：它使用一个名为 moment 的库和一个名为 **hapi** 的框架，通过传递一个 URL 参数中的时间戳来提供等效的 ISO
    日期。编写该服务的语言是 Node.js，但你不需要成为该语言的专家，只需能够阅读 JavaScript 即可。像所有 Node.js 应用一样，它配有一个
    `package.json` 文件，用于描述项目及其依赖项：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Some fields of the `package.json` are customized, but the important parts are
    the dependencies and the scripts sections.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`package.json` 的某些字段是自定义的，但重要部分是依赖项和脚本部分。'
- en: 'Now, one important file is left; the Dockerfile:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，剩下一个重要的文件；Dockerfile：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In order to test our service, let''s build the Docker image:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的服务，让我们构建 Docker 镜像：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After a few seconds (or a bit more), our image is ready to use. Just run it:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后（或者稍久一些），我们的镜像已经准备好使用。只需运行它：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And that''s it. In order to test it, use curl to get some results:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些。为了测试它，可以使用 curl 获取一些结果：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will return a JSON with the ISO Date representation of the timestamp passed
    as a URL parameter, as you can see in your terminal.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个 JSON，包含以 ISO 日期格式表示的时间戳，正如你在终端看到的那样。
- en: 'The UTC date service is very much the same but with different code and a different
    interface:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: UTC 日期服务基本相同，只是代码和接口不同：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see, there are some changes:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，做了一些改动：
- en: The port is `3001`
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端口是 `3001`
- en: The date returned is the UTC date (which is basically the ISO Date without timezone
    information)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回的日期是 UTC 日期（基本上是没有时区信息的 ISO 日期）
- en: 'We also have a Dockerfile, which is the same as for the ISO Date service, and
    a `package.json`, which is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个 Dockerfile，和 ISO 日期服务的相同，还有一个 `package.json`，如下所示：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'These are minor changes (just the description and name). In total, you should
    have these files in the UTC date service:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是小的改动（仅是描述和名称）。总的来说，你应该在 UTC 日期服务中拥有以下文件：
- en: Dockerfile (the same as ISO Date Service)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dockerfile（与 ISO 日期服务相同）
- en: '`index.js` with the code from earlier'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含之前代码的 `index.js`
- en: '`package.json`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`package.json`'
- en: If you want to make your life easier, just clone the repository at `git@github.com:dgonzalez/chronos.git`
    so that you have all the code ready to be executed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让自己的生活更轻松，只需克隆 `git@github.com:dgonzalez/chronos.git` 仓库，这样你就有了所有准备执行的代码。
- en: 'Now in order to test that everything is correct, build the Docker image:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了测试一切是否正确，构建 Docker 镜像：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And then run it:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行它：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once it is started, we should have our service listening on port `3001`. You
    can check this by executing curl, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动，我们应该有我们的服务在 `3001` 端口上监听。你可以通过执行 curl 来检查，如下所示：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This, in a manner similar to ISO Date Service, should return a JSON with the
    date but in a UTC format in this case.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似于 ISO 日期服务的方式，这应该返回一个 JSON 格式的日期，但这次是 UTC 格式。
- en: Aggregator service
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合服务
- en: The `aggregator` service is the microservice that, as the name indicates, aggregates
    the other two (or more) services and provides a front API for consumers so that
    all the logic behind the scenes gets encapsulated. Even though it is not perfect,
    this is a common pattern because it allows us to play with the idea of circuit
    breaking as well as manage the errors on a dedicated layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`aggregator` 服务是一个微服务，顾名思义，它聚合了其他两个（或更多）服务，并为消费者提供前端 API，使得所有幕后逻辑得以封装。虽然它并不完美，但这是一个常见模式，因为它允许我们使用断路器的概念并且在专用层中管理错误。'
- en: 'In our case, the service is quite simple. First, let''s take a look at the
    code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，服务相当简单。首先，让我们看一下代码：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In order to simplify the understanding of the code, we did not use promises
    or async/await in it at the cost of having a nested `callback` (which is quite
    simple to read).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化代码的理解，我们没有使用 promises 或 async/await，而是选择了嵌套的 `callback`（这很容易阅读）。
- en: 'Here are a few points to note from the preceding code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从前面的代码中需要注意的一些要点：
- en: We are calling the services by name (`utcdate-service` and `isodate-service`),
    leveraging the communication to the Kubernetes DNS
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过名称调用服务（`utcdate-service` 和 `isodate-service`），利用与 Kubernetes DNS 的通信
- en: Before returning, `aggregator` service issues a call to the two services and
    returns a JSON object with the aggregated information
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在返回之前，`aggregator` 服务会调用两个服务，并返回一个包含聚合信息的 JSON 对象
- en: In order to test this service, we would need to create DNS entries (or host
    entries) pointing to `isodate-service` and `utcdate-service`, which is harder
    than testing it in Kubernetes, so we will skip the testing for now.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个服务，我们需要创建指向 `isodate-service` 和 `utcdate-service` 的 DNS 条目（或主机条目），这比在
    Kubernetes 中测试更为复杂，因此我们暂时跳过测试。
- en: 'As with any node application, the `aggregator` service needs a `package.json`
    to install the dependencies and control a few aspects:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何 Node 应用一样，`aggregator` 服务需要一个 `package.json` 来安装依赖项并控制一些方面：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `package.json` is very important. The scripts block particularly instruct
    us on what to do when the `npm start` command is executed by the Docker container
    based on the image defined in the Dockerfile:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`package.json` 非常重要，特别是脚本块，它指示我们当 Docker 容器基于 Dockerfile 中定义的镜像执行 `npm start`
    命令时该做什么：'
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'By now, you should have three files:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该有三个文件：
- en: '`index.js`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index.js`'
- en: '`Dockerfile`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dockerfile`'
- en: '`package.json`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`package.json`'
- en: 'Build the `docker` container with the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令构建 `docker` 容器：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Check whether it works as expected:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 检查它是否按预期工作：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Even though the server won't be able to resolve requests because it does not
    know how to communicate with `isodate-service` and `utcdate-service`, it should
    start.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 即使服务器无法解析请求，因为它不知道如何与 `isodate-service` 和 `utcdate-service` 通信，它仍然应该启动。
- en: Pushing the images to Google Container Registry
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将镜像推送到 Google Container Registry
- en: 'So as of now, we have three images in our local repository:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的本地仓库中已有三个镜像：
- en: '`iso-date-service`'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iso-date-service`'
- en: '`utc-date-service`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`utc-date-service`'
- en: '`aggregator`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregator`'
- en: 'These three images live in your computer but unfortunately, our Kubernetes
    cluster in the GKE won''t be able to reach them. The solution for this problem
    is to push these images into a Docker registry that will be reachable by our cluster.
    Google Cloud provides us with a Docker registry, which is extremely convenient
    for using with GKE due to several reasons:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个镜像现在存在于你的计算机上，但不幸的是，我们在 GKE 中的 Kubernetes 集群将无法访问它们。解决这个问题的方法是将这些镜像推送到一个
    Docker 注册中心，这样我们的集群就能访问它们。谷歌云提供了一个 Docker 注册中心，非常适合与 GKE 配合使用，原因有很多：
- en: '**Data containment**: The data never leaves the Google network'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据隔离**：数据永远不会离开 Google 网络'
- en: '**Integration**: Services in GCP can interact with implicit authentication'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：GCP中的服务可以通过隐式认证进行交互'
- en: '**Automation**: This integrates with GitHub and other services so that we can
    build our images, automatically creating a pipeline of continuous delivery of
    images.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化**：这与 GitHub 和其他服务集成，使我们能够构建镜像，自动创建持续交付镜像的管道。'
- en: 'Before setting up a continuous delivery pipeline with Git, we are going to
    push the images manually in order to understand how it works. **Google Container
    Registry** (**GCR**) is replicated across the globe, so the first thing that you
    need to do is choose where you want to store your images:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置与 Git 的持续交付管道之前，我们将手动推送镜像，以便理解其工作原理。**Google 容器注册中心** (**GCR**) 在全球范围内都有复制，因此你需要做的第一件事是选择存储镜像的地方：
- en: '`us.gcr.io` hosts your images in the United States'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`us.gcr.io`在美国托管你的镜像'
- en: '`eu.gcr.io` hosts your images in the European Union'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eu.gcr.io`在欧盟托管你的镜像'
- en: '`asia.gcr.io` hosts your images in Asia'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`asia.gcr.io`在亚洲托管你的镜像'
- en: 'In my case, `eu.gcr.io` is the perfect match. Then, we need our project ID.
    This can be found by clicking on the project name in the top bar of the console:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，`eu.gcr.io`是完美的匹配。接下来，我们需要我们的项目ID。可以通过点击控制台顶部栏中的项目名称找到它：
- en: '![](img/eb52b1bc-ae8c-407b-a489-cd48f4d28360.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb52b1bc-ae8c-407b-a489-cd48f4d28360.png)'
- en: 'In my case, the project ID is `implementing-modern-devops`*.* Now, the third
    component is the name of the image that we already have. With these three components,
    we can build the name for our Docker images URL:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的案例中，项目ID是`implementing-modern-devops`*.* 现在，第三个组件是我们已经拥有的镜像名称。通过这三个组件，我们可以构建出
    Docker 镜像 URL 的名称：
- en: '`eu.gcr.io/isodate-service:1.0`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eu.gcr.io/isodate-service:1.0`'
- en: '`eu.gcr.io/utcdate-service:1.0`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eu.gcr.io/utcdate-service:1.0`'
- en: '`eu.gcr.io/aggregator:1.0`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eu.gcr.io/aggregator:1.0`'
- en: The 1.0 part is the version of our image. If not specified, the default is latest
    but we are going to version the images for traceability.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 1.0部分是我们镜像的版本。如果没有指定，默认是latest，但我们将为镜像打上版本号，以便追溯。
- en: 'Now it is time to tag our images as appropriate. First up is the ISO Date service:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候为我们的镜像打上合适的标签了。首先是 ISO 日期服务：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, there''s the UTC date service:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是 UTC 日期服务：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And finally, we have the `aggregator` service:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了`aggregator`服务：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is the mechanism that Docker uses to identify where to push the images:
    Docker reads the name of our image and identifies the URL to which the image is
    to be pushed. In this case, as we are using a private registry (Google Container
    Registry is private), we need to use credentials, but using the `gcloud` command,
    it becomes quite easy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Docker 用来识别镜像推送地址的机制：Docker 读取我们的镜像名称，并识别出镜像要推送到的 URL。在这种情况下，由于我们使用的是私有注册中心（Google
    容器注册中心是私有的），我们需要使用凭据，但使用 `gcloud` 命令，操作变得相当简单：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now it''s time for the `isodate-service`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候处理`isodate-service`了：
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And finally, there''s `utcdate-service`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里是`utcdate-service`：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Be careful; the project ID will change, so customize the command to fit your
    configuration.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 小心，项目ID会发生变化，因此请根据你的配置自定义命令。
- en: After a bit of time (it can take up to a few minutes to push the three images
    to GCR), the images should be up in our private instance of the Google Container
    Registry.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在稍等片刻之后（将镜像推送到 GCR 可能需要几分钟时间），这些镜像应该已经上传到我们私有的 Google 容器注册中心实例中了。
- en: '![](img/c7094dba-e9d3-4cd4-aa95-f0a96b5bc299.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7094dba-e9d3-4cd4-aa95-f0a96b5bc299.png)'
- en: 'Let''s recap what we have done:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们所做的：
- en: We've built the images locally
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经在本地构建了镜像
- en: We've tagged the images with the appropriated name so that we can push them
    to GCR
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们给镜像打上了适当的标签，以便能够将它们推送到 GCR
- en: We've pushed the images to GCR using `gcloud`
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`gcloud`推送了镜像到 GCR
- en: This is fairly straightforward, but it can be tricky if you have not done it
    before. All our images are sitting in our private container registry, ready to
    be used.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常直接，但如果你以前没有做过，可能会有点棘手。我们所有的镜像都保存在我们的私人容器注册表中，随时可以使用。
- en: Setting up a continuous delivery pipeline for images
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置镜像的持续交付流水线
- en: 'Now that we have deployed our images to GCR, we need to automate the process
    so that we minimize the manual intervention. In order to do that, we are going
    to use the Build Triggers section of our Google Container Registry. In this case,
    we are going to use GitHub as it is the industry standard for Git repositories
    management. Create an account at [https://www.github.com](https://www.github.com)
    (if you don''t have one already) and then create three repositories:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已将镜像部署到 GCR，我们需要自动化这个过程，以最小化人工干预。为此，我们将使用 Google 容器注册表中的构建触发器部分。在这种情况下，我们将使用
    GitHub，因为它是业界标准的 Git 代码库管理工具。创建一个[https://www.github.com](https://www.github.com)账号（如果你还没有的话），然后创建三个代码库：
- en: '`aggregator`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggregator`'
- en: '`isodate-service`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`isodate-service`'
- en: '`utcdate-service`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`utcdate-service`'
- en: These can be public but, in the future, if you are working with private code,
    you should either create private repositories (which you need to pay for) or select
    a different provider, such as the source code repositories in Google Cloud Platform.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可以是公开的，但如果你将来要处理私有代码，应该创建私有代码库（需要付费）或选择其他提供商，比如 Google Cloud Platform 中的源代码库。
- en: 'The first thing that we need to do is push the code for the three services
    into the repositories. Github will give you the instructions to that, but basically,
    the process is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是将三个服务的代码推送到代码库中。Github 会提供相关指令，基本上，过程如下：
- en: Clone the repository
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆代码库
- en: Add the code from the preceding section as appropriate
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要添加前述部分的代码
- en: Push the code into the remote repository
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代码推送到远程代码库
- en: 'My GitHub username is `dgonzalez` and the commands to push the code for the
    `aggregator` are as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我的 GitHub 用户名是`dgonzalez`，推送`aggregator`代码的命令如下：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now copy the code from the `aggregator` into the newly created folder with
    the `clone` command and execute (inside the `aggregator` folder):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将`aggregator`中的代码复制到新创建的文件夹中，使用`clone`命令并执行（在`aggregator`文件夹内）：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Commit the changes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 提交更改：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And then `push` them to the remote repository:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将它们`push`到远程代码库：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After these commands, your repository should look like what is shown in the
    following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行这些命令后，你的代码库应该会像下面的截图所示：
- en: '![](img/12ee0676-400d-493d-bb9c-43255664f4ba.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12ee0676-400d-493d-bb9c-43255664f4ba.png)'
- en: The commands that we used are quite basic Git commands. You probably know about
    Git, but if you don't, I would recommend that you follow some tutorials, such
    as [https://try.github.io/levels/1/challenges/1](https://try.github.io/levels/1/challenges/1).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的命令是非常基础的 Git 命令。你可能对 Git 已经有所了解，但如果你不熟悉，我建议你跟随一些教程，比如[https://try.github.io/levels/1/challenges/1](https://try.github.io/levels/1/challenges/1)。
- en: 'Now that we have our repository ready, it is time to go back to GCP to set
    up the Build triggers for our pipeline. The first thing that we need to do is
    go to the triggers section of the Container Registry in Google Cloud Platform:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的代码库已经准备好了，是时候回到 GCP 设置我们流水线的构建触发器了。我们需要做的第一件事是进入 Google Cloud Platform
    的容器注册表中的触发器部分：
- en: '![](img/a20a64c6-ebc1-4b2d-95d6-06da69d2577e.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a20a64c6-ebc1-4b2d-95d6-06da69d2577e.png)'
- en: 'This functionality allows us to create triggers that fire off the build of
    our images based on events. There are several approaches to triggering strategies.
    In this case, we are going to build the image based on the creation of new tags.
    The most common way of doing this is by monitoring changes in the master branch,
    but I am a big fan of versioning. Think about this: containers are immutable artifacts:
    Once created, they should not be altered, but what if there is an issue with the
    code inside the container? The strategy is to branch off from the master and then
    create what is called a hot-fix build. With tagging, we can do this too but by
    branching from a tag instead of from the master, which has the following benefits:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能允许我们创建触发器，这些触发器会根据事件触发镜像的构建。有几种触发策略。在这种情况下，我们将基于新标签的创建来构建镜像。最常见的做法是监控主分支的变化，但我非常支持版本管理。想一想：容器是不可变的工件：一旦创建，就不应更改，但如果容器内部的代码出现问题怎么办？这个策略是从主分支分支出来，然后创建所谓的热修复构建。通过标签，我们也可以做到这一点，只不过是从标签而不是从主分支进行分支，这有以下好处：
- en: The master can change without firing events
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主分支可以更改，而不会触发事件
- en: Tags cannot be accidentally created (so no accidental releases)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签不能被意外创建（因此不会出现意外发布）
- en: Version is kept in the source code manager instead of in the code
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本保存在源代码管理器中，而不是在代码中
- en: You can correlate a tag to a build the artifact
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以将标签与构建的工件关联起来。
- en: 'That said, it is perfectly valid to use the master as a reference point and
    other combinations: the important lesson here is to stick to a procedure and make
    it clear to everyone.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，完全可以将主分支作为参考点并使用其他组合：这里的重要教训是坚持一个程序并让每个人都清楚。
- en: 'Click on **Create Trigger** and select GitHub. Once you click on Next, it will
    let you select the project from a list; then, click on Next again. Now we get
    presented with a form and a few options:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**创建触发器**，然后选择GitHub。点击“下一步”后，它会让你从列表中选择项目；然后再次点击“下一步”。现在我们会看到一个表单和几个选项：
- en: '![](img/fccfa568-afa3-4ed3-8eb7-297b79f719e7.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fccfa568-afa3-4ed3-8eb7-297b79f719e7.png)'
- en: We are going to use the Dockerfile instead of `cloudbuild.yaml` (the latter
    is GCP-specific) and set the trigger on the tag; the image name has to match the
    repositories created in the preceding section (remember the `eu.*` name and check
    the name of the repository as well).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Dockerfile而不是`cloudbuild.yaml`（后者是GCP特有的）并设置触发器以便在标签上触发；镜像名称必须与前面章节中创建的仓库相匹配（记住`eu.*`名称并检查仓库名称）。
- en: 'Once created, nothing happens. Our repository has no tags, so nothing has been
    built. Let''s create a tag:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，什么也不会发生。我们的仓库没有标签，因此没有构建任何内容。让我们创建一个标签：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will create a tag, and now we need to push it to the server:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个标签，现在我们需要将其推送到服务器：
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we go back to GCP Container Registry and check what happened: a new build
    has been triggered, pushing version 1.0 to the registry for the `aggregator` image:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们回到GCP容器注册表，查看发生了什么：一个新的构建已被触发，将版本1.0推送到`aggregator`镜像的注册表中：
- en: '![](img/4a9568a4-5124-4dbf-aa40-0a16cbf4dd73.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a9568a4-5124-4dbf-aa40-0a16cbf4dd73.png)'
- en: From now on, if we create a new tag in our repository, GCP is going to build
    an image for us, which can be correlated to a commit in GitHub so that we can
    fully trace what is in every environment of our build. It does not get better
    than this.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，如果我们在仓库中创建新的标签，GCP将为我们构建镜像，并且可以与GitHub中的提交相关联，这样我们就能完全追踪到我们构建的每个环境中的内容。没有比这更好的了。
- en: This whole build and push could have been done with Jenkins, as you learned
    in the chapter 4 (continous integration), but I am of the opinion that if someone
    can take care of your problems for a reasonable price, it's better than solving
    them yourself and add more moving parts to your already complex system. In this
    case, the registry, the build pipeline, and the automation are taken care of by
    Google Cloud Platform.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 整个构建和推送过程本可以通过Jenkins完成，正如你在第4章（持续集成）中所学到的那样，但我认为如果有人能够以合理的价格处理你的问题，胜过自己去解决它们并在已经复杂的系统中增加更多的可变因素。在这种情况下，注册表、构建流水线和自动化由Google
    Cloud Platform负责。
- en: Setting up Jenkins
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Jenkins
- en: 'In the preceding section, we leveraged the image operations to Google Cloud
    Platform, but now, we need to manage Kubernetes in a CI/CD fashion from somewhere.
    In this case, we are going to use Jenkins for this purpose. We have several options
    here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们利用了Google Cloud Platform的镜像操作，但现在，我们需要从某个地方以CI/CD方式管理Kubernetes。在这种情况下，我们将使用Jenkins来实现这个目的。我们在这里有几种选择：
- en: Deploy Jenkins in Kubernetes
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署 Jenkins
- en: Install Jenkins in baremetal
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在裸机上安装 Jenkins
- en: Install Jenkins in a container outside of Kubernetes
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 外部的容器中安装 Jenkins
- en: Lately, Jenkins has become Kubernetes-friendly with a plugin that allows Jenkins
    to spawn slaves when required in a containerized fashion so that it leverages
    the provisioning and destruction of hardware to Kubernetes. This is a more than
    interesting approach when your cluster is big enough (50+ machines), but when
    your cluster is small, it may be problematic as it can lead into a noisy neighborhood.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Jenkins 变得对 Kubernetes 更加友好，提供了一个插件，可以在需要时以容器化的方式启动 Jenkins 的从节点，从而将硬件的配置和销毁交给
    Kubernetes 来管理。当你的集群足够大（50 台以上机器）时，这是一种非常有趣的方式，但如果集群较小，这可能会导致邻居噪音问题。
- en: 'I am a big fan of segregation: CI/CD should be able to talk to your production
    infrastructure but should not be running in the same hardware for two reasons:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常推崇分离原则：CI/CD 应该能够与生产基础设施进行通信，但出于两个原因，它不应与生产硬件共用一台机器：
- en: Resource consumption
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源消耗
- en: Vulnerabilities
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 漏洞
- en: 'Think about it: a CI/CD software is, by default, vulnerable to attackers as
    it needs to execute commands through an interface; therefore, you are giving access
    to the underlying infrastructure to a potential attacker.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想：CI/CD 软件默认情况下对攻击者是脆弱的，因为它需要通过界面执行命令；因此，你实际上是将底层基础设施的访问权限交给了潜在的攻击者。
- en: 'My advice: start simple. If the company is small, I would go for Jenkins in
    a container with a volume mounted and evolve the infrastructure up to a point
    where your cluster is big enough to accommodate Jenkins without a significant
    impact; move it into your cluster in a dedicated namespace.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是：从简单开始。如果公司规模较小，我会选择将 Jenkins 部署在容器中并挂载卷，逐步发展基础设施，直到集群足够大，可以容纳 Jenkins
    而不会对性能造成显著影响；然后将其移入专用的命名空间中。
- en: In the chapter 4 (Continuous Integration), we set up Jenkins in a container
    without making use of any volume, which can be problematic as the configuration
    might be lost across restarts. Now, we are going to set up Jenkins in bare metal
    so that we have another way of managing Jenkins.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 章（持续集成）中，我们在没有使用任何卷的情况下将 Jenkins 部署在容器中，这可能会带来问题，因为配置可能会在重启后丢失。现在，我们将把
    Jenkins 安装在裸机上，这样就有了另一种管理 Jenkins 的方式。
- en: The first thing that we need to do is create a machine in GCP.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一台 GCP 机器。
- en: '![](img/1c198a4a-eeb8-400a-a43c-622afad3885c.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c198a4a-eeb8-400a-a43c-622afad3885c.png)'
- en: 'The preceding screenshot is the configuration of my Jenkins machine. A few
    important aspects are as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图是我 Jenkins 机器的配置。以下是几个重要的方面：
- en: Ubuntu instead of Debian (I selected the latest LTS version of Ubuntu)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ubuntu 而不是 Debian（我选择了最新的 Ubuntu LTS 版本）
- en: Small instance (we can scale that later on)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型实例（我们可以稍后扩展）
- en: Changes might need to be done to the firewall in order to access Jenkins
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能需要更改防火墙设置，以便能够访问 Jenkins
- en: Everything else is standard. We are not attaching a static IP to Jenkins as
    this is just a demo, but you probably want to do that, as you learned earlier,
    as well as have an entry in the DNS that can have a static reference point for
    your CI server.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其他配置都是标准的。我们没有为 Jenkins 配置静态 IP，因为这只是一个演示，但你可能希望配置静态 IP，就像你之前学到的那样，并在 DNS 中添加一个条目，以便为你的
    CI 服务器提供一个静态的参考点。
- en: It also would be a good exercise to do this in Terraform as well so that you
    can manage your infrastructure in an **Infrastructure as code** (**IaC**) fashion.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 将其以 Terraform 的方式执行也是一个不错的练习，这样你就可以以**基础设施即代码**（**IaC**）的方式管理基础设施。
- en: Once the machine has spun up, it is time to install Jenkins. We are going to
    follow the official guide, which can be found at [https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Ubuntu](https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Ubuntu).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦机器启动，就可以安装 Jenkins。我们将按照官方指南进行操作，指南链接如下：[https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Ubuntu](https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Ubuntu)。
- en: 'Using the web SSH Terminal from the Google Cloud platform, open a shell to
    your newly created machine and execute the following commands:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Google Cloud 平台的 Web SSH 终端，打开新创建机器的 shell 并执行以下命令：
- en: '[PRE29]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, add the Jenkins repository:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，添加 Jenkins 仓库：
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, update the list of packages:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，更新软件包列表：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And finally, install Jenkins:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，安装 Jenkins：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'That''s it. Once the preceding command is finished, Jenkins should be installed
    and can be started, stopped, and restarted as a service. To ensure that it is
    running, execute the following command:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。一旦前面的命令完成，Jenkins 应该已经安装好，可以作为服务启动、停止和重启。为了确保它在运行，执行以下命令：
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now if we browse the public IP in our server on port `8080`, we get the initial
    screen for Jenkins.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们浏览服务器上的公共 IP 和端口 `8080`，我们将看到 Jenkins 的初始屏幕。
- en: You might need to tweak the firewall to allow access to port `8080` on this
    machine.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要调整防火墙以允许访问该机器上的端口`8080`。
- en: 'This initial screen is familiar, and we need to get the password to initialize
    Jenkins. This password is in the logs:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个初始屏幕很熟悉，我们需要获取密码以初始化 Jenkins。这个密码在日志中：
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Enter the password and initialize Jenkins (suggested plugins). This might take
    a while; meanwhile, we also need to set up the Gcloud SDK. First, switch to the
    user Jenkins:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输入密码并初始化 Jenkins（建议的插件）。这可能需要一些时间；与此同时，我们还需要设置 Gcloud SDK。首先，切换到 Jenkins 用户：
- en: '[PRE35]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And then just execute the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后只需执行以下命令：
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once the installation finishes. you need to open a new shell for the changes
    to take effect. Do that and install `kubectl`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你需要打开一个新的 shell 以使更改生效。这样做并安装 `kubectl`：
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now that we have the `Kubectl` binary in our system, we need to connect it to
    a cluster, but first, it's time to create a cluster. As you learned in previous
    chapters, just create a cluster with three machines of a small size. Once it is
    created, connect to the cluster from the Jenkins machine, as shown in the previous
    chapter, but first, run `gcloud init` to configure a new `auth` session (option
    2) with your account.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们系统中有了 `Kubectl` 二进制文件，我们需要将其连接到一个集群，但首先是时候创建一个集群了。正如你在前几章中学到的，只需创建一个包含三台小型机器的集群。创建完成后，如前一章所示，连接
    Jenkins 机器到集群，但首先，运行 `gcloud init` 来配置一个新的 `auth` 会话（选项 2），使用你的帐户。
- en: 'Once you are done, make sure that `kubectl` can talk to your cluster by executing
    a test command, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，确保 `kubectl` 可以与集群通信，通过执行以下测试命令：
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You should list the three nodes that compound your cluster. Now we need to
    make `kubectl` accessible to the user `jenkins`. Just run the following command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该列出组成你集群的三个节点。现在我们需要让 `kubectl` 对 `jenkins` 用户可用。只需运行以下命令：
- en: '[PRE39]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Change the owner to Jenkins:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有者更改为 Jenkins：
- en: 'Now, going back to Jenkins, set up the admin user as shown in the following
    screenshot:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到 Jenkins，按照以下截图设置管理员用户：
- en: '![](img/bf32a1e9-cc5f-40d5-81c0-576c61610ef9.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf32a1e9-cc5f-40d5-81c0-576c61610ef9.png)'
- en: Click on Save and Finish, and it's done.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 点击保存并完成，就完成了。
- en: 'Before we start creating jobs, we need to make the binary kubectl available
    to the user `jenkins`. Login as root and execute:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始创建作业之前，我们需要让 `jenkins` 用户可以使用二进制文件 kubectl。以 root 用户登录并执行：
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This will make sure that the `kubectl` command for `jenkins` points to the SDK
    installed by the `jenkins` user in the preceding steps.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保 `jenkins` 的 `kubectl` 命令指向前面步骤中 `jenkins` 用户安装的 SDK。
- en: 'Now, we have everything:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一切：
- en: Jenkins
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jenkins
- en: The Google Cloud SDK
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud SDK
- en: A GKE cluster
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 GKE 集群
- en: A connection between Jenkins and GKE
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jenkins 与 GKE 之间的连接
- en: 'Before proceeding, we are going to make sure that everything works as expected.
    Go to Jenkins and create a new free style project and add a build step with the
    following command:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将确保一切按预期工作。进入 Jenkins 创建一个新的自由风格项目，并添加一个构建步骤，使用以下命令：
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Save the project and run it. The output should be very similar to what is shown
    in the following screenshot:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 保存项目并运行。输出应该与以下截图非常相似：
- en: '![](img/e63279c7-568e-4da9-b303-3dcd3e054129.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e63279c7-568e-4da9-b303-3dcd3e054129.png)'
- en: This indicates that we are good to go.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示我们可以开始了。
- en: In general, Jenkins and other CI systems should never be exposed over the internet.
    Never. It only takes a weak password for someone to destroy your system if it
    is accessible to the public. In this case, as an illustrative example, we have
    not configured the firewall, but in your company, you should allow access only
    from the IP of your office.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Jenkins 和其他 CI 系统绝不应暴露在互联网上。绝不。只需要一个弱密码，如果公开访问，任何人都可以摧毁你的系统。在这个示例中，我们没有配置防火墙，但在你的公司，应该仅允许来自办公室
    IP 的访问。
- en: Continuous delivery for your application
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为你的应用程序实现持续交付
- en: 'Up until now, we have set up a few elements:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经设置了一些元素：
- en: A GitHub repository with our code (`aggregator`)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含我们代码的 GitHub 仓库（`aggregator`）
- en: A continuous delivery pipeline in GCP for our Docker image that gets fired once
    we tag the code
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes cluster
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jenkins connected to the preceding cluster
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are going to set up the continuous delivery pipeline for our code and
    the Kubernetes infrastructure. This pipeline is going to be actioned by a Jenkins
    job, which we will trigger manually.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that all that you have read about **Continuous Delivery**
    (**CD**) is about transparently shipping code to production without any human
    intervention, but here we are, with a few events that need manual steps in order
    to action the build. I have worked in some places where continuous delivery is
    triggered automatically by changes in the master branch of your repository, and
    after few incidents, I really believe that a manual trigger is a fair price to
    pay for having an enormous amount of control over the deployments.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: For example, when publishing the image, by creating a tag manually in order
    to build our image, we are adding a barrier so that no one accidentally commits
    code to master and publishes a version that might be unstable or, even worse,
    insecure. Now we are going to do something similar, but the job that releases
    our code is going to be actioned manually in Jenkins, so by controlling the access
    to Jenkins, we have an audit trail of who did what, plus we get role-based access
    control for free. We can assign roles to the people of our team, preventing the
    most inexperienced developers from creating a mess without supervision but still
    allowing enough agility to release code in an automated fashion.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that we need to do is create a repository that we are going
    to call `aggregator-kubernetes` in GitHub to host all our YAML files with the
    Kubernetes resources. We will do this for `utcdate-service` and `isodate-service`,
    but let's do the `aggregator` first.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have created our repository, we need to create our Kubernetes objects
    to deploy and expose the service. In short, our system is going to look like what
    is shown in the following diagram:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a07e42bb-57df-461d-9a7d-6507bfe87dfa.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: On the above picture, we can see the Kubernetes objects (*ReplicaSet* and *Service*)
    that we need to create for each application (deployments are omitted). In red,
    we can see the application itself. For now, we are focusing on the `aggregator`,
    so we need to create a `ReplicaSet` that is going to be managed by a Deployment
    and a Service of the `LoadBalancer` that is going to expose our API to the rest
    of the world through a `gcloud` load balancer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The first element that we need is our deployment:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This is nothing that we wouldn''t expect. It''s a simple deployment object
    with the image that our automatic build process has created for us (remember,
    we created a tag with the version 1.0...also remember to customize it to your
    project). In our new repository, `aggregator-kubernetes`, save this file under
    a folder called objects with the name `deployment.yaml`. Now it is time to create
    the service that is going to expose our application:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Again, it''s very straightforward: a service that exposes anything tagged with
    app: `aggregator-service` to the outside world via a load balancer in Google cloud.
    Save it inside the objects folder with the name `service.yaml`. Now it is time
    to commit the changes and push them to your GitHub repository:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And then, execute this:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And finally, look at this:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'By now, you have all the code for the infrastructure of the `aggregator` sitting
    in your GitHub repository with a layout similar to the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dadd3ced-77eb-4f4f-bd46-4c6b93ec62ad.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Inside the objects folder, you can find the two YAML files: `deployment.yaml`
    and `service.yaml`. We can run these files locally with `kubectl` (connecting
    them to the cluster first) in order to verify that they are working as expected
    (and I recommend that you do this).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to set up a Jenkins job to articulate our build. Create a new
    freestyle project in Jenkins with the following configuration:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddf3b25b-2d47-411c-bdb2-e5e22096a795.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: First, look at the GitHub repository. As you can see, it is creating an error,
    and that's is only because GitHub needs an SSH key to identify the clients. GitHub
    explains how to generate and configure such keys at [https://help.github.com/articles/connecting-to-github-with-ssh/](https://help.github.com/articles/connecting-to-github-with-ssh/).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Once you have added the credentials with the private key that was generated,
    the error should be removed (remember, the type of credentials is 'SSH key with
    username', and your username has to match the one in GitHub).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can play a lot with Cit options, such as creating a tag on every build
    in order to trace what is going in your system or even building from tags. We
    are going to build the master branch: no tags this time.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to add our only build step for this job:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddd76fe0-bbc2-43bb-b970-37c0e93d9813.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: As you learned in previous chapters, with `kubectl apply`, we can pretty much
    rule the world. In this case, we are adding our folder with the `yamls` as a parameter;
    therefore, `kubectl` is going to action on Kubernetes with the YAML definitions
    that we are going to create.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the job and run it. Once it finishes, it should be successful with a log
    similar to the following one:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c11d724-c5f9-4c03-8cf8-692944c61f8d.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: This job might fail a few times as there are many moving parts. By now, you
    have enough knowledge to troubleshoot the integration of these parts.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: That's it. Our Continuous Delivery (CD) pipeline is working. From now on, if
    we want to make changes to our `aggregator`, we just need to add/modify files
    to our code repository, tag them with a new version, modify our `aggregator-kubernetes`
    definitions to point to the new image, and kick off our Jenkins job.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two extra steps:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Create a tag
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kick off a job manually
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the price you pay for having a lot of control in our deployment but
    with a bit of a secret sauce: we are set for a great deployment flexibility, as
    we are going to see in the next section, but first, you should repeat the same
    exercise for `utcdate-service` and `isodate-service` so that we have our full
    system running. If you want to save a lot of time or check whether you are going
    in the right direction, check out my repository at [https://github.com/dgonzalez/chronos](https://github.com/dgonzalez/chronos).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Inside every service, there is a folder called definitions that contains the
    Kubernetes objects to make everything work.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful with the name of the services: the aggregator is expecting to be
    able to resolve `isodate-service` and `utcdate-service` from the DNS, so your
    Services (Kubernetes objects) should be named accordingly.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Regular release
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are all set; if you''ve completed the deployment of `utcdate-service`
    and `isodate-service`, a fully working system should be installed on Kubernetes.
    The way it works is very simple: When you get the URL of the `aggregator` in the
    `/dates/{timestamp}` path, replacing timestamp with a valid UNIX timestamp, the
    service will contact `utcdate-service` and `isodate-service` and get the timestamp
    converted into the UTC and ISO formats. In my case, the load balancer provided
    by Google Cloud Platform will lead to the URL: `http://104.155.35.237/dates/1111111111`.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'It will have the following response:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You can play around with it for a bit, but it is nothing fancy: just a simple
    demo system that makes microservices and their automation easy to understand.
    In this case, we are not running any test, but for a continuous delivery pipeline,
    testing is a must (we will talk about this later).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Now as the title of the section suggests, we are going to create a new version
    of our application and release it using our continuous delivery pipeline.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new version it is going to be very simple but quite illustrative. On the
    `aggregator`, replace `index.js` with the following code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In the highlighted part, we have added a new section to the return object that
    basically returns the raw timestamp. Now it is time to commit the changes, but
    first, let''s follow a good practice. Create a branch:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This is going to create a local branch called `raw-timestamp`. Now commit the
    changes created in the preceding code:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'And push the branch to GitHub:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'If we visit the GitHub interface now, we''ll notice that something has changed:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94a75462-4832-4a69-978c-77d086d82638.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'It is suggesting that we create a Pull requests. Basically, a pull request
    is a request to add code to a repository. Click on Compare & pull request and
    then add a description in the new form and click on Create pull request. This
    is the outcome:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff9cab24-1687-43ea-937e-3c057d53bd90.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: 'There are three tabs:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Conversation
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commits
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Files changed
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one is a list of comments by the participants. The second tab is the
    list of commits that they pushed into the server, and the third one is the list
    of changes in diff style with additions and deletions, where you can drop comments
    asking for changes or suggesting better ways of doing things. In big projects,
    the master branch is usually blocked, and the only way to push code into it is
    via pull requests in order to enforce the review of the code.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Once you are happy, click on Merge pull request and merge the code. This pushes
    the changes into the master branch (needs confirmation).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to create a tag. This can be done via the GitHub interface.
    If you click on the release link (beside the number of contributors above the
    list of files), it brings you to the releases page:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7b704c6-b5c7-49bc-8faa-dbe31290bc09.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: 'There, you can see the tag that we created earlier from the terminal and a
    button called Draft a new release. Click on it, and it will show a new form:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51671f01-b9e8-4222-a646-f0174fe0537c.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: 'Fill in the details, as shown here, and create the release. This creates a
    tag that is connected to our container registry in Google Cloud Platform, and
    by now (it is very quick), a new version of our image should be available:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20fb4c23-b1a4-472b-9243-5de53186b262.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: As you can see, there is a good level of control over what is going into our
    production buckets (registry and cluster). Now the only step left is to release
    the new version into Kubernetes. Go back to the repository called `aggregator-kubernetes`
    (we created it in the preceding section) and change the tag of the image in the
    `deployment.yaml` file from `eu.gcr.io/implementing-modern-devops/aggregator-service:1.0`
    to `eu.gcr.io/implementing-modern-devops/aggregator-service:2.0`. Be aware that
    the project needs to be tailored to your configuration.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, commit and push the changes (from `aggregator-kubernetes`
    folder):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now everything is ready. We are at the edge of the cliff. If we click on Run
    in the job that we created in Jenkins, the new version of the software is going
    to be deployed in Kubernetes with zero downtime (depending on your configuration,
    as seen earlier); we have the control. We can decide when is the best time to
    release, and we have an easy way to roll back: revert the changes and click on
    Run again in Jenkins.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Once you are comfortable with the changes, run the job that we created in Jenkins
    (in my case, `aggregator-kubernetes`).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'If you hit the same URL as earlier (`http://104.155.35.237/dates/1111111111`),
    the result should have changed a bit:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The new version is up. As you can imagine, this is a fairly powerful argument
    to adopt DevOps: release software transparently to the users with minimal effort
    (create a tag and run a job in Jenkins).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to execute the same deployment but using a
    technique called blue-green deployment, which consist on release the new version
    in a private mode running in the production environment in order for us to test
    the features before making them available to the general public.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployment
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to execute a blue-green deployment, first, we need to roll back to
    version 1.0\. Edit `deployment.yaml` in `aggregator-kubernetes`, adjust the image
    to the tag `1.0`, and push the changes to GitHub. Once that is done, run a job
    called `aggregator-kubernetes` in Jenkins, and there you go; we have rolled back
    to version 1.0\. Leave version 2.0 of the image in the registry as we are going
    to use it.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'A blue-green deployment is a technique used to release software to production
    that is not visible to the general public, so we can test it before making it
    available to everyone. Kubernetes makes this extremely simple: the only thing
    we need to do is duplicate the resources in `aggregator-``kubernetes` and assign
    to them different names and tags. For example, this is our `deployment-bluegreen.yaml`:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'And this is our `service-bluegreen.yaml`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As you can see, we have created a vertical slice of our app with a different
    set of selectors/tags; therefore, our original version is working, but now, we
    have a new service called `aggregator-service-bluegreen` that serves the new version
    of our application via a load balancer, which we can check via the Kubernetes
    interface (using the `kubectl proxy` command, as explained earlier):'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b9183a2-6b2c-4f68-be7b-27b4bb0da73a.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: 'If you play around the two external endpoints, you can see the difference:
    the new one is returning the raw payload as well as the dates in the ISO format
    and in UTC timezone (version 2.0), whereas the old one only returns the dates
    (version 1.0).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now in what we call the blue status: we are happy with our release and
    we are sure that our software works with our production configuration without
    affecting any of our customers. If there was any problem, no customers would have
    noticed it. Now it is time to go to the green phase. We have two options here:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Remove the `aggregator-bluegreen` deployment and all its children (ReplicaSet
    and pods as well as the `aggregator-service-bluegreen` service) and upgrade our
    base deployment (`aggregator`)
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the labels for the selector in the aggregator service and make it point
    to the new Pods
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, I am a big fan of the first option as it keeps things simple, but
    it is your choice; also it's a good time for experimenting. Changing the selector
    in the service has an immediate effect, and it is probably the easy route if you
    are in a hurry.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with complex systems, I always try to go over a blue-green deployment
    phase to remove stress from the team. Think about it: instead of thinking that
    everything is solid, you are actually verifying that everything works as expected
    with no surprises, so the psychological factor of the uncertainty is gone at the
    moment of release.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we are going to visit another type of release, which introduces
    a new pod into the running system so we get to expose it to the users but only
    to a subset of them, so if something goes wrong, it does not kill the system;
    it just produces some errors. Before proceeding, make sure that you return your
    cluster to the original status: just a deployment called `aggregator` with its
    pods (remove the blue-green deployment).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployment
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a story about the name of this type of deployment, which very interesting.
    Before all the gas detectors, miners used to bring a canary (the bird) into the
    mines, as they are extremely sensitive to dangerous gases. Everybody was working
    normally but keeping an eye on the bird. If, for some reason, the bird died, everybody
    would leave the mine in order to avoid getting poisoned or even killed.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what we are going to do: introduce a new version of our software,
    which will actually produce errors if there is any problem so we only impact a
    limited number of customers.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is done via the YAMl files using the selectors that our service
    is targeting but with a new version of our app. Before continuing, make sure that
    you have only one deployment called `aggregator` with two pods running the version
    1.0 of our app (as shown in the Regular Release section).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in `aggregator-kubernetes`, create a file (inside objects folder) with
    the following content:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Here''s an easy explanation: we are creating a new deployment, with only one
    Pod, with the same tags that the original deployment pods (`aggregator`) has;
    therefore, the `aggregator-service` is going to target this pod as well: three
    in total.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Push the changes to GitHub and run the job `aggregator-kubernetes`, which will
    apply this configuration to our cluster. Now open the endpoint that we used earlier
    for testing, in my case, `http://104.155.35.237/dates/1111111111`, and keep refreshing
    the URL a few times. Approximately, one-third of the requests should come back
    with the raw timestamp (new version of the app) and two-third should come back
    without it (version 1.0).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that everything went well via the Kubernetes dashboard by checking
    the `aggregator-service`:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbfa0659-b273-4808-80db-dccdf247f518.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: Here, you can see the newly created pod being targeted by our service. I usually
    leave this status for a few hours/days (depending on the release), and once I
    am happy, I remove the `canary` deployment and apply the configuration to the
    `aggregator` deployment. You can play with the number of replicas as well in order
    to change the percentage of the users that get the new version or even gradually
    increase the number of `canaries` and decrease the number of regular pods until
    the application is completely rolled out.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: This strategy is followed by big companies such as Google to release new features
    with a lot of success. I am a big fan of using it as starting point when the system
    is big enough (10 *+* pods running), but I would be reluctant to do that in a
    small system as the percentage of affected requests would be too big (33.3% in
    the preceding example).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter was pretty intense: we set up a CD pipeline as well as visited
    the most common release strategies, which, using Kubernetes, were within the reach
    of our hand. Everything was automated except a couple of checkpoints that were
    left on purpose so that we could control what was going in our system (just for
    peace of mind). This was the climax of the book: even though the examples were
    basic, they provided you with enough tools to set up something similar in your
    company in order to get the benefit of working with microservices but padding
    the operational overhead that they involve as well as facilitating the release
    of new versions.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will learn an important aspect of continuous delivery:
    monitoring. With the right monitoring in place we can remove a lot of stress from
    the releases so that our engineers are more confident on being able to catch errors
    early leading into smoother rollouts and a lower production bug count.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
