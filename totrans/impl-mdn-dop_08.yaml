- en: Release Management â€“ Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Release management has been always the boring part of software development.
    It is the discussion where people from different teams (operations, management,
    development, and so on) put all the details together to plan how to deploy a new
    version of one of the apps from the company (or various others).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is usually a big event that happens at 4 a.m. in the morning, and it is
    a binary event: we either succeed in releasing the new version or we fail and
    have to roll back.'
  prefs: []
  type: TYPE_NORMAL
- en: Stress and tension are the common denominators in these type of deployments,
    and above everything else, we are playing against the statistics.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you are going to learn how to create a continuous delivery
    pipeline and deploy a microservices-based system to update it, keeping all the
    lights on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will specifically cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Playing against the statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a continuous delivery pipeline for images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Jenkins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous delivery for your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing against the statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have spoken several times about the Big Bang event that a deployment is.
    This is something that I always try to avoid when I set up a new system: releases
    should be smooth events that can be done at any time without effort and you should
    be able to roll back within minutes with little to no effort.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This might be a gigantic task, but once you provide a solid foundation to your
    engineers, marvelous things happen: they start being more efficient. If you provide
    a solid base that will give them confidence that within a few clicks (or commands)
    that they can return the system to a stable state, you will have removed a big
    part of the complexity of any software system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s talk about statistics. When we are creating a deployment plan, we are
    creating a system configured in series: it is a finite list of steps that will
    result in our system being updated:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy a JAR file into a server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop the old Spring Boot app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the properties files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start the new Spring Boot app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If any of the steps fail, the whole system fails. This is what we call a series
    system: the failure of any of the components (steps) compromises the full system.
    Let''s assume that every step has a 1% failure ratio. 1% seems quite an acceptable
    number...until we connect them in a series. From the preceding example, let''s
    assume that we have a deployment with 10 steps. These steps have a 1% of failure
    rate which is equals to 99% of success rate or 0.99 reliability. Connecting them
    in a series means that the whole reliability of our system can be expressed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that our system has a 90.43 percent success rate or, in other words,
    9.57 percent failure rate. Things have changed dramatically: nearly 1 in every
    10 deployments is going to fail, which, by any means, is quite far from the 1
    percent of the individual steps that we quoted earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: Nearly 10 percent is quite a lot for depending on the systems, and it might
    be a risk that we are not willing to take, so why don't we work to reduce this
    risk to an acceptable level? Why don't we shift this risk to a previous step that
    does not compromise production and we reduce our deployment to a simple switch
    (on/off) that we can disconnect at any time?
  prefs: []
  type: TYPE_NORMAL
- en: These are two concepts called canary and blue green deployments, and we are
    going to study how to use them in Kubernetes so that we reduce the risk of our
    deployments failing, as well as the stress of the **big bang event** that a deployment
    means in traditional software development.
  prefs: []
  type: TYPE_NORMAL
- en: The test system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to articulate a continuous delivery pipeline, we need a system to play
    with, and after some talks and demos, I have developed one that I tend to use,
    as it has pretty much no business logic and leaves a lot of space to think about
    the underlying infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'I call the system **Chronos**, and as you can guess, its purpose is related
    to the management of time zones and formats of dates. The system is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/351f1bf2-923e-416b-8cba-ccdea3091760.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have three services:'
  prefs: []
  type: TYPE_NORMAL
- en: An API aggregator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A service that translates a timestamp into a date in ISO format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A service that translates a timestamp into a date in UTC format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These services work in coordination to translate a timestamp into a date in
    different formats, but it is also open to extensions as we can aggregate more
    services to add more capabilities and expose them through the API Aggregator.
  prefs: []
  type: TYPE_NORMAL
- en: Every service will be packed into a different Docker image, deployed as a Deployment
    in Kubernetes and exposed via Services (externals and internals) to the cluster
    and to the outer world in the case of the API Aggregator.
  prefs: []
  type: TYPE_NORMAL
- en: ISO date and UTC date services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ISO date service simply takes a timestamp and returns the equivalent date
    using the ISO format. Let''s take a look at its code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This service itself is very simple: it uses a library called moment and a framework
    called **hapi** to provide the ISO Date equivalent to a timestamp passed as a
    URL parameter. The language used to write the service is Node.js, but you don''t
    need to be an expert in the language; you should just be able to read JavaScript.
    As with every Node.js application, it comes with a `package.json` that is used
    to describe the project and its dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Some fields of the `package.json` are customized, but the important parts are
    the dependencies and the scripts sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, one important file is left; the Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to test our service, let''s build the Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds (or a bit more), our image is ready to use. Just run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And that''s it. In order to test it, use curl to get some results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This will return a JSON with the ISO Date representation of the timestamp passed
    as a URL parameter, as you can see in your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UTC date service is very much the same but with different code and a different
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, there are some changes:'
  prefs: []
  type: TYPE_NORMAL
- en: The port is `3001`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The date returned is the UTC date (which is basically the ISO Date without timezone
    information)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also have a Dockerfile, which is the same as for the ISO Date service, and
    a `package.json`, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'These are minor changes (just the description and name). In total, you should
    have these files in the UTC date service:'
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile (the same as ISO Date Service)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index.js` with the code from earlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`package.json`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to make your life easier, just clone the repository at `git@github.com:dgonzalez/chronos.git`
    so that you have all the code ready to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now in order to test that everything is correct, build the Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And then run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it is started, we should have our service listening on port `3001`. You
    can check this by executing curl, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This, in a manner similar to ISO Date Service, should return a JSON with the
    date but in a UTC format in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregator service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `aggregator` service is the microservice that, as the name indicates, aggregates
    the other two (or more) services and provides a front API for consumers so that
    all the logic behind the scenes gets encapsulated. Even though it is not perfect,
    this is a common pattern because it allows us to play with the idea of circuit
    breaking as well as manage the errors on a dedicated layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the service is quite simple. First, let''s take a look at the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In order to simplify the understanding of the code, we did not use promises
    or async/await in it at the cost of having a nested `callback` (which is quite
    simple to read).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few points to note from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We are calling the services by name (`utcdate-service` and `isodate-service`),
    leveraging the communication to the Kubernetes DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before returning, `aggregator` service issues a call to the two services and
    returns a JSON object with the aggregated information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to test this service, we would need to create DNS entries (or host
    entries) pointing to `isodate-service` and `utcdate-service`, which is harder
    than testing it in Kubernetes, so we will skip the testing for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any node application, the `aggregator` service needs a `package.json`
    to install the dependencies and control a few aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `package.json` is very important. The scripts block particularly instruct
    us on what to do when the `npm start` command is executed by the Docker container
    based on the image defined in the Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'By now, you should have three files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`index.js`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dockerfile`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`package.json`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Build the `docker` container with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Check whether it works as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Even though the server won't be able to resolve requests because it does not
    know how to communicate with `isodate-service` and `utcdate-service`, it should
    start.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the images to Google Container Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So as of now, we have three images in our local repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '`iso-date-service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`utc-date-service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aggregator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three images live in your computer but unfortunately, our Kubernetes
    cluster in the GKE won''t be able to reach them. The solution for this problem
    is to push these images into a Docker registry that will be reachable by our cluster.
    Google Cloud provides us with a Docker registry, which is extremely convenient
    for using with GKE due to several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data containment**: The data never leaves the Google network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration**: Services in GCP can interact with implicit authentication'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automation**: This integrates with GitHub and other services so that we can
    build our images, automatically creating a pipeline of continuous delivery of
    images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before setting up a continuous delivery pipeline with Git, we are going to
    push the images manually in order to understand how it works. **Google Container
    Registry** (**GCR**) is replicated across the globe, so the first thing that you
    need to do is choose where you want to store your images:'
  prefs: []
  type: TYPE_NORMAL
- en: '`us.gcr.io` hosts your images in the United States'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eu.gcr.io` hosts your images in the European Union'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`asia.gcr.io` hosts your images in Asia'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In my case, `eu.gcr.io` is the perfect match. Then, we need our project ID.
    This can be found by clicking on the project name in the top bar of the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb52b1bc-ae8c-407b-a489-cd48f4d28360.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In my case, the project ID is `implementing-modern-devops`*.* Now, the third
    component is the name of the image that we already have. With these three components,
    we can build the name for our Docker images URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '`eu.gcr.io/isodate-service:1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eu.gcr.io/utcdate-service:1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eu.gcr.io/aggregator:1.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 1.0 part is the version of our image. If not specified, the default is latest
    but we are going to version the images for traceability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to tag our images as appropriate. First up is the ISO Date service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, there''s the UTC date service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, we have the `aggregator` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the mechanism that Docker uses to identify where to push the images:
    Docker reads the name of our image and identifies the URL to which the image is
    to be pushed. In this case, as we are using a private registry (Google Container
    Registry is private), we need to use credentials, but using the `gcloud` command,
    it becomes quite easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time for the `isodate-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, there''s `utcdate-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Be careful; the project ID will change, so customize the command to fit your
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: After a bit of time (it can take up to a few minutes to push the three images
    to GCR), the images should be up in our private instance of the Google Container
    Registry.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7094dba-e9d3-4cd4-aa95-f0a96b5bc299.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s recap what we have done:'
  prefs: []
  type: TYPE_NORMAL
- en: We've built the images locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've tagged the images with the appropriated name so that we can push them
    to GCR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've pushed the images to GCR using `gcloud`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is fairly straightforward, but it can be tricky if you have not done it
    before. All our images are sitting in our private container registry, ready to
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a continuous delivery pipeline for images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have deployed our images to GCR, we need to automate the process
    so that we minimize the manual intervention. In order to do that, we are going
    to use the Build Triggers section of our Google Container Registry. In this case,
    we are going to use GitHub as it is the industry standard for Git repositories
    management. Create an account at [https://www.github.com](https://www.github.com)
    (if you don''t have one already) and then create three repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '`aggregator`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isodate-service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`utcdate-service`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These can be public but, in the future, if you are working with private code,
    you should either create private repositories (which you need to pay for) or select
    a different provider, such as the source code repositories in Google Cloud Platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we need to do is push the code for the three services
    into the repositories. Github will give you the instructions to that, but basically,
    the process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Clone the repository
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the code from the preceding section as appropriate
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Push the code into the remote repository
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'My GitHub username is `dgonzalez` and the commands to push the code for the
    `aggregator` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now copy the code from the `aggregator` into the newly created folder with
    the `clone` command and execute (inside the `aggregator` folder):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Commit the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And then `push` them to the remote repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After these commands, your repository should look like what is shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12ee0676-400d-493d-bb9c-43255664f4ba.png)'
  prefs: []
  type: TYPE_IMG
- en: The commands that we used are quite basic Git commands. You probably know about
    Git, but if you don't, I would recommend that you follow some tutorials, such
    as [https://try.github.io/levels/1/challenges/1](https://try.github.io/levels/1/challenges/1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our repository ready, it is time to go back to GCP to set
    up the Build triggers for our pipeline. The first thing that we need to do is
    go to the triggers section of the Container Registry in Google Cloud Platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a20a64c6-ebc1-4b2d-95d6-06da69d2577e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This functionality allows us to create triggers that fire off the build of
    our images based on events. There are several approaches to triggering strategies.
    In this case, we are going to build the image based on the creation of new tags.
    The most common way of doing this is by monitoring changes in the master branch,
    but I am a big fan of versioning. Think about this: containers are immutable artifacts:
    Once created, they should not be altered, but what if there is an issue with the
    code inside the container? The strategy is to branch off from the master and then
    create what is called a hot-fix build. With tagging, we can do this too but by
    branching from a tag instead of from the master, which has the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: The master can change without firing events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tags cannot be accidentally created (so no accidental releases)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version is kept in the source code manager instead of in the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can correlate a tag to a build the artifact
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That said, it is perfectly valid to use the master as a reference point and
    other combinations: the important lesson here is to stick to a procedure and make
    it clear to everyone.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Create Trigger** and select GitHub. Once you click on Next, it will
    let you select the project from a list; then, click on Next again. Now we get
    presented with a form and a few options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fccfa568-afa3-4ed3-8eb7-297b79f719e7.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to use the Dockerfile instead of `cloudbuild.yaml` (the latter
    is GCP-specific) and set the trigger on the tag; the image name has to match the
    repositories created in the preceding section (remember the `eu.*` name and check
    the name of the repository as well).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once created, nothing happens. Our repository has no tags, so nothing has been
    built. Let''s create a tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a tag, and now we need to push it to the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we go back to GCP Container Registry and check what happened: a new build
    has been triggered, pushing version 1.0 to the registry for the `aggregator` image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a9568a4-5124-4dbf-aa40-0a16cbf4dd73.png)'
  prefs: []
  type: TYPE_IMG
- en: From now on, if we create a new tag in our repository, GCP is going to build
    an image for us, which can be correlated to a commit in GitHub so that we can
    fully trace what is in every environment of our build. It does not get better
    than this.
  prefs: []
  type: TYPE_NORMAL
- en: This whole build and push could have been done with Jenkins, as you learned
    in the chapter 4 (continous integration), but I am of the opinion that if someone
    can take care of your problems for a reasonable price, it's better than solving
    them yourself and add more moving parts to your already complex system. In this
    case, the registry, the build pipeline, and the automation are taken care of by
    Google Cloud Platform.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Jenkins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding section, we leveraged the image operations to Google Cloud
    Platform, but now, we need to manage Kubernetes in a CI/CD fashion from somewhere.
    In this case, we are going to use Jenkins for this purpose. We have several options
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Jenkins in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Jenkins in baremetal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Jenkins in a container outside of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lately, Jenkins has become Kubernetes-friendly with a plugin that allows Jenkins
    to spawn slaves when required in a containerized fashion so that it leverages
    the provisioning and destruction of hardware to Kubernetes. This is a more than
    interesting approach when your cluster is big enough (50+ machines), but when
    your cluster is small, it may be problematic as it can lead into a noisy neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: 'I am a big fan of segregation: CI/CD should be able to talk to your production
    infrastructure but should not be running in the same hardware for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Resource consumption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vulnerabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think about it: a CI/CD software is, by default, vulnerable to attackers as
    it needs to execute commands through an interface; therefore, you are giving access
    to the underlying infrastructure to a potential attacker.'
  prefs: []
  type: TYPE_NORMAL
- en: 'My advice: start simple. If the company is small, I would go for Jenkins in
    a container with a volume mounted and evolve the infrastructure up to a point
    where your cluster is big enough to accommodate Jenkins without a significant
    impact; move it into your cluster in a dedicated namespace.'
  prefs: []
  type: TYPE_NORMAL
- en: In the chapter 4 (Continuous Integration), we set up Jenkins in a container
    without making use of any volume, which can be problematic as the configuration
    might be lost across restarts. Now, we are going to set up Jenkins in bare metal
    so that we have another way of managing Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that we need to do is create a machine in GCP.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c198a4a-eeb8-400a-a43c-622afad3885c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding screenshot is the configuration of my Jenkins machine. A few
    important aspects are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ubuntu instead of Debian (I selected the latest LTS version of Ubuntu)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small instance (we can scale that later on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes might need to be done to the firewall in order to access Jenkins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everything else is standard. We are not attaching a static IP to Jenkins as
    this is just a demo, but you probably want to do that, as you learned earlier,
    as well as have an entry in the DNS that can have a static reference point for
    your CI server.
  prefs: []
  type: TYPE_NORMAL
- en: It also would be a good exercise to do this in Terraform as well so that you
    can manage your infrastructure in an **Infrastructure as code** (**IaC**) fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Once the machine has spun up, it is time to install Jenkins. We are going to
    follow the official guide, which can be found at [https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Ubuntu](https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Ubuntu).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the web SSH Terminal from the Google Cloud platform, open a shell to
    your newly created machine and execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, add the Jenkins repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, update the list of packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, install Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it. Once the preceding command is finished, Jenkins should be installed
    and can be started, stopped, and restarted as a service. To ensure that it is
    running, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now if we browse the public IP in our server on port `8080`, we get the initial
    screen for Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: You might need to tweak the firewall to allow access to port `8080` on this
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'This initial screen is familiar, and we need to get the password to initialize
    Jenkins. This password is in the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter the password and initialize Jenkins (suggested plugins). This might take
    a while; meanwhile, we also need to set up the Gcloud SDK. First, switch to the
    user Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And then just execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the installation finishes. you need to open a new shell for the changes
    to take effect. Do that and install `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the `Kubectl` binary in our system, we need to connect it to
    a cluster, but first, it's time to create a cluster. As you learned in previous
    chapters, just create a cluster with three machines of a small size. Once it is
    created, connect to the cluster from the Jenkins machine, as shown in the previous
    chapter, but first, run `gcloud init` to configure a new `auth` session (option
    2) with your account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done, make sure that `kubectl` can talk to your cluster by executing
    a test command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You should list the three nodes that compound your cluster. Now we need to
    make `kubectl` accessible to the user `jenkins`. Just run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Change the owner to Jenkins:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, going back to Jenkins, set up the admin user as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf32a1e9-cc5f-40d5-81c0-576c61610ef9.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on Save and Finish, and it's done.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start creating jobs, we need to make the binary kubectl available
    to the user `jenkins`. Login as root and execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: This will make sure that the `kubectl` command for `jenkins` points to the SDK
    installed by the `jenkins` user in the preceding steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have everything:'
  prefs: []
  type: TYPE_NORMAL
- en: Jenkins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Google Cloud SDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GKE cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A connection between Jenkins and GKE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before proceeding, we are going to make sure that everything works as expected.
    Go to Jenkins and create a new free style project and add a build step with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the project and run it. The output should be very similar to what is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e63279c7-568e-4da9-b303-3dcd3e054129.png)'
  prefs: []
  type: TYPE_IMG
- en: This indicates that we are good to go.
  prefs: []
  type: TYPE_NORMAL
- en: In general, Jenkins and other CI systems should never be exposed over the internet.
    Never. It only takes a weak password for someone to destroy your system if it
    is accessible to the public. In this case, as an illustrative example, we have
    not configured the firewall, but in your company, you should allow access only
    from the IP of your office.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous delivery for your application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we have set up a few elements:'
  prefs: []
  type: TYPE_NORMAL
- en: A GitHub repository with our code (`aggregator`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A continuous delivery pipeline in GCP for our Docker image that gets fired once
    we tag the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jenkins connected to the preceding cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are going to set up the continuous delivery pipeline for our code and
    the Kubernetes infrastructure. This pipeline is going to be actioned by a Jenkins
    job, which we will trigger manually.
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that all that you have read about **Continuous Delivery**
    (**CD**) is about transparently shipping code to production without any human
    intervention, but here we are, with a few events that need manual steps in order
    to action the build. I have worked in some places where continuous delivery is
    triggered automatically by changes in the master branch of your repository, and
    after few incidents, I really believe that a manual trigger is a fair price to
    pay for having an enormous amount of control over the deployments.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when publishing the image, by creating a tag manually in order
    to build our image, we are adding a barrier so that no one accidentally commits
    code to master and publishes a version that might be unstable or, even worse,
    insecure. Now we are going to do something similar, but the job that releases
    our code is going to be actioned manually in Jenkins, so by controlling the access
    to Jenkins, we have an audit trail of who did what, plus we get role-based access
    control for free. We can assign roles to the people of our team, preventing the
    most inexperienced developers from creating a mess without supervision but still
    allowing enough agility to release code in an automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that we need to do is create a repository that we are going
    to call `aggregator-kubernetes` in GitHub to host all our YAML files with the
    Kubernetes resources. We will do this for `utcdate-service` and `isodate-service`,
    but let's do the `aggregator` first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have created our repository, we need to create our Kubernetes objects
    to deploy and expose the service. In short, our system is going to look like what
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a07e42bb-57df-461d-9a7d-6507bfe87dfa.png)'
  prefs: []
  type: TYPE_IMG
- en: On the above picture, we can see the Kubernetes objects (*ReplicaSet* and *Service*)
    that we need to create for each application (deployments are omitted). In red,
    we can see the application itself. For now, we are focusing on the `aggregator`,
    so we need to create a `ReplicaSet` that is going to be managed by a Deployment
    and a Service of the `LoadBalancer` that is going to expose our API to the rest
    of the world through a `gcloud` load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first element that we need is our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This is nothing that we wouldn''t expect. It''s a simple deployment object
    with the image that our automatic build process has created for us (remember,
    we created a tag with the version 1.0...also remember to customize it to your
    project). In our new repository, `aggregator-kubernetes`, save this file under
    a folder called objects with the name `deployment.yaml`. Now it is time to create
    the service that is going to expose our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, it''s very straightforward: a service that exposes anything tagged with
    app: `aggregator-service` to the outside world via a load balancer in Google cloud.
    Save it inside the objects folder with the name `service.yaml`. Now it is time
    to commit the changes and push them to your GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, execute this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'By now, you have all the code for the infrastructure of the `aggregator` sitting
    in your GitHub repository with a layout similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dadd3ced-77eb-4f4f-bd46-4c6b93ec62ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inside the objects folder, you can find the two YAML files: `deployment.yaml`
    and `service.yaml`. We can run these files locally with `kubectl` (connecting
    them to the cluster first) in order to verify that they are working as expected
    (and I recommend that you do this).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to set up a Jenkins job to articulate our build. Create a new
    freestyle project in Jenkins with the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddf3b25b-2d47-411c-bdb2-e5e22096a795.png)'
  prefs: []
  type: TYPE_IMG
- en: First, look at the GitHub repository. As you can see, it is creating an error,
    and that's is only because GitHub needs an SSH key to identify the clients. GitHub
    explains how to generate and configure such keys at [https://help.github.com/articles/connecting-to-github-with-ssh/](https://help.github.com/articles/connecting-to-github-with-ssh/).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have added the credentials with the private key that was generated,
    the error should be removed (remember, the type of credentials is 'SSH key with
    username', and your username has to match the one in GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can play a lot with Cit options, such as creating a tag on every build
    in order to trace what is going in your system or even building from tags. We
    are going to build the master branch: no tags this time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to add our only build step for this job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddd76fe0-bbc2-43bb-b970-37c0e93d9813.png)'
  prefs: []
  type: TYPE_IMG
- en: As you learned in previous chapters, with `kubectl apply`, we can pretty much
    rule the world. In this case, we are adding our folder with the `yamls` as a parameter;
    therefore, `kubectl` is going to action on Kubernetes with the YAML definitions
    that we are going to create.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the job and run it. Once it finishes, it should be successful with a log
    similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c11d724-c5f9-4c03-8cf8-692944c61f8d.png)'
  prefs: []
  type: TYPE_IMG
- en: This job might fail a few times as there are many moving parts. By now, you
    have enough knowledge to troubleshoot the integration of these parts.
  prefs: []
  type: TYPE_NORMAL
- en: That's it. Our Continuous Delivery (CD) pipeline is working. From now on, if
    we want to make changes to our `aggregator`, we just need to add/modify files
    to our code repository, tag them with a new version, modify our `aggregator-kubernetes`
    definitions to point to the new image, and kick off our Jenkins job.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two extra steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a tag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kick off a job manually
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the price you pay for having a lot of control in our deployment but
    with a bit of a secret sauce: we are set for a great deployment flexibility, as
    we are going to see in the next section, but first, you should repeat the same
    exercise for `utcdate-service` and `isodate-service` so that we have our full
    system running. If you want to save a lot of time or check whether you are going
    in the right direction, check out my repository at [https://github.com/dgonzalez/chronos](https://github.com/dgonzalez/chronos).'
  prefs: []
  type: TYPE_NORMAL
- en: Inside every service, there is a folder called definitions that contains the
    Kubernetes objects to make everything work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful with the name of the services: the aggregator is expecting to be
    able to resolve `isodate-service` and `utcdate-service` from the DNS, so your
    Services (Kubernetes objects) should be named accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: Regular release
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are all set; if you''ve completed the deployment of `utcdate-service`
    and `isodate-service`, a fully working system should be installed on Kubernetes.
    The way it works is very simple: When you get the URL of the `aggregator` in the
    `/dates/{timestamp}` path, replacing timestamp with a valid UNIX timestamp, the
    service will contact `utcdate-service` and `isodate-service` and get the timestamp
    converted into the UTC and ISO formats. In my case, the load balancer provided
    by Google Cloud Platform will lead to the URL: `http://104.155.35.237/dates/1111111111`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It will have the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'You can play around with it for a bit, but it is nothing fancy: just a simple
    demo system that makes microservices and their automation easy to understand.
    In this case, we are not running any test, but for a continuous delivery pipeline,
    testing is a must (we will talk about this later).'
  prefs: []
  type: TYPE_NORMAL
- en: Now as the title of the section suggests, we are going to create a new version
    of our application and release it using our continuous delivery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new version it is going to be very simple but quite illustrative. On the
    `aggregator`, replace `index.js` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In the highlighted part, we have added a new section to the return object that
    basically returns the raw timestamp. Now it is time to commit the changes, but
    first, let''s follow a good practice. Create a branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This is going to create a local branch called `raw-timestamp`. Now commit the
    changes created in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'And push the branch to GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'If we visit the GitHub interface now, we''ll notice that something has changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94a75462-4832-4a69-978c-77d086d82638.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is suggesting that we create a Pull requests. Basically, a pull request
    is a request to add code to a repository. Click on Compare & pull request and
    then add a description in the new form and click on Create pull request. This
    is the outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff9cab24-1687-43ea-937e-3c057d53bd90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are three tabs:'
  prefs: []
  type: TYPE_NORMAL
- en: Conversation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Files changed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one is a list of comments by the participants. The second tab is the
    list of commits that they pushed into the server, and the third one is the list
    of changes in diff style with additions and deletions, where you can drop comments
    asking for changes or suggesting better ways of doing things. In big projects,
    the master branch is usually blocked, and the only way to push code into it is
    via pull requests in order to enforce the review of the code.
  prefs: []
  type: TYPE_NORMAL
- en: Once you are happy, click on Merge pull request and merge the code. This pushes
    the changes into the master branch (needs confirmation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to create a tag. This can be done via the GitHub interface.
    If you click on the release link (beside the number of contributors above the
    list of files), it brings you to the releases page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7b704c6-b5c7-49bc-8faa-dbe31290bc09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There, you can see the tag that we created earlier from the terminal and a
    button called Draft a new release. Click on it, and it will show a new form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51671f01-b9e8-4222-a646-f0174fe0537c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fill in the details, as shown here, and create the release. This creates a
    tag that is connected to our container registry in Google Cloud Platform, and
    by now (it is very quick), a new version of our image should be available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20fb4c23-b1a4-472b-9243-5de53186b262.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there is a good level of control over what is going into our
    production buckets (registry and cluster). Now the only step left is to release
    the new version into Kubernetes. Go back to the repository called `aggregator-kubernetes`
    (we created it in the preceding section) and change the tag of the image in the
    `deployment.yaml` file from `eu.gcr.io/implementing-modern-devops/aggregator-service:1.0`
    to `eu.gcr.io/implementing-modern-devops/aggregator-service:2.0`. Be aware that
    the project needs to be tailored to your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is done, commit and push the changes (from `aggregator-kubernetes`
    folder):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now everything is ready. We are at the edge of the cliff. If we click on Run
    in the job that we created in Jenkins, the new version of the software is going
    to be deployed in Kubernetes with zero downtime (depending on your configuration,
    as seen earlier); we have the control. We can decide when is the best time to
    release, and we have an easy way to roll back: revert the changes and click on
    Run again in Jenkins.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you are comfortable with the changes, run the job that we created in Jenkins
    (in my case, `aggregator-kubernetes`).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you hit the same URL as earlier (`http://104.155.35.237/dates/1111111111`),
    the result should have changed a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The new version is up. As you can imagine, this is a fairly powerful argument
    to adopt DevOps: release software transparently to the users with minimal effort
    (create a tag and run a job in Jenkins).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to execute the same deployment but using a
    technique called blue-green deployment, which consist on release the new version
    in a private mode running in the production environment in order for us to test
    the features before making them available to the general public.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to execute a blue-green deployment, first, we need to roll back to
    version 1.0\. Edit `deployment.yaml` in `aggregator-kubernetes`, adjust the image
    to the tag `1.0`, and push the changes to GitHub. Once that is done, run a job
    called `aggregator-kubernetes` in Jenkins, and there you go; we have rolled back
    to version 1.0\. Leave version 2.0 of the image in the registry as we are going
    to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'A blue-green deployment is a technique used to release software to production
    that is not visible to the general public, so we can test it before making it
    available to everyone. Kubernetes makes this extremely simple: the only thing
    we need to do is duplicate the resources in `aggregator-``kubernetes` and assign
    to them different names and tags. For example, this is our `deployment-bluegreen.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is our `service-bluegreen.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have created a vertical slice of our app with a different
    set of selectors/tags; therefore, our original version is working, but now, we
    have a new service called `aggregator-service-bluegreen` that serves the new version
    of our application via a load balancer, which we can check via the Kubernetes
    interface (using the `kubectl proxy` command, as explained earlier):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b9183a2-6b2c-4f68-be7b-27b4bb0da73a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you play around the two external endpoints, you can see the difference:
    the new one is returning the raw payload as well as the dates in the ISO format
    and in UTC timezone (version 2.0), whereas the old one only returns the dates
    (version 1.0).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now in what we call the blue status: we are happy with our release and
    we are sure that our software works with our production configuration without
    affecting any of our customers. If there was any problem, no customers would have
    noticed it. Now it is time to go to the green phase. We have two options here:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove the `aggregator-bluegreen` deployment and all its children (ReplicaSet
    and pods as well as the `aggregator-service-bluegreen` service) and upgrade our
    base deployment (`aggregator`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the labels for the selector in the aggregator service and make it point
    to the new Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, I am a big fan of the first option as it keeps things simple, but
    it is your choice; also it's a good time for experimenting. Changing the selector
    in the service has an immediate effect, and it is probably the easy route if you
    are in a hurry.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with complex systems, I always try to go over a blue-green deployment
    phase to remove stress from the team. Think about it: instead of thinking that
    everything is solid, you are actually verifying that everything works as expected
    with no surprises, so the psychological factor of the uncertainty is gone at the
    moment of release.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we are going to visit another type of release, which introduces
    a new pod into the running system so we get to expose it to the users but only
    to a subset of them, so if something goes wrong, it does not kill the system;
    it just produces some errors. Before proceeding, make sure that you return your
    cluster to the original status: just a deployment called `aggregator` with its
    pods (remove the blue-green deployment).'
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a story about the name of this type of deployment, which very interesting.
    Before all the gas detectors, miners used to bring a canary (the bird) into the
    mines, as they are extremely sensitive to dangerous gases. Everybody was working
    normally but keeping an eye on the bird. If, for some reason, the bird died, everybody
    would leave the mine in order to avoid getting poisoned or even killed.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what we are going to do: introduce a new version of our software,
    which will actually produce errors if there is any problem so we only impact a
    limited number of customers.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is done via the YAMl files using the selectors that our service
    is targeting but with a new version of our app. Before continuing, make sure that
    you have only one deployment called `aggregator` with two pods running the version
    1.0 of our app (as shown in the Regular Release section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in `aggregator-kubernetes`, create a file (inside objects folder) with
    the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s an easy explanation: we are creating a new deployment, with only one
    Pod, with the same tags that the original deployment pods (`aggregator`) has;
    therefore, the `aggregator-service` is going to target this pod as well: three
    in total.'
  prefs: []
  type: TYPE_NORMAL
- en: Push the changes to GitHub and run the job `aggregator-kubernetes`, which will
    apply this configuration to our cluster. Now open the endpoint that we used earlier
    for testing, in my case, `http://104.155.35.237/dates/1111111111`, and keep refreshing
    the URL a few times. Approximately, one-third of the requests should come back
    with the raw timestamp (new version of the app) and two-third should come back
    without it (version 1.0).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that everything went well via the Kubernetes dashboard by checking
    the `aggregator-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbfa0659-b273-4808-80db-dccdf247f518.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, you can see the newly created pod being targeted by our service. I usually
    leave this status for a few hours/days (depending on the release), and once I
    am happy, I remove the `canary` deployment and apply the configuration to the
    `aggregator` deployment. You can play with the number of replicas as well in order
    to change the percentage of the users that get the new version or even gradually
    increase the number of `canaries` and decrease the number of regular pods until
    the application is completely rolled out.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy is followed by big companies such as Google to release new features
    with a lot of success. I am a big fan of using it as starting point when the system
    is big enough (10 *+* pods running), but I would be reluctant to do that in a
    small system as the percentage of affected requests would be too big (33.3% in
    the preceding example).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter was pretty intense: we set up a CD pipeline as well as visited
    the most common release strategies, which, using Kubernetes, were within the reach
    of our hand. Everything was automated except a couple of checkpoints that were
    left on purpose so that we could control what was going in our system (just for
    peace of mind). This was the climax of the book: even though the examples were
    basic, they provided you with enough tools to set up something similar in your
    company in order to get the benefit of working with microservices but padding
    the operational overhead that they involve as well as facilitating the release
    of new versions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will learn an important aspect of continuous delivery:
    monitoring. With the right monitoring in place we can remove a lot of stress from
    the releases so that our engineers are more confident on being able to catch errors
    early leading into smoother rollouts and a lower production bug count.'
  prefs: []
  type: TYPE_NORMAL
