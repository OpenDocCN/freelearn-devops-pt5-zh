- en: Chapter 15. Self-Healing Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '|   | *Healing takes courage, and we all have courage, even if we have to dig
    a little to find it.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Tori Amos* |'
  prefs: []
  type: TYPE_TB
- en: Let's face it. The systems we are creating are not perfect. Sooner or later,
    one of our applications will fail, one of our services will not be able to handle
    the increased load, one of our commits will introduce a fatal bug, a piece of
    hardware will break, or something entirely unexpected will happen.
  prefs: []
  type: TYPE_NORMAL
- en: How do we fight the unexpected? Most of us are trying to develop a bullet proof
    system. We are attempting to create what no one did before. We strive for the
    ultimate perfection, hoping that the result will be a system that does not have
    any bugs, is running on hardware that never fails, and can handle any load. Here's
    a tip. There is no such thing as perfection. No one is perfect, and nothing is
    without fault. That does not mean that we should not strive for perfection. We
    should, when time and resources are provided. However, we should also embrace
    the inevitable, and design our systems not to be perfect, but able to recuperate
    from failures, and able to predict likely future. We should hope for the best
    but prepare for the worst.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of examples of resilient systems outside software engineering,
    none of them better than life itself. We can take ourselves, humanity, as an example.
    We're the result of a very long experiment based on small and incremental evolutionary
    improvements, performed over millions of years. We can learn a lot from a human
    body, and apply that knowledge to our software and hardware. One of the fascinating
    abilities we (humans) possess is the capacity to self-heal.
  prefs: []
  type: TYPE_NORMAL
- en: Human body has an amazing capacity to heal itself. The most fundamental unit
    of human body is cell. Throughout our life, cells inside our body are working
    to bring us back to a state of equilibrium. Each cell is a dynamic, living unit
    that is continuously monitoring and adjusting its own processes, working to restore
    itself according to the original DNA code it was created with, and to maintain
    balance within the body. Cells have the ability to heal themselves, as well as
    to make new cells that replace those that have been permanently damaged or destroyed.
    Even when a large number of cells are destroyed, the surrounding cells replicate
    to make new cells, thereby quickly replacing the cells that were destroyed. This
    ability does not make us, individuals, immune to death, but it does make us very
    resilient. We are continuously attacked by viruses. We succumb to diseases and
    yet, in most cases, we come out victorious. However, looking at us as individuals
    would mean that we are missing the big picture. Even when our own lives end, the
    life itself not only survives, but thrives, ever growing, and ever adapting.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of a computer system as a human body that consists of cells of
    various types. They can be hardware or software. When they are software units,
    the smaller they are, the easier it is for them to self-heal, recuperate from
    failures, multiply, or even get destroyed when that is needed. We call those small
    units microservices, and they can, indeed, have behaviors similar to those observed
    in a human body. The microservices-based system we are building can be made in
    a way that is can self-heal. That is not to say that self-healing we are about
    to explore is applicable only to microservices. It is not. However, like most
    other techniques we explored, self-healing can be applied to almost any type of
    architecture, but provides best results when combined with microservices. Just
    like life that consists of individuals that form a whole ecosystem, each computer
    system is part of something bigger. It communicates, cooperates, and adapts to
    other systems forming a much larger whole.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing Levels and Types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software systems, the self-healing term describes any application, service,
    or a system that can discover that it is not working correctly and, without any
    human intervention, make the necessary changes to restore itself to the normal
    or designed state. Self-healing is about making the system capable of making its
    decisions by continually checking and optimizing its state and automatically adapting
    to changing conditions. The goal is to make fault tolerant and responsive system
    capable of responding to changes in demand and recuperation from failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-healing systems can be divided into three levels, depending on size and
    type of resources we are monitoring, and acting upon. Those levels are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Application level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll explore each of those three types separately.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing on the Application Level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Application level healing is the ability of an application, or a service, to
    heal itself internally. Traditionally, we're used to capturing problems through
    exceptions and, in most cases, logging them for further examination. When such
    an exception occurs, we tend to ignore it and move on (after logging), as if nothing
    happened, hoping for the best in the future. In other cases, we tend to stop the
    application if an exception of certain type occurs. An example would be a connection
    to a database. If the connection is not established when the application starts,
    we often stop the whole process. If we are a bit more experienced, we might try
    to repeat the attempt to connect to the database. Hopefully, those attempts are
    limited, or we might easily enter a never ending loop, unless database connection
    failure was temporary and the DB gets back online soon afterwards. With time,
    we got better ways to deal with problems inside applications. One of them is Akka.
    It's usage of supervisor, and design patterns it promotes, allow us to create
    internally self-healing applications and services. Akka is not the only one. Many
    other libraries and frameworks enable us to create fault tolerant applications
    capable of recuperation from potentially disastrous circumstances. Since we are
    trying to be agnostic to programming languages, I'll leave it to you, dear reader,
    investigation of ways to self-heal your applications internally. Bear in mind
    that self-healing in this context refers to internal processes and does not provide,
    for example, recuperation from failed processes. Moreover, if we adopt microservices
    architecture, we can quickly end up with services written in different languages,
    using different frameworks, and so on. It is truly up to developers of each service
    to design it in a way that it can heal itself and recuperate from failures.
  prefs: []
  type: TYPE_NORMAL
- en: Let's jump into the second level.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing on the System Level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the application level healing that depends on a programming language
    and design patterns that we apply internally, system level self-healing can be
    generalized and be applied to all services and applications, independently from
    their internals. This is the type of self-healing that we can design on the level
    of the whole system. While there are many things that can happen at the system
    level, the two most commonly monitored aspects are failures of processes and response
    time. If a process fails, we need to redeploy the service, or restart the process.
    On the other hand, if the response time is not adequate, we need to scale, or
    descale, depending whether we reached upper or lower response time limits. Recuperating
    from process failures is often not enough. While such actions might restore our
    system to the desired state, human intervention is often still needed. We need
    to investigate the cause of the failure, correct the design of the service, or
    fix a bug. That is, self-healing often goes hand in hand with investigation of
    the causes of that failure. The system automatically recuperates and we (humans)
    try to learn from those failures, and improve the system as a whole. For that
    reason, some kind of a notification is required as well. In both cases (failures
    and increased traffic), the system needs to monitor itself and take some actions.
  prefs: []
  type: TYPE_NORMAL
- en: How does the system monitor itself? How does it check the status of its components?
    There are many ways, but two most commonly used are TTLs and pings.
  prefs: []
  type: TYPE_NORMAL
- en: Time-To-Live
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Time-to-live** (**TTL**) checks expect a service, or an application, to periodically
    confirm that it is operational. The system that receives TTL signals keeps track
    of the last known reported state for a given TTL. If that state is not updated
    within a predefined period, the monitoring system assumes that the service failed
    and needs to be restored to its designed state. For example, a healthy service
    could send an HTTP request announcing that it is alive. If the process the service
    is running in fails, it will be incapable to send the request, TTL will expire,
    and reactive measures will be executed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main problem with TTL is coupling. Applications and services need to be
    tied to the monitoring system. Implementing TTL would be one of the microservices
    anti-patterns since we are trying to design them in a way that they are as autonomous
    as possible. Moreover, microservices should have a clear function and a single
    purpose. Implementing TTL requests inside them would add additional functionality
    and complicate the development:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time-To-Live](img/B05848_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-01 – System level self-healing with time-to-live (TTL)
  prefs: []
  type: TYPE_NORMAL
- en: Pinging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea behind pinging is to check the state of an application, or a service,
    externally. The monitoring system should ping each service periodically and, if
    no response is received, or the content of the response is not adequate, execute
    healing measures. Pinging can come in many forms. If a service exposes HTTP API,
    it is often a simple request, where desired response should be HTTP status in
    2XX range. In other cases, when HTTP API is not exposed, pinging can be done with
    a script, or any other method that can validate the state of the service.
  prefs: []
  type: TYPE_NORMAL
- en: Pinging is opposite from TTL, and, when possible, is a preferable way of checking
    the status of individual parts of the system. It removes repetition, coupling,
    and complications that could occur when implementing TTL inside each service.
  prefs: []
  type: TYPE_NORMAL
- en: '![Pinging](img/B05848_15_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-02 – System level self-healing with pings
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing on the Hardware Level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Truth be told, there is no such a thing as hardware self-healing. We cannot
    have a process that will automatically heal failed memory, repare broken hard
    disk, fix malfunctioning CPU, and so on. What healing on this level truly means
    is redeployment of services from an unhealthy to one of the healthy nodes. As
    with the system level, we need to periodically check the status of different hardware
    components, and act accordingly. Actually, most healing caused due to hardware
    level will happen at the system level. If hardware is not working correctly, chances
    are that the service will fail, and thus be fixed by system level healing. Hardware
    level healing is more related to preventive types of checks that we''ll discuss
    shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing on the Hardware Level](img/B05848_15_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-03 – Hardware level self-healing
  prefs: []
  type: TYPE_NORMAL
- en: Besides the division based on the check levels, we can also divide it based
    on the moment actions are taken. We can react to a failure, or we can try to prevent
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Reactive healing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the organizations that implemented some kind of self-healing systems
    focused on reactive healing. After a failure is detected, the system reacts and
    restores itself to the designed state. A service process is dead, ping returns
    the code 404 (not found), corrective actions are taken, and the service is operational
    again. This works no matter whether service failed because its process failed,
    or the whole node stopped being operational (assuming that we have a system that
    can redeploy to a healthy node). This is the most important type of healing and,
    at the same time, the easiest one to implement. As long as we have all the checks
    in place, as well as actions that should be performed in case of a failure, and
    we have each service scaled to at least two instances distributed on separate
    physical nodes, we should (almost) never have downtime. I said almost never because,
    for example, the whole datacenter might loose power, thus stopping all nodes.
    It's all about evaluating risks against costs of preventing those risks.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it is worthwhile to have two datacenters in different locations,
    and in other cases it's not. The objective is to strive towards zero-downtime,
    while accepting that some cases are not worthwhile trying to prevent.
  prefs: []
  type: TYPE_NORMAL
- en: No matter whether we are striving for zero-downtime, or almost zero-downtime,
    reactive self-healing should be a must for any but smallest settings, especially
    since it does not require big investment. You might invest in spare hardware,
    or you might invest in separate datacenters. Those decisions are not directly
    related with self-healing, but with the level of risks that are acceptable for
    a given use case. Reactive self-healing investment is primarily in knowledge how
    to do it, and time to implement it. While time is an investment in itself, we
    can spend it wisely, and create a general solution that would work for (almost)
    all cases, thus reducing the time we need to spend implementing such a system.
  prefs: []
  type: TYPE_NORMAL
- en: Preventive healing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea behind preventive healing is to predict the problems we might have
    in the future, and act in a way that those problems are avoided. How do we predict
    the future? To be more precise, what data do we use to predict the future?
  prefs: []
  type: TYPE_NORMAL
- en: Relatively easy, but less reliable way of predicting the future, is to base
    assumptions on (near) real-time data. For example, if one of the HTTP requests
    we're using to check the health of a service responded in more than 500 milliseconds,
    we might want to scale that service. We can even do the opposite. Following the
    same example, if it took less than 100 milliseconds to receive the response, we
    might want to descale the service, and reassign those resources to another one
    that might need it more. The problem with taking into account the current status
    when predicting the future is variability. If it took a long time between the
    request and the response, it might indeed be the sign that scaling is needed,
    but it might also be a temporary increase in traffic, and the next check (after
    the traffic spike is gone) will deduce that there is a need to descale. If microservices
    architecture is applied, this can be a minor issue, since they are small and easy
    to move around. They are easy to scale, and descale. Monolithic applications are
    often much more problematic if this strategy is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: If historical data is taken into account, preventive healing becomes much more
    reliable but, at the same time, much more complicated to implement. Information
    (response times, CPU, memory, and so on) needs to be stored somewhere and, often
    complex, algorithms need to be employed to evaluate tendencies, and make conclusions.
    For example, we might observe that, during the last hour, memory usage has been
    steadily increasing, and that it reached a critical point of, let's say, 90%.
    That would be a clear indication that the service that is causing that increase
    needs to be scaled. The system could also take into account longer period of time,
    and deduce that every Monday there is a sudden increase in traffic, and scale
    services well in advance to prevent long responses. What would be, for example,
    the meaning of a steady increase in memory usage from the moment a service is
    deployed, and sudden decrease when a new version is released? Probably memory
    leaks and, in such a case, the system would need to restart the application when
    certain threshold is reached, and hope that developers would fix the issue (hence
    the need for notifications).
  prefs: []
  type: TYPE_NORMAL
- en: Let us change the focus, and discuss architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter the internal processes and tools, every self-healing system will have
    some common elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the very beginning, there is a cluster. A single server cannot be made fault
    tolerant. If a piece of its hardware fails, there is nothing we can do to heal
    that. There is no readily available replacement. Therefore, the system must start
    with a cluster. It can be composed out of two or two hundred servers. The size
    is not of the essence, but the ability to move from one hardware to another in
    the case of a failure. Bear in mind that we always need to evaluate benefits versus
    costs. If financially viable, we would have at least two physically and geographically
    separated datacenters. In such a case, if there is a power outage in one, the
    other one would be fully operational. However, in many instances that is not a
    financially viable option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-04 – Self-healing system architecture: Everything starts with a cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the cluster up and running, we can begin deploying our services.
    However, managing services inside a cluster without some orchestrator is tedious,
    at best. It requires time and often ends up with a very unbalanced usage of resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-05 – Self-healing system architecture: Services are deployed to the
    cluster, but with a very unbalanced usage of resources'
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, people treat a cluster as a set of individual servers, which
    is wrong, knowing that today we have tools at our disposal that can help us do
    the orchestration in a much better way. With Docker Swarm, Kubernetes, or Apache
    Mesos, we can solve the orchestration within a cluster. Cluster orchestration
    is important, not only to ease the deployment of our services, but also as a way
    to provide fast re-deployments to healthy nodes in case of a failure (be it of
    software or hardware nature). Bear in mind that we need at least two instances
    of every service running behind a proxy. Given such a situation, if one instance
    fails, the others can take over its load, thus avoiding any downtime while the
    system re-deploys the failed instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-06 – Self-healing system architecture: Some deployment orchestrator
    is required to distribute services across the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basis of any self-healing system is monitoring of the state of deployed
    services, or applications, as well as the underlying hardware. The only way we
    can monitor them is to have information about their existence. That information
    can be available in many different forms, ranging from manually maintained configuration
    files, through traditional databases, all the way until highly available distributed
    service registries like `Consul`, `etcd`, or `Zookeeper`. In some cases, the service
    registry can be chosen by us, while in others it comes as part of the cluster
    orchestrator. For example, Docker Swarm has the flexibility that allows it to
    work with a couple of registries, while Kubernetes is tied to `etcd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-07 – Self-healing system architecture: Primary requirement for monitoring
    the state of the system is to have the information of the system stored service
    registry'
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter the tool we choose to act as a service registry, the next obstacle
    is to put the information into the service registry of choice. The principle is
    a simple one. Something needs to monitor hardware and services and update the
    registry whenever a new one is added, or an existing one is removed. There are
    plenty of tools that can do that. We are already familiar with Registrator, which
    fulfills this role pretty well. As with service registries, some cluster orchestrators
    already come with their own ways to register and de-register services. No matter
    which tool we choose, the primary requirement is to be able to monitor the cluster
    and send information to service registry in near-realtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-08 – Self-healing system architecture: Service registry is useless
    if no mechanism will monitor the system and store new information'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the cluster with services up and running, and we have the
    information about the system in the service registry, we can employ some health
    monitoring that will detect anomalies. Such a tool needs to know not only what
    the desired state is, but, also, what the actual situation is at any moment. Consul
    Watches can fulfill this role while Kubernetes and Mesos come with their own tools
    for this type of tasks. In a more traditional environment, Nagios or Icinga (only
    to name a few), can fulfill this role as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-09 – Self-healing system architecture: With all the relevant information
    stored in a service registry, some health monitoring tools can utilize it to verify
    whether the desired state is maintained'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next piece of the puzzle is a tool that would be able to execute corrective
    actions. When the health monitor detects an anomaly, it would send a message to
    perform a corrective measure. As a minimum, that corrective action should send
    a signal to the cluster orchestrator, which, in turn, would redeploy the failed
    service. Even if a failure was caused by a hardware problem, cluster orchestrator
    would (temporarily) fix that by redeploying the service to a healthy node. In
    most cases, corrective actions are not that simple. There could be a mechanism
    to notify interested parties, record what happened, revert to an older version
    of the service, and so on. We already adopted Jenkins, and it is a perfect fit
    to act as the tool that can receive a message from the health monitor and, as
    a result, initiate corrective actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-10 – Self-healing system architecture: As a minimum, corrective action
    should send a signal to the cluster orchestrator to redeploy the service that
    failed'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process, as it is for now, is dealing only with reactive healing. The system
    is continuously monitored and, if a failure is detected, corrective actions are
    taken, which, in turn, will restore the system to the desired state. Can we take
    it a step further and try to accomplish preventive healing? Can we predict the
    future and act accordingly? In many cases we can, in some we can''t. We cannot
    know that a hard disk will fail tomorrow. We cannot predict that there will be
    an outage today at noon. However, in some cases, we can see that the traffic is
    increasing, and will soon reach a point that will require some of our services
    to be scaled. We can predict that a marketing campaign we are about to launch
    will increase the load. We can learn from our mistakes, and teach the system how
    to behave in certain situations. The essential elements of such a set of processes
    are similar to those we should employ for reactive healing. We need a place to
    store data and a process that collects them. Unlike service registry that deals
    with a relatively small amount of data and benefits from being distributed, preventive
    healing requires quite bigger storage and capabilities that would allow us to
    perform some analytic operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-11 – Self-healing system architecture: Preventive healing requires
    historical data to be analyzed'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly to the registrator service, we''ll also need some data collector
    that will be sending historical data. That data can be quite massive and include,
    but not be limited by, CPU, HD, network traffic, system and service logs, and
    so on. Unlike the registrator that listens to events, mostly generated by the
    cluster orchestrator, data collector should be continuously collecting data, digesting
    the input, and producing an output that should be stored as historical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-12 – Self-healing system architecture: Preventive healing requires
    vast quantities of data to be collected continuously'
  prefs: []
  type: TYPE_NORMAL
- en: 'We already used some of the tools needed for reactive self-healing. Docker
    Swarm can be used as the cluster orchestrator, Registrator and Consul for service
    discovery, and Jenkins for performing, among other duties, corrective actions.
    The only tool that we haven''t used are two subsets of Consul; checks and watches.
    Preventive healing will require exploration of some new processes and tools, so
    we''ll leave it for later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-Healing Architecture](img/B05848_15_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-13 – Self-healing system architecture: One of the combinations of
    tools'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if we can set up a sample reactive self-healing system.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing with Docker, Consul Watches, and Jenkins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The good news is that we already used all the tools that we require to make
    a reactive self-healing system. We have Swarm that will make sure that containers
    will be deployed to healthy nodes (or at least nodes that are operational). We
    have Jenkins that can be used to execute the healing process and, potentially,
    send notifications. Finally, we can use Consul not only to store service information,
    but also to perform health checks and send requests to Jenkins. The only piece
    we haven't used until now are Consul watches that can be programmed to perform
    health checks.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note about how Consul does health checks is that it differs from
    traditional way Nagios and other similar tools are operating. Consul avoids the
    thundering herd problem by using gossip, and only alerts on state changes.
  prefs: []
  type: TYPE_NORMAL
- en: As always, we'll start by creating VMs we'll use throughout the rest of the
    chapter. We'll create the familiar combination of one `cd` and three `swarm` servers
    (one master and two nodes).
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the Environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following command will create the four VMs we'll use in this chapter. We'll
    create the `cd` node and use it to provision the other nodes with Ansible. This
    VM will also host Jenkins, that will be an important part of the self-healing
    process. The other three VMs will form the Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With all the VMs operational, we can proceed and set up the Swarm cluster. We'll
    start by provisioning the cluster in the same way as we did before, and then discuss
    changes we need to make it self-heal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the time has come to provision the `cd` server with Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We reached the point where the whole cluster is operational, and Jenkins server
    will be up and running soon. We set one Swarm master (`swarm-master`), two Swarm
    nodes (`swarm-node-1` and `swarm-node-2`), and one server with Ansible and, soon
    to be running, Jenkins (`cd`). Feel free to continue reading while Jenkins provisioning
    is running. We won't need it right away.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Consul Health Checks and Watches for Monitoring Hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can send instructions to Consul to perform periodic checks of services or
    entire nodes. It does not come with predefined checks. Instead, it runs scripts,
    performs HTTP requests, or wait for TTL signals defined by us. While the lack
    of predefined checks might seem like a disadvantage, it gives us the freedom to
    design the process as we see fit. In case we're using scripts to perform checks,
    Consul will expect them to exit with certain codes. If we exit from the check
    script with the `code 0`, Consul will assume that everything works correctly.
    Exit `code 1` is expected to be a warning, and the exit `code 2` is an error.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by creating a few scripts that will perform hardware checks. Getting
    information of, let's say, hard disk utilization is relatively easy with the `df`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We used the `-h` argument to output `human-readable` information, and the output
    is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Bear in mind that in your case the output might be slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we truly need are numbers from the root directory (the third row in the
    output). We can filter the output of the `df` command so that only the row with
    the value `/` of the last column is displayed. After the filter, we should extract
    the percentage of used disk space (column 5). While we are extracting data, we
    might just as well get the disk size (column 2), and the amount of used space
    (column 3). Data that we extract should be stored as variables that we could use
    later on. The commands we can use to accomplish all that is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Since the value that represents the used space percentage contains the `%` sign,
    we removed the last character before assigning the value to the `used_percent`
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can double-check whether the variables we created contain correct values
    with a simple `printf` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the last command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The only thing left is to exit with 1 (warning) or 2 (error) when a threshold
    is reached. We''ll define the error threshold as 95% and warning as 80%. The only
    thing missing is a simple `if`/`elif`/`else` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For testing purposes, we put echos. The script that we are about to make should
    exit with `2`, `1` or `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move into the `swarm-master` node, create the script, and test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll start by creating a directory where Consul scripts will reside:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create the script with the commands we practiced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try it out. Since there''s quite a lot of free disk space, the script
    should echo the disk usage and return zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The command provided an output similar to the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily display the exit code of the last command with `$?`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The echo returned zero, and the script seems to be working. You can test the
    rest of exit codes by modifying the threshold to be below the current disk usage.
    I'll leave that to you, as a simple exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Consul check threshold exercise**'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the `disk.sh` script in a way that warning and error thresholds are lower
    than the current HD usage. Test the changes by running the script and outputting
    the exit code. Once the exercise is done, revert the script to its original values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the script that checks the disk usage, we should tell Consul
    about its existence. Consul uses JSON format for specifying checks. The definition
    that utilizes the script we just created is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'That JSON would tell Consul that there is a check with the ID `disk`, name
    `Disk utilization` and notes `Critical 95% util`, `warning 80% util`. The `name`
    and `notes` are purely for visualization purposes (as you''ll see soon). Next,
    we are specifying the path to the script to be `/data/consul/scripts/disk.sh`.
    Finally, we are telling Consul to run the script every `10` seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When we started Consul (through the Ansible playbook), we specified that configuration
    files are located in the `/data/consul/config/` directory. We still need to reload
    it, so that it picks up the new file we just created. The easiest way to reload
    Consul is by sending it the `HUP` signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We managed to create hard disk checks in Consul. It will run the script every
    ten seconds and, depending on its exit code, determine the health of the node
    it runs on (in this case `swarm-master`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul Health Checks and Watches for Monitoring Hardware](img/B05848_15_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-14 – Hard disk checks in Consul
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the Consul UI by opening `http://10.100.192.200:8500/ui/`
    from a browser. Once the UI is opened, please click the **Nodes** button, and
    then the `swarm-master` node. Among other information, you''ll see two checks.
    One of them is `Serf Health Status`. It''s Consul''s internal check based on TTL.
    If one of the Consul nodes is down, that information will be propagated throughout
    the cluster. The check check, called `Disk utilization`, is the one we just created,
    and, hopefully, the status is `passing`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul Health Checks and Watches for Monitoring Hardware](img/B05848_15_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-15 – Hard disk checks in Consul UI
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how easy it is to add a check in Consul, we should define what
    action should be performed when a check fails. We do that through Consul watches.
    As with checks, Consul does not offer an out-of-the-box final solution. It provides
    a mechanism for us to create the solution that fits our needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consul supports seven different types of watches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**key**: Watch a specific KV pair'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**keyprefix**: Watch a prefix in the KV store'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**services**: Watch the list of available services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nodes**: Watch the list of nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**service**: Watch the instances of a service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**checks**: Watch the value of health checks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**event**: Watch for custom user events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the types is useful in certain situations and, together, they provide
    a very comprehensive framework for building your self-healing, fault tolerant,
    system. We'll concentrate on the `checks` type, since it will allow us to utilize
    the hard disk check we created earlier. Please consult the watches documentation
    for more info.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by creating the script that will be run by Consul watcher. The
    `manage_watches.sh` script is as follows (please don''t run it):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We started by defining `RED` and `NC` variables that will help us paint critical
    parts of the output in red. Then, we are reading the Consul input and storing
    it into the `JSON` variable. That is followed by the creation of `STATUS_ARRAY`
    and `CHECK_ID_ARRAY` arrays that will hold `Status` and `CheckID` values for each
    element from the JSON. Finally, those arrays allow us to iterate through each
    item, and send a POST request to Jenkins to build the `hardware-notification`
    job (we'll take a look at it later). The request uses Jenkins friendly format
    for passing the `CHECK_ID` and `STATUS` variables. Please consult Jenkins remote
    access API for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the script that will be executed whenever there is a check
    with the `warning` or `critical` status, we''ll inform Consul about its existence.
    The Consul watches definition is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This definition should be self-explanatory. We defined two watches, both of
    type `checks`. The first one will be run in case of a `warning`, and the second
    when a check is in the `critical` state. We're trying to keep things simple by
    specifying, in both instances, the same handler `manage_watches.sh`. In a real
    world setting, you should differentiate those two states and run different actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the watches file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Before we proceed, and reload Consul, we should have a quick discussion about
    the Jenkins job `hardware-notification`. It was already created when we provisioned
    Jenkins. Its configuration can be seen by opening `http://10.100.198.200:8080/job/hardware-notification/configure`.
    It contains two parameters, `checkId` and `status`. We're using those two parameters
    as a way to avoid creating separate jobs for each hardware check. Whenever Consul
    watcher sends the POST request to build this job, it passes values to those two
    variables. In the build phase, we are simply running an `echo` command that sends
    values of those two variables to standard output (`STDOUT`). In a real world situation,
    this job would do some actions. For example, if disk space is low, it could remove
    unused logs and temporary files. Another example would be creation of additional
    nodes, if we're using one of the cloud services like Amazon AWS. In some other
    situations, no automated reaction is possible. In any case, besides concrete actions
    like those, this job should also send some kind of a notification (email, instant
    messaging, and so on) so that operators are informed about the potential problem.
    Since those situations would be difficult to reproduce locally, the initial definition
    of this job does nothing of the sort. I'll leave it up to you to extend it for
    your own needs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**The hardware-notification Jenkins job exercise**'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the `hardware-notification` Jenkins job so that logs are deleted in case
    the `checkId` value is `disk`. Create mock logs (feel free to use the `touch`
    command to create files) on the server and run the job manually. Once the job
    build is finished, confirm that the logs were indeed removed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul Health Checks and Watches for Monitoring Hardware](img/B05848_15_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-16 – Settings screen of the Jenkins job hardware-notification
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem we have right now is that the hard disk on the `swarm-master` node
    is mostly empty, thus preventing us from testing the system we just set up. We''ll
    have to change the thresholds defined in the `disk.sh`. Let''s modify the 80%
    warning threshold to `2%`. Current HD usage is surely more than that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s reload Consul and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing we should check is the watches log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The relevant part of the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Please note that it might take a few seconds until Consul's check is run. If
    you did not receive the similar output from logs, repeat the `cat` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the JSON that consul sent to the script and that the request to
    build the Jenkins job `hardware-notification` has been dispatched. We can also
    take a look at the Jenkins Console Output of this job by opening `http://10.100.198.200:8080/job/hardware-notification/lastBuild/console`
    URL in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul Health Checks and Watches for Monitoring Hardware](img/B05848_15_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-17 – Console output of the Jenkins job hardware-notification
  prefs: []
  type: TYPE_NORMAL
- en: Since, at this moment, we have only one Consul check used for hard disk utilization,
    we should implement at least one more. The suitable candidate is memory. Even
    if we do not do any corrective action when some hardware check fails, having the
    information in Consul is already very useful in itself.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the process, we can do better, and use Ansible to set
    up everything. Besides, different checks should be set up not only in the `swarm-master`
    node but also in the rest of the cluster, and we don't want to do that manually
    unless it's for learning purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed, let''s exit the `swarm-master` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Automatically Setting Up Consul Health Checks and Watches for Monitoring Hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this moment, we have one hardware watcher configured only in the `swarm-master`
    node. Now that we are familiar with the way Consul watches work, we can use Ansible
    to deploy hardware monitoring to all the nodes of the Swarm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll run the Ansible playbook first, and then explore the roles that were
    used to setup the checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `swarm-healing.yml` playbook is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The only difference, when compared with the `swarm.yml` playbook, is the usage
    of the `consul-healing` role. Those two roles (`consul` and `consul-healing`)
    are very similar. The major difference is that the latter copies few more files
    to destination servers (`roles/consul-healing/files/consul_check.json, roles/consul-healing/files/disk.sh`,
    and `roles/consul-healing/files/mem.sh`). We already created all those files manually,
    except the `mem.sh` that is used to check memory, and follows the similar logic
    as the `disk.sh` script. The `roles/consul-healing/templates/manage_watches.sh`
    and `roles/consul-healing/templates/watches.json` files are defined as templates
    so that a few things can be customized through Ansible variables. All in all,
    we are mostly replicating manual steps through Ansible, so that provisioning and
    configuration of the whole cluster can be done automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Please open the `http://10.100.192.200:8500/ui/#/dc1/nodes` URL, and click on
    any of the nodes. You'll notice that each has `Disk utilization` and `Memory utilization`
    watches that, in the case of a failure, will start the build of the Jenkins job
    `hardware-notification/`.
  prefs: []
  type: TYPE_NORMAL
- en: While watching hardware resources, and performing predefined actions in case
    a threshold is reached, is interesting and useful, there is often a limitation
    to corrective actions that can be taken. If, for example, a whole node is down,
    the only thing we can do, in most cases, is to send a notification to someone
    who will manually investigate the problem. The real benefits are obtained by monitoring
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Consul Health Checks and Watches for Monitoring Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we dive into service checks and watches, let's initiate deployment of
    our `books-ms` container. That way we'll use our time wisely, and discuss the
    subject while Jenkins is working hard to have the service up and running.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by indexing the branches defined in the Jenkins job `books-ms`.
    Please open it in a browser, click the **Branch Indexing** link located in the
    left-hand menu, and follow it with **Run Now**. Once the indexing is done, Jenkins
    will detect that the `swarm` branch matches the filter, create the subproject,
    and run the first build. When finished, we'll have the `books-ms` service deployed
    to the cluster, and we'll be able to experiment with more self-healing techniques.
    You can monitor the build progress from the console screen.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in self-healing is identifying that something is wrong. On the
    system level, we can observe services we're deploying and, if one of them does
    not respond, perform some corrective actions. We can continue using Consul checks
    in a similar manner as with did with memory and disk verifications. The major
    difference is that this time we'll be better of by using `http` instead `script`
    checks. Consul will perform periodic requests to our services, and send failures
    to the watches we already set up.
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, we should discuss what should be checked. Should we check
    each service container? Should we check auxiliary containers like databases? Should
    we care about containers at all? Each of those checks can be useful depending
    on specific scenarios. In our case, we'll use a more general approach and monitor
    the service as a whole. Are we losing control if we are not monitoring each container
    separately? The answer to that question depends on the goals we're trying to accomplish.
    What do we care about? Do we care if all containers are running, or whether our
    services are working and performing as expected? If we'd need to choose, I'd say
    that the latter is more important. If our service is scaled to five instances
    and it continues performing well even after two of them stop working, there is
    probably nothing we should do. Only if service as a whole stops working, or if
    it doesn't perform as expected, some corrective actions should be taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike hardware checks that benefit from uniformity, and should be located
    in one place, system checks can vary from one service to another. In order to
    avoid dependencies between a team that maintains a service and a team in charge
    of the overall CD processes, we''ll keep check definitions inside the service
    code repository. That way, service team has full freedom to define checks they
    think are appropriate for the service they''re developing. Since parts of the
    checks are variables, we''ll define them through the Consul Template format. We''ll,
    also, employ naming convention and always use the same name for the file. The
    `consul_check.ctmpl` describes checks for the `books-ms` service, and is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We defined not only checks but also the service named `books-ms`, the tag `service`,
    port it is running on and the address. Please note that, since this is the definition
    of the service as a whole, the port is `80`. In our case, the service as a whole
    is accessible through the proxy, no matter how many containers we deploy, nor
    ports they are running on. The address is obtained from Consul, through the `proxy/ip`
    key. This service should behave the same, no matter which color is currently deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the service is defined, we proceed with the checks (in this case only
    one). Each check has an ID and a name, which are used for informational purposes
    only. The key entry is `http` that defines the address Consul will use to ping
    this service. Finally, we specified that ping should be performed every ten seconds
    and that the timeout should be one second. How do we use this template? To answer
    that question, we should explore the Jenkinsfile, located in the `master` branch
    of the `books-ms` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The only significant difference, when compared with Jenkinsfiles we used in
    previous chapters, is the last line that invokes the `updateChecks` function from
    the `roles/jenkins/files/scripts/workflow-util.groovy` utility script. The function
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In a nutshell, the function copies the file `consul_check.ctmpl` to the `swarm-master`
    node, and runs Consul Template. The result is the creation of, yet another, Consul
    configuration file that will perform service checks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the checks defined, we should take a closer look at the `roles/consul-healing/templates/manage_watches.sh`
    script. The relevant part is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we aim at performing two types of checks (hardware and services), we
    had to introduce an `if`/`else` statement. When hardware failure is discovered
    (`mem` or `disk`), build request is sent to the Jenkins job `hardware-notification`.
    This part is the same as the definition we created earlier. On the other hand,
    we''re assuming that any other type of checks is related to services, and a request
    is sent to the `service-redeploy` job. In our case, when `books-ms` service fails,
    Consul will send a request to build the `service-redeploy` job, and pass `books-ms`
    as the `serviceName` parameter. We''re creating this job in Jenkins in the same
    way as we created others. The main difference is the usage of the `roles/jenkins/templates/service-redeploy.groovy`
    script. The content is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: You probably noticed that the script is much shorter than the `Jenkinsfile`
    we used before. We could easily use the same script to redeploy as the one we're
    using for deployment, and the end result would be (almost) the same. However,
    the objectives differ. One of the crucial requirements is speed. If our service
    failed, we want to redeploy is as fast as possible, while having into account
    as many different scenarios as possible. One of the important differences is that
    we are not running tests during redeployment. All tests already passed during
    deployment, or the service would not be running in the first place and there would
    be nothing to fail. Besides, the same set of tests running against the same release
    will always produce the same result, or our tests are flaky and unreliable, indicating
    grave mistakes in the testing process. You'll also notice that building and pushing
    to the registry is missing. We do not want to build and deploy a new release,
    that's what deployment is for. We want to get the latest release back to production
    as soon as possible. Our need is to restore the system to the same state as it
    was before the service failed. Now that we covered what is, intentionally, missing
    from the redeployment script, let's go through it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first change is in the way how we obtain the number of instances that should
    be running. Up until now, Jenkinsfile, residing in the service repository, was
    deciding how many instances to deploy. We had the statement `def instances = 1`
    in the Jenkinsfile. However, since this redeployment job should be used for all
    services, we had to create a new function called `getInstances` that will retrieve
    the number stored in Consul. It represents the `desired` number of instances,
    and corresponds with the value specified in the Jenkinsfile. Without it, we would
    risk deploying a fixed number of containers and, potentially, destroying someone
    else''s intention. Maybe developers decided to run two instances of the service,
    or maybe they scaled it to five after realizing that the load is too big. For
    that reason, we have to discover how many instances to deploy, and put that information
    to good use. The `getInstances` function defined in the `roles/jenkins/files/scripts/workflow-util.groovy`
    script is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The function sends a simple request to Consul and returns the number of instances
    of the specified service.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are deleting the job workspace directory before cloning the code from
    GitHub. This removal of the files is necessary since the Git repository is different
    from one service to another, and Git repository cannot be cloned on top of the
    other. We don't need all the code, but rather few configuration files, specifically,
    those for Docker Compose and Consul. Never the less, it's easier if we clone everything.
    If the repository is big, you might consider getting only the files you need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now that all the files we'll need (and many more that we won't) are in the workspace,
    we can initiate the redeployment. Before we proceed, let's discuss what might
    have caused the failure in the first place. We can identify three main culprits.
    One of the nodes stopped working, one of the infrastructure services is down (Swarm,
    Consul, and so on), or our own service failed. We'll skip the first possibility
    and leave it for later. If one of the infrastructure services stopped working,
    we could fix that by running Ansible playbooks. On the other hand, if the cluster
    is operating as expected, all we have to do is redeploy the container with our
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore provisioning with Ansible. The part of the script that runs
    Ansible playbooks is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The major difference, when compared with the previous Jenkins Workflow scripts,
    is that, this time, provisioning is inside the `try`/`catch` block. The reason
    is a possible node failure. If the culprit for this redeployment is one malfunctioning
    node, provisioning will fail. That''s not a problem in itself if the rest of the
    script is run. For that reason, we have this script block under `try`/`catch`,
    thus ensuring that the script continues running no matter the provisioning result.
    After all, if a node is not working, Swarm will redeploy the service somewhere
    else (explained in more detail later on). Let''s move onto the next use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Those two lines are the same as in the deployment script in the Jenkinsfile.
    The only, subtle, difference is that the number of instances is not hardcoded,
    but, as we saw earlier, discovered.
  prefs: []
  type: TYPE_NORMAL
- en: That's it for now. With the script we explored, we have two out of three scenarios
    covered. Our system will recover if one of infrastructure or one of our services
    fails. Let's try it out.
  prefs: []
  type: TYPE_NORMAL
- en: We'll stop one of the infrastructure services and see whether the system will
    get restored to the original state. There is probably no better candidate than
    `nginx`. It is part of our services infrastructure and, without it, none of our
    services work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without nginx, our service is not accessible through the port `80`. At no point,
    Consul will know that `nginx` failed. Instead, Consul checker will detect that
    the `books-ms` service is not operational, and initiate a new build of the Jenkins
    job `service-redeploy`. As a result, provisioning and redeployment will be executed.
    Part of Ansible provisioning is in charge of ensuring that, among others, nginx
    is running:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's enter the `swarm-master` node and stop the `nginx` container.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'With nginx process dead, the `books-ms` service is not accessible (at least
    not through the port `80`). We can confirm that by sending an HTTP request to
    it. Please bear in mind that Consul will initiate redeployment through Jenkins,
    so hurry up before it becomes operational again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, `curl` returned the `Connection refused` error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also take a look at the Consul UI. The Service `books-ms` check should
    be in the critical state. You can click on the `swarm-master` link to get more
    details about all the service running on that node and their statuses. As a side
    note, `books-ms` is registered as running on the `swarm-master` server because
    that''s where the proxy is. There is also `books-ms-blue` or `books-ms-green`
    service that contains data specific to deployed containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul Health Checks and Watches for Monitoring Services](img/B05848_15_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-18 – Consul status screen with one check in the critical status
  prefs: []
  type: TYPE_NORMAL
- en: Finally, We can take a look at the service-redeploy console screen. The redeployment
    process should be on the way, or, more likely, finished by now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the build of the `service-redeploy` job is finished, everything should
    be restored to the original status, and we can use our service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the response is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The proxy service has been, indeed, redeployed, and everything is working as
    expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'What would happen if, instead stopping one of the infrastructure services,
    we remove the `book-ms` instance entirely? Let''s remove the service container,
    and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and open the `service-redeploy` Jenkins console screen. It might take
    a couple of seconds until Consul initiates a new build. Once started, all we need
    to do is wait a bit longer, until the build finishes running. Once you see the
    **Finished: Success** message, we can double check whether the service is indeed
    operational:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting Up Consul Health Checks and Watches for Monitoring Services](img/B05848_15_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-19 – Output of the service-redeploy build
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The combined output of both commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Our service is, indeed, running and accessible through the proxy. The system
    healed itself. We can stop almost any process, on any of the Swarm nodes, and,
    with a few seconds delay, system will restore itself to the previous state. The
    only thing we haven''t tried is to stop the whole node. Such an action would require
    a few more changes to our scripts. We''ll explore those changes later on. Please
    be aware that this is a demo setting and it does not mean that the system, as
    it is now, is ready for production. On the other hand, it''s not far either. With
    a bit of tweaking, you could consider applying this to you system. You might want
    to add some notifications (email, Slack, and so on) and adapt the process to your
    needs. The important part is the process. Once we understand what we want, and
    how to get there, the rest is usually only a question of time:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process we have, at this moment, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Consul performs periodic HTTP requests, runs custom scripts or waits for **time-to-live**
    (**TTL**) messages from services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case Consul's request does not return status code `200`, the script returns
    a non-zero exit code, or TTL message was not received, Consul sends a request
    to Jenkins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upon receiving a request from Consul, Jenkins initiates redeployment process,
    sends notification messages, and so on:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Setting Up Consul Health Checks and Watches for Monitoring Services](img/B05848_15_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-20 – Checking and healing Consul pinging services
  prefs: []
  type: TYPE_NORMAL
- en: We explored a few examples of reactive healing. Those were, by no means, exhaustive
    enough to provide you with everything you need to set up your own system, but,
    hopefully, provided you with a path that you should explore in more depth, and
    adapt to your own needs. Right now, we'll move our attention to preventive measure
    we can take. We'll examine scheduled scaling and descaling. It is a good candidate
    as an introduction to preventive healing since it is probably the easiest one
    to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Preventive Healing through Scheduled Scaling and Descaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preventive healing is a huge topic in itself and, in all but the simplest scenarios,
    requires historical data that can be used to analyze the system and predict the
    future. Since, at this moment, we neither have the data, nor the tools to generate
    them, we'll start with a very simple example that does not require any of those.
  prefs: []
  type: TYPE_NORMAL
- en: The scenario we'll explore is as follows. We are working on an online book store.
    Marketing department decided that, starting from the new years eve, all readers
    will be able to purchase books with a discount. The campaign will last for a day,
    and we expect it to generate a huge interest. In technical terms, that means that
    during 24 hours, starting from midnight, January the first, our system will be
    under heavy load. What should we do? We already have processes and tools that
    allow us to scale our system (or parts that will be most affected). What we need
    to do is scale selected services before the campaign starts and, once it's finished,
    restore it to the original state. The problem is that no one wants to celebrate
    new years eve in the office. We can fix that easily with Jenkins. We can create
    a scheduled job that will scale, and, later on, descale our services. With this
    problem solved, another one emerges. To how many instances should we scale? We
    can define a number in advance but, in that way, we risk making a mistake. For
    example, we might decide to scale to three instances (at this moment we have only
    one). Between today and the start of the campaign, due to some other reason, the
    number of instances might increase to five. In such a scenario, not only that
    we would not increase the capacity of our system, but would accomplish quite contrary.
    Our scheduled job would descale the service from five to three. The solution might
    be to use relative values. Instead of specifying that the system should be scaled
    to three instances, we should set it up in a way that the number of instances
    should be increased by two. If there is one instance running, such a process would
    launch two more and increase the overall number to three. On the other hand, if
    someone already scaled the service to five, the end result would be seven containers
    running inside our cluster. The similar logic can be employed after the campaign
    is finished. We can create the second scheduled job that would decrease the number
    of running instances by two. From three, to one. From five, to three. It does
    not matter how many will be running at that moment since we would decrease that
    number by two.
  prefs: []
  type: TYPE_NORMAL
- en: This process of preventive healing is similar to the usage of vaccinations.
    Their primary use is not to heal an existing infection, but to develop immunity
    that will prevent them from spreading in the first place. In the same way, we'll
    schedule scaling (and later on descaling), in order to prevent the increased load
    affecting our system in unexpected ways. Instead of healing an infected system,
    we'll prevent it from getting into bad shape.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see such the process in action.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please open the Jenkins `books-ms-scale` configuration screen. The job configuration
    is very straightforward. It has one parameter called `scale` with the default
    value of `2`. It can be adjusted when starting a build. `Build Triggers` is set
    to `build periodically` with the value `45 23 31 12`. If you already used cron
    scheduling, this should look familiar. The format is `MINUTE HOUR DOM MONTH DOW`.
    The first number represents minutes, the second hours, the third is the day of
    a month, followed by month and the day of the week. Asterisk, can be translated
    to any. So, the value we are using is fourty fifth minute of the twenty third
    hour, on thirty first day of the twelfth month. In other words, fifteen minutes
    before new years eve. That is more than enough time for us to increase the number
    of instances before the campaign starts. For more information about the scheduling
    format, please click the icon with a question mark located right of the `Schedule*`
    field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third, at last, part of the job configuration is the following Workflow
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Since there is no real reason to duplicate the code, we are using the helper
    functions from the `roles/jenkins/files/scripts/workflow-util.groovy` script.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the number of instances we want to run. We do that by
    adding the value of the `scale` parameter (defaults to two) to the number of instances
    our service is currently using. We get the latter by invoking the `getInstances`
    function we already utilized in a couple of cases throughout the book. That new
    value is put to Consul through the `putInstances` function. Finally, we run a
    build of the `service-redeploy` job which does the redeployment that we need.
    To summarize, since the `service-redeploy` job reads the number of instances from
    Consul, all we had to do in this script, before invoking the `service-redeploy`
    build, was to change the `scale` value in Consul. From there on, `service-redeploy`
    job will do what''s needed to scale the number of containers. By invoking the
    `service-redeploy` job, we avoided replicating the code that is already used elsewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preventive Healing through Scheduled Scaling and Descaling](img/B05848_15_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-21 – Configuration of the books-ms-scale job representing scheduled
    scaling
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have two paths we can take. One is to wait until new years eve and confirm
    that the job works. I will take liberty and assume that you do not have so much
    patience, and proceed with the alternative. We''ll run the job manually. Before
    we do that, let''s take a quick look at the current situation inside our Swarm
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The combined output of the commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We can see that only one instance of the books-ms service is running (`booksms_app-blue_1`)
    and that Consul has the value of `1` stored as the `books-ms/instances` key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run the `books-ms-scale` Jenkins job. If everything works as expected,
    it should increase the number of `books-ms` instances by two, resulting in three
    in total. Please open the `books-ms-scale` build screen and click the **Build**
    button. You can monitor the progress by opening the `books-ms-scale` console screen.
    You''ll see that, after storing the new number of instances in Consul, it will
    invoke a build of the `service-redeploy` job. After a few seconds, the build will
    finish, and we''ll be able to verify the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The combined output of the commands is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, this time, three instances of the service are running. We can
    observe the same result from the Consul UI, by navigating to the `key/value books-ms/instances
    screen`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preventive Healing through Scheduled Scaling and Descaling](img/B05848_15_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-22 – Consul UI Key/Value books-ms/instances screen
  prefs: []
  type: TYPE_NORMAL
- en: Our system is now ready to take the increased load during those 24 hours. As
    you saw, we were very generous by scheduling it to run 15 minutes before the due
    date. The execution of the build lasted only a couple of seconds. We could speed
    it even more by skipping the provisioning part of the `service-redeploy` job.
    I'll leave that to you as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Add conditional to the service-redeploy job**'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the service-redeploy Jenkins job so that provisioning is optional. You'll
    have to add a new parameter that accepts boolean value and add an if/else statement
    to the workflow script. Make sure that the parameter has the default value set
    to true so that provisioning is always performed unless specified otherwise. Once
    finished, switch to the configuration of the `books-ms-scale` job and modify it
    so that the call to the service-redeploy job passes the signal to skip provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: What happens after 24 hours passes, and the campaign is over? The Jenkins job
    `books-ms-descale` will be run. It is the same as the `books-ms-scale` job with
    two notable differences. The `scale` parameter is set to `-2` and it is scheduled
    to run on the second of January, fifteen minutes after midnight (`15 0 2 1 *`).
    We gave our system fifteen minutes of cool-down time. The Workflow script is the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run it by opening the `books-ms-descale` build screen, and clicking
    the **Build** button. It will reduce the number of instances by two, and run a
    build of the `service-redeploy` job. Once finished, we can have another look at
    our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The combined output of the commands is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We are back where we started. The campaign is finished, and the service is reduced
    from three instances to one. The value in Consul is restored as well. The system
    survived horde of visitors desperately trying to benefit from our new years eve
    discount, business is happy that we were able to serve them all, and life continues
    as it was.
  prefs: []
  type: TYPE_NORMAL
- en: We could have created different formulas to accomplish our goals. It could be
    as simple as multiplying the number of existing instances. That would give us
    a bit more realistic scenario. Instead of adding two new containers, we could
    have multiplied them by two. If three were running before, six would be running
    afterwards. As you can imagine, those formulas can often be much more complicated.
    More importantly, they would require much more consideration. If, instead of running
    one, we were running fifty different services, we would not apply the same formula
    to all of them. Some would need to be scaled a lot, some not so much, while other
    not at all. The best way to proceed would be to employ some kind of stress tests
    that would tell us which pieces of the system require scaling, and how much that
    scaling should be. There's plethora of tools that can run those tests, with JMeter
    and Gatling (my favorite) being only a few.
  prefs: []
  type: TYPE_NORMAL
- en: I mentioned, at the beginning of this chapter, that preventive healing is based
    on historical data. This was a very poor, yet very efficient and simple way of
    demonstrating that. In this case, historical data was in our heads. We knew that
    a marketing campaign will increase the load on our service, and acted in a way
    that potential problems are avoided. The real, and much more complicated, way
    to create preventive healing require more than our memory. It requires a system
    capable of storing and analyzing data. We'll discuss requirements for such a system
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Reactive Healing with Docker Restart Policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Those more familiar with Docker might be asking why I did not mention Docker
    restart policies. On a first look, they seem to be a very effective way to recuperate
    failed containers. They are, indeed, the easiest way to define when to restart
    containers. We can use the `--restart` flag on `docker run` (or the equivalent
    Docker Compose definition), and the container will be restarted on exit. The following
    table summarizes the currently supported restart policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Policy | Result |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `no` | Do not automatically restart the container when it exits. This is
    the default. |'
  prefs: []
  type: TYPE_TB
- en: '| `on-failure[:max-retries]` | Restart only if the container exits with a non-zero
    exit status. Optionally, limit the number of restart retries the Docker daemon
    attempts. |'
  prefs: []
  type: TYPE_TB
- en: '| `always` | Always restart the container regardless of the exit status. When
    you specify always, the Docker daemon will try to restart the container indefinitely.
    The container will also always start on daemon startup, regardless of the current
    state of the container. |'
  prefs: []
  type: TYPE_TB
- en: '| `unless-stopped` | Always restart the container regardless of the exit status,
    but do not start it on daemon startup if the container has been put to a stopped
    state before. |'
  prefs: []
  type: TYPE_TB
- en: An example of the usage of restart policy is as follows (please do not run it).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In that case, mongo would be restarted up to three times. The restart would
    occur only if the process running inside the mongo container exits with a non-zero
    status. If we stop that container, the restart policy would not be applied.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with restart policies is that there are too many corner cases not
    contemplated. The process running inside a container might fail due problems not
    directly related to the container that failed. For example, a service inside the
    container might be trying to connect to a database through a proxy. It might have
    been designed to stop if the connection could not be established. If, for some
    reason, the node with the proxy is not operational, it doesn't matter how many
    times we restart the container, the result will always be the same. There is nothing
    wrong in trying, but, sooner or later, someone needs to be notified about the
    problem. Maybe provisioning scripts need to be run to restore the system to the
    desired state. Maybe more nodes need to be added to the cluster. Maybe even the
    whole data center is not operational. No matter the cause, there are many more
    possible paths that could be taken than what restart policy permits. For those
    reasons, we do need a more robust system to deal with all those circumstances,
    and we are already on the way of creating it. The flow we have established is
    much more robust than simple restart policies, and it already covers the same
    problems as those that can be solved with the Docker restart policy. Actually,
    as it is now, we have many more paths covered. We perform containers orchestration
    with Docker Swarm that will make sure that our services are deployed to the most
    suited nodes inside the cluster. We use Ansible that is continuously (with each
    deploy) provisioning the cluster, thus ensuring that the whole infrastructure
    is in the correct state. We are using Consul in combination with Registrator and
    Consul Template for service discovery, making sure that the registry of all our
    services is always up to date. Finally, Consul health checks are continuously
    monitoring the state of our cluster and, in case of a failure, sends requests
    to Jenkins that will initiate appropriate corrective actions.
  prefs: []
  type: TYPE_NORMAL
- en: We are utilizing the Docker's slogan *batteries included but removable* to our
    benefit by extending the system to suit our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Combining On-Premise with Cloud Nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I won't start a discussion whether to use on-premise servers or cloud hosting.
    Both have their advantages and disadvantages. The decision what to use depends
    on individual needs. Besides, such an attempt would be better suited inside the
    clustering and scaling chapter. However, there is a clear use case in favour of
    cloud hosting that would suit very well the needs of, at least, one of the scenarios
    from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud hosting shines when we need a temporary increase in the cluster capacity.
    A good example would be our fictional scenario with the new years eve campaign.
    We needed to boost our capacity for a day. If you are already hosting all your
    servers in the cloud, this scenario would require a few more nodes to be created
    and, later on, destroyed, once the load is reduced to its former size. On the
    other hand, if you use on-premise hosting, that would be an opportunity to contract
    cloud hosting only for those additional nodes. Buying a new set of servers that
    will be used only during a short period is very costly, especially if we take
    into account that the cost cosists not only of hardware price, but also maintenance.
    If, in such cases, we use cloud nodes, the invoice would be paid only for the
    time we use them (assuming that we destroy them afterwards). Since we have all
    the scripts for provisioning and deploying services, the setup of those nodes
    would be almost effortless.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Personally, I prefer the combination of on-premise and cloud hosting. My on-premise
    servers are fulfilling the need for the minimum capacity, while cloud hosting
    nodes are being created (and destroyed) whenever that capacity needs to be temporarily
    increased. Please note that such a combination is only my personal preference,
    and might not apply to your use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The important part is that everything you learned from this book is equally
    applicable to both situations (on-premise or cloud). The only significant difference
    is that you should not be using Vagrant on production servers. We are using it
    only to create quickly virtual machines on you laptop. If you are looking for
    a way to create production VMs in a similar way as with Vagrant, I suggest you
    explore another HashiCorp product called **Packer**.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Healing Summary (So Far)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we built so far is, in some cases, close to what Kubernetes and Mesos offer
    out of the box, while in others exceeds their functionality. The real advantage
    of the system we are working on is its the ability to fine-tune it to your needs.
    That is not to say that Kubernetes and Mesos should not be used. You should, at
    least, be familiar with them. Do not take anyone's word for granted (not even
    mine). Try them out and make your own conclusions. There are as many use cases
    as there are projects, and each is different from the other. While in some cases
    the system we built would provides a good base to build upon, there are others
    where, for example, Kubernetes or Mesos might be more appropriate. I could not
    fit all the possible combinations in detail inside a single book. That would increase
    it to an unmanageable size.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, I choose to explore ways we can build systems that are highly extensible.
    Almost any piece we used by now can be extended, or replaced with another. I feel
    that this approach gives you more possibilities to adapt examples to your own
    needs and, at the same time, learn not only how something works, but why we chose
    it.
  prefs: []
  type: TYPE_NORMAL
- en: We went far from the humble beginnings of this book, and we are not yet done.
    The exploration of self-healing systems will continue. However, first we need
    turn our attention to different ways of collecting data generated inside our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As the first part of the self-healing subject is closing to an end, let us destroy
    our VMs, and start the new chapter fresh.
  prefs: []
  type: TYPE_NORMAL
- en: 'You know what follows next. We''ll destroy everything we did, and begin the
    next chapter fresh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
