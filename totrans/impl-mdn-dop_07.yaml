- en: Docker Swarm and Kubernetes - Clustering Infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how powerful Docker is but we have not unleashed the full
    potential of containers. You have learned how to run containers on a single host
    with the local resources without the possibility of clustering our hardware resources
    in a way that allows us to uniformly use them as one big host. This has a lot
    of benefits, but the most obvious one is that we provide a middleware between
    developers and ops engineers that acts as a common language so that we don't need
    to go to the ops team and ask them for a machine of a given size. we just provide
    the definition of our service and the Docker clustering technology will take care
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to dive deep into deploying and managing applications
    on Kubernetes, but we will also take a look at how Docker Swarm works.
  prefs: []
  type: TYPE_NORMAL
- en: 'People usually tend to see Kubernetes and Docker Swarm as competitors, but
    in my experience, they solve different problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes** is focused on advanced microservices topologies that offer all
    the potential of years of experience running containers in Google'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Swarm** offers the most straightforward clustering capabilities for
    running applications in a very simple way'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, Kubernetes is more suited for advanced applications, whereas Docker
    Swarm is a version of Docker on steroids.
  prefs: []
  type: TYPE_NORMAL
- en: 'This comes at a cost: managing a Kubernetes cluster can be very hard, whereas
    managing a Docker Swarm cluster is fairly straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: There are other clustering technologies that are used in the current DevOps
    ecosystem, such as DC/OS or Nomad, but unfortunately, we need to focus on the
    ones that are, in my opinion, the most suited for DevOps and focus specifically
    on Kubernetes that, in my opinion, is eating the DevOps market.
  prefs: []
  type: TYPE_NORMAL
- en: Why clustering ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](0b33ce27-d21e-4021-87ad-6865c65b7e33.xhtml), *DevOps in the Real
    World,* you learned about organizational alignment and why is important to shift
    roles in a company to accommodate DevOps tools. It is not okay anymore to just
    be a developer or a sysadmin; now you need to be a full stack DevOps engineer
    in order to get success in any project. Full stack DevOps means that you need
    to understand the business and the technology used in the organisation. Think
    about it; if you became a civil engineer instead of an IT engineer, it is mandatory
    to know the local rules (the business) plus the commercial names of the tools
    used to build roads and bridges (the technology) but also be able to coordinate
    their building (ops). Maybe not every engineer needs to know everything but they
    need to be aware of in the full picture in order to ensure the success of the
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to containers and DevOps, making concepts simple for everyone to
    understand is something that's mandatory nowadays. You want to ensure that all
    the engineers in your project are able to trace the software from conception (requirements)
    to deployment (ops) but also have in mind predictability so that the business
    people that barely speak tech are able to plan strategies around the products
    that you build.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the keys to achieving the flow described here is predictability, and
    the way to achieve predictability is making uniform and repeatable use of your
    resources. As you learned earlier, cloud data centers such as Amazon Web Services
    or Google Cloud Platform provide us with a virtually unlimited pool of resources
    that can be used to build our systems in a traditional way:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the size of the VMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provision VMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Or, if we want to draw a diagram so we can understand it better, it would be
    similar to the next one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a68c76ed-810f-4527-bbbc-c2226fefe7fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are a few considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: Clear separation between Development and Operations (this may vary depending
    on the size of your company
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software components owned by Development and deployments and configuration owned
    by Operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some servers might be relatively underutilized (**Server 1**) and on a very
    low load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This has been the picture for 40 odd years of software development, and it
    is still the picture if we are running Docker containers, but there are few problems
    in it:'
  prefs: []
  type: TYPE_NORMAL
- en: If a problem arises in **Component 3** in production, who is responsible for
    it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is a configuration mismatch, who will fix it if developers are not
    supposed to see what is going on in production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Server 1** is running a software component that might be called only once
    or twice a day (imagine an authentication server for workstations); do we need
    a full VM just for it?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we scale our services in a transparent manner?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These questions can be answered, but usually, they get an answer too late in
    the game plus "the hidden requirements" are only seen once the problems arise
    at the worst possible time:'
  prefs: []
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-healing infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circuit breaking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During college years, one of the things in common across all the different
    subjects was reusability and extensibility. Your software should be extensible
    and reusable so that we can potentially build libraries of components creating
    the engineering sweet spot (not just software development): build once, use everywhere.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This has been completely overlooked in the operations part of the software
    development until recent years. If you get a job as a Java developer in a company,
    there is a set of accepted practices that every single Java developer in the world
    knows and makes use of so you can nearly hit the ground running without too many
    problems (in theory). Now let''s raise a question: if all the Java apps follow
    the same practices and set of common patterns, why does every single company deploy
    them in a different way?'
  prefs: []
  type: TYPE_NORMAL
- en: A continuous delivery pipeline has the same requirements in pretty much every
    company in the IT world, but I have seen at least three different ways of organizing
    it with a huge amount of custom magic happening that only one or two people within
    the company know of.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clusters are here to save us. Let''s reshuffle the image from before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e0e0ad8-b84a-468c-bc50-e7d0bd7f2627.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we have solved few of our problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now development and ops are connected via a middleware: the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components can be replicated (refer to component 1 and component 1') without
    provisioning extra hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DevOps engineers are the glue between the two teams (development and ops), making
    things happen at a fast pace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The stability of the full system does not depend on a single server (or component)
    as the cluster is built in a way that can accept some level of failure by just
    degrading performance or taking down the less critical services: it is okay to
    sacrifice e-mailing in order to keep the accounting processes of the company running.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And about the hidden requirements. Well, this is where we need to make a decision
    about which clustering technology we want to use as they approach the service
    discovery, load balancing, and auto-scaling from different angles.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in previous chapters, Docker is a fantastic tool that follows
    the most modern architectural principles used for running applications packed
    as containers. In this case, Docker Swarm runs only Docker containers, ignoring
    other technologies that, at the moment, are not suitable for production, such
    as Rkt. Even Docker is quite new to the scene up to a point that some companies
    hesitate in deploying it in their production systems, as there is not so much
    expertise in the market as well as many doubts about security or how Docker works
    in general.
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker Swarm** is the clustered version of Docker, and it solves the problem
    described in the previous section in a very simple manner: pretty much all the
    docker commands that you learned in the Docker chapter works in Docker Swarm so
    that we can federate our hardware without actually taking care of the hardware
    itself. Just add nodes to the pool of resources and Swarm will take care of them,
    leveraging the way we build our systems to purely containers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Swarm is not something that we need to install aside from the Docker
    engine: it comes embedded into it and it is a mode rather than a server itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm is evolving quite quickly and it is dragging Docker itself along
    as more and more features are being baked into it due to its usage in the Swarm
    mode. The most interesting part of this is how we can leverage our Docker knowledge
    into it without any extra as the swarm mode of our Docker engine takes care of
    the resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also a problem: we are limited by the Docker API, whereas with Kubernetes
    (we will come back to it in a second), we are not only limited by the Docker API,
    but we can also extend the Kubernetes API to add new objects to fulfill our needs.'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm can be operated through `docker-compose` (up to a certain extent),
    which provides a decent approach to infrastructure as code but is not very comprehensive
    when our application is somehow complex.
  prefs: []
  type: TYPE_NORMAL
- en: In the current IT market, Kubernetes seems to be the clear winner of the orchestration
    battle, and as such, we are going to focus on it, but if you want to learn more
    about Docker Swarm, the official documentation can be found at [https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/).
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is the jewel of the crown of the containers orchestration. The product
    itself was vamped by Google leveraging years of knowledge on how to run containers
    in production. Initially, it was an internal system used to run Google services,
    but at some point, it became a public project. Nowadays, it is an open source
    project maintained by few companies (Red Hat, Google, and so on) and is used by
    thousands of companies.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this, the demand for Kubernetes engineers has skyrocketed
    up to a point that companies are willing to hire people without expertise in the
    field but with a good attitude to learn new technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes has become so popular due to, in my opinion, the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: It solves all the deployment problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It automates micro services' operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a common language to connect ops and development with a clean interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once it is setup, it is very easy to operate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nowadays, one of the biggest problems in companies that want to shorten the
    delivery life cycle is the **red tape that has grown around the delivery process**.
    Quarter releases are not acceptable anymore in a market where a company of five
    skilled engineers can overtake a classic bank due to the fact that they can cut
    the red tape and streamline a delivery process that allows them to release multiple
    times a day.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of my professional activities is to speak at conferences (meet-ups in Dublin,
    RebelCon in Cork, **Google Developer Groups** (**GDGs**) in multiple places, Google
    IO Extended) and I always use the same words in all the talks: release management
    should stop being a big bang event that stops the world for three hours in order
    to release a new version of your company''s application and **s**tart being a
    painless process that can be rolled back at any time so that we remove the majority
    of the stress from it by providing the tools to manage a faulty release.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This (not just this, but mainly this) is Kubernetes: a set of tools and virtual
    objects that will provide the engineers with a framework that can be used to streamline
    all the operations around our apps:'
  prefs: []
  type: TYPE_NORMAL
- en: Scale up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero downtime rollouts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canary deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rollbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secret management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes is built in a technology-agnostic way. Docker is the main container
    engine, but all the components were designed with interchangeability in mind:
    once Rkt is ready, it will be easy to switch to Rkt from Docker, which gives an
    interesting perspective to the users as they don''t get tied to a technology in
    particular so that avoiding vendor locking becomes easier. This applies to the
    software defined network and other Kubernetes components as well.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the pain points is the steep learning curve for setting it up as well
    as for using it.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is very complex, and being skilled in its API and operations can
    take any smart engineer a few weeks, if not months, but once you are proficient
    in it, the amount of time that you can save completely pays off all the time spent
    learning it.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the same way, setting up a cluster is not easy up to a point that companies
    have started selling Kubernetes as a service: they care about maintaining the
    cluster and you care about using it.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the (once again, in my opinion) most advanced providers for Kubernetes
    is the **Google Container Engine** (**GKE**), and it is the one that we are going
    to use for the examples in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I was planning the contents of this chapter, I had to make a decision
    between two items:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Showing how to build applications around Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I was thinking about it for a few days but then I realized something: there
    is a lot of information and about half a dozen methods to set up a cluster and
    none of them are official. Some of them are supported by the official Kubernetes
    GitHub repository, but there is no (at the time of writing this) official and
    preferred way of setting up a Kubernetes instance either on premises or in the
    cloud, so the method chosen to explain how to deploy the cluster might be obsolete
    by the time this book hits the market. The following options are the most common
    ways of setting up a Kubernetes cluster currently:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kops**: The name stands for Kubernetes operations and it is a command-line
    interface for operating clusters: creating, destroying, and scaling them with
    a few commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubeadm**: Kubeadm is alpha at the moment and breaking changes can be integrated
    at any time into the source code. It brings the installation of Kubernetes to
    the execution of a simple command in every node that we want to incorporate to
    the cluster in the same way as we would do if it was Docker Swarm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tectonic**: Tectonic is a product from CoreOS to install Kubernetes in a
    number of providers (AWS, Open Stack, Azure) pretty much painlessly. It is free
    for clusters up to nine nodes and I would highly recommend that, at the very least,
    you play around it to learn about the cluster topology itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ansible**: Kubernetes'' official repository also provides a set of playbooks
    to install a Kubernetes cluster on any VM provider as well as on bare metal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these options are very valid to set up a cluster from scratch as they
    automate parts of Kubernetes architecture by hiding the details and the full picture.
    If you really want to learn about the internals of Kubernetes, I would recommend
    a guide written by Kelsey Hightower called Kubernetes the hard way, which basically
    shows you how to set up everything around Kubernetes, from the etcd cluster needed
    to share information across nodes to the certificates used to communicate with
    `kubectl`, the remote control for Kubernetes. This guide can be found at [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way).
  prefs: []
  type: TYPE_NORMAL
- en: And it is maintained and up to date with new versions of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: As you can guess from this explanation, in this chapter, you are going to learn
    about the architecture of Kubernetes, but mainly, we will focus on how to deploy
    and operate applications on Kubernetes so that by the end of this chapter, we
    have a good understanding of how we can benefit from an already running cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes logical architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first problem that you will find once you start playing with Kubernetes
    is creating a mental map on how and where everything runs in Kubernetes as well
    as how everything is connected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, it took me few weeks to fully understand how it all was wiring
    up, but once I had the picture in my mind, I drew something similar to what is
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02b1c383-6951-4f06-a713-c3934fe10ace.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is Kubernetes on a very high level: a master node that orchestrates the
    running of containers grouped in pods across different Nodes (they used to be
    called minions but not anymore).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This mental map helps us understand how everything is wired up and brings up
    a new concept: the pod. A pod is basically a set of one or more containers running
    in orchestration to achieve a single task. For example, think about a cache and
    a cache warmer: they can run in different containers but on the same pod so that
    the cache warmer can be packed as an individual application. We will come back
    to this later on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this picture, we are also able to identify different physical components:'
  prefs: []
  type: TYPE_NORMAL
- en: Master
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The master is the node that runs all support services such as DNS (for service
    discovery) as well as the API server that allows us to operate the cluster. Ideally,
    your cluster should have more than one master, but in my opinion, being able to
    recover a master quickly is more important than having a high availability configuration.
    After all, if the master goes down, usually, it is possible to keep everything
    running until we recover the master that usually is as simple as spawning a new
    VM (on the cloud) with the same template as the old master was using.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to have a master running with the IP Tables blocking connections
    to key ports so that it does not join the cluster and remove the IP Tables rules
    once you want the master to become the lead of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The nodes are basically workers: they follow instructions from the master in
    order to deploy and keep applications alive as per the specified configuration.
    They use a software called Kubelet, which is basically the Kubernetes agent that
    orchestrates the communication with the master.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the networking, there are two layers of network in here:'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hardware network is what we all know and that is used to interconnect the
    VMs on the cluster. It is defined in our cloud provider (AWS, Google Cloud Platform,
    and so on), and there is nothing special about it, just bear in mind that ideally,
    this network should be a high profile network (Gigabyte Ethernet) as the inter-node
    traffic can be quite high.
  prefs: []
  type: TYPE_NORMAL
- en: The software network (or **Software Defined Network**, **SDN**) is a network
    that runs on top of Kubernetes middleware and is shared between all the nodes
    via **etcd**, which is basically a distributed key value storage that is used
    by Kubernetes as a coordination point to share information about several components.
  prefs: []
  type: TYPE_NORMAL
- en: 'This SDN is used to interconnect the pods: the IPs are virtual IPs that do
    not really exist in the external network and only the nodes (and master) know
    about. They are used to rout the traffic across different nodes so that if an
    app on the node 1 needs to reach a pod living in the **Node 3**, with this network,
    the application will be able to reach it using the standard `http/tcp` stack.
    This network would look similar to what is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ff1c1a6-6e62-44c8-aeed-57a30964dedd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explain this a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: The addresses on the network 192.168.0.0/16 are the physical addresses. They
    are used to interconnect the VMs that compound the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The addresses on the network 10.0.0.0/24 are the software defined network addresses.
    They are not reachable from outside the cluster and only the nodes are able to
    resolve these addresses and forward the traffic to the right target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking is a fairly important topic in Kubernetes, and currently, the most
    common bottleneck in performance is that traffic forwarding is common across nodes
    (we will come back to this later on in this chapter), and this causes extra inter-node
    traffic that might cause a general slowdown of the applications running in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In general and for now, this is all we need to know about the Kubernetes architecture.
    The main idea behind Kubernetes is to provide a uniform set of resources that
    can be used as a single computing unit with easy zero downtime operations. As
    of now, we really don't know how to use it, but the important thing is that we
    have a mental model of the big picture in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a cluster in GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing we need to start playing with in Kubernetes is a cluster. There
    are several options, but we are going to use GKE as we have already signed for
    the trial and there should be enough credit in there for going through the full
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Another option if you did not sign for the trial on GCP is Minikube. Minikube
    is an out-of-the-box, easy-to-install local cluster that runs on VMs and is a
    very good tool for experimenting with new features without being afraid of breaking
    something.
  prefs: []
  type: TYPE_NORMAL
- en: The Minikube project can be found at [https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube).
  prefs: []
  type: TYPE_NORMAL
- en: Its documentation is fairly comprehensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to create a cluster in GCP, the first thing we need to do is open
    the container engine in the online console in GCP that will show something similar
    to what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/317e21d3-388e-4dc9-b6bc-fc66f39a85c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This means that you have no clusters at the moment. Click on the Create a container
    cluster button and fill in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44a48f26-1eee-4542-b5fb-5ac6df3c3ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just a few considerations here:'
  prefs: []
  type: TYPE_NORMAL
- en: Give a comprehensive name to the cluster. In my case, I named it `testing-cluster`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose a zone that is close to you geographically, in my case, `europe-west1-c`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding the cluster version, choose the default one. This is the version of
    Kubernetes that you want your cluster to run. It can be seamlessly upgraded later.
    Also, be aware that Kubernetes releases a new version every 2 months (apporximately),
    so by the time you are reading this book, it is most likely that there will be
    a more modern version available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The machine type should also be the standard one (1 vCPU 3.75 GB of RAM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size is the number of machines that we want to use in our cluster. Three is
    a good number for testing and it can also be increased (or decreased later on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everything else should be default. Auto-upgrade and auto-repair are beta functionalities
    that I would hesitate to use in a production cluster yet. These two options allow
    GCP to take actions if there is a new version of Kubernetes available or one of
    the nodes breaks for some reason.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the form is completed click on Create Cluster, and that is everything.
    Now Google is provisioning a cluster for us. In order to check what is going on,
    open the tab of the Compute Engine in the GCP and you should see something similar
    to what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2890b604-2b63-4fd3-97db-22e6d31a8fca.png)'
  prefs: []
  type: TYPE_IMG
- en: Three machines have been created in the compute engine with the prefix "`gke-`",
    which means that they belong to the GKE, **K** is for **Kubernetes**. They are
    regular machines, and there's nothing special about them aside from the fact that
    Google has provisioned all the software required to set up a Kubernetes Node,
    but where is the master?
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the interesting thing about running Kubernetes in Google Cloud Platform:
    they look after your master so there is no need to worry about the high availability
    or upgrading it as it is done automatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The master of our cluster is hosting one of the key components of our whole
    cluster: the API server. All the operations in Kubernetes are done via the API
    server with a component called `kubectl`. Kubectl stands for Kubernetes Control
    and is basically a terminal program that you can install on your local machine
    (or in a continuous integration server), add the configuration for a given cluster,
    and start issuing commands to our cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to install `kubectl`. In the previous chapters, we already
    installed the Google Cloud SDK (`gcloud` command), which can be used to install
    `kubectl` with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And that''s it. Now we can use `kubectl` in our system as if it was any other
    command, but we need to add our cluster configuration. As of now, `kubectl` is
    not configured to operate our cluster, so the first thing we need to do is fetch
    the required configuration. Google Cloud Platform makes it very easy. If you open
    the Google Container Engine tab, it now should look similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78bc4e64-e930-49b3-bd60-83a16b3d782b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is a button called Connect on the right-hand side of
    the screen. By clicking on it, you will be presented with the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e2ca1a1-c476-4fe8-8241-9aacb15e9323.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The date in the form will be slightly different (as the name of your cluster
    and project will be different), but there are two commands presented in there:'
  prefs: []
  type: TYPE_NORMAL
- en: A `gcloud` command to get the configuration of our Kubernetes cluster in our
    local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `kubectl` command to start the proxy into the Kuberentes Dashboard UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first command is easy. Just execute it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output will be similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'So, what happened here is that `gcloud` fetched the configuration and installed
    it locally for us to operate the cluster. You can try this by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will output the list of nodes in your cluster. Kubectl is a very extensive
    command-line tool. With it, we can do pretty much anything inside the cluster,
    as we will learn in the rest of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second command in the preceding screenshot is used to start a proxy in
    Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let's explain what happened here. Kubernetes makes heavy usage of client certificates.
    In order to communicate with the master, our machine needs to proxy the requests
    sending the certificate to validate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we browse to the URL in the preceding screenshot now, `http://localhost:8001/ui`,
    we get presented with the Kubernetes dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f4986fd-fc75-4ae3-82d9-4d605efc0768.png)'
  prefs: []
  type: TYPE_IMG
- en: The dashboard is basically a nice way of presenting all the information of our
    running cluster to the end users. It is also possible to operate the cluster up
    to a certain extent from the dashboard, but my recommendation will be to master
    `kubectl` as it is way more powerful. On the dashboard, we can see a lot of information,
    such as the state of the nodes, the items deployed into the cluster (Pods, Replica
    Sets, Daemon Sets, and so on), and the namespaces as well as many other elements.
  prefs: []
  type: TYPE_NORMAL
- en: Explore around a bit and get yourself familiar with the dashboard as it is a
    nice tool to actually see things happening in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes divides the workloads into namespaces. A namespace is a virtual
    cluster that allows the engineers to segregate resources (up to a point) across
    different teams. It is also used by Kubernetes to run its own internal components.
    This is important because Kubernetes spreads the key components across different
    nodes to ensure high availability. In this case, we have three components that
    are running on every node:'
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes proxy (`kube-proxy`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes DNS (`kube-dns`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Kubernetes dashboard is what we just have seen: a user interface to represent
    the information within the Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes proxy is a proxy that the nodes use to resolve IP addresses in the
    SDN from Pods addresses to node addresses so that the cluster is able to redirect
    the traffic to the right Node.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes DNS is basically a load balancing and service discovery mechanism.
    In the next section, you will learn about the building blocks that we can use
    for deploying applications to Kubernetes. In particular, Services are strongly
    coupled with this DNS service in a way that in order to locate an application
    within Kubernetes, we just need to know its name and the configuration of the
    Service that groups the Pods compounding the given application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fact that we are running these components in every node enables Kubernetes
    to enter into an autopilot mode in case of a master going down: applications will
    continue working (in the majority of the cases) even without a master, so losing
    a master is not a catastrophic event.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have configured `kubectl` in our machines, it is time to learn about
    the building blocks that we can use in Kubernetes in order to build extremely
    robust applications.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding section, you learned about the cluster topology, but now we
    need the tools to run applications on it. We have already introduced one of the
    Kubernetes building blocks: the Pod. In this section, we are going to look at
    some of the most important API objects (building blocks) that Kubernetes provide
    in order to build our applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When I started learning Kubernetes, I was working in the second company that
    was deploying applications in a continuous delivery way, and I always had a question
    in mind: why are different companies trying to solve the same problem in different
    ways?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then I realized why: The element missing was the domain-specific language for
    continuous delivery. The lack of a common standard and well understood way of
    rolling out applications was preventing them to work efficiently and deliver value
    early in the chain. Everybody knows what a load balancer is or a proxy or many
    other elements that are involved in the deployment of a new version of an app,
    but the way people uses the in, say, imaginative ways is where the problem lies.
    If you hire a new engineer, their previous knowledge of continuous delivery becomes
    obsolete as they need to learn your way of doing things.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes solves this problem with a set of objects (Pods, ReplicaSets, DameonSets,
    and so on) that are described in YAML files (or JSON). Once we finish this section,
    we will already have enough knowledge to be able to, from the YAML or JSON files
    defining our resources, build a diagram about what the system looks like. These
    files, alongside the Docker images, are enough for Kubernetes to run our system,
    and we will look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods are the most basic element of the Kubernetes API. A Pod basically is a
    set of containers that work together in order to provide a service or part of
    it. The concept of Pod is something that can be misleading. The fact that we can
    run several containers working together suggests that we should be sticking the
    frontend and backend of our application on a single pod as they work together.
    Even though we can do this, it is a practice that I would strongly suggest you
    avoid. The reason for this is that by bundling together the frontend and the backend,
    we are losing a lot of flexibility that Kubernetes is providing us with, such
    as autoscaling, load balancing, or canary deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, pods contain a single container and it is, by far, the most common
    use case, but there are few legitimate use cases for multi-container pods:'
  prefs: []
  type: TYPE_NORMAL
- en: Cache and cache warmer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precalculating and serving HTML pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File upload and file processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, all of these are activities that are strongly coupled together,
    but if the feeling is that the containers within a pod are working toward different
    tasks (such as backend and frontend), it might be worth placing them in different
    Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two options for communication between containers inside a pod:'
  prefs: []
  type: TYPE_NORMAL
- en: Filesystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local network interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As Pods are indivisible elements running on a single machine, volumes mounted
    in all the containers of a pod are shared: files created in a container within
    a pod can be accessed from other containers mounting the same volume.'
  prefs: []
  type: TYPE_NORMAL
- en: The local network interface or loopback is what we commonly know as localhost.
    Containers inside a pod share the same network interface; therefore, they can
    communicate via localhost (or `127.0.0.1`) on the exposed ports.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, Kubernetes relies heavily on **Yet Another Markup Language**
    (**YAML**) files to configure API elements. In order to deploy a pod, we need
    to create a yaml file, but first, just create a folder called **deployments**,
    where we are going to create all the descriptors that we will be created on this
    section. Create a file called `pod.yaml` (or `pod.yml`) with the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the preceding `yaml` is fairly descriptive, but some points
    need clarification:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`: This is the version of the Kubernetes API that we are going to
    use to define our resource (in this case, pod). Kuberentes is a living project
    that evolves very quickly. The version is the mechanism used to avoid deprecating
    resources with new releases. In general, Kuberentes works with three branches:
    alpha, beta, and stable. In the preceding case, we are using the stable version.
    More information can be found at [https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: In this section, we are defining one of the most powerful discovery
    mechanisms that I have ever seen: the pattern matching. The section label, specifically,
    will be used later on to expose pods with certain **l**abels to the outer world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec`: This is where we define our container. In this case, we are deploying
    an `nginx` instance so that we can easily see how everything works without focusing
    too much on the application itself. As expected, the image and the exposed port
    have been specified. We have also defined the CPU and memory limitations for this
    Pod, so we prevent an outbreak in resource consumption (note that the YAML file
    is requesting the resources; they might not be available so the pod will operate
    with lower profile resources).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the simplest configuration for an item that we can create in Kubernetes.
    Now it''s time to deploy the resource in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce an output similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Disclaimer: there are several ways of creating a resource, but in this book,
    I will use `apply` as much as possible. Another possibility would be to use `create`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The advantage that `apply` has over create is that apply does a three-way diff
    between the previous version, the current version, and the changes that you want
    to apply and decides how is best to update the resource. This is letting Kubernetes
    do what it does best: automate container orchestration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With create, Kubernetes does not save the state of the resource, and if we
    want to run apply afterward in order to gracefully change the state of a resource,
    a warning is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This means that we can push our system to an unstable state for few seconds,
    which might not be acceptable depending on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have applied our YAML file, we can use `kubectl` to see what is going
    on in Kubernetes. Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output our pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a6b0b4e-706a-4686-b11f-43592c818f96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can do this for other elements of our cluster, such as the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And this will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4b67f6b-c276-4f97-917e-e1e95b2eeb2b.png)'
  prefs: []
  type: TYPE_IMG
- en: The `kubectl get` works for all the workflows in Kubernetes and the majority
    of the API objects.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of seeing what is going on in Kubernetes is using the dashboard.
    Now that we have created a pod, open the dashboard at `http://localhost:8001/ui`
    and navigate to the pods section on the left-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in order to access the dashboard, first, you need to execute `kubectl
    proxy` on a Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'There; you will see the list of the current deployed pods, in this case, just
    nginx. Click on it and the screen should look very similar to what is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15fd80e2-8bb2-4d68-830f-ffb7d6fb62a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we get a ton of information, from the memory and CPU that the pod is
    consuming to the node where it is running and a few other valuable items, such
    as the annotations applied to the pod. We can get this using the ''`describe`''
    command of `kubectl`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Annotations are a new concept and are the data around our API element, in this
    case, our pod. If you click on Last applied configuration in the Details section,
    you can see the data from the YAML file, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9577ed4e-2e88-4681-aeaf-42ca6d67379e.png)'
  prefs: []
  type: TYPE_IMG
- en: And this relates to the three-way diff that was explained earlier and is used
    by Kubernetes to decide the best way of upgrading a resource without getting into
    an inconsistent state.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of now, our pod is running in Kubernetes but is not connected to the outer
    world; therefore, there is no way to open a browser and navigate to the nginx
    home page from outside the cluster. One thing that we can do is open a remote
    session to a bash Terminal in the container inside the pod in a manner similar
    to what we would do with Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: And we are in. Effectively, we have gained access to a root terminal inside
    our container and we can execute any command. We will use this functionality later
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have seen how pods work, you might have a few questions abound what
    Kubernetes is supposed to do:'
  prefs: []
  type: TYPE_NORMAL
- en: How can we scale pods?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we roll out new versions of an application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we access our application?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will answer all these questions, but first, we need to know other 'building
    blocks'.
  prefs: []
  type: TYPE_NORMAL
- en: Replica Sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we know how to deploy applications in pods. The sole concept of pod
    is very powerful, but it lacks robustness. It is actually impossible to define
    scaling policies or even make sure that the pods remain alive if something happens
    (such as a node going down). This might be okay in some situations, but here is
    an interesting question. If we are biting the bullet on the overhead of maintaining
    a Kubernetes cluster, why don't we take the benefits of it?
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do that, we need to work with **Replica Sets**. A Replica Set is
    like a traffic cop in a road full of pods: they make sure that the traffic flows
    and everything works without crashing and moving the pods around so that we make
    the best use of the road (our cluster, in this case).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Replica Sets are actually an update of a much older item: the Replication Controller.
    The reason for the upgrade is the labeling and selecting of resources, which we
    will see visit when we dive deep into the API item called Service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a Replica Set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this a YAML file that is basically fairly easy to understand but might
    require some explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we have used the extensions API on the version `v1beta1`. If
    you remember from the pod section (previously), Kubernetes has three branches:
    stable, alpha, and beta. The complete reference can be found in the official documentation,
    and it is very likely to change often as Kubernetes is a vibrant and always evolving
    project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the spec section is where the important things happen: we have defined a
    set of labels for the Replica Set, but we have also defined a pod (in this case,
    with a single container) and specified that we want three instances of it (replicas:
    three).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple and effective. Now we have defined a resource called Replica Set, which
    allows us to deploy a pod and keep it alive as per configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the command returns, we should see the following message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s verify it using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As the output of the preceding command, you should see the Replica Set explaining
    that there are three desired pods, three actually deployed, and three ready. Note
    the difference between current and ready: a pod might be deployed but still not
    ready to process requests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have specified that our `replicaset` should keep three pods alive. Let''s
    verify this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'No surprises here: our `replicaset` has created three pods, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1055e82-5872-477b-91a8-6d200347d72b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have four pods:'
  prefs: []
  type: TYPE_NORMAL
- en: One created in the preceding section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three created by the Replica Set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s kill one of the pods and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And now, query how many pods are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/508833b4-48b0-4dc0-a0e4-119ead927a8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Bingo! Our `replicaset` has created a new pod (you can see which one in the
    AGE column). This is immensely powerful. We have gone from a world where a pod
    (an application) being killed wakes you up at 4 a.m. in the morning to take action
    to a world where when one of our application dies, Kubernetes revives it for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at what happened in the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bf89559-f168-4880-bac5-c038679168b7.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can expect, the Replica Set has created the pods for you. You can try
    to kill them from the interface as well (the period icon to the very right of
    every pod will allow you to do that), but the Replica Set will re-spawn them for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are going to do something that might look like it''s from out of this
    world: we are going to scale our application with a single command, but first,
    edit `replicaset.yml` and change the `replicas` field from three to five.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the file and execute this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now take a look at the dashboard again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f92bebd-c741-495b-a01b-3b9be7fb475d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, Kubernetes is creating pods for us following the instructions
    of the Replica Set, `nginx-rs`. In the preceding sreenshot, we can see one pod
    whose icon is not green, and that is because its status is Pending, but after
    a few seconds, the status becomes Ready, just like any other pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also very powerful, but there is a catch: who scales the application
    if the load spike happens at 4 a.m. in the morning? Well, Kubernetes provides
    a solution for this: Horizontal Pod Autoscalers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding command, we have specified that Kubernetes should attach
    a Horizontal Pod Autoscalers to our Replica Set. If you browse the Replica Set
    in the dashboard again, the situation has changed dramatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f948bb0-7e82-4b40-b53e-dc9d12fac20f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explain what happened here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have attached an Horizontal Pod Autoscalers to our Replica Set: minimum
    `1` pod, maximum `10`, and the trigger for creating or destroying pods is the
    CPU utilization going over `80%` on a given pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Replica Set has scaled down to one pod because there is no load on the system,
    but it will scale back to up to 10 nodes if required and stay there for as long
    as the burst of requests is going on and scale back to the minimum required resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now this is actually the dream of any sysadmin: no-hassle autoscaling and self-healing
    infrastructure. As you can see, Kubernetes starts making sense altogether, but
    there is one thing disturbing in the autoscaler part. It was a command that we
    ran in the terminal, but it is captured nowhere. So how can we keep track of our
    infrastructure (yes, an Horizontal Pod Autoscaler is part of the infrastructure)?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, there is an alternative; we can create a YAML file that describes our
    Horizontal Pod Autoscaler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'First, from the dashboard, remove `HorizontalPodAutoscaler` created from the
    previous example. Then, write the preceding content into a file called `horizontalpodautoscaler.yml`
    and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This should have the same effect as the `autoscale` command but with two obvious
    benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: We can control more parameters, such as the name of the HPA, or add metadata
    to it, such as labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We keep our infrastructure as code within reach so we know what is going on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second point is extremely important: we are in the age of the infrastructure
    as code and Kubernetes leverages this powerful concept in order to provide traceability
    and readability. Later on, in [Chapter 8](127a7b5f-4bd7-4290-bea0-3e8db867e4af.xhtml),
    *Release Management – Continuous Delivery*, you will learn how to create a continuous
    delivery pipeline with Kubernetes in a very easy way that works on 90% of the
    software projects.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the preceding command returns, we can check on the dashboard and see that
    effectively, our Replica Set has attached an Horizontal Pod Autoscaler as per
    our configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though the Replica Set is a very powerful concept, there is one part of
    it that we have not talked about: what happens when we apply a new configuration
    to a Replica Set in order to upgrade our applications? How does it handle the
    fact that we want to keep our application alive 100% of the time without service
    interruption?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the answer is simple: it doesn''t. If you apply a new configuration to
    a Replica Set with a new version of the image, the Replica Set will destroy all
    the Pods and create newer ones without any guaranteed order or control. In order
    to ensure that our application is always up with a guaranteed minimum amount of
    resources (Pods), we need to use Deployments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, take a look at what a deployment looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it is very similar to a Replica Set, but there is a new section:
    strategy. In strategy, we are defining how our `rollout` is going to work, and
    we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RollingUpdate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Recreate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RollingUpdate` is the default option as it seems the most versatile in modern
    24/7 applications: It coordinates two replica sets and starts shutting down pods
    from the old replica set at the same time that it is creating them in the new
    Replica Set. This is very powerful because it ensures that our application always
    stays up. Kubernetes decides what is best to coordinate the pods'' rescheduling,
    but you can influence this decision with two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxUnavailable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxSurge`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one defines how many pods we can loose from our Replica Set in order
    to perform a `rollout`. As an example, if our Replica Set has three replicas,
    a `rollout` with the `maxUnavailable` value of `1` will allow Kubernetes to transition
    to the new Replica Set with only two pods in the status `Ready` at some point.
    In this example, `maxUnavailable` is `0`; therefore, Kubernetes will always keep
    three pods alive.
  prefs: []
  type: TYPE_NORMAL
- en: '`MaxSurge` is similar to maxUnavailable, but it goes the other way around:
    it defines how many pods above the replicas can be scheduled by Kubernetes. In
    the preceding example, with three replicas with `maxSurge` set on `1`, the maximum
    amount of pods at a given time in our `rollout` will be `4`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Playing with these two parameters as well as the replicas'' number, we can
    achieve quite interesting effects. For example, by specifying three replicas with
    `maxSurge 1` and `maxUnavailable 1`, we are forcing Kubernetes to move the pods
    one by one in a very conservative way: we might have four pods during the `rollout`,
    but we will never go below three available pods.'
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the strategies, Recreate basically destroys all the pods and
    creates them again with the new configuration without taking uptime into account.
    This might be indicated in some scenarios, but I would strongly suggest that you
    use `RollingUpdate` when possible (pretty much always) as it leads to smoother
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to attach a Horizontal Pod Autoscaler to a Deployment in
    the same way that we would do with a Replica Set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test our deployment. Create a file called `deployment.yml` and apply
    it to our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the command returns, we can go to the Kubernetes dashboard (`localhost:8001/ui`
    with the proxy active) and check what happened in the Deployments section in the
    menu on the left-hand side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da68eada-ed69-491f-b51a-851d95ceb92f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have a new Deployment called `nginx-deployment`, which has created a Replica
    Set that also contains the specified pods. In the preceding command, we have passed
    a new parameter: `--record`. This saves the command in the `rollout` history of
    our deployment so that we can query the `rollout` history of a given deployment
    to see the changes applied to it. In this case, just execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show you all the actions that altered the status of a deployment
    called `nginx-deployment`. Now, let''s execute some change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We have used `kubectl` to change the version of the `nginx` container back
    to version 1.9.1 (`kubectl` is very versatile; the official documentation offers
    shortcuts for pretty much everything), and a few things happened. The first one
    is that a new Replica Set has been created and the pods have been moved over to
    it from the old replica set. We can verify this in the Replica Sets section of
    the menu on the left-hand side of the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/914506b4-95bc-4bb3-a8f7-cbc5da1cc2d1.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the old replica set has 0 pods, whereas the new one that took
    over has three pods. This all happened without you noticing it, but it is a very
    clever workflow with a lot of work from the Kubernetes community and the companies
    behind it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second thing that happened was that we have a new entry in our rollout
    history. Let''s check it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Which one should produce an output similar to the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1947f72-a1f1-410f-b4e1-8d946788f00e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we have two entries that describe the changes applied to our deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have been into IT for few years, by now, you have reached the conclusion
    that a rollback strategy is always necessary because bugs flowing into production
    are the reality no matter how good our QA is. I am a big fan of building the systems
    in a way that deployments are unimportant events (from a technical point of view),
    as shown with Kuberentes, and the engineers always have an easy way out if things
    start to fail in production. Deployments offer an easy rollback if something goes
    wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the preceding and browse back to the dashboard on the **Replica Sets**
    section again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18a385bd-ca4c-44b3-8b36-6a0508f690cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That''s right. In a matter of seconds, we have gone from instability (a broken
    build) to the safety of the old known version without interrupting the service
    and without involving half of the IT department: a simple command brings back
    the stability to the system. The rollback command has a few configurations, and
    we can even select the revision where we want to jump to.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how powerful Kubernetes is and this is how simple our life becomes
    by using Kubernetes as the middleware of our enterprise: a modern CD pipeline
    assembled in a few lines of configuration that works in the same way in all the
    companies in the world by facilitating command `rollouts` and rollbacks. That''s
    it...simple and efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Right now, it feels like we know enough to move our applications to Kubernetes,
    but there is one thing missing. So far, up until now, we have just run predefined
    containers that are not exposed to the outer world. In short, there is no way
    to reach our application from outside the cluster. You are going to learn how
    to do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we were able to deploy containers into Kubernetes and keep them
    alive by making use of pods, Replica Sets, and Horizontal Pods Autoscalers as
    well as Deployments, but so far, you have not learned how to expose applications
    to the outer world or make use of service discovery and balancing within Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Services** are responsible for all of the above. A Service in Kubernetes
    is not an element as we are used to it. A Service is an abstract concept used
    to give entity to a group of pods through pattern matching and expose them to
    different channels via the same interface: a set of labels attached to a Pod that
    get matched against a selector (another set of labels and rules) in order to group
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a service on top of the deployment created in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Easy and straightforward, but there''s one detail: the selector section has
    a hidden message for us. The selectors are the mechanisms that Kubernetes uses
    to connect components via pattern matching algorithms. Let''s explain what pattern
    matching is. In the preceding Service, we are specifying that we want to select
    all the Pods that have a label with the `app` key and the `nginx` value. If you
    go back to the previous section, you''ll understand our deployment has these labels
    in the pod specification. This is a match; therefore, our service will select
    these pods. We can check this by browsing in the dashboard in the Services section
    and clicking on `nginx-service`, but first, you need to create the `service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, check out the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e9b3f8f-cae8-4dee-8969-95fad1d7814e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there are three pods selected, and they all belong to the deployment
    `nginx` that we created in the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: Don't remove the deployment from the previous section; otherwise, there will
    be no pods to select by our service.
  prefs: []
  type: TYPE_NORMAL
- en: 'This screen has a lot of interesting information. The first piece of information
    is that the service has an IP: this IP is denominated as `clusterIP`. Basically,
    it is an IP within the cluster that can be reached by our pods and other elements
    in Kubernetes. There is also a field called `Type`, which allows us to chose the
    service type. There are three types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ClusterIP`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NodePort`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LoadBalancer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ClusterIP` is what we just created and explained.'
  prefs: []
  type: TYPE_NORMAL
- en: '`NodePort` is another type of service that is rarely used in Cloud but is very
    common on premises. It allocates a port on all the nodes to expose our application.
    This allows Kubernetes to define the ingress of the traffic into our pods. This
    is challenging for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It generates extra traffic in our internal network as the nodes need to forward
    the traffic across to reach the pods (imagine a cluster of 100 nodes that has
    an app with only three pods, it is very unlikely to hit the node that is running
    one of them).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ports are allocated randomly so you need to query the Kubernetes API to
    know the allocated port.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LoadBalancer` is the jewel in the crown here. When you create a service of
    type `LoadBalancer`, a cloud load balancer is provisioned so that the client applications
    hit the load balancer that redirects the traffic into the correct nodes. As you
    can imagine, for a cloud environment where infrastructure is created and destroyed
    in matter of seconds, this is the ideal situation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the previous screenshot, we can see another piece of interesting
    information: the internal endpoints. This is the service discovery mechanism that
    Kubernetes is using to locate our applications. What we have done here is connect
    the pods of our application to a name: `nginx-service`. From now on, no matter
    what happens, the only thing that our apps need to know in order to reach our
    `nginx` pods is that there is a service called `nginx` that knows how to locate
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to test this, we are going to run an instance of a container called
    `busybox`, which is basically the Swiss army knife of command-line tools. Run
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will present us with a shell inside the container called
    `busybox` running in a pod so we are inside the Kubernetes cluster and, more importantly,
    inside the network so that we can see what is going on. Be aware that the preceding
    command runs just a pod: no deployment or replica set is created, so once you
    exit the shell, the pod is finalized and resources are destroyed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we get the prompt inside `busybox`, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This should return something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay, what happened here? When we created a service, we assigned a name to
    it: `nginx-service`. This name has been used to register it in an internal DNS
    for service discovery. As mentioned earlier, the DNS service is running on Kubernetes
    and is reachable from all the Pods so that it is a centralised repository of common
    knowledge. There is another way that the Kubernetes engineers have created in
    order to carry on with the service discovery: the environment variables. In the
    same prompt, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This command outputs all the environment variables, but there are few that
    are relevant to our recently defined service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'These variables, injected by Kubernetes at creation time, define where the
    applications can find our service. There is one problem with this approach: the
    environment variables are injected at creation time, so if our service changes
    during the life cycle of our pods, these variables become obsolete and the pod
    has to be restarted in order to inject the new values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All this magic happens through the selector mechanism on Kubernetes. In this
    case, we have used the equal selector: a label must match in order for a pod (or
    an object in general) to be selected. There are quite a few options, and at the
    time of writing this, this is still evolving. If you want to learn more about
    selectors, here is the official documentation: [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/).'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, services are used in Kubernetes to glue our applications together.
    Connecting applications with services allows us to build systems based on microservices
    by coupling REST endpoints in the API with the name of the service that we want
    to reach on the DNS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up until now, you have learned how to expose our applications to the rest of
    our cluster, but how do we expose our applications to the outer world? You have
    also learned that there is a type of service that can be used for this: `LoadBalancer`.
    Let''s take a look at the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one change in the preceding definition: the service type is now `LoadBalancer`.
    The best way to explain what this causes is by going to the Services section of
    the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f694bd9-47e9-49dd-9f6d-1b0d2fdfd334.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, our newly created service got assigned an external endpoint.
    If you browse it, bingo! The `nginx` default page is rendered.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have created two services, `nginx-service` and `nginx-service-lb`, of the
    type `ClusterIP` and `LoadBalancer`, respectively, which both point to the same
    pods that belong to a deployment and are managed through a replica set. This can
    be a bit confusing, but the following diagram will explain it better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1647789c-12c0-459c-866b-7956a0919600.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram is the perfect explanation of what we've built in this
    section. As you can see, the load balancer is outside of Kubernetes, but everything
    else is inside our cluster as virtual elements of an API.
  prefs: []
  type: TYPE_NORMAL
- en: Other Building Blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, you learned the basics needed to deploy applications
    into Kubernetes successfully. The API objects that we visited are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicaSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Kubernetes, there are many other building blocks that can be used to build
    more advanced applications; every few months, the Kubernetes engineers add new
    elements to improve or add functionality.
  prefs: []
  type: TYPE_NORMAL
- en: One example of these additions is the ReplicaSet that was designed to replace
    another item called ReplicationController. The main difference between the ReplicationController
    and the ReplicaSet is that the latter one has a more advance semantics label selection
    for the Pods that were recently re-engineered in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: As a new product, Kuberentes is constantly changing (in fact, it is possible
    that by the time that you read this book, the core elements might have changed),
    so the engineers try to keep the compatibility across different versions so that
    people are not urged to upgrade in a short period of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other examples of more advanced building blocks are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PetSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs and CronJobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CronJobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to go in deep to the full stack in Kubernetes, we would need a full
    book (or more!). Let's visit some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Daemon Sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Daemon Sets are an API element used to **ensure that a Pod is running in all
    (or some) nodes**. One of the assumptions in Kubernetes is that the pod should
    not worry about which node is being run, but that said, there might be a situation
    where we want to ensure that we run at least one pod on each node for a number
    of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to do that, Kubernetes provides an API element called Daemon Set. Through
    a combination of labels and selectors, we can define something called **affinity**,
    which can be used to run our pods on certain nodes (we might have specific hardware
    requirements that only a few nodes are able to provide so that we can use tags
    and selectors to provide a hint to the pods to relocate to certain nodes).
  prefs: []
  type: TYPE_NORMAL
- en: 'Daemon Sets have several ways to be contacted, from the DNS through a headless
    service (a service that works as a load balancer instead of having a cluster IP
    assigned) to the node IP, but Daemon Sets work best when they are the initiators
    of the communication: something happens (an event) and a Daemon Set sends an event
    with information about that event (for example, a node is running low on space).'
  prefs: []
  type: TYPE_NORMAL
- en: PetSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PetSets** are an interesting concept within Kubernetes: they are strong named
    resources whose naming is supposed to stay the same for a long term. As of now,
    a pod does not have a strong entity within a Kubernetes cluster: you need to create
    a service in order to locate a pod as they are ephemeral. Kubernetes can reschedule
    them at any time without prior notice for changing their name, as we have seen
    before. If you have a deployment running in Kubernetes and kill one of the pods,
    its name changes from (for example) *pod-xyz* to *pod-abc i*n an unpredictable
    way. so we cannot know which names to use in our application to connect to them
    beforehand.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with a Pet Set, this changes completely. A pet set has an ordinal
    order, so it is easy to guess the name of the pod. Let''s say that we have deployed
    a Pet Set called mysql, which defines pods running a MySQL server. If we have
    three replicas, the naming will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mysql-0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mysql-1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mysql-2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we can bake this knowledge in our application to reach them. This is suboptimal
    but good enough: we are still coupling services by name (DNS service discovery
    has this limitation), but it works in all cases and is a sacrifice that is worth
    paying for because in return, we get a lot of flexibility. The ideal situation
    in service discovery is where our system does not need to know even the name of
    the application carrying the work: just throw the message into the ether (the
    network) and the appropriated server will pick it up and respond accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pet Sets have been replaced in later versions of Kubernetes with another item
    called **Stateful Set.** The Stateful Set is an improvement over the Pet Set mainly
    in how Kubernetes manages the **master knowledge to avoid a split brain situation**:
    where two different elements think that they are in control.'
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Job** in Kubernetes is basically an element that spawns the defined number
    of pods and waits for them to finish before completing its life cycle. It is very
    useful when there is a need to run a one-off task, such as rotating logs or migrating
    data across databases.
  prefs: []
  type: TYPE_NORMAL
- en: Cron jobs have the same concept as Jobs, but they get triggered by time instead
    of a one-off process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both in combination are very powerful tools to keep any system running. If
    you think about how we rotate logs without Kubernetes via ssh, it is quite risky:
    there is no control (by default) over who is doing what, and usually, there is
    no review process in the ssh operations carried by an individual.'
  prefs: []
  type: TYPE_NORMAL
- en: With this approach, it is possible to create a Job and get other engineers to
    review it before running it for extra safety.
  prefs: []
  type: TYPE_NORMAL
- en: Secrets and configuration management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On Docker in general, as of today, secrets are being passed into containers
    via environment variables. This is very insecure: first, there is no control over
    who can access what, and second, environment variables are not designed to act
    as secrets and a good amount of commercial software (and open source) outputs
    them into the standard output as part of bootstrapping. Needless to say, that''s
    rather inconvenient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes has solved this problem quite gracefully: instead of passing an
    environment variable to our container, a volume is mounted with the secret on
    a file (or several) ready to be consumed.'
  prefs: []
  type: TYPE_NORMAL
- en: By default, Kubernetes injects a few secrets related to the cluster into our
    containers so that they can interact with the API and so on, but it is also possible
    to create your own secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to create secrets:'
  prefs: []
  type: TYPE_NORMAL
- en: Using `kubectl`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining an API element of type secret and using `kubectl` to deploy it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first way is fairly straightforward. Create a folder called *secrets* in
    your current work folder and execute the following commands inside it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates two files with two strings (simple strings as of now). Now it
    is time to create the secret in Kubernetes using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And that''s it. Once we are done, we can query the secrets using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This, in my case, returns two secrets:'
  prefs: []
  type: TYPE_NORMAL
- en: A service account token injected by the cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My newly created secret (`my-secrets`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second way of creating a secret is by defining it in a `yaml` file and
    deploying it via `kubectl`. Take a look at the following definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'First, the values for `secret1` and `secret2`, seem to be encrypted, but they
    are not; they are just encoded in `base64`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the values that you can see here. The type of the secret is
    Opaque, which is the default type of secret, and the rest seems fairly straightforward.
    Now create the secret with kubectl (save the preceding content in a file called
    `secret.yml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: And that's it. If you query the secrets again, note that there should be a new
    one called `my-secret-yaml`. It is also possible to list and see the secrets in
    the dashboard on the Secrets link in the menu on left-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to use them. In order to use the secret, two things need to
    be done:'
  prefs: []
  type: TYPE_NORMAL
- en: Claim the secret as a volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mount the volume from the secret
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at a `Pod` using a secret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'So, you have learned a new thing here: `kubectl` also understands JSON. If
    you don''t like YAML, it is possible to write your definitions in JSON without
    any side-effects.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, looking at the JSON file, we can see how first, the secret is declared
    as a volume and then how the secret is mounted in the path/secrets.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to verify this, just run a command in your container to check it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This should list the two files that we have created, `secret1.txt` and `secret2.txt`,
    containing the data that we have also specified.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes- moving on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned enough to run simple applications in Kubernetes,
    but even though we cannot claim ourselves to be experts, we got the head start
    in becoming experts. Kubernetes is a project that evolves at the speed of light,
    and the best thing that you can do to keep yourself updated is follow the project
    on GitHub at [https://github.com/kubernetes](https://github.com/kubernetes).
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes community is very responsive with issues raised by the users
    and are also very keen on getting people to contribute to the source code and
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: If you keep working with Kubernetes, some help will be required. The official
    documentation is quite complete, and even though it feels like it needs a reshuffle
    sometimes, it is usually enough to keep you going.
  prefs: []
  type: TYPE_NORMAL
- en: The best way that I've found to learn Kubernetes is by experimenting in Minikube
    (or a test cluster) before jumping into a bigger commitment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at a good amount of concepts required to deploy an
    application on Kubernetes. As mentioned earlier, it is impossible to cover everything
    abound Kubernetes in a single chapter, but with the amount of knowledge from this
    chapter, we are going to be able to set up a continuous delivery pipeline in the
    following chapter in a way that we automate zero downtime deployments without
    the big bang effect (the big deployment that stops the world), enabling our organization
    to move faster.
  prefs: []
  type: TYPE_NORMAL
