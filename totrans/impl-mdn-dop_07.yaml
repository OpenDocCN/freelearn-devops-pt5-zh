- en: Docker Swarm and Kubernetes - Clustering Infrastructure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm 和 Kubernetes - 集群基础设施
- en: So far, we have seen how powerful Docker is but we have not unleashed the full
    potential of containers. You have learned how to run containers on a single host
    with the local resources without the possibility of clustering our hardware resources
    in a way that allows us to uniformly use them as one big host. This has a lot
    of benefits, but the most obvious one is that we provide a middleware between
    developers and ops engineers that acts as a common language so that we don't need
    to go to the ops team and ask them for a machine of a given size. we just provide
    the definition of our service and the Docker clustering technology will take care
    of it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了 Docker 的强大，但我们还没有释放容器的全部潜力。你已经学会了如何在单一主机上运行容器，并利用本地资源，但没有办法将我们的硬件资源以一种可以将其统一作为一个大主机来使用的方式进行集群。这带来了许多好处，但最明显的一点是，我们为开发人员和运维工程师之间提供了一个中间件，它作为一种通用语言，这样我们就不需要去找运维团队请求指定大小的机器了。我们只需提供我们的服务定义，Docker
    集群技术就会处理这些事情。
- en: In this chapter, we are going to dive deep into deploying and managing applications
    on Kubernetes, but we will also take a look at how Docker Swarm works.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨如何在 Kubernetes 上部署和管理应用程序，但我们也会看看 Docker Swarm 是如何工作的。
- en: 'People usually tend to see Kubernetes and Docker Swarm as competitors, but
    in my experience, they solve different problems:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 人们通常倾向于将 Kubernetes 和 Docker Swarm 视为竞争对手，但根据我的经验，它们解决的是不同的问题：
- en: '**Kubernetes** is focused on advanced microservices topologies that offer all
    the potential of years of experience running containers in Google'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes** 专注于先进的微服务拓扑，提供了多年的容器运行经验的全部潜力，源自 Google。'
- en: '**Docker Swarm** offers the most straightforward clustering capabilities for
    running applications in a very simple way'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker Swarm** 提供了最直接的集群能力，用于以非常简单的方式运行应用程序。'
- en: In short, Kubernetes is more suited for advanced applications, whereas Docker
    Swarm is a version of Docker on steroids.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Kubernetes 更适合于高级应用，而 Docker Swarm 是一种增强版的 Docker。
- en: 'This comes at a cost: managing a Kubernetes cluster can be very hard, whereas
    managing a Docker Swarm cluster is fairly straightforward.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这有一个代价：管理 Kubernetes 集群可能非常困难，而管理 Docker Swarm 集群相对简单。
- en: There are other clustering technologies that are used in the current DevOps
    ecosystem, such as DC/OS or Nomad, but unfortunately, we need to focus on the
    ones that are, in my opinion, the most suited for DevOps and focus specifically
    on Kubernetes that, in my opinion, is eating the DevOps market.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的 DevOps 生态系统中还存在其他集群技术，如 DC/OS 或 Nomad，但不幸的是，我们需要关注那些在我看来最适合 DevOps 的技术，特别是
    Kubernetes，我认为它正在吞噬 DevOps 市场。
- en: Why clustering ?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要集群？
- en: In [Chapter 1](0b33ce27-d21e-4021-87ad-6865c65b7e33.xhtml), *DevOps in the Real
    World,* you learned about organizational alignment and why is important to shift
    roles in a company to accommodate DevOps tools. It is not okay anymore to just
    be a developer or a sysadmin; now you need to be a full stack DevOps engineer
    in order to get success in any project. Full stack DevOps means that you need
    to understand the business and the technology used in the organisation. Think
    about it; if you became a civil engineer instead of an IT engineer, it is mandatory
    to know the local rules (the business) plus the commercial names of the tools
    used to build roads and bridges (the technology) but also be able to coordinate
    their building (ops). Maybe not every engineer needs to know everything but they
    need to be aware of in the full picture in order to ensure the success of the
    project.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 1 章](0b33ce27-d21e-4021-87ad-6865c65b7e33.xhtml)《*现实世界中的 DevOps*》中，你学习了组织对齐以及为什么在公司中调整角色以适应
    DevOps 工具是如此重要。现在仅仅作为开发者或系统管理员已经不再足够；你现在需要成为一名全栈 DevOps 工程师，才能在任何项目中获得成功。全栈 DevOps
    意味着你需要理解企业和组织中使用的技术。想一想，如果你成为了一名土木工程师，而不是 IT 工程师，你必须了解当地的规则（业务），以及用于建造道路和桥梁的工具的商业名称（技术），同时还要能够协调它们的建设（运维）。也许并不是每个工程师都需要知道所有的内容，但他们需要了解整个图景，以确保项目的成功。
- en: Coming back to containers and DevOps, making concepts simple for everyone to
    understand is something that's mandatory nowadays. You want to ensure that all
    the engineers in your project are able to trace the software from conception (requirements)
    to deployment (ops) but also have in mind predictability so that the business
    people that barely speak tech are able to plan strategies around the products
    that you build.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 回到容器和 DevOps，今天让每个人都能理解的概念是必不可少的。你需要确保项目中的所有工程师都能够追溯软件的整个过程，从构思（需求）到部署（运维），同时也要考虑可预测性，这样那些几乎不懂技术的业务人员也能围绕你所构建的产品规划策略。
- en: 'One of the keys to achieving the flow described here is predictability, and
    the way to achieve predictability is making uniform and repeatable use of your
    resources. As you learned earlier, cloud data centers such as Amazon Web Services
    or Google Cloud Platform provide us with a virtually unlimited pool of resources
    that can be used to build our systems in a traditional way:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这里描述的流的关键之一是可预测性，而实现可预测性的方法是对资源进行统一和可重复的使用。正如你之前学到的那样，像 Amazon Web Services
    或 Google Cloud Platform 这样的云数据中心为我们提供了一个几乎无限的资源池，可以按照传统方式构建我们的系统：
- en: Define the size of the VMs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义虚拟机的大小
- en: Provision VMs
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置虚拟机
- en: Install the software
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装软件
- en: Maintain it
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护它
- en: 'Or, if we want to draw a diagram so we can understand it better, it would be
    similar to the next one:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们想绘制一个图表，以便更好地理解，它将类似于下图：
- en: '![](img/a68c76ed-810f-4527-bbbc-c2226fefe7fc.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a68c76ed-810f-4527-bbbc-c2226fefe7fc.png)'
- en: 'Here are a few considerations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几点需要考虑：
- en: Clear separation between Development and Operations (this may vary depending
    on the size of your company
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发和运维之间的明确分离（这可能会根据公司规模有所不同）
- en: Software components owned by Development and deployments and configuration owned
    by Operations
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由开发部门拥有的软件组件以及由运维部门拥有的部署和配置
- en: Some servers might be relatively underutilized (**Server 1**) and on a very
    low load
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些服务器可能相对低效（**服务器 1**），负载非常低
- en: 'This has been the picture for 40 odd years of software development, and it
    is still the picture if we are running Docker containers, but there are few problems
    in it:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是40多年来软件开发的情景，如果我们在运行 Docker 容器时仍然是这种情况，但其中有几个问题：
- en: If a problem arises in **Component 3** in production, who is responsible for
    it?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在生产环境中**组件 3**出现问题，谁负责处理？
- en: If there is a configuration mismatch, who will fix it if developers are not
    supposed to see what is going on in production?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果配置不匹配，当开发人员不应该看到生产环境中的情况时，谁来修复它？
- en: '**Server 1** is running a software component that might be called only once
    or twice a day (imagine an authentication server for workstations); do we need
    a full VM just for it?'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器 1**运行着一个可能一天只调用一次或两次的软件组件（比如工作站的认证服务器）；我们需要为它配置一个完整的虚拟机吗？'
- en: How do we scale our services in a transparent manner?
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何以透明的方式扩展我们的服务？
- en: 'These questions can be answered, but usually, they get an answer too late in
    the game plus "the hidden requirements" are only seen once the problems arise
    at the worst possible time:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题是可以解答的，但通常它们的答案往往来得太晚，而“隐藏的需求”只有在最糟糕的时刻才会显现出来：
- en: Service discovery
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现
- en: Load balancing
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Self-healing infrastructure
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自愈基础设施
- en: Circuit breaking
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电路断开
- en: 'During college years, one of the things in common across all the different
    subjects was reusability and extensibility. Your software should be extensible
    and reusable so that we can potentially build libraries of components creating
    the engineering sweet spot (not just software development): build once, use everywhere.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在大学时期，所有不同学科的共同点之一是可重用性和可扩展性。你的软件应该具有可扩展性和可重用性，这样我们就可以创建组件库，形成工程的最佳实践（不仅仅是软件开发）：构建一次，到处使用。
- en: 'This has been completely overlooked in the operations part of the software
    development until recent years. If you get a job as a Java developer in a company,
    there is a set of accepted practices that every single Java developer in the world
    knows and makes use of so you can nearly hit the ground running without too many
    problems (in theory). Now let''s raise a question: if all the Java apps follow
    the same practices and set of common patterns, why does every single company deploy
    them in a different way?'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发的运维部分，这一点一直被完全忽视，直到近年来才开始受到重视。如果你在一家公司担任 Java 开发人员，那么有一套被世界上每一个 Java 开发人员接受并使用的实践，这样你几乎可以毫无问题地快速上手（理论上是这样）。现在我们提出一个问题：如果所有的
    Java 应用都遵循相同的实践和公共模式，为什么每家公司却以不同的方式部署它们？
- en: A continuous delivery pipeline has the same requirements in pretty much every
    company in the IT world, but I have seen at least three different ways of organizing
    it with a huge amount of custom magic happening that only one or two people within
    the company know of.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IT 世界中，持续交付流水线几乎每个公司都需要，但我见过至少三种不同的组织方式，而且这些方式背后有着大量只有一两个人知道的定制“魔法”。
- en: 'Clusters are here to save us. Let''s reshuffle the image from before:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 集群在这里拯救我们。让我们重新排列一下之前的图像：
- en: '![](img/9e0e0ad8-b84a-468c-bc50-e7d0bd7f2627.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e0e0ad8-b84a-468c-bc50-e7d0bd7f2627.png)'
- en: 'In this case, we have solved few of our problems:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们解决了我们的一些问题：
- en: 'Now development and ops are connected via a middleware: the cluster.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，开发和运维通过中间件连接起来：集群。
- en: Components can be replicated (refer to component 1 and component 1') without
    provisioning extra hardware.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组件可以被复制（参考组件 1 和组件 1'），无需额外的硬件支持。
- en: DevOps engineers are the glue between the two teams (development and ops), making
    things happen at a fast pace.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DevOps 工程师是两个团队（开发和运维）之间的纽带，让事情以更快的节奏推进。
- en: 'The stability of the full system does not depend on a single server (or component)
    as the cluster is built in a way that can accept some level of failure by just
    degrading performance or taking down the less critical services: it is okay to
    sacrifice e-mailing in order to keep the accounting processes of the company running.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个系统的稳定性并不依赖于单一服务器（或组件），因为集群是以能够接受一定程度故障的方式构建的，这样可以通过降低性能或关闭较不关键的服务来容忍故障：为了保持公司财务流程的正常运行，牺牲邮件服务是可以接受的。
- en: And about the hidden requirements. Well, this is where we need to make a decision
    about which clustering technology we want to use as they approach the service
    discovery, load balancing, and auto-scaling from different angles.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 说到隐藏的需求。这就是我们需要决定使用哪种集群技术的地方，因为它们从不同角度处理服务发现、负载均衡和自动扩展。
- en: Docker Swarm
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker Swarm
- en: As we have seen in previous chapters, Docker is a fantastic tool that follows
    the most modern architectural principles used for running applications packed
    as containers. In this case, Docker Swarm runs only Docker containers, ignoring
    other technologies that, at the moment, are not suitable for production, such
    as Rkt. Even Docker is quite new to the scene up to a point that some companies
    hesitate in deploying it in their production systems, as there is not so much
    expertise in the market as well as many doubts about security or how Docker works
    in general.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中看到的，Docker 是一个非常棒的工具，它遵循了运行作为容器打包的应用的最现代的架构原则。在这种情况下，Docker Swarm
    仅运行 Docker 容器，忽略了其他目前不适用于生产环境的技术，比如 Rkt。甚至 Docker 本身也在某种程度上是新兴的，以至于一些公司在将其部署到生产系统时犹豫不决，因为市场上没有太多的专业知识，也存在许多关于安全性或
    Docker 工作原理的疑虑。
- en: '**Docker Swarm** is the clustered version of Docker, and it solves the problem
    described in the previous section in a very simple manner: pretty much all the
    docker commands that you learned in the Docker chapter works in Docker Swarm so
    that we can federate our hardware without actually taking care of the hardware
    itself. Just add nodes to the pool of resources and Swarm will take care of them,
    leveraging the way we build our systems to purely containers.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker Swarm** 是 Docker 的集群版，它以非常简单的方式解决了上一节中描述的问题：几乎所有你在 Docker 章节中学到的 Docker
    命令都可以在 Docker Swarm 中使用，这样我们就可以在不直接管理硬件的情况下联邦化我们的硬件资源。只需将节点添加到资源池中，Swarm 就会管理它们，并充分利用我们构建纯容器系统的方式。'
- en: 'Docker Swarm is not something that we need to install aside from the Docker
    engine: it comes embedded into it and it is a mode rather than a server itself.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 不是我们需要单独安装的东西，它是嵌入在 Docker 引擎中的一种模式，而不是一个独立的服务器。
- en: Docker Swarm is evolving quite quickly and it is dragging Docker itself along
    as more and more features are being baked into it due to its usage in the Swarm
    mode. The most interesting part of this is how we can leverage our Docker knowledge
    into it without any extra as the swarm mode of our Docker engine takes care of
    the resources.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 正在快速发展，并且随着更多特性的加入，它正在带动 Docker 本身的发展，特别是在 Swarm 模式下使用时。最有趣的部分是，我们如何在没有额外操作的情况下利用我们的
    Docker 知识，因为 Docker 引擎的 Swarm 模式会自动处理资源。
- en: 'This is also a problem: we are limited by the Docker API, whereas with Kubernetes
    (we will come back to it in a second), we are not only limited by the Docker API,
    but we can also extend the Kubernetes API to add new objects to fulfill our needs.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个问题：我们受限于 Docker API，而在 Kubernetes 中（稍后我们会详细讲解），我们不仅不受 Docker API 的限制，还可以扩展
    Kubernetes API 来添加新对象，以满足我们的需求。
- en: Docker Swarm can be operated through `docker-compose` (up to a certain extent),
    which provides a decent approach to infrastructure as code but is not very comprehensive
    when our application is somehow complex.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Docker Swarm 可以通过 `docker-compose` 进行操作（在一定程度上），它为基础设施即代码提供了一个不错的方案，但当我们的应用程序变得复杂时，它并不是特别全面。
- en: In the current IT market, Kubernetes seems to be the clear winner of the orchestration
    battle, and as such, we are going to focus on it, but if you want to learn more
    about Docker Swarm, the official documentation can be found at [https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的 IT 市场中，Kubernetes 似乎是编排战斗的明显赢家，因此我们将重点关注它，但如果你想了解更多关于 Docker Swarm 的内容，可以参考官方文档，链接为
    [https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/)。
- en: Kubernetes
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes
- en: Kubernetes is the jewel of the crown of the containers orchestration. The product
    itself was vamped by Google leveraging years of knowledge on how to run containers
    in production. Initially, it was an internal system used to run Google services,
    but at some point, it became a public project. Nowadays, it is an open source
    project maintained by few companies (Red Hat, Google, and so on) and is used by
    thousands of companies.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是容器编排的皇冠上的明珠。该产品本身由 Google 利用多年的生产容器运行经验进行改进。最初，它是一个内部系统，用于运行 Google
    的服务，但在某个时刻，它成为了一个公开的项目。如今，它是一个由少数几家公司（如 Red Hat、Google 等）维护的开源项目，并被成千上万的公司使用。
- en: At the time of writing this, the demand for Kubernetes engineers has skyrocketed
    up to a point that companies are willing to hire people without expertise in the
    field but with a good attitude to learn new technologies.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Kubernetes 工程师的需求已经飙升，达到了公司愿意聘用那些虽然在该领域没有专业知识，但有良好学习态度的人来学习新技术的程度。
- en: 'Kubernetes has become so popular due to, in my opinion, the following factors:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为 Kubernetes 之所以变得如此流行，主要有以下几个原因：
- en: It solves all the deployment problems
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它解决了所有的部署问题
- en: It automates micro services' operations
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它自动化微服务的运维
- en: It provides a common language to connect ops and development with a clean interface
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一种通用的语言，将运维和开发连接起来，拥有一个清晰的接口
- en: Once it is setup, it is very easy to operate
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦设置完成，它非常容易操作
- en: Nowadays, one of the biggest problems in companies that want to shorten the
    delivery life cycle is the **red tape that has grown around the delivery process**.
    Quarter releases are not acceptable anymore in a market where a company of five
    skilled engineers can overtake a classic bank due to the fact that they can cut
    the red tape and streamline a delivery process that allows them to release multiple
    times a day.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，许多公司希望缩短交付周期，而其中最大的一个问题就是**围绕交付过程积累的繁文缛节**。在一个市场中，五名熟练的工程师能够赶超传统银行，因为他们能够消除繁文缛节，简化交付流程，使得他们可以每天发布多次。
- en: 'One of my professional activities is to speak at conferences (meet-ups in Dublin,
    RebelCon in Cork, **Google Developer Groups** (**GDGs**) in multiple places, Google
    IO Extended) and I always use the same words in all the talks: release management
    should stop being a big bang event that stops the world for three hours in order
    to release a new version of your company''s application and **s**tart being a
    painless process that can be rolled back at any time so that we remove the majority
    of the stress from it by providing the tools to manage a faulty release.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我的职业活动之一是参加各种会议（在都柏林的聚会、科克的 RebelCon、多个地方的 **Google Developer Groups** (**GDGs**)、Google
    IO Extended），我在所有的演讲中总是使用相同的话语：发布管理应该不再是一个重大的事件，让整个世界停顿三小时才能发布公司应用的新版本，而应该变成一个无痛的过程，可以随时回滚，从而通过提供管理故障发布的工具来减轻大部分压力。
- en: 'This (not just this, but mainly this) is Kubernetes: a set of tools and virtual
    objects that will provide the engineers with a framework that can be used to streamline
    all the operations around our apps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这（不仅仅是这个，但主要是这个）就是 Kubernetes：一套工具和虚拟对象，它们为工程师提供了一个框架，可以用来简化所有与我们应用相关的操作：
- en: Scale up
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展规模
- en: Scale down
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩减规模
- en: Zero downtime rollouts
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零停机发布
- en: Canary deployments
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀发布
- en: Rollbacks
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚
- en: Secret management
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密钥管理
- en: 'Kubernetes is built in a technology-agnostic way. Docker is the main container
    engine, but all the components were designed with interchangeability in mind:
    once Rkt is ready, it will be easy to switch to Rkt from Docker, which gives an
    interesting perspective to the users as they don''t get tied to a technology in
    particular so that avoiding vendor locking becomes easier. This applies to the
    software defined network and other Kubernetes components as well.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是以技术无关的方式构建的。Docker 是主要的容器引擎，但所有组件都设计为具有可互换性：一旦 Rkt 就绪，切换到 Rkt 会很容易，这给用户带来了有趣的视角，因为他们不会被某一种特定技术绑定，这样避免供应商锁定就变得更加容易。这同样适用于软件定义网络和其他
    Kubernetes 组件。
- en: One of the pain points is the steep learning curve for setting it up as well
    as for using it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个痛点是设置和使用 Kubernetes 的陡峭学习曲线。
- en: Kubernetes is very complex, and being skilled in its API and operations can
    take any smart engineer a few weeks, if not months, but once you are proficient
    in it, the amount of time that you can save completely pays off all the time spent
    learning it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 非常复杂，熟练掌握其 API 和操作可能需要几周，甚至几个月的时间，但一旦你精通它，你所节省的时间完全能够弥补所有学习所花费的时间。
- en: 'On the same way, setting up a cluster is not easy up to a point that companies
    have started selling Kubernetes as a service: they care about maintaining the
    cluster and you care about using it.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，设置集群并不容易，甚至有公司开始将 Kubernetes 作为一种服务出售：它们负责维护集群，你负责使用它。
- en: One of the (once again, in my opinion) most advanced providers for Kubernetes
    is the **Google Container Engine** (**GKE**), and it is the one that we are going
    to use for the examples in this book.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来（再次强调，这是我的个人看法），Kubernetes 的一个最先进的提供商是 **Google 容器引擎** (**GKE**)，这也是我们将在本书中用于示例的提供商。
- en: 'When I was planning the contents of this chapter, I had to make a decision
    between two items:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在规划本章内容时，我必须在两个选项之间做出决定：
- en: Setting up a cluster
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置集群
- en: Showing how to build applications around Kubernetes
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示如何围绕 Kubernetes 构建应用程序
- en: 'I was thinking about it for a few days but then I realized something: there
    is a lot of information and about half a dozen methods to set up a cluster and
    none of them are official. Some of them are supported by the official Kubernetes
    GitHub repository, but there is no (at the time of writing this) official and
    preferred way of setting up a Kubernetes instance either on premises or in the
    cloud, so the method chosen to explain how to deploy the cluster might be obsolete
    by the time this book hits the market. The following options are the most common
    ways of setting up a Kubernetes cluster currently:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我考虑了几天，后来我意识到一件事：有大量的信息和大约半打方法来设置集群，而且没有一个是官方的。其中一些方法得到了官方 Kubernetes GitHub
    仓库的支持，但在撰写本文时（截至撰写时），并没有官方的、首选的设置 Kubernetes 实例的方法，无论是在本地还是在云端。因此，选择的讲解集群部署的方法可能在本书上市时已经过时。以下是目前最常见的设置
    Kubernetes 集群的方式：
- en: '**Kops**: The name stands for Kubernetes operations and it is a command-line
    interface for operating clusters: creating, destroying, and scaling them with
    a few commands.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kops**：这个名字代表 Kubernetes 操作，它是一个用于操作集群的命令行接口：通过几条命令创建、销毁和扩展集群。'
- en: '**Kubeadm**: Kubeadm is alpha at the moment and breaking changes can be integrated
    at any time into the source code. It brings the installation of Kubernetes to
    the execution of a simple command in every node that we want to incorporate to
    the cluster in the same way as we would do if it was Docker Swarm.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeadm**：目前，Kubeadm 还处于 alpha 阶段，任何时候都可能会有破坏性更新集成到源代码中。它通过在每个我们希望加入集群的节点上执行简单的命令，简化了
    Kubernetes 的安装，就像我们使用 Docker Swarm 时的做法一样。'
- en: '**Tectonic**: Tectonic is a product from CoreOS to install Kubernetes in a
    number of providers (AWS, Open Stack, Azure) pretty much painlessly. It is free
    for clusters up to nine nodes and I would highly recommend that, at the very least,
    you play around it to learn about the cluster topology itself.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tectonic**：Tectonic 是 CoreOS 推出的一个产品，用于在多个云服务提供商（如 AWS、Open Stack、Azure）上轻松安装
    Kubernetes。它对于最多九个节点的集群是免费的，我强烈建议你至少尝试一下，以了解集群拓扑结构。'
- en: '**Ansible**: Kubernetes'' official repository also provides a set of playbooks
    to install a Kubernetes cluster on any VM provider as well as on bare metal.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ansible**：Kubernetes 官方仓库还提供了一套 playbooks，用于在任何虚拟机提供商或裸机上安装 Kubernetes 集群。'
- en: All of these options are very valid to set up a cluster from scratch as they
    automate parts of Kubernetes architecture by hiding the details and the full picture.
    If you really want to learn about the internals of Kubernetes, I would recommend
    a guide written by Kelsey Hightower called Kubernetes the hard way, which basically
    shows you how to set up everything around Kubernetes, from the etcd cluster needed
    to share information across nodes to the certificates used to communicate with
    `kubectl`, the remote control for Kubernetes. This guide can be found at [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些选项都是从头开始搭建集群的有效方法，因为它们通过隐藏细节和全貌，自动化了 Kubernetes 架构的部分内容。如果你真的想了解 Kubernetes
    的内部工作原理，我推荐 Kelsey Hightower 写的一本指南《Kubernetes the hard way》，它展示了如何围绕 Kubernetes
    设置一切，从需要在节点间共享信息的 etcd 集群，到用于与 `kubectl`（Kubernetes 的远程控制工具）通信的证书。你可以在 [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)
    找到这本指南。
- en: And it is maintained and up to date with new versions of Kubernetes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 而且它会随着 Kubernetes 新版本的发布而更新。
- en: As you can guess from this explanation, in this chapter, you are going to learn
    about the architecture of Kubernetes, but mainly, we will focus on how to deploy
    and operate applications on Kubernetes so that by the end of this chapter, we
    have a good understanding of how we can benefit from an already running cluster.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从这个解释中可以猜到的，在这一章中，你将学习 Kubernetes 的架构，但主要，我们将专注于如何在 Kubernetes 上部署和操作应用程序，以便在本章结束时，我们能够充分理解如何从一个已经运行的集群中获益。
- en: Kubernetes logical architecture
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 逻辑架构
- en: The first problem that you will find once you start playing with Kubernetes
    is creating a mental map on how and where everything runs in Kubernetes as well
    as how everything is connected.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你开始使用 Kubernetes，你会发现第一个问题是如何在脑海中构建一个关于 Kubernetes 中一切如何运行以及如何连接的思维地图。
- en: 'In this case, it took me few weeks to fully understand how it all was wiring
    up, but once I had the picture in my mind, I drew something similar to what is
    shown in the following diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我花了几周时间才完全理解这一切是如何连接的，但一旦我脑海中有了这个全貌，我画出了类似于下图所示的结构：
- en: '![](img/02b1c383-6951-4f06-a713-c3934fe10ace.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02b1c383-6951-4f06-a713-c3934fe10ace.png)'
- en: 'This is Kubernetes on a very high level: a master node that orchestrates the
    running of containers grouped in pods across different Nodes (they used to be
    called minions but not anymore).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Kubernetes 的一个非常高层次的概述：一个主节点负责协调容器的运行，这些容器被分组在不同节点上的 pods 中运行（它们曾经被称为 minions，但现在已经不再使用这个术语）。
- en: 'This mental map helps us understand how everything is wired up and brings up
    a new concept: the pod. A pod is basically a set of one or more containers running
    in orchestration to achieve a single task. For example, think about a cache and
    a cache warmer: they can run in different containers but on the same pod so that
    the cache warmer can be packed as an individual application. We will come back
    to this later on.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这张思维地图帮助我们理解一切如何连接起来，并引入了一个新的概念：pod。Pod 本质上是一组一个或多个容器，它们在协同操作中执行单一任务。例如，想象一个缓存和一个缓存预热器：它们可以在不同的容器中运行，但在同一个
    pod 上，这样缓存预热器就可以作为一个独立应用程序来打包。我们稍后会再讨论这个概念。
- en: 'With this picture, we are also able to identify different physical components:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这张图，我们还能够识别出不同的物理组件：
- en: Master
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点
- en: Nodes
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点
- en: The master is the node that runs all support services such as DNS (for service
    discovery) as well as the API server that allows us to operate the cluster. Ideally,
    your cluster should have more than one master, but in my opinion, being able to
    recover a master quickly is more important than having a high availability configuration.
    After all, if the master goes down, usually, it is possible to keep everything
    running until we recover the master that usually is as simple as spawning a new
    VM (on the cloud) with the same template as the old master was using.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点是运行所有支持服务的节点，如DNS（用于服务发现）以及API服务器，允许我们操作集群。理想情况下，您的集群应该有多个主节点，但在我看来，能够快速恢复主节点比拥有高可用性配置更重要。毕竟，如果主节点宕机，通常可以保持一切正常运行，直到我们恢复主节点，通常只需生成一个使用与旧主节点相同模板的新VM（在云上）即可。
- en: It is also possible to have a master running with the IP Tables blocking connections
    to key ports so that it does not join the cluster and remove the IP Tables rules
    once you want the master to become the lead of your cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以运行一个带有IP Tables阻止连接到关键端口的主节点，以防它加入集群，并在希望主节点成为集群领导时移除IP Tables规则。
- en: 'The nodes are basically workers: they follow instructions from the master in
    order to deploy and keep applications alive as per the specified configuration.
    They use a software called Kubelet, which is basically the Kubernetes agent that
    orchestrates the communication with the master.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 节点基本上是工作节点：它们根据指令从主节点部署和维持应用程序的运行，按照指定的配置。它们使用一个名为Kubelet的软件，这基本上是Kubernetes的代理程序，负责与主节点进行通信。
- en: 'Regarding the networking, there are two layers of network in here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关于网络，这里有两层网络：
- en: Hardware network
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件网络
- en: Software network
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件网络
- en: The hardware network is what we all know and that is used to interconnect the
    VMs on the cluster. It is defined in our cloud provider (AWS, Google Cloud Platform,
    and so on), and there is nothing special about it, just bear in mind that ideally,
    this network should be a high profile network (Gigabyte Ethernet) as the inter-node
    traffic can be quite high.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件网络是我们所熟知的，用于连接集群中的虚拟机（VM）。它在我们的云服务提供商（如AWS，Google Cloud Platform等）中定义，并没有什么特别之处，只需记住，理想情况下，这个网络应该是高性能网络（千兆以太网），因为节点间的流量可能会很大。
- en: The software network (or **Software Defined Network**, **SDN**) is a network
    that runs on top of Kubernetes middleware and is shared between all the nodes
    via **etcd**, which is basically a distributed key value storage that is used
    by Kubernetes as a coordination point to share information about several components.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 软件网络（或**软件定义网络，SDN**）是运行在Kubernetes中间件之上的网络，通过**etcd**与所有节点共享，etcd基本上是一种分布式键值存储，Kubernetes用它作为协调点来共享关于多个组件的信息。
- en: 'This SDN is used to interconnect the pods: the IPs are virtual IPs that do
    not really exist in the external network and only the nodes (and master) know
    about. They are used to rout the traffic across different nodes so that if an
    app on the node 1 needs to reach a pod living in the **Node 3**, with this network,
    the application will be able to reach it using the standard `http/tcp` stack.
    This network would look similar to what is shown in the following figure:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个SDN用于连接各个Pod：IP地址是虚拟IP，不在外部网络中实际存在，只有节点（和主节点）知道。它们用于在不同节点之间路由流量，因此，如果节点1上的应用程序需要访问**节点3**上的Pod，使用这个网络，应用程序可以通过标准的`http/tcp`协议栈进行访问。这个网络看起来与下图类似：
- en: '![](img/6ff1c1a6-6e62-44c8-aeed-57a30964dedd.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ff1c1a6-6e62-44c8-aeed-57a30964dedd.png)'
- en: 'Let''s explain this a bit:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简单解释一下这个网络结构：
- en: The addresses on the network 192.168.0.0/16 are the physical addresses. They
    are used to interconnect the VMs that compound the cluster.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络上的地址192.168.0.0/16是物理地址。它们用于连接组成集群的虚拟机（VM）。
- en: The addresses on the network 10.0.0.0/24 are the software defined network addresses.
    They are not reachable from outside the cluster and only the nodes are able to
    resolve these addresses and forward the traffic to the right target.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络上的地址10.0.0.0/24是软件定义网络地址。它们无法从集群外部访问，只有节点能够解析这些地址并将流量转发到正确的目标。
- en: Networking is a fairly important topic in Kubernetes, and currently, the most
    common bottleneck in performance is that traffic forwarding is common across nodes
    (we will come back to this later on in this chapter), and this causes extra inter-node
    traffic that might cause a general slowdown of the applications running in Kubernetes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是 Kubernetes 中一个相当重要的话题，目前性能瓶颈最常见的原因是节点之间的流量转发（我们将在本章稍后讨论），这会导致额外的节点间流量，从而可能导致
    Kubernetes 中运行的应用程序普遍变慢。
- en: In general and for now, this is all we need to know about the Kubernetes architecture.
    The main idea behind Kubernetes is to provide a uniform set of resources that
    can be used as a single computing unit with easy zero downtime operations. As
    of now, we really don't know how to use it, but the important thing is that we
    have a mental model of the big picture in a Kubernetes cluster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，现阶段我们需要了解的 Kubernetes 架构就是这些。Kubernetes 的核心理念是提供一组统一的资源，这些资源可以作为一个单一的计算单元，支持零停机操作的简便实现。目前，我们还不清楚如何使用它，但重要的是我们已经有了对
    Kubernetes 集群整体架构的理解。
- en: Setting up a cluster in GCP
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 GCP 上设置集群
- en: The first thing we need to start playing with in Kubernetes is a cluster. There
    are several options, but we are going to use GKE as we have already signed for
    the trial and there should be enough credit in there for going through the full
    book.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Kubernetes 中要开始操作的第一件事就是集群。虽然有多种选择，但我们将使用 GKE，因为我们已经注册了试用，并且账户里应该有足够的信用额度来完成整个书中的内容。
- en: Another option if you did not sign for the trial on GCP is Minikube. Minikube
    is an out-of-the-box, easy-to-install local cluster that runs on VMs and is a
    very good tool for experimenting with new features without being afraid of breaking
    something.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有在 GCP 上注册试用，另一个选择是 Minikube。Minikube 是一个开箱即用、易于安装的本地集群，它运行在虚拟机上，是一个非常好的工具，可以在不担心破坏任何东西的情况下尝试新特性。
- en: The Minikube project can be found at [https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube 项目可以在 [https://github.com/kubernetes/minikube](https://github.com/kubernetes/minikube)
    上找到。
- en: Its documentation is fairly comprehensive.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它的文档相当全面。
- en: 'In order to create a cluster in GCP, the first thing we need to do is open
    the container engine in the online console in GCP that will show something similar
    to what is shown in the following screenshot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 GCP 上创建一个集群，首先我们需要做的是在 GCP 的在线控制台中打开容器引擎，它将显示类似于以下截图的内容：
- en: '![](img/317e21d3-388e-4dc9-b6bc-fc66f39a85c1.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/317e21d3-388e-4dc9-b6bc-fc66f39a85c1.png)'
- en: 'This means that you have no clusters at the moment. Click on the Create a container
    cluster button and fill in the following form:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你目前没有集群。点击“创建容器集群”按钮，并填写以下表单：
- en: '![](img/44a48f26-1eee-4542-b5fb-5ac6df3c3ce5.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44a48f26-1eee-4542-b5fb-5ac6df3c3ce5.png)'
- en: 'Just a few considerations here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几点需要考虑的事项：
- en: Give a comprehensive name to the cluster. In my case, I named it `testing-cluster`.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给集群起个全面的名字。在我的例子中，我命名为 `testing-cluster`。
- en: Choose a zone that is close to you geographically, in my case, `europe-west1-c`.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个地理位置上靠近你的区域，在我的例子中是 `europe-west1-c`。
- en: Regarding the cluster version, choose the default one. This is the version of
    Kubernetes that you want your cluster to run. It can be seamlessly upgraded later.
    Also, be aware that Kubernetes releases a new version every 2 months (apporximately),
    so by the time you are reading this book, it is most likely that there will be
    a more modern version available.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于集群版本，选择默认版本。这是你希望集群运行的 Kubernetes 版本，之后可以无缝升级。此外，请注意 Kubernetes 每大约两个月发布一个新版本，因此到你读这本书时，很可能会有更现代的版本可用。
- en: The machine type should also be the standard one (1 vCPU 3.75 GB of RAM).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器类型也应该选择标准类型（1 个 vCPU，3.75 GB 的 RAM）。
- en: Size is the number of machines that we want to use in our cluster. Three is
    a good number for testing and it can also be increased (or decreased later on).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群的规模是我们希望在集群中使用的机器数量。三台是测试的一个不错选择，也可以在以后增加或减少。
- en: Everything else should be default. Auto-upgrade and auto-repair are beta functionalities
    that I would hesitate to use in a production cluster yet. These two options allow
    GCP to take actions if there is a new version of Kubernetes available or one of
    the nodes breaks for some reason.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其他内容应该是默认的。自动升级和自动修复是 Beta 功能，我现在还不建议在生产集群中使用。这两个选项允许 GCP 在有新版本的 Kubernetes
    发布或某个节点因某种原因发生故障时自动采取措施。
- en: 'Once the form is completed click on Create Cluster, and that is everything.
    Now Google is provisioning a cluster for us. In order to check what is going on,
    open the tab of the Compute Engine in the GCP and you should see something similar
    to what is shown in the following screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2890b604-2b63-4fd3-97db-22e6d31a8fca.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Three machines have been created in the compute engine with the prefix "`gke-`",
    which means that they belong to the GKE, **K** is for **Kubernetes**. They are
    regular machines, and there's nothing special about them aside from the fact that
    Google has provisioned all the software required to set up a Kubernetes Node,
    but where is the master?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the interesting thing about running Kubernetes in Google Cloud Platform:
    they look after your master so there is no need to worry about the high availability
    or upgrading it as it is done automatically.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The master of our cluster is hosting one of the key components of our whole
    cluster: the API server. All the operations in Kubernetes are done via the API
    server with a component called `kubectl`. Kubectl stands for Kubernetes Control
    and is basically a terminal program that you can install on your local machine
    (or in a continuous integration server), add the configuration for a given cluster,
    and start issuing commands to our cluster.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to install `kubectl`. In the previous chapters, we already
    installed the Google Cloud SDK (`gcloud` command), which can be used to install
    `kubectl` with the following command:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And that''s it. Now we can use `kubectl` in our system as if it was any other
    command, but we need to add our cluster configuration. As of now, `kubectl` is
    not configured to operate our cluster, so the first thing we need to do is fetch
    the required configuration. Google Cloud Platform makes it very easy. If you open
    the Google Container Engine tab, it now should look similar to the following one:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78bc4e64-e930-49b3-bd60-83a16b3d782b.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there is a button called Connect on the right-hand side of
    the screen. By clicking on it, you will be presented with the following form:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e2ca1a1-c476-4fe8-8241-9aacb15e9323.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'The date in the form will be slightly different (as the name of your cluster
    and project will be different), but there are two commands presented in there:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: A `gcloud` command to get the configuration of our Kubernetes cluster in our
    local machine
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `kubectl` command to start the proxy into the Kuberentes Dashboard UI
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first command is easy. Just execute it:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And the output will be similar to the following one:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So, what happened here is that `gcloud` fetched the configuration and installed
    it locally for us to operate the cluster. You can try this by running the following
    command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will output the list of nodes in your cluster. Kubectl is a very extensive
    command-line tool. With it, we can do pretty much anything inside the cluster,
    as we will learn in the rest of this chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'The second command in the preceding screenshot is used to start a proxy in
    Kubernetes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will output the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let's explain what happened here. Kubernetes makes heavy usage of client certificates.
    In order to communicate with the master, our machine needs to proxy the requests
    sending the certificate to validate them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we browse to the URL in the preceding screenshot now, `http://localhost:8001/ui`,
    we get presented with the Kubernetes dashboard:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f4986fd-fc75-4ae3-82d9-4d605efc0768.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: The dashboard is basically a nice way of presenting all the information of our
    running cluster to the end users. It is also possible to operate the cluster up
    to a certain extent from the dashboard, but my recommendation will be to master
    `kubectl` as it is way more powerful. On the dashboard, we can see a lot of information,
    such as the state of the nodes, the items deployed into the cluster (Pods, Replica
    Sets, Daemon Sets, and so on), and the namespaces as well as many other elements.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Explore around a bit and get yourself familiar with the dashboard as it is a
    nice tool to actually see things happening in your cluster.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes divides the workloads into namespaces. A namespace is a virtual
    cluster that allows the engineers to segregate resources (up to a point) across
    different teams. It is also used by Kubernetes to run its own internal components.
    This is important because Kubernetes spreads the key components across different
    nodes to ensure high availability. In this case, we have three components that
    are running on every node:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes dashboard
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes proxy (`kube-proxy`)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes DNS (`kube-dns`)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Kubernetes dashboard is what we just have seen: a user interface to represent
    the information within the Kubernetes cluster.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes proxy is a proxy that the nodes use to resolve IP addresses in the
    SDN from Pods addresses to node addresses so that the cluster is able to redirect
    the traffic to the right Node.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes DNS is basically a load balancing and service discovery mechanism.
    In the next section, you will learn about the building blocks that we can use
    for deploying applications to Kubernetes. In particular, Services are strongly
    coupled with this DNS service in a way that in order to locate an application
    within Kubernetes, we just need to know its name and the configuration of the
    Service that groups the Pods compounding the given application.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The fact that we are running these components in every node enables Kubernetes
    to enter into an autopilot mode in case of a master going down: applications will
    continue working (in the majority of the cases) even without a master, so losing
    a master is not a catastrophic event.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Once we have configured `kubectl` in our machines, it is time to learn about
    the building blocks that we can use in Kubernetes in order to build extremely
    robust applications.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes building blocks
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the preceding section, you learned about the cluster topology, but now we
    need the tools to run applications on it. We have already introduced one of the
    Kubernetes building blocks: the Pod. In this section, we are going to look at
    some of the most important API objects (building blocks) that Kubernetes provide
    in order to build our applications.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'When I started learning Kubernetes, I was working in the second company that
    was deploying applications in a continuous delivery way, and I always had a question
    in mind: why are different companies trying to solve the same problem in different
    ways?'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Then I realized why: The element missing was the domain-specific language for
    continuous delivery. The lack of a common standard and well understood way of
    rolling out applications was preventing them to work efficiently and deliver value
    early in the chain. Everybody knows what a load balancer is or a proxy or many
    other elements that are involved in the deployment of a new version of an app,
    but the way people uses the in, say, imaginative ways is where the problem lies.
    If you hire a new engineer, their previous knowledge of continuous delivery becomes
    obsolete as they need to learn your way of doing things.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes solves this problem with a set of objects (Pods, ReplicaSets, DameonSets,
    and so on) that are described in YAML files (or JSON). Once we finish this section,
    we will already have enough knowledge to be able to, from the YAML or JSON files
    defining our resources, build a diagram about what the system looks like. These
    files, alongside the Docker images, are enough for Kubernetes to run our system,
    and we will look at a few examples.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods are the most basic element of the Kubernetes API. A Pod basically is a
    set of containers that work together in order to provide a service or part of
    it. The concept of Pod is something that can be misleading. The fact that we can
    run several containers working together suggests that we should be sticking the
    frontend and backend of our application on a single pod as they work together.
    Even though we can do this, it is a practice that I would strongly suggest you
    avoid. The reason for this is that by bundling together the frontend and the backend,
    we are losing a lot of flexibility that Kubernetes is providing us with, such
    as autoscaling, load balancing, or canary deployments.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, pods contain a single container and it is, by far, the most common
    use case, but there are few legitimate use cases for multi-container pods:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Cache and cache warmer
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precalculating and serving HTML pages
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File upload and file processing
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, all of these are activities that are strongly coupled together,
    but if the feeling is that the containers within a pod are working toward different
    tasks (such as backend and frontend), it might be worth placing them in different
    Pods.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two options for communication between containers inside a pod:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Filesystem
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local network interface
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As Pods are indivisible elements running on a single machine, volumes mounted
    in all the containers of a pod are shared: files created in a container within
    a pod can be accessed from other containers mounting the same volume.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: The local network interface or loopback is what we commonly know as localhost.
    Containers inside a pod share the same network interface; therefore, they can
    communicate via localhost (or `127.0.0.1`) on the exposed ports.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a pod
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, Kubernetes relies heavily on **Yet Another Markup Language**
    (**YAML**) files to configure API elements. In order to deploy a pod, we need
    to create a yaml file, but first, just create a folder called **deployments**,
    where we are going to create all the descriptors that we will be created on this
    section. Create a file called `pod.yaml` (or `pod.yml`) with the following content:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, the preceding `yaml` is fairly descriptive, but some points
    need clarification:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`: This is the version of the Kubernetes API that we are going to
    use to define our resource (in this case, pod). Kuberentes is a living project
    that evolves very quickly. The version is the mechanism used to avoid deprecating
    resources with new releases. In general, Kuberentes works with three branches:
    alpha, beta, and stable. In the preceding case, we are using the stable version.
    More information can be found at [https://kubernetes.io/docs/concepts/overview/kubernetes-api/](https://kubernetes.io/docs/concepts/overview/kubernetes-api/).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: In this section, we are defining one of the most powerful discovery
    mechanisms that I have ever seen: the pattern matching. The section label, specifically,
    will be used later on to expose pods with certain **l**abels to the outer world.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec`: This is where we define our container. In this case, we are deploying
    an `nginx` instance so that we can easily see how everything works without focusing
    too much on the application itself. As expected, the image and the exposed port
    have been specified. We have also defined the CPU and memory limitations for this
    Pod, so we prevent an outbreak in resource consumption (note that the YAML file
    is requesting the resources; they might not be available so the pod will operate
    with lower profile resources).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the simplest configuration for an item that we can create in Kubernetes.
    Now it''s time to deploy the resource in our cluster:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will produce an output similar to the following one:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Disclaimer: there are several ways of creating a resource, but in this book,
    I will use `apply` as much as possible. Another possibility would be to use `create`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The advantage that `apply` has over create is that apply does a three-way diff
    between the previous version, the current version, and the changes that you want
    to apply and decides how is best to update the resource. This is letting Kubernetes
    do what it does best: automate container orchestration.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'With create, Kubernetes does not save the state of the resource, and if we
    want to run apply afterward in order to gracefully change the state of a resource,
    a warning is produced:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This means that we can push our system to an unstable state for few seconds,
    which might not be acceptable depending on your use case.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have applied our YAML file, we can use `kubectl` to see what is going
    on in Kubernetes. Execute the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will output our pod:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a6b0b4e-706a-4686-b11f-43592c818f96.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'We can do this for other elements of our cluster, such as the nodes:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And this will output the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4b67f6b-c276-4f97-917e-e1e95b2eeb2b.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: The `kubectl get` works for all the workflows in Kubernetes and the majority
    of the API objects.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Another way of seeing what is going on in Kubernetes is using the dashboard.
    Now that we have created a pod, open the dashboard at `http://localhost:8001/ui`
    and navigate to the pods section on the left-hand side.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in order to access the dashboard, first, you need to execute `kubectl
    proxy` on a Terminal.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'There; you will see the list of the current deployed pods, in this case, just
    nginx. Click on it and the screen should look very similar to what is shown here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15fd80e2-8bb2-4d68-830f-ffb7d6fb62a8.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Here, we get a ton of information, from the memory and CPU that the pod is
    consuming to the node where it is running and a few other valuable items, such
    as the annotations applied to the pod. We can get this using the ''`describe`''
    command of `kubectl`, as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Annotations are a new concept and are the data around our API element, in this
    case, our pod. If you click on Last applied configuration in the Details section,
    you can see the data from the YAML file, as shown in the following screenshot:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9577ed4e-2e88-4681-aeaf-42ca6d67379e.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: And this relates to the three-way diff that was explained earlier and is used
    by Kubernetes to decide the best way of upgrading a resource without getting into
    an inconsistent state.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'As of now, our pod is running in Kubernetes but is not connected to the outer
    world; therefore, there is no way to open a browser and navigate to the nginx
    home page from outside the cluster. One thing that we can do is open a remote
    session to a bash Terminal in the container inside the pod in a manner similar
    to what we would do with Docker:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And we are in. Effectively, we have gained access to a root terminal inside
    our container and we can execute any command. We will use this functionality later
    on.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have seen how pods work, you might have a few questions abound what
    Kubernetes is supposed to do:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: How can we scale pods?
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we roll out new versions of an application?
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we access our application?
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will answer all these questions, but first, we need to know other 'building
    blocks'.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Replica Sets
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we know how to deploy applications in pods. The sole concept of pod
    is very powerful, but it lacks robustness. It is actually impossible to define
    scaling policies or even make sure that the pods remain alive if something happens
    (such as a node going down). This might be okay in some situations, but here is
    an interesting question. If we are biting the bullet on the overhead of maintaining
    a Kubernetes cluster, why don't we take the benefits of it?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do that, we need to work with **Replica Sets**. A Replica Set is
    like a traffic cop in a road full of pods: they make sure that the traffic flows
    and everything works without crashing and moving the pods around so that we make
    the best use of the road (our cluster, in this case).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Replica Sets are actually an update of a much older item: the Replication Controller.
    The reason for the upgrade is the labeling and selecting of resources, which we
    will see visit when we dive deep into the API item called Service.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a Replica Set:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Again, this a YAML file that is basically fairly easy to understand but might
    require some explanation:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we have used the extensions API on the version `v1beta1`. If
    you remember from the pod section (previously), Kubernetes has three branches:
    stable, alpha, and beta. The complete reference can be found in the official documentation,
    and it is very likely to change often as Kubernetes is a vibrant and always evolving
    project.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the spec section is where the important things happen: we have defined a
    set of labels for the Replica Set, but we have also defined a pod (in this case,
    with a single container) and specified that we want three instances of it (replicas:
    three).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple and effective. Now we have defined a resource called Replica Set, which
    allows us to deploy a pod and keep it alive as per configuration.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test it:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once the command returns, we should see the following message:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s verify it using `kubectl`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As the output of the preceding command, you should see the Replica Set explaining
    that there are three desired pods, three actually deployed, and three ready. Note
    the difference between current and ready: a pod might be deployed but still not
    ready to process requests.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'We have specified that our `replicaset` should keep three pods alive. Let''s
    verify this:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'No surprises here: our `replicaset` has created three pods, as shown in the
    following screenshot:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1055e82-5872-477b-91a8-6d200347d72b.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: 'We have four pods:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: One created in the preceding section
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three created by the Replica Set
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s kill one of the pods and see what happens:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And now, query how many pods are running:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/508833b4-48b0-4dc0-a0e4-119ead927a8e.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: Bingo! Our `replicaset` has created a new pod (you can see which one in the
    AGE column). This is immensely powerful. We have gone from a world where a pod
    (an application) being killed wakes you up at 4 a.m. in the morning to take action
    to a world where when one of our application dies, Kubernetes revives it for us.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at what happened in the dashboard:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bf89559-f168-4880-bac5-c038679168b7.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: As you can expect, the Replica Set has created the pods for you. You can try
    to kill them from the interface as well (the period icon to the very right of
    every pod will allow you to do that), but the Replica Set will re-spawn them for
    you.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are going to do something that might look like it''s from out of this
    world: we are going to scale our application with a single command, but first,
    edit `replicaset.yml` and change the `replicas` field from three to five.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the file and execute this:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now take a look at the dashboard again:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f92bebd-c741-495b-a01b-3b9be7fb475d.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: As you can see, Kubernetes is creating pods for us following the instructions
    of the Replica Set, `nginx-rs`. In the preceding sreenshot, we can see one pod
    whose icon is not green, and that is because its status is Pending, but after
    a few seconds, the status becomes Ready, just like any other pod.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also very powerful, but there is a catch: who scales the application
    if the load spike happens at 4 a.m. in the morning? Well, Kubernetes provides
    a solution for this: Horizontal Pod Autoscalers.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute the following command:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With the preceding command, we have specified that Kubernetes should attach
    a Horizontal Pod Autoscalers to our Replica Set. If you browse the Replica Set
    in the dashboard again, the situation has changed dramatically:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f948bb0-7e82-4b40-b53e-dc9d12fac20f.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explain what happened here:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'We have attached an Horizontal Pod Autoscalers to our Replica Set: minimum
    `1` pod, maximum `10`, and the trigger for creating or destroying pods is the
    CPU utilization going over `80%` on a given pod.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Replica Set has scaled down to one pod because there is no load on the system,
    but it will scale back to up to 10 nodes if required and stay there for as long
    as the burst of requests is going on and scale back to the minimum required resources.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now this is actually the dream of any sysadmin: no-hassle autoscaling and self-healing
    infrastructure. As you can see, Kubernetes starts making sense altogether, but
    there is one thing disturbing in the autoscaler part. It was a command that we
    ran in the terminal, but it is captured nowhere. So how can we keep track of our
    infrastructure (yes, an Horizontal Pod Autoscaler is part of the infrastructure)?'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, there is an alternative; we can create a YAML file that describes our
    Horizontal Pod Autoscaler:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'First, from the dashboard, remove `HorizontalPodAutoscaler` created from the
    previous example. Then, write the preceding content into a file called `horizontalpodautoscaler.yml`
    and run the following command:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This should have the same effect as the `autoscale` command but with two obvious
    benefits:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: We can control more parameters, such as the name of the HPA, or add metadata
    to it, such as labels
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We keep our infrastructure as code within reach so we know what is going on
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second point is extremely important: we are in the age of the infrastructure
    as code and Kubernetes leverages this powerful concept in order to provide traceability
    and readability. Later on, in [Chapter 8](127a7b5f-4bd7-4290-bea0-3e8db867e4af.xhtml),
    *Release Management – Continuous Delivery*, you will learn how to create a continuous
    delivery pipeline with Kubernetes in a very easy way that works on 90% of the
    software projects.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Once the preceding command returns, we can check on the dashboard and see that
    effectively, our Replica Set has attached an Horizontal Pod Autoscaler as per
    our configuration.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though the Replica Set is a very powerful concept, there is one part of
    it that we have not talked about: what happens when we apply a new configuration
    to a Replica Set in order to upgrade our applications? How does it handle the
    fact that we want to keep our application alive 100% of the time without service
    interruption?'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the answer is simple: it doesn''t. If you apply a new configuration to
    a Replica Set with a new version of the image, the Replica Set will destroy all
    the Pods and create newer ones without any guaranteed order or control. In order
    to ensure that our application is always up with a guaranteed minimum amount of
    resources (Pods), we need to use Deployments.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'First, take a look at what a deployment looks like:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As you can see, it is very similar to a Replica Set, but there is a new section:
    strategy. In strategy, we are defining how our `rollout` is going to work, and
    we have two options:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '`RollingUpdate`'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Recreate`'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RollingUpdate` is the default option as it seems the most versatile in modern
    24/7 applications: It coordinates two replica sets and starts shutting down pods
    from the old replica set at the same time that it is creating them in the new
    Replica Set. This is very powerful because it ensures that our application always
    stays up. Kubernetes decides what is best to coordinate the pods'' rescheduling,
    but you can influence this decision with two parameters:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '`maxUnavailable`'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxSurge`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first one defines how many pods we can loose from our Replica Set in order
    to perform a `rollout`. As an example, if our Replica Set has three replicas,
    a `rollout` with the `maxUnavailable` value of `1` will allow Kubernetes to transition
    to the new Replica Set with only two pods in the status `Ready` at some point.
    In this example, `maxUnavailable` is `0`; therefore, Kubernetes will always keep
    three pods alive.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '`MaxSurge` is similar to maxUnavailable, but it goes the other way around:
    it defines how many pods above the replicas can be scheduled by Kubernetes. In
    the preceding example, with three replicas with `maxSurge` set on `1`, the maximum
    amount of pods at a given time in our `rollout` will be `4`.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Playing with these two parameters as well as the replicas'' number, we can
    achieve quite interesting effects. For example, by specifying three replicas with
    `maxSurge 1` and `maxUnavailable 1`, we are forcing Kubernetes to move the pods
    one by one in a very conservative way: we might have four pods during the `rollout`,
    but we will never go below three available pods.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the strategies, Recreate basically destroys all the pods and
    creates them again with the new configuration without taking uptime into account.
    This might be indicated in some scenarios, but I would strongly suggest that you
    use `RollingUpdate` when possible (pretty much always) as it leads to smoother
    deployments.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to attach a Horizontal Pod Autoscaler to a Deployment in
    the same way that we would do with a Replica Set.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s test our deployment. Create a file called `deployment.yml` and apply
    it to our cluster:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once the command returns, we can go to the Kubernetes dashboard (`localhost:8001/ui`
    with the proxy active) and check what happened in the Deployments section in the
    menu on the left-hand side:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da68eada-ed69-491f-b51a-851d95ceb92f.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
- en: 'We have a new Deployment called `nginx-deployment`, which has created a Replica
    Set that also contains the specified pods. In the preceding command, we have passed
    a new parameter: `--record`. This saves the command in the `rollout` history of
    our deployment so that we can query the `rollout` history of a given deployment
    to see the changes applied to it. In this case, just execute the following:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will show you all the actions that altered the status of a deployment
    called `nginx-deployment`. Now, let''s execute some change:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We have used `kubectl` to change the version of the `nginx` container back
    to version 1.9.1 (`kubectl` is very versatile; the official documentation offers
    shortcuts for pretty much everything), and a few things happened. The first one
    is that a new Replica Set has been created and the pods have been moved over to
    it from the old replica set. We can verify this in the Replica Sets section of
    the menu on the left-hand side of the dashboard:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/914506b4-95bc-4bb3-a8f7-cbc5da1cc2d1.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: As you can see, the old replica set has 0 pods, whereas the new one that took
    over has three pods. This all happened without you noticing it, but it is a very
    clever workflow with a lot of work from the Kubernetes community and the companies
    behind it.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'The second thing that happened was that we have a new entry in our rollout
    history. Let''s check it out:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Which one should produce an output similar to the following one:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1947f72-a1f1-410f-b4e1-8d946788f00e.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: Now we have two entries that describe the changes applied to our deployment.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have been into IT for few years, by now, you have reached the conclusion
    that a rollback strategy is always necessary because bugs flowing into production
    are the reality no matter how good our QA is. I am a big fan of building the systems
    in a way that deployments are unimportant events (from a technical point of view),
    as shown with Kuberentes, and the engineers always have an easy way out if things
    start to fail in production. Deployments offer an easy rollback if something goes
    wrong:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Execute the preceding and browse back to the dashboard on the **Replica Sets**
    section again:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18a385bd-ca4c-44b3-8b36-6a0508f690cc.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: 'That''s right. In a matter of seconds, we have gone from instability (a broken
    build) to the safety of the old known version without interrupting the service
    and without involving half of the IT department: a simple command brings back
    the stability to the system. The rollback command has a few configurations, and
    we can even select the revision where we want to jump to.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how powerful Kubernetes is and this is how simple our life becomes
    by using Kubernetes as the middleware of our enterprise: a modern CD pipeline
    assembled in a few lines of configuration that works in the same way in all the
    companies in the world by facilitating command `rollouts` and rollbacks. That''s
    it...simple and efficient.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Right now, it feels like we know enough to move our applications to Kubernetes,
    but there is one thing missing. So far, up until now, we have just run predefined
    containers that are not exposed to the outer world. In short, there is no way
    to reach our application from outside the cluster. You are going to learn how
    to do that in the next section.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Services
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we were able to deploy containers into Kubernetes and keep them
    alive by making use of pods, Replica Sets, and Horizontal Pods Autoscalers as
    well as Deployments, but so far, you have not learned how to expose applications
    to the outer world or make use of service discovery and balancing within Kubernetes.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '**Services** are responsible for all of the above. A Service in Kubernetes
    is not an element as we are used to it. A Service is an abstract concept used
    to give entity to a group of pods through pattern matching and expose them to
    different channels via the same interface: a set of labels attached to a Pod that
    get matched against a selector (another set of labels and rules) in order to group
    them.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create a service on top of the deployment created in the previous
    section:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Easy and straightforward, but there''s one detail: the selector section has
    a hidden message for us. The selectors are the mechanisms that Kubernetes uses
    to connect components via pattern matching algorithms. Let''s explain what pattern
    matching is. In the preceding Service, we are specifying that we want to select
    all the Pods that have a label with the `app` key and the `nginx` value. If you
    go back to the previous section, you''ll understand our deployment has these labels
    in the pod specification. This is a match; therefore, our service will select
    these pods. We can check this by browsing in the dashboard in the Services section
    and clicking on `nginx-service`, but first, you need to create the `service`:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, check out the dashboard:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e9b3f8f-cae8-4dee-8969-95fad1d7814e.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: As you can see, there are three pods selected, and they all belong to the deployment
    `nginx` that we created in the preceding section.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Don't remove the deployment from the previous section; otherwise, there will
    be no pods to select by our service.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'This screen has a lot of interesting information. The first piece of information
    is that the service has an IP: this IP is denominated as `clusterIP`. Basically,
    it is an IP within the cluster that can be reached by our pods and other elements
    in Kubernetes. There is also a field called `Type`, which allows us to chose the
    service type. There are three types:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '`ClusterIP`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NodePort`'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LoadBalancer`'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ClusterIP` is what we just created and explained.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '`NodePort` is another type of service that is rarely used in Cloud but is very
    common on premises. It allocates a port on all the nodes to expose our application.
    This allows Kubernetes to define the ingress of the traffic into our pods. This
    is challenging for two reasons:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: It generates extra traffic in our internal network as the nodes need to forward
    the traffic across to reach the pods (imagine a cluster of 100 nodes that has
    an app with only three pods, it is very unlikely to hit the node that is running
    one of them).
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ports are allocated randomly so you need to query the Kubernetes API to
    know the allocated port.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LoadBalancer` is the jewel in the crown here. When you create a service of
    type `LoadBalancer`, a cloud load balancer is provisioned so that the client applications
    hit the load balancer that redirects the traffic into the correct nodes. As you
    can imagine, for a cloud environment where infrastructure is created and destroyed
    in matter of seconds, this is the ideal situation.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the previous screenshot, we can see another piece of interesting
    information: the internal endpoints. This is the service discovery mechanism that
    Kubernetes is using to locate our applications. What we have done here is connect
    the pods of our application to a name: `nginx-service`. From now on, no matter
    what happens, the only thing that our apps need to know in order to reach our
    `nginx` pods is that there is a service called `nginx` that knows how to locate
    them.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to test this, we are going to run an instance of a container called
    `busybox`, which is basically the Swiss army knife of command-line tools. Run
    the following command:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding command will present us with a shell inside the container called
    `busybox` running in a pod so we are inside the Kubernetes cluster and, more importantly,
    inside the network so that we can see what is going on. Be aware that the preceding
    command runs just a pod: no deployment or replica set is created, so once you
    exit the shell, the pod is finalized and resources are destroyed.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we get the prompt inside `busybox`, run the following command:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This should return something similar to the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Okay, what happened here? When we created a service, we assigned a name to
    it: `nginx-service`. This name has been used to register it in an internal DNS
    for service discovery. As mentioned earlier, the DNS service is running on Kubernetes
    and is reachable from all the Pods so that it is a centralised repository of common
    knowledge. There is another way that the Kubernetes engineers have created in
    order to carry on with the service discovery: the environment variables. In the
    same prompt, run the following command:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This command outputs all the environment variables, but there are few that
    are relevant to our recently defined service:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'These variables, injected by Kubernetes at creation time, define where the
    applications can find our service. There is one problem with this approach: the
    environment variables are injected at creation time, so if our service changes
    during the life cycle of our pods, these variables become obsolete and the pod
    has to be restarted in order to inject the new values.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'All this magic happens through the selector mechanism on Kubernetes. In this
    case, we have used the equal selector: a label must match in order for a pod (or
    an object in general) to be selected. There are quite a few options, and at the
    time of writing this, this is still evolving. If you want to learn more about
    selectors, here is the official documentation: [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, services are used in Kubernetes to glue our applications together.
    Connecting applications with services allows us to build systems based on microservices
    by coupling REST endpoints in the API with the name of the service that we want
    to reach on the DNS.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Up until now, you have learned how to expose our applications to the rest of
    our cluster, but how do we expose our applications to the outer world? You have
    also learned that there is a type of service that can be used for this: `LoadBalancer`.
    Let''s take a look at the following definition:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'There is one change in the preceding definition: the service type is now `LoadBalancer`.
    The best way to explain what this causes is by going to the Services section of
    the dashboard:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f694bd9-47e9-49dd-9f6d-1b0d2fdfd334.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
- en: As you can see, our newly created service got assigned an external endpoint.
    If you browse it, bingo! The `nginx` default page is rendered.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'We have created two services, `nginx-service` and `nginx-service-lb`, of the
    type `ClusterIP` and `LoadBalancer`, respectively, which both point to the same
    pods that belong to a deployment and are managed through a replica set. This can
    be a bit confusing, but the following diagram will explain it better:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1647789c-12c0-459c-866b-7956a0919600.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram is the perfect explanation of what we've built in this
    section. As you can see, the load balancer is outside of Kubernetes, but everything
    else is inside our cluster as virtual elements of an API.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Other Building Blocks
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, you learned the basics needed to deploy applications
    into Kubernetes successfully. The API objects that we visited are as follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Pod
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicaSet
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Kubernetes, there are many other building blocks that can be used to build
    more advanced applications; every few months, the Kubernetes engineers add new
    elements to improve or add functionality.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: One example of these additions is the ReplicaSet that was designed to replace
    another item called ReplicationController. The main difference between the ReplicationController
    and the ReplicaSet is that the latter one has a more advance semantics label selection
    for the Pods that were recently re-engineered in Kubernetes.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: As a new product, Kuberentes is constantly changing (in fact, it is possible
    that by the time that you read this book, the core elements might have changed),
    so the engineers try to keep the compatibility across different versions so that
    people are not urged to upgrade in a short period of time.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Other examples of more advanced building blocks are the following:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSet
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PetSets
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs and CronJobs
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CronJobs
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to go in deep to the full stack in Kubernetes, we would need a full
    book (or more!). Let's visit some of them.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Daemon Sets
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Daemon Sets are an API element used to **ensure that a Pod is running in all
    (or some) nodes**. One of the assumptions in Kubernetes is that the pod should
    not worry about which node is being run, but that said, there might be a situation
    where we want to ensure that we run at least one pod on each node for a number
    of reasons:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Collect logs
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the hardware
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to do that, Kubernetes provides an API element called Daemon Set. Through
    a combination of labels and selectors, we can define something called **affinity**,
    which can be used to run our pods on certain nodes (we might have specific hardware
    requirements that only a few nodes are able to provide so that we can use tags
    and selectors to provide a hint to the pods to relocate to certain nodes).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Daemon Sets have several ways to be contacted, from the DNS through a headless
    service (a service that works as a load balancer instead of having a cluster IP
    assigned) to the node IP, but Daemon Sets work best when they are the initiators
    of the communication: something happens (an event) and a Daemon Set sends an event
    with information about that event (for example, a node is running low on space).'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: PetSets
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PetSets** are an interesting concept within Kubernetes: they are strong named
    resources whose naming is supposed to stay the same for a long term. As of now,
    a pod does not have a strong entity within a Kubernetes cluster: you need to create
    a service in order to locate a pod as they are ephemeral. Kubernetes can reschedule
    them at any time without prior notice for changing their name, as we have seen
    before. If you have a deployment running in Kubernetes and kill one of the pods,
    its name changes from (for example) *pod-xyz* to *pod-abc i*n an unpredictable
    way. so we cannot know which names to use in our application to connect to them
    beforehand.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with a Pet Set, this changes completely. A pet set has an ordinal
    order, so it is easy to guess the name of the pod. Let''s say that we have deployed
    a Pet Set called mysql, which defines pods running a MySQL server. If we have
    three replicas, the naming will be as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '`mysql-0`'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mysql-1`'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mysql-2`'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we can bake this knowledge in our application to reach them. This is suboptimal
    but good enough: we are still coupling services by name (DNS service discovery
    has this limitation), but it works in all cases and is a sacrifice that is worth
    paying for because in return, we get a lot of flexibility. The ideal situation
    in service discovery is where our system does not need to know even the name of
    the application carrying the work: just throw the message into the ether (the
    network) and the appropriated server will pick it up and respond accordingly.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'Pet Sets have been replaced in later versions of Kubernetes with another item
    called **Stateful Set.** The Stateful Set is an improvement over the Pet Set mainly
    in how Kubernetes manages the **master knowledge to avoid a split brain situation**:
    where two different elements think that they are in control.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  id: totrans-392
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Job** in Kubernetes is basically an element that spawns the defined number
    of pods and waits for them to finish before completing its life cycle. It is very
    useful when there is a need to run a one-off task, such as rotating logs or migrating
    data across databases.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Cron jobs have the same concept as Jobs, but they get triggered by time instead
    of a one-off process.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'Both in combination are very powerful tools to keep any system running. If
    you think about how we rotate logs without Kubernetes via ssh, it is quite risky:
    there is no control (by default) over who is doing what, and usually, there is
    no review process in the ssh operations carried by an individual.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: With this approach, it is possible to create a Job and get other engineers to
    review it before running it for extra safety.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Secrets and configuration management
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On Docker in general, as of today, secrets are being passed into containers
    via environment variables. This is very insecure: first, there is no control over
    who can access what, and second, environment variables are not designed to act
    as secrets and a good amount of commercial software (and open source) outputs
    them into the standard output as part of bootstrapping. Needless to say, that''s
    rather inconvenient.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes has solved this problem quite gracefully: instead of passing an
    environment variable to our container, a volume is mounted with the secret on
    a file (or several) ready to be consumed.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: By default, Kubernetes injects a few secrets related to the cluster into our
    containers so that they can interact with the API and so on, but it is also possible
    to create your own secrets.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to create secrets:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Using `kubectl`
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining an API element of type secret and using `kubectl` to deploy it
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first way is fairly straightforward. Create a folder called *secrets* in
    your current work folder and execute the following commands inside it:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This creates two files with two strings (simple strings as of now). Now it
    is time to create the secret in Kubernetes using `kubectl`:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'And that''s it. Once we are done, we can query the secrets using `kubectl`:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This, in my case, returns two secrets:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: A service account token injected by the cluster
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My newly created secret (`my-secrets`)
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second way of creating a secret is by defining it in a `yaml` file and
    deploying it via `kubectl`. Take a look at the following definition:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'First, the values for `secret1` and `secret2`, seem to be encrypted, but they
    are not; they are just encoded in `base64`:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will return the values that you can see here. The type of the secret is
    Opaque, which is the default type of secret, and the rest seems fairly straightforward.
    Now create the secret with kubectl (save the preceding content in a file called
    `secret.yml`):'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: And that's it. If you query the secrets again, note that there should be a new
    one called `my-secret-yaml`. It is also possible to list and see the secrets in
    the dashboard on the Secrets link in the menu on left-hand side.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is time to use them. In order to use the secret, two things need to
    be done:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Claim the secret as a volume
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mount the volume from the secret
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at a `Pod` using a secret:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'So, you have learned a new thing here: `kubectl` also understands JSON. If
    you don''t like YAML, it is possible to write your definitions in JSON without
    any side-effects.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Now, looking at the JSON file, we can see how first, the secret is declared
    as a volume and then how the secret is mounted in the path/secrets.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to verify this, just run a command in your container to check it:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This should list the two files that we have created, `secret1.txt` and `secret2.txt`,
    containing the data that we have also specified.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes- moving on
  id: totrans-430
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned enough to run simple applications in Kubernetes,
    but even though we cannot claim ourselves to be experts, we got the head start
    in becoming experts. Kubernetes is a project that evolves at the speed of light,
    and the best thing that you can do to keep yourself updated is follow the project
    on GitHub at [https://github.com/kubernetes](https://github.com/kubernetes).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes community is very responsive with issues raised by the users
    and are also very keen on getting people to contribute to the source code and
    documentation.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: If you keep working with Kubernetes, some help will be required. The official
    documentation is quite complete, and even though it feels like it needs a reshuffle
    sometimes, it is usually enough to keep you going.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: The best way that I've found to learn Kubernetes is by experimenting in Minikube
    (or a test cluster) before jumping into a bigger commitment.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at a good amount of concepts required to deploy an
    application on Kubernetes. As mentioned earlier, it is impossible to cover everything
    abound Kubernetes in a single chapter, but with the amount of knowledge from this
    chapter, we are going to be able to set up a continuous delivery pipeline in the
    following chapter in a way that we automate zero downtime deployments without
    the big bang effect (the big deployment that stops the world), enabling our organization
    to move faster.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
