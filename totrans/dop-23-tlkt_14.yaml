- en: Creating a Production-Ready Kubernetes Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a Kubernetes cluster is not trivial. We have to make many choices,
    and we can easily get lost in the myriad of options. The number of permutations
    is getting close to infinite and, yet, our clusters need to be configured consistently.
    Experience from the first attempt to set up a cluster can easily convert into
    a nightmare that will haunt you for the rest of your life.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Docker Swarm that packs almost everything into a single binary, Kubernetes
    clusters require quite a few separate components running across the nodes. Setting
    them up can be very easy, or it can become a challenge. It all depends on the
    choices we make initially. One of the first things we need to do is choose a tool
    that we'll use to create a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If we'd decide to install a Docker Swarm cluster, all we'd need to do is to
    install Docker engine on all the servers, and execute `docker swarm init` or `docker
    swarm join` command on each of the nodes. That's it. Docker packs everything into
    a single binary. Docker Swarm setup process is as simple as it can get. The same
    cannot be said for Kubernetes. Unlike Swarm that is highly opinionated, Kubernetes
    provides much higher freedom of choice. It is designed around extensibility. We
    need to choose among many different components. Some of them are maintained by
    the core Kubernetes project, while others are provided by third-parties. Extensibility
    is probably one of the main reasons behind Kubernetes' rapid growth. Almost every
    software vendor today is either building components for Kubernetes or providing
    a service that sits on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the intelligent design and the fact that it solves problems related
    to distributed, scalable, fault-tolerant, and highly available systems, Kubernetes'
    power comes from adoption and support from a myriad of individuals and companies.
    You can use that power, as long as you understand that it comes with responsibilities.
    It's up to you, dear reader, to choose how will your Kubernetes cluster look like,
    and which components it'll host. You can decide to build it from scratch, or you
    can use one of the hosted solutions like **Google Cloud Platform** (**GCE**) Kubernetes
    Engine. There is a third option though. We can choose to use one of the installation
    tools. Most of them are highly opinionated with a limited amount of arguments
    we can use to tweak the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking that creating a cluster from scratch using `kubeadm` cannot
    be that hard. You'd be right if running Kubernetes is all we need. But, it isn't.
    We need to make it fault tolerant and highly available. It needs to stand the
    test of time. Constructing a robust solution would require a combination of Kubernetes
    core and third-party components, AWS know-how, and quite a lot of custom scripts
    that would tie the two together. We won't go down that road. At least, not now.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use **Kubernetes Operations** (**kops**) to create a cluster. It is somewhere
    in the middle between do-it-yourself-from-scratch and hosted solutions (for example,
    GCE). It's an excellent fit for both newbies and veterans. You'll learn which
    components are required for running a Kubernetes cluster. You'll be able to make
    some choices. And, yet, we won't go down the rabbit hole of setting up the cluster
    from scratch. Believe me, that hole is very deep, and it might take us a very
    long time to get out of it.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, this would be a great place to explain the most significant components
    of a Kubernetes cluster. Heck, you were probably wondering why we didn't do that
    early on when we began the journey. Still, we'll postpone the discussion for a
    while longer. I believe it'll be better to create a cluster first and discuss
    the components through live examples. I feel that it's easier to understand something
    we can see and touch, instead of keeping it purely on the theoretical level.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, we'll create a cluster first, and discuss its components later.
  prefs: []
  type: TYPE_NORMAL
- en: Since I already mentioned that we'll use **kops** to create a cluster, we'll
    start with a very brief introduction to the project behind it.
  prefs: []
  type: TYPE_NORMAL
- en: What is kubernetes operations (kops) project?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you visit **Kubernetes Operations** (**kops**) ([https://github.com/kubernetes/kops](https://github.com/kubernetes/kops))
    project, the first sentence you'll read is that it is "the easiest way to get
    a production-grade Kubernetes cluster up and running." In my humble opinion, that
    sentence is accurate only if we exclude **Google Kubernetes Engine** (**GKE**).
    Today (February 2018), other hosting vendors did not yet release their Kubernetes-as-a-service
    solutions. Amazon's **Elastic Container Service for Kubernetes** (**EKS**) ([https://aws.amazon.com/eks/](https://aws.amazon.com/eks/))
    is still not open to the public. **Azure Container Service** (**AKS**) ([https://azure.microsoft.com/en-us/services/kubernetes-service/](https://azure.microsoft.com/en-us/services/kubernetes-service/))
    is also a new addition that still has a few pain points. By the time you read
    this, all major hosting providers might have their solutions. Still, I prefer
    kops since it provides almost the same level of simplicity without taking away
    the control of the process. It allows us to tweak the cluster more than we would
    be permitted with hosted solutions. It is entirely open source, it can be stored
    in version control, and it is not designed to lock you into a vendor.
  prefs: []
  type: TYPE_NORMAL
- en: If your hosting vendor is AWS, kops is, in my opinion, the best way to create
    a Kubernetes cluster. Whether that's true for GCE, is open for debate since GKE
    works great. We can expect kops to be extended in the future to other vendors.
    For example, at the time of this writing, VMWare is in alpha and should be stable
    soon. Azure and Digital Ocean support are being added as I write this.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use kops to create a Kubernetes cluster in AWS. This is the part of the
    story that might get you disappointed. You might have chosen to run Kubernetes
    somewhere else. Don't be depressed. Almost all Kubernetes clusters follow the
    same principles even though the method of setting them up might differ. The principles
    are what truly matters, and I'm confident that, once you set it up successfully
    on AWS, you'll be able to transfer that knowledge anywhere else.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for choosing AWS lies in its adoption. It is the hosting vendor with,
    by far, the biggest user-base. If I'd have to place a blind bet on your choice,
    it would be AWS solely because that is statistically the most likely choice. I
    could not explore all the options in a single chapter. If I am to go through all
    hosting vendors and different projects that might help with the installation,
    we'd need to dedicate a whole book to that. Instead, I invite you to explore the
    subject further once you're finished with installing Kubernetes in AWS with kops.
    As an alternative, ping me on `slack.devops20toolkit.com` or send me an email
    to `viktor@farcic.com` and I'll give you a hand. If I receive enough messages,
    I might even dedicate a whole book to Kubernetes installations.
  prefs: []
  type: TYPE_NORMAL
- en: I went astray from kops...
  prefs: []
  type: TYPE_NORMAL
- en: Kops lets us create a production-grade Kubernetes cluster. That means that we
    can use it not only to create a cluster, but also to upgrade it (without downtime),
    update it, or destroy it if we don't need it anymore. A cluster cannot be called
    "production grade" unless it is highly available and fault tolerant. We should
    be able to execute it entirely from the command line if we'd like it to be automated.
    Those and quite a few other things are what kops provides, and what makes it great.
  prefs: []
  type: TYPE_NORMAL
- en: Kops follows the same philosophy as Kubernetes. We create a set of JSON or YAML
    objects which are sent to controllers that create a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss what kops can and cannot do in more detail soon. For now, we'll
    jump into the hands-on part of this chapter and ensure that all the prerequisites
    for the installation are set.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for the cluster setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll continue using the specifications from the `vfarcic/k8s-specs` repository,
    so the first thing we'll do is to go inside the directory where we cloned it,
    and pull the latest version.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `14-aws.sh` ([https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134](https://gist.github.com/vfarcic/04af9efcd1c972e8199fc014b030b134))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I will assume that you already have an AWS account. If that's not the case,
    please head over to Amazon Web Services ([https://aws.amazon.com/](https://aws.amazon.com/))
    and sign-up.
  prefs: []
  type: TYPE_NORMAL
- en: If you are already proficient with AWS, you might want to skim through the text
    that follows and only execute the commands.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we should do is get the AWS credentials.
  prefs: []
  type: TYPE_NORMAL
- en: Please open Amazon EC2 Console ([https://console.aws.amazon.com/ec2/v2/home](https://console.aws.amazon.com/ec2/v2/home)),
    click on your name from the top-right menu and select My Security Credentials.
    You will see the screen with different types of credentials. Expand the Access
    Keys (Access Key ID and Secret Access Key) section and click the Create New Access
    Key button. Expand the Show Access Key section to see the keys.
  prefs: []
  type: TYPE_NORMAL
- en: You will not be able to view the keys later on, so this is the only chance you'll
    have to *Download Key File*.
  prefs: []
  type: TYPE_NORMAL
- en: We'll put the keys as environment variables that will be used by the **AWS Command
    Line Interface** (**AWS CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Please replace `[...]` with your keys before executing the commands that follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We'll need to install AWS **Command Line Interface** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    and gather info about your account.
  prefs: []
  type: TYPE_NORMAL
- en: If you haven't already, please open the Installing the AWS Command Line Interface
    ([https://docs.aws.amazon.com/cli/latest/userguide/installing.html](https://docs.aws.amazon.com/cli/latest/userguide/installing.html))
    page, and follow the installation method best suited for your OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note to Windows users: I found the most convenient way to get AWS CLI installed
    on Windows is to use Chocolatey ([https://chocolatey.org/](https://chocolatey.org/)).
    Download and install Chocolatey, then run `choco install awscli` from an Administrator
    Command Prompt. Later on in the chapter, Chocolatey will be used to install jq.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you're done, we'll confirm that the installation was successful by outputting
    the version.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note to Windows users: You might need to reopen your *GitBash* terminal for
    the changes to the environment variable `PATH` to take effect.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output (from my laptop) is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Amazon EC2 is hosted in multiple locations worldwide. These locations are composed
    of regions and availability zones. Each region is a separate geographic area composed
    of multiple isolated locations known as availability zones. Amazon EC2 provides
    you the ability to place resources, such as instances, and data in multiple locations.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll define the environment variable `AWS_DEFAULT_REGION` that will tell
    AWS CLI which region we'd like to use by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For now, please note that you can change the value of the variable to any other
    region, as long as it has at least three availability zones. We'll discuss the
    reasons for using `us-east-2` region and the need for multiple availability zones
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll create a few **Identity and Access Management** (**IAM**) resources.
    Even though we could create a cluster with the user you used to register to AWS,
    it is a good practice to create a separate account that contains only the privileges
    we''ll need for the exercises that follow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll create an IAM group called `kops`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We don't care much for any of the information from the output except that it
    does not contain an error message thus confirming that the group was created successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll assign a few policies to the group thus providing the future users
    of the group with sufficient permissions to create the objects we'll need.
  prefs: []
  type: TYPE_NORMAL
- en: Since our cluster will consist of EC2 ([https://aws.amazon.com/ec2/](https://aws.amazon.com/ec2/))
    instances, the group will need to have the permissions to create and manage them.
    We'll need a place to store the state of the cluster so we'll need access to S3
    ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)). Furthermore, we need
    to add VPCs ([https://aws.amazon.com/vpc/](https://aws.amazon.com/vpc/)) to the
    mix so that our cluster is isolated from prying eyes. Finally, we'll need to be
    able to create additional IAMs.
  prefs: []
  type: TYPE_NORMAL
- en: In AWS, user permissions are granted by creating policies. We'll need *AmazonEC2FullAccess*,
    *AmazonS3FullAccess*, *AmazonVPCFullAccess*, and *IAMFullAccess*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The commands that attach the required policies to the `kops` group are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a group with the sufficient permissions, we should create a
    user as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Just as when we created the group, the contents of the output are not important,
    except as a confirmation that the command was executed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'The user we created does not yet belong to the `kops` group. We''ll fix that
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we'll need access keys for the newly created user. Without them, we
    would not be able to act on its behalf.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We created access keys and stored the output in the `kops-creds` file. Let's
    take a quick look at its content.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Please note that I removed the values of the keys. I do not yet trust you enough
    with the keys of my AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: We need the `SecretAccessKey` and `AccessKeyId` entries. So, the next step is
    to parse the content of the `kops-creds` file and store those two values as the
    environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.
  prefs: []
  type: TYPE_NORMAL
- en: In the spirit of full automation, we'll use `jq` ([https://stedolan.github.io/jq/](https://stedolan.github.io/jq/))
    to parse the contents of the `kops-creds` file. Please download and install the
    distribution suited for your OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note to Windows users: Using Chocolatey, install `jq` from an Administrator
    Command Prompt via `choco install jq`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We used `cat` to output contents of the file and combined it with `jq` to filter
    the input so that only the field we need is retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, all the AWS CLI commands will not be executed by the administrative
    user you used to register to AWS, but as `kops`.
  prefs: []
  type: TYPE_NORMAL
- en: It is imperative that the `kops-creds` file is secured and not accessible to
    anyone but people you trust. The best method to secure it depends from one organization
    to another. No matter what you do, do not write it on a post-it and stick it to
    your monitor. Storing it in one of your GitHub repositories is even worse.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we should decide which availability zones we'll use. So, let's take a
    look at what's available in the `us-east-2` region.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the region has three availability zones. We'll store them in
    an environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'A note to Windows users: Please use `tr ''\r\n'' '', ''` instead of `tr ''\n''
    '',''` in the command that follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Just as with the access keys, we used `jq` to limit the results only to the
    zone names, and we combined that with `tr` that replaced new lines with commas.
    The second command removes the trailing comma.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the last command that echoed the values of the environment variable
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We'll discuss the reasons behind the usage of three availability zones later
    on. For now, just remember that they are stored in the environment variable `ZONES`.
  prefs: []
  type: TYPE_NORMAL
- en: The last preparation step is to create SSH keys required for the setup. Since
    we might create some other artifacts during the process, we'll create a directory
    dedicated to the creation of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'SSH keys can be created through the `aws ec2` command `create-key-pair`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We created a new key pair, filtered the output so that only the `KeyMaterial`
    is returned, and stored it in the `devops23.pem` file.
  prefs: []
  type: TYPE_NORMAL
- en: For security reasons, we should change the permissions of the `devops23.pem`
    file so that only the current user can read it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we'll need only the public segment of the newly generated SSH key,
    so we'll use `ssh-keygen` to extract it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: All those steps might look a bit daunting if this is your first contact with
    AWS. Nevertheless, they are pretty standard. No matter what you do in AWS, you'd
    need to perform, more or less, the same actions. Not all of them are mandatory,
    but they are good practice. Having a dedicated (non-admin) user and a group with
    only required policies is always a good idea. Access keys are necessary for any
    `aws` command. Without SSH keys, no one can enter into a server.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that we're finished with the prerequisites, and we can turn
    our attention towards creating a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a kubernetes cluster in AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start by deciding the name of our soon to be created cluster. We'll choose
    to call it `devops23.k8s.local`. The latter part of the name (`.k8s.local`) is
    mandatory if we do not have a DNS at hand. It's a naming convention kops uses
    to decide whether to create a gossip-based cluster or to rely on a publicly available
    domain. If this would be a "real" production cluster, you would probably have
    a DNS for it. However, since I cannot be sure whether you do have one for the
    exercises in this book, we'll play it safe, and proceed with the gossip mode.
  prefs: []
  type: TYPE_NORMAL
- en: We'll store the name into an environment variable so that it is easily accessible.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: When we create the cluster, kops will store its state in a location we're about
    to configure. If you used Terraform, you'll notice that kops uses a very similar
    approach. It uses the state it generates when creating the cluster for all subsequent
    operations. If we want to change any aspect of a cluster, we'll have to change
    the desired state first, and then apply those changes to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, when creating a cluster in AWS, the only option for storing the
    state are `Amazon S3` ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/))
    buckets. We can expect availability of additional stores soon. For now, S3 is
    our only option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that creates an S3 bucket in our region is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We created a bucket with a unique name and the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: For simplicity, we'll define the environment variable `KOPS_STATE_STORE`. Kops
    will use it to know where we store the state. Otherwise, we'd need to use `--store`
    argument with every `kops` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: There's only one thing missing before we create the cluster. We need to install
    kops.
  prefs: []
  type: TYPE_NORMAL
- en: If you are a **MacOS user**, the easiest way to install `kops` is through `Homebrew`
    ([https://brew.sh/](https://brew.sh/)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As an alternative, we can download a release from GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If, on the other hand, you''re a **Linux user**, the commands that will install
    `kops` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Finally, if you are a **Windows user**, you cannot install *kops*. At the time
    of this writing, its releases do not include Windows binaries. Don't worry. I
    am not giving up on you, dear *Windows user*. We'll manage to overcome the problem
    soon by exploiting Docker's ability to run any Linux application. The only requirement
    is that you have Docker for Windows ([https://www.docker.com/docker-windows](https://www.docker.com/docker-windows))
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: I already created a Docker image that contains `kops` and its dependencies.
    So, we'll create an alias `kops` that will create a container instead running
    a binary. The result will be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that creates the `kops` alias is as follows. Execute it only if
    you are a **Windows user**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We won't go into details of all the arguments the `docker run` command uses.
    Their usage will become clear when we start using `kops`. Just remember that we
    are passing all the environment variables we might use as well as mounting the
    SSH key and the directory where `kops` will store `kubectl` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: We are, finally, ready to create a cluster. But, before we do that, we'll spend
    a bit of time discussing the requirements we might have. After all, not all clusters
    are created equal, and the choices we are about to make might severely impact
    our ability to accomplish the goals we might have.
  prefs: []
  type: TYPE_NORMAL
- en: The first question we might ask ourselves is whether we want to have high-availability.
    It would be strange if anyone would answer no. Who doesn't want to have a cluster
    that is (almost) always available? Instead, we'll ask ourselves what the things
    that might bring our cluster down are.
  prefs: []
  type: TYPE_NORMAL
- en: When a node is destroyed, Kubernetes will reschedule all the applications that
    were running inside it into the healthy nodes. All we have to do is to make sure
    that, later on, a new server is created and joined the cluster, so that its capacity
    is back to the desired values. We'll discuss later how are new nodes created as
    a reaction to failures of a server. For now, we'll assume that will happen somehow.
  prefs: []
  type: TYPE_NORMAL
- en: Still, there is a catch. Given that new nodes need to join the cluster, if the
    failed server was the only master, there is no cluster to join. All is lost. The
    part is where master servers are. They host the critical components without which
    Kubernetes cannot operate.
  prefs: []
  type: TYPE_NORMAL
- en: So, we need more than one master node. How about two? If one fails, we still
    have the other one. Still, that would not work.
  prefs: []
  type: TYPE_NORMAL
- en: Every piece of information that enters one of the master nodes is propagated
    to the others, and only after the majority agrees, that information is committed.
    If we lose majority (50%+1), masters cannot establish a quorum and cease to operate.
    If one out of two masters is down, we can get only half of the votes, and we would
    lose the ability to establish the quorum. Therefore, we need three masters or
    more. Odd numbers greater than one are "magic" numbers. Given that we won't create
    a big cluster, three should do.
  prefs: []
  type: TYPE_NORMAL
- en: With three masters, we are safe from a failure of any single one of them. Given
    that failed servers will be replaced with new ones, as long as only one master
    fails at the time, we should be fault tolerant and have high availability.
  prefs: []
  type: TYPE_NORMAL
- en: Always set an odd number greater than one for master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The whole idea of having multiple masters does not mean much if an entire data
    center goes down.
  prefs: []
  type: TYPE_NORMAL
- en: Attempts to prevent a data center from failing are commendable. Still, no matter
    how well a data center is designed, there is always a scenario that might cause
    its disruption. So, we need more than one data center. Following the logic behind
    master nodes, we need at least three. But, as with almost anything else, we cannot
    have any three (or more) data centers. If they are too far apart, the latency
    between them might be too high. Since every piece of information is propagated
    to all the masters in a cluster, slow communication between data centers would
    severely impact the cluster as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, we need three data centers that are close enough to provide low
    latency, and yet physically separated, so that failure of one does not impact
    the others. Since we are about to create the cluster in AWS, we'll use **availability
    zones** (**AZs**) which are physically separated data centers with low latency.
  prefs: []
  type: TYPE_NORMAL
- en: Always spread your cluster between at least three data centers which are close
    enough to warrant low latency.
  prefs: []
  type: TYPE_NORMAL
- en: There's more to high-availability to running multiple masters and spreading
    a cluster across multiple availability zones. We'll get back to this subject later.
    For now, we'll continue exploring the other decisions we have to make.
  prefs: []
  type: TYPE_NORMAL
- en: Which networking shall we use? We can choose between *kubenet*, *CNI*, *classic*,
    and *external* networking.
  prefs: []
  type: TYPE_NORMAL
- en: The classic Kubernetes native networking is deprecated in favor of kubenet,
    so we can discard it right away.
  prefs: []
  type: TYPE_NORMAL
- en: The external networking is used in some custom implementations and for particular
    use cases, so we'll discard that one as well.
  prefs: []
  type: TYPE_NORMAL
- en: That leaves us with kubenet and CNI.
  prefs: []
  type: TYPE_NORMAL
- en: '**Container Network Interface** (**CNI**) allows us to plug in a third-party
    networking driver. Kops supports Calico ([https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/](https://docs.projectcalico.org/v2.0/getting-started/kubernetes/installation/hosted/)),
    flannel ([https://github.com/coreos/flannel](https://github.com/coreos/flannel)),
    Canal (Flannel + Calico) ([https://github.com/projectcalico/canal](https://github.com/projectcalico/canal)),
    kopeio-vxlan ([https://github.com/kopeio/networking](https://github.com/kopeio/networking)),
    kube-router ([https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer](https://github.com/kubernetes/kops/blob/master/docs/networking.md#kube-router-example-for-cni-ipvs-based-service-proxy-and-network-policy-enforcer)),
    romana ([https://github.com/romana/romana](https://github.com/romana/romana)),
    weave ([https://github.com/weaveworks-experiments/weave-kube](https://github.com/weaveworks-experiments/weave-kube)),
    and `amazon-vpc-routed-eni` ([https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend](https://github.com/kubernetes/kops/blob/master/docs/networking.md#amazon-vpc-backend))
    networks. Each of those networks comes with pros and cons and differs in its implementation
    and primary objectives. Choosing between them would require a detailed analysis
    of each. We''ll leave a comparison of all those for some other time and place.
    Instead, we''ll focus on `kubenet`.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubenet is kops' default networking solution. It is Kubernetes native networking,
    and it is considered battle tested and very reliable. However, it comes with a
    limitation. On AWS, routes for each node are configured in AWS VPC routing tables.
    Since those tables cannot have more than fifty entries, kubenet can be used in
    clusters with up to fifty nodes. If you're planning to have a cluster bigger than
    that, you'll have to switch to one of the previously mentioned CNIs.
  prefs: []
  type: TYPE_NORMAL
- en: Use kubenet networking if your cluster is smaller than fifty nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that using any of the networking solutions is easy. All we
    have to do is specify the `--networking` argument followed with the name of the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we won't have the time and space to evaluate all the CNIs, we'll
    use kubenet as the networking solution for the cluster we're about to create.
    I encourage you to explore the other options on your own (or wait until I write
    a post or a new book).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are left with only one more choice we need to make. What will be
    the size of our nodes? Since we won't run many applications, `t2.small` should
    be more than enough and will keep AWS costs to a minimum. `t2.micro` is too small,
    so we elected the second smallest among those AWS offers.
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that we did not mention persistent volumes. We'll explore
    them in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that creates a cluster using the specifications we discussed is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We specified that the cluster should have three masters and one worker node.
    Remember, we can always increase the number of workers, so there's no need to
    start with more than what we need at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: The sizes of both worker nodes and masters are set to `t2.small`. Both types
    of nodes will be spread across the three availability zones we specified through
    the environment variable `ZONES`. Further on, we defined the public key and the
    type of networking.
  prefs: []
  type: TYPE_NORMAL
- en: We used `--kubernetes-version` to specify that we prefer to run version `v1.8.4`.
    Otherwise, we'd get a cluster with the latest version considered stable by kops.
    Even though running latest stable version is probably a good idea, we'll need
    to be a few versions behind to demonstrate some of the features kops has to offer.
  prefs: []
  type: TYPE_NORMAL
- en: By default, kops sets `authorization` to `AlwaysAllow`. Since this is a simulation
    of a production-ready cluster, we changed it to `RBAC`, which we already explored
    in one of the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The `--yes` argument specifies that the cluster should be created right away.
    Without it, `kops` would only update the state in the S3 bucket, and we'd need
    to execute `kops apply` to create the cluster. Such two-step approach is preferable,
    but I got impatient and would like to see the cluster in all its glory as soon
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the `kubectl` context was changed to point to the new cluster
    which is starting, and will be ready soon. Further down are a few suggestions
    of the next actions. We'll skip them, for now.
  prefs: []
  type: TYPE_NORMAL
- en: A note to Windows users
  prefs: []
  type: TYPE_NORMAL
- en: Kops was executed inside a container. It changed the context inside the container
    that is now gone. As a result, your local `kubectl` context was left intact. We'll
    fix that by executing `kops export kubecfg --name ${NAME}` and `export KUBECONFIG=$PWD/config/kubecfg.yaml`.
    The first command exported the config to `/config/kubecfg.yaml`. That path was
    specified through the environment variable `KUBECONFIG` and is mounted as `config/kubecfg.yaml`
    on local hard disk. The latter command exports `KUBECONFIG` locally. Through that
    variable, kubectl is now instructed to use the configuration in `config/kubecfg.yaml`
    instead of the default one. Before you run those commands, please give AWS a few
    minutes to create all the EC2 instances and for them to join the cluster. After
    waiting and executing those commands, you'll be all set.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use kops to retrieve the information about the newly created cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This information does not tell us anything new. We already knew the name of
    the cluster and the zones it runs in.
  prefs: []
  type: TYPE_NORMAL
- en: How about `kubectl cluster-info`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the master is running as well as KubeDNS. The cluster is probably
    ready. If in your case KubeDNS did not appear in the output, you might need to
    wait for a few more minutes.
  prefs: []
  type: TYPE_NORMAL
- en: We can get more reliable information about the readiness of our new cluster
    through the `kops validate` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: That is useful. We can see that the cluster uses four instance groups or, to
    use AWS terms, four **auto-scaling groups** (**ASGs**). There's one for each master,
    and there's one for all the (worker) nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The reason each master has a separate ASG lies in need to ensure that each is
    running in its own **availability zone** (**AZ**). That way we can guarantee that
    failure of the whole AZ will affect only one master. Nodes (workers), on the other
    hand, are not restricted to any specific AZ. AWS is free to schedule nodes in
    any AZ that is available.
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss ASGs in more detail later on.
  prefs: []
  type: TYPE_NORMAL
- en: Further down the output, we can see that there are four servers, three with
    masters, and one with worker node. All are ready.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we got the confirmation that our `cluster devops23.k8s.local is ready`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the information we got so far, we can describe the cluster through the
    *figure 14-1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41026644-f0e5-4485-94ef-86b733e7c2e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-1: The servers that form the Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: There's apparently much more to the cluster than what is depicted in the *figure
    14-1*. So, let's try to discover the goodies kops created for us.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the components that constitute the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When kops created the VMs (EC2 instances), the first thing it did was to execute
    *nodeup*. It, in turn, installed a few packages. It made sure that Docker, Kubelet,
    and Protokube are up and running.
  prefs: []
  type: TYPE_NORMAL
- en: '**Docker** runs containers. It would be hard for me to imagine that you don''t
    know what Docker does, so we''ll skip to the next in line.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubelet** is Kubernetes'' node agent. It runs on every node of a cluster,
    and its primary purpose is to run Pods. Or, to be more precise, it ensures that
    the containers described in PodSpecs are running as long as they are healthy.
    It primarily gets the information about the Pods it should run through Kubernetes''
    API server. As an alternative, it can get the info through files, HTTP endpoints,
    and HTTP servers.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Docker and Kubelet, **Protokube** is specific to kops. Its primary responsibilities
    are to discover master disks, to mount them, and to create manifests. Some of
    those manifests are used by Kubelet to create system-level Pods and to make sure
    that they are always running.
  prefs: []
  type: TYPE_NORMAL
- en: Besides starting the containers defined through Pods in the manifests (created
    by Protokube), Kubelet also tries to contact the API server which, eventually,
    is also started by it. Once the connection is established, Kubelet registers the
    node where it is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'All three packages are running on all the nodes, no matter whether they are
    masters or workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/895f0fd4-538c-4178-886e-beefe7b0362b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-2: The servers that form the Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the system-level Pods currently running in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, quite a few core components are running.
  prefs: []
  type: TYPE_NORMAL
- en: We can divide core (or system-level) components into two groups. Master components
    run only on masters. In our case, they are `kube-apiserver`, `kube-controller-manager`,
    `kube-scheduler`, `etcd`, and `dns-controller`. Node components run on all the
    nodes, both masters and workers. We already discussed a few of those. In addition
    to Protokube, Docker, and Kubelet, we got `kube-proxy`, as one more node component.
    Since this might be the first time you heard about those core components, we'll
    briefly explain each of their functions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes API Server** (`kube-apiserver`) accepts requests to create, update,
    or remove Kubernetes resources. It listens on ports `8080` and `443`. The former
    is insecure and is only reachable from the same server. Through it, the other
    components can register themselves without requiring a token. The former port
    (`443`) is used for all external communications with the API Server. That communication
    can be user-facing like, for example, when we send a `kubectl` command. Kubelet
    also uses `443` port to reach the API server and register itself as a node.'
  prefs: []
  type: TYPE_NORMAL
- en: No matter who initiates communication with the API Server, its purpose is to
    validate and configure API object. Among others, those can be Pods, Services,
    ReplicaSets, and others. Its usage is not limited to user-facing interactions.
    All the components in the cluster interact with the API Server for the operations
    that require a cluster-wide shared state.
  prefs: []
  type: TYPE_NORMAL
- en: The shared state of the cluster is stored in `etcd` ([https://github.com/coreos/etcd](https://github.com/coreos/etcd)).
    It is a key/value store where all cluster data is kept, and it is highly available
    through consistent data replication. It is split into two Pods, where `etcd-server`
    holds the state of the cluster and `etcd-server-events` stores the events.
  prefs: []
  type: TYPE_NORMAL
- en: Kops creates an **EBS volume** for each `etcd` instance. It serves as its storage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes Controller Manager** (`kube-controller-manager`) is in charge
    of running controllers. You already saw a few controllers in action like `ReplicaSets`
    and `Deployments`. Apart from object controllers like those, `kube-controller-manager`
    is also in charge of Node Controllers responsible for monitoring servers and responding
    when one becomes unavailable.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes Scheduler** (`kube-scheduler`) watches the API Server for new
    Pods and assigns them to a node. From there on, those Pods are run by Kubelet
    on the allocated node.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DNS Controller** (`dns-controller`) allows nodes and users to discover the
    API Server.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes Proxy** (`kube-proxy`) reflects Services defined through the API
    Server. It is in charge of TCP and UDP forwarding. It runs on all nodes of the
    cluster (both masters and workers).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b27529b-a97b-4ebd-b2a1-52e9976c3280.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-3: The core components of the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: There's much more going on in our new cluster. For now, we explored only the
    major components.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we'll try to update our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how much we plan, we will never manage to have a cluster with capacity
    that should serve us equally well today as tomorrow. Things change, and we'll
    need to be able to adapt to those changes. Ideally, our cluster should increase
    and decrease its capacity automatically by evaluating metrics and firing alerts
    that would interact with kops or directly with AWS. However, that is an advanced
    topic that we won't be able to cover. For now, we'll limit the scope to manual
    cluster updates.
  prefs: []
  type: TYPE_NORMAL
- en: With kops, we cannot update the cluster directly. Instead, we edit the desired
    state of the cluster stored, in our case, in the S3 bucket. Once the state is
    changed, kops will make the necessary changes to comply with the new desire.
  prefs: []
  type: TYPE_NORMAL
- en: We'll try to update the cluster so that the number of worker nodes is increased
    from one to two. In other words, we want to add one more server to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the sub-commands provided through `kops edit`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the available commands, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We have three types of edits we can make. We did not set up federation, so that
    one is out of the game. You might think that `cluster` would provide the possibility
    to create a new worker node. However, that is not the case. If you execute `kops
    edit cluster --name $NAME`, you'll see that nothing in the configuration indicates
    how many nodes we should have. That is normal considering that we should not create
    servers in AWS directly. Just as Kubernetes, AWS also prefers declarative approach
    over imperative. At least, when dealing with EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of sending an imperative instruction to create a new node, we'll change
    the value of the **Auto-Scaling Group** (**ASG**) related to worker nodes. Once
    we change ASG values, AWS will make sure that it complies with the new desire.
    It'll not only create a new server to comply with the new ASG sizes, but it will
    also monitor EC2 instances and maintain the desired number in case one of them
    fails.
  prefs: []
  type: TYPE_NORMAL
- en: So, we'll choose the third `kops edit` option.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We executed `kops edit ig` command, where `ig` is one of the aliases of `instancegroup`.
    We specified the name of the cluster with the `--name` argument. Finally, we set
    the type of the servers to `nodes`. As a result, we are presented with the `InstanceGroup`
    config for the Auto-Scaling Group associated with worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Bear in mind that what you're seeing on the screen is not the standard output
    (`stdout`). Instead, the configuration is opened in your default editor. In my
    case, that is `vi`.
  prefs: []
  type: TYPE_NORMAL
- en: We can see some useful information from this config. For example, the `image`
    used to create EC2 instances is based on Debian. It is custom made for kops. The
    `machineType` represents EC2 size which is set to `t2.small`. Further down, you
    can see that we're running the VMs in three subnets or, since we're in AWS, three
    availability zones.
  prefs: []
  type: TYPE_NORMAL
- en: The parts of the config we care about are the `spec` entries `maxSize` and `minSize`.
    Both are set to `1` since that is the number of worker nodes we specified when
    we created the cluster. Please change the values of those two entries to `2`,
    save, and exit.
  prefs: []
  type: TYPE_NORMAL
- en: If you're using `vi` as your default editor, you'll need to press *I* to enter
    into the `insert` mode. From there on, you can change the values. Once you're
    finished editing, please press the *ESC* key, followed by `:wq`. Colon (`:`) allows
    us to enter commands, `w` is translated to save, and `q` to quit. Don't forget
    to press the enter key. If, on the other hand, you are not using `vi`, you're
    on your own. I'm sure that you'll know how to operate your default editor. If
    not, Google is your friend.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ce16a02-cba2-4136-b00d-e4282ebd6cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-4: The process behind the `kops edit` command'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we changed the configuration, we need to tell kops that we want it
    to update the cluster to comply with the new desired state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the last few lines, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We can see that kops set our `kubectl` context to the cluster we updated. There
    was no need for that since that was already our context, but it did that anyway.
    Further on, we got the confirmation that the changes `have been applied to the
    cloud`.
  prefs: []
  type: TYPE_NORMAL
- en: The last sentence is interesting. It informed us that we can use `kops rolling-update`.
    The `kops update` command applies all the changes to the cluster at once. That
    can result in downtime. For example, if we wanted to change the image to a newer
    version, running `kops update` would recreate all the worker nodes at once. As
    a result, we'd have downtime from the moment instances are shut down until the
    new ones are created, and Kubernetes schedules the Pods in them. Kops knows that
    such an action should not be allowed so, if the update requires that servers are
    replaced, it does nothing expecting that you'll execute `kops rolling-update`
    afterward. That is not our case. Adding new nodes does not require restarts or
    replacement of the existing servers.
  prefs: []
  type: TYPE_NORMAL
- en: The `kops rolling-update` command intends to apply the changes without downtime.
    It would apply them to one server at the time so that most of the servers are
    always running. In parallel, Kubernetes would be rescheduling the Pods that were
    running on the servers that were brought down.
  prefs: []
  type: TYPE_NORMAL
- en: As long as our applications are scaled, `kops rolling-update` should not produce
    downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what happened when we executed the `kops update` command.
  prefs: []
  type: TYPE_NORMAL
- en: Kops retrieved the desired state from the S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kops sent requests to AWS API to change the values of the workers ASG.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AWS modified the values of the workers ASG by increasing them by 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ASG created a new EC2 instance to comply with the new sizing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Protokube installed Kubelet and Docker and created the manifest file with the
    list of Pods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubelet read the manifest file and run the container that forms the `kube-proxy`
    Pod (the only Pod on the worker nodes).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kubelet sent a request to the `kube-apiserver` (through the `dns-controller`)
    to register the new node and join it to the cluster. The information about the
    new node is stored in `etcd`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is almost identical to the one used to create the nodes of the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a24d8159-561d-4817-b0ea-89d5683c7081.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-5: The process behind the `kops update` command'
  prefs: []
  type: TYPE_NORMAL
- en: Unless you are a very slow reader, ASG created a new EC2 instance, and Kubelet
    joined it to the cluster. We can confirm that through the `kops validate` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We can see that now we have two nodes (there was one before) and that they are
    located somewhere inside the three `us-east-2` availability zones.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can use `kubectl` to confirm that Kubernetes indeed added the
    new worker node to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: That was easy, wasn't it? From now on, we can effortlessly add or remove nodes.
  prefs: []
  type: TYPE_NORMAL
- en: How about upgrading?
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading the cluster manually
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process to upgrade the cluster depends on what we want to do.
  prefs: []
  type: TYPE_NORMAL
- en: If we'd like to upgrade it to a specific Kubernetes version, we can execute
    a similar process like the one we used to add a new worker node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Just as before, we are about to edit the cluster definition. The only difference
    is that this time we're not editing a specific instance group, but the cluster
    as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: If you explore the YAML file in front of you, you'll see that it contains the
    information we specified when we created the cluster, combined with the kops default
    values that we omitted to set.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we're interested in `kubernetesVersion`. Please find it and change
    the version from `v1.8.4` to `v1.8.5`. Save and exit.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we modified the desired state of the cluster, we can proceed with `kops
    update`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The last line of the output indicates that we *must specify* `--yes` *to apply
    changes*. Unlike the previous time we executed `kops update`, now we did not specify
    the argument `--yes`. As a result, we got a preview, or a dry-run, of what would
    happen if we apply the change. Previously, we added a new worker node, and that
    operation did not affect the existing servers. We were brave enough to update
    the cluster without previewing which resources will be created, updated, or destroyed.
    However, this time we are upgrading the servers in the cluster. Existing nodes
    will be replaced with new ones, and that is potentially dangerous operation. Later
    on, we might trust kops to do what's right and skip the preview altogether. But,
    for now, we should evaluate what will happen if we proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Please go through the output. You'll see a git-like diff of the changes that
    will be applied to some of the resources that form the cluster. Take your time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are confident with the changes, we can apply them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of the output states that `changes may require instances to restart:
    kops rolling-update cluster`. We already saw that message before but, this time,
    the update was not performed. The reason is simple, even though not necessarily
    very intuitive. We can update auto-scaling groups since that results in creation
    or destruction of nodes. But, when we need to replace them, as in this case, it
    would be disastrous to execute a simple update. Updating everything at once would,
    at best, produce a downtime. In our case, it''s even worse. Destroying all the
    masters at once would likely result in a loss of quorum. A new cluster might not
    be able to recuperate.'
  prefs: []
  type: TYPE_NORMAL
- en: All in all, kops requires an extra step when "big bang" updating of the cluster
    might result in undesirable results. So, we need to execute the `kops rolling-update`
    command. Since we're still insecure, we'll run a preview first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We can see that all the nodes require an update. Since we already evaluated
    the changes through the output of the `kops update` command, we'll proceed and
    apply rolling updates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The rolling update process started, and it will take approximately 30 minutes
    to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll explore the output as it comes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The output starts with the same information we got when we asked for a preview,
    so there''s not much to comment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Instead of destroying the first node, kops picked one masters and drained it.
    That way, the applications running on it can shut down gracefully. We can see
    that it drained `etcd-server-events`, `etcd-server-ip`, `kube-apiserver`, `kube-controller-manager`,
    `kube-proxy`, `kube-scheduler` Pods running on the server `ip-172-20-40-167`.
    As a result, Kubernetes rescheduled them to one of the healthy nodes. That might
    not be true for all the Pods but only for those that can be rescheduled.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We can see that after draining finished, the master node was stopped. Since
    each master is associated with an auto-scaling group, AWS will detect that the
    node is no more, and start a new one. Once the new server is initialized, `nodeup`
    will execute and install Docker, Kubelet, and Protokube. The latter will create
    the manifest that will be used by Kubelet to run the Pods required for a master
    node. Kubelet will also register the new node with one of the healthy masters.
  prefs: []
  type: TYPE_NORMAL
- en: That part of the process is the same as the one executed when creating a new
    cluster or when adding new servers. It is the part that takes longest to complete
    (around five minutes).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, after waiting for everything to settle, kops validated the
    cluster, thus confirming that upgrade of the first master node finished successfully.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf42f01c-d9a5-4b8a-8e49-38cee4dd01d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-6: Rolling upgrade of one of the master nodes'
  prefs: []
  type: TYPE_NORMAL
- en: As soon as it validated the upgrade of the first master, kops proceeded with
    the next node. During next ten to fifteen minutes, the same process will be repeated
    with the other two masters. Once all three are upgraded, kops will execute the
    same process with the worker nodes, and we'll have to wait for another ten to
    fifteen minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Finally, once all the servers were upgraded, we can see that rolling update
    was completed.
  prefs: []
  type: TYPE_NORMAL
- en: The experience was positive but long. Auto-scaling groups need a bit of time
    to detect that a server is down. It takes a minute or two for a new VM to be created
    and initialized. Docker, Kubelet, and Protokube need to be installed. Containers
    that form core Pods need to be pulled. All in all, quite a few things need to
    happen.
  prefs: []
  type: TYPE_NORMAL
- en: The upgrade process would be faster if kops would use immutable approach and
    bake everything into images (AMIs). However, the choice was made to decouple OS
    with packages and core Pods, so the installation needs to be done at runtime.
    Also, the default distribution is Debian, which is not as light as, let's say,
    CoreOS. Due to those, and a few other design choices, the process is somehow lengthy.
    When combined with inevitable time AWS needs to do its part of the process, we're
    looking at over five minutes of upgrade duration for each node in a cluster. Even
    with only five nodes, the whole process is around thirty minutes. If we'd have
    a bigger cluster, it could take hours, or even days to upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: Even though it takes considerable time to upgrade, the process is hands-free.
    If we are brave enough, we can let kops do its job and spend our time working
    on something more exciting. Assuming that our applications are designed to be
    scalable and fault-tolerant, we won't experience downtime. That is what matters
    much more than whether we'll be able to watch the process unfold. If we trust
    the system, we can just as well run it in the background and ignore it. However,
    earning trust is hard. We need to successfully run the process a few times before
    we put our fate in it. Even then, we should build a robust monitoring and alerting
    system that will notify us if things go wrong. Unfortunately, we won't cover those
    subjects in this book. You'll have to wait for the next one or explore it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to our cluster and verify that Kubernetes was indeed upgraded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Judging by versions of each of the nodes, all were upgraded to `v1.8.5`. The
    process worked.
  prefs: []
  type: TYPE_NORMAL
- en: Try to upgrade often. As a rule of thumb, you should upgrade one minor release
    at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you are a couple of minor releases behind the stable kops-recommended
    release, it's better if you execute multiple rolling upgrades (one for each minor
    release) than to jump to the latest at once. By upgrading to the next minor release,
    you'll minimize potential problems and simplify rollback if required.
  prefs: []
  type: TYPE_NORMAL
- en: Even though kops is fairly reliable, you should not trust it blindly. It's relatively
    easy to create a small testing cluster running the same release as production,
    execute the upgrade process, and validate that everything works as expected. Once
    finished, you can destroy the test cluster and avoid unnecessary expenses.
  prefs: []
  type: TYPE_NORMAL
- en: Don't trust anyone. Test upgrades in a separate cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading the cluster automatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We edited cluster's desired state before we started the rolling update process.
    While that worked well, we're likely to always upgrade to the latest stable version.
    In those cases, we can execute the `kops upgrade` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that this time we skipped the preview by setting the `--yes` argument.
    The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the current Kubernetes version is `v1.8.5` and, in case we choose
    to proceed, it will be upgraded to the latest which, at the time of this writing,
    is `v1.8.6`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as before, we can see from the last entry that `changes may require instances
    to restart: kops rolling-update cluster`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: I'll skip commenting on the output since it is the same as the previous time
    we upgraded the cluster. The only significant difference, from the process perspective,
    is that we did not edit cluster's desired state by specifying the version we want,
    but initiated the process through the `kops upgrade` command. Everything else
    was the same in both cases.
  prefs: []
  type: TYPE_NORMAL
- en: If we are to create a test cluster and write a set of tests that verify the
    upgrade process, we could execute the upgrade process periodically. We could,
    for example, create a job in Jenkins that would upgrade every month. If there
    isn't new Kubernetes release, it would do nothing. If there is, it would create
    a new cluster with the same release as production, upgrade it, validate that everything
    works as expected, destroy the testing cluster, upgrade the production cluster,
    and run another round of test. However, it takes time and experience to get to
    that point. Until then, manually executed upgrades are the way to go.
  prefs: []
  type: TYPE_NORMAL
- en: We are missing one more thing before we can deploy applications to our simulation
    of a production cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need a way to access the cluster. So far, we saw that we can, at least, interact
    with the Kubernetes API. Every time we executed `kubectl`, it communicated with
    the cluster through the API server. That communication is established through
    AWS Elastic Load Balancer (ELB). Let's take a quick look at it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Judging from the `Listener` section, we can see that only port `443` is opened,
    thus allowing only SSL requests. The three instances belong to managers. We can
    safely assume that this load balancer is used only for the access to Kubernetes
    API. In other words, we are still missing access to worker nodes through which
    we'll be able to communicate with our applications. We'll come back to this issue
    in a moment.
  prefs: []
  type: TYPE_NORMAL
- en: The entry that matters, from user's perspective, is `DNSName`. That is the address
    we need to use if we want to communicate with Kubernetes' API Server. Load Balancer
    is there to ensure that we have a fixed address and that requests will be forwarded
    to one of the healthy masters.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the name of the load balancer is `api-devops23-k8s-local-ivnbim`. It
    is important that you remember that it starts with `api-devops23`. You'll see
    soon why the name matters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm that the `DNSName` is indeed the door to the API by examining
    `kubectl` configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the `devops23.k8s.local` is set to use `amazonaws.com` subdomain
    as the server address and that it is the current context. That is the DNS of the
    ELB.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df4178fe-c7a7-4479-927a-9e9f757388d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-7: Load balancer behind Kubernetes API Server'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we can access the API does not get us much closer to having a
    way to access applications we are soon to deploy. We already learned that we can
    use Ingress to channel requests to a set of ports (usually `80` and `443`). However,
    even if we deploy Ingress, we still need an entry point to the worker nodes. We
    need another load balancer sitting above the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, kops has a solution.
  prefs: []
  type: TYPE_NORMAL
- en: We can use kops' add-ons to deploy additional core services. You can get the
    list of those currently available by exploring directories in [https://github.com/kubernetes/kops/tree/master/addons](https://github.com/kubernetes/kops/tree/master/addons).
    Even though most of them are useful, we'll focus only on the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Add-ons are, in most cases, Kubernetes resources defined in a YAML file. All
    we have to do is pick the addon we want, choose the version we prefer, and execute
    `kubectl create`. We'll create the resources defined in `ingress-nginx` version
    `v1.6.0`.
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into details behind the definition YAML file we are about to use
    to create the resources kops assembled for us. I'll leave that up to you. Instead,
    we'll proceed with `kubectl create`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We can see that quite a few resources were created in the Namespace `kube-ingress`.
    Let's take a look what's inside.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: We can see that it created two deployments, which created two `ReplicaSets`,
    which created Pods. In addition, we got two Services as well. As a result, Ingress
    is running inside our cluster and are a step closer to being able to test it.
    Still, we need to figure out how to access the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: One of the two Services (`ingress-nginx`) is `LoadBalancer`. We did not explore
    that type when we discussed Services.
  prefs: []
  type: TYPE_NORMAL
- en: '`LoadBalancer` Service type exposes the service externally using a cloud provider''s
    load balancer. `NodePort` and `ClusterIP` services, to which the external load
    balancer will route, are automatically created. Ingress is "intelligent" enough
    to know how to create and configure an AWS ELB. All it needed is an annotation
    `service.beta.kubernetes.io/aws-load-balancer-proxy-protocol` (defined in the
    YAML file).'
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice that the `ingress-nginx` Service published port `30107` and mapped
    it to `80`. `30430` was mapped to `443`. This means that, from inside the cluster,
    we should be able to send HTTP requests to `30107` and HTTPS to `30430`. However,
    that is only part of the story. Since the Service is the `LoadBalancer` type,
    we should expect some changes to AWS **Elastic Load Balancers** (**ELBs**) as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Let's check the state of the load balancers in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, limited to the relevant parts, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We can observe from the output that a new load balancer was added.
  prefs: []
  type: TYPE_NORMAL
- en: The new load balancer publishes port `80` (HTTP) and maps it to `30107`. This
    port is the same as the `ingress-nginx` Service published. Similarly, the LB published
    port `443` (HTTPS) and mapped it to `30430`. From the `Instances` section, we
    can see that it currently maps to the two worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Further down, we can see the `DNSName`. We should retrieve it but, unfortunately,
    `LoadBalancerName` does not follow any format. However, we do know that now there
    are two load balancers and that the one dedicated to masters has a name that starts
    with `api-devops23`. So, we can retrieve the other LB by specifying that it should
    not contain that prefix. We'll use `jq` instruction `not` for that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The command that retrieves DNS from the new load balancer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We'll come back to the newly created Ingress and the load balancer soon. For
    now, we'll move on and deploy the `go-demo-2` application.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying resources to a Kubernetes cluster running in AWS is no different from
    deployments anywhere else, including Minikube. That's one of the big advantages
    of Kubernetes, or of any other container scheduler. We have a layer of abstraction
    between hosting providers and our applications. As a result, we can deploy (almost)
    any YAML definition to any Kubernetes cluster, no matter where it is. That's huge.
    It gives up a very high level of freedom and allows us to avoid vendor locking.
    Sure, we cannot effortlessly switch from one scheduler to another, meaning that
    we are "locked" into the scheduler we chose. Still, it's better to depend on an
    open source project than on a commercial hosting vendor like AWS, GCE, or Azure.
  prefs: []
  type: TYPE_NORMAL
- en: We need to spend time setting up a Kubernetes cluster, and the steps will differ
    from one hosting provider to another. However, once a cluster is up-and-running,
    we can create any Kubernetes resource (almost) entirely ignoring what's underneath
    it. The result is the same no matter whether our cluster is AWS, GCE, Azure, on-prem,
    or anywhere else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get back to the task at hand and create `go-demo-2` resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We moved back to the repository''s root directory, and created the resources
    defined in `aws/go-demo-2.yml`. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Next, we should wait until `go-demo-2-api` Deployment is rolled out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can validate that the application is running and is accessible
    through the DNS provided by the AWS **Elastic Load Balancer** (**ELB**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: We got response code `200` and the message `hello, world!`. The Kubernetes cluster
    we set up in AWS works!
  prefs: []
  type: TYPE_NORMAL
- en: When we sent the request to the ELB dedicated to workers, it performed round-robin
    and forwarded it to one of the healthy nodes. Once inside the worker, the request
    was picked by the `nginx` Service, forwarded to Ingress, and, from there, to one
    of the containers that form the replicas of the `go-demo-2-api` ReplicaSet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93b6e8a9-7415-46fe-b8c6-3ca283045100.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-8: Load balancer behind Kubernetes worker nodes'
  prefs: []
  type: TYPE_NORMAL
- en: It might be worth pointing out that containers that form our applications are
    always running in worker nodes. Master servers, on the other hand, are entirely
    dedicated to running Kubernetes system. That does not mean that we couldn't create
    a cluster in the way that masters and workers are combined into the same servers,
    just as we did with Minikube. However, that is risky, and we're better off separating
    the two types of nodes. Masters are more reliable when they are running on dedicated
    servers. Kops knows that, and it does not even allow us to mix the two.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring high-availability and fault-tolerance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cluster would not be reliable if it wouldn't be fault tolerant. Kops intents
    to make it so, but we're going to validate that anyways.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s retrieve the list of worker node instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: We used `aws ec2 describe-instances` to retrieve all the instances (five in
    total). The output was sent to `jq`, which filtered them by the security group
    dedicated to worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: We'll terminate one of the worker nodes. To do that, we'll pick a random one,
    and retrieve its ID.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: We used the same command as before and added `tail -n 1`, so that the output
    is limited to a single line (entry). We stored the result in the `INSTANCE_ID`
    variable. Now we know which instance to terminate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the output that the instance is shutting down. We can confirm
    that by listing all the instances from the security group `nodes.devops23.k8s.local`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we are now running only one instance. All that's left is to wait
    for a minute, and repeat the same command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: This time, we can see that there are again two instances. The only difference
    is that this time one of the instance IDs is different.
  prefs: []
  type: TYPE_NORMAL
- en: AWS auto-scaling group discovered that the instances do not match the desired
    number, and it created a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fact that AWS created a node to replace the one we terminated does not
    mean that the new server joined the Kubernetes cluster. Let''s verify that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: If you were fast enough, your output should also show that there is only one
    (worker) `node`. Once AWS created a new server, it takes a bit of time until Docker,
    Kubelet, and Protokube are installed, containers are pulled and run, and the node
    is registered through one of the masters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try it again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: This time, the number of (worker) nodes is back to two. Our cluster is back
    in the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: What we just experienced is, basically, the same as when we executed the rolling
    upgrade. The only difference is that we terminated an instance as a way to simulate
    a failure. During the upgrade process, kops does the same. It shuts down one instance
    at a time and waits until the cluster goes back to the desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to do a similar test with master nodes. The only difference is that
    you'll have to use `masters` instead of `nodes` as the prefix of the security
    group name. Since everything else is the same, I trust you won't need instructions
    and explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Giving others access to the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unless you''re planning to be the only person in your organization with the
    access to the cluster, you''ll need to create a `kubectl` configuration that you
    can distribute to your coworkers. Let''s see the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'We went back to the `cluster` directory, created the sub-directory `config`,
    and exported `KUBECONFIG` variable with the path to the file where we''d like
    to store the configuration. Now we can execute `kops export`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the latter command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: Now you can pass that configuration to one of your coworkers, and he'll have
    the same access as you.
  prefs: []
  type: TYPE_NORMAL
- en: Truth be told, you should create a new user and a password or, even better,
    an SSH key and let each user in your organization access the cluster with their
    own authentication. You should also create RBAC permissions for each user or a
    group of users. We won't go into the steps how to do that since they are already
    explained in the [Chapter 12](36d9d538-dc85-4366-bd95-72076b27cb28.xhtml), *Securing
    Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: Destroying the cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter is almost finished, and we do not need the cluster anymore. We want
    to destroy it as soon as possible. There's no good reason to keep it running when
    we're not using it. But, before we proceed with the destructive actions, we'll
    create a file that will hold all the environment variables we used in this chapter.
    That will help us the next time we want to recreate the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'We echoed the variables with the values into the `kops` file, and now we can
    delete the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Kops removed references of the cluster from our `kubectl` configuration and
    proceeded to delete all the AWS resources it created. Our cluster is no more.
    We can proceed and delete the S3 bucket as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: We will not remove the IAM resources (group, user, access key, and policies).
    It does not cost to keep them in AWS, and we'll save ourselves from re-running
    the commands that create them. However, I will list the commands as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: Do NOT execute following commands. They are only a reference. We'll need those
    resources in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have a production-ready Kubernetes cluster running in AWS. Isn't that something
    worthwhile a celebration?
  prefs: []
  type: TYPE_NORMAL
- en: Kops proved to be relatively easy to use. We executed more `aws` than `kops`
    commands. If we exclude them, the whole cluster can be created with a single `kops`
    command. We can easily add or remove worker nodes. Upgrades are simple and reliable,
    if a bit long. The important part is that through rolling upgrades we can avoid
    downtime.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few `kops` command we did not explore. I feel that now you know
    the important parts and that you will be able to figure out the rest through the
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: You might be inclined to think that you are ready to apply everything you learned
    so far. Do not open that champagne bottle you've been saving for special occasions.
    There's still one significant topic we need to explore. We postponed the discussion
    about stateful services since we did not have the ability to use external drives.
    We did use volumes, but they were all local, and do not qualify as persistent.
    Failure of a single server would prove that. Now that we are running a cluster
    in AWS, we can explore how to deploy stateful applications.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes operations (kops) compared to Docker for AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker for AWS (D4AWS) quickly became the preferable way to create a Docker
    Swarm cluster in AWS (and Azure). Similarly, kops is the most commonly used tool
    to create Kubernetes clusters in AWS. At least, at the time of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: The result, with both tools, is more or less the same. Both create Security
    Groups, VPCs, Auto-Scaling Groups, Elastic Load Balancers, and everything else
    a cluster needs. In both cases, Auto-Scaling Groups are in charge of creating
    EC2 instances. Both rely on external storage to keep the state of the cluster
    (kops in S3 and D4AWS in DynamoDB). In both cases, EC2 instances brought to life
    by Auto-Scaling Groups know how to run system-level services and join the cluster.
    If we exclude the fact that one solution runs Docker Swarm and that the other
    uses Kubernetes, there is no significant functional difference if we observe only
    the result (the cluster). So, we'll focus on user experience instead.
  prefs: []
  type: TYPE_NORMAL
- en: Both tools can be executed from the command line and that's where we can spot
    the first difference.
  prefs: []
  type: TYPE_NORMAL
- en: Docker for AWS relies on CloudFormation templates, so we need to execute `aws
    cloudformation` command. Docker provides a template and we should use parameters
    to customize it. In my humble opinion, the way CloudFormation expects us to pass
    parameters is just silly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: Having to write something like `ParameterKey=ManagerSize,ParameterValue=3` instead
    of `ManagerSize=3` is annoying at best.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample command that creates Kubernetes cluster using `kops` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Isn't that easier and more intuitive?
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, kops is a binary with everything we would expect. We can, for example,
    execute `kops --help` and see the available options and a few examples. If we'd
    like to know which parameters are available with Docker For AWS, we'd need to
    go through the template. That's definitely less intuitive and more difficult than
    running `kops create cluster --help`. Even if we don't mind browsing through the
    Docker For AWS template, we still don't have examples at hand (from the command
    line, not browser). From user experience perspective, kops wins over Docker For
    AWS if we restrict the comparison only to command line interface. Simply put,
    executing a well-defined binary dedicated to managing a cluster is better than
    executing `aws cloudformation` commands with remote templates.
  prefs: []
  type: TYPE_NORMAL
- en: Did Docker make a mistake for choosing CloudFormation? I don't think so. Even
    if command line experience is suboptimal, it is apparent that they wanted to provide
    an experience native to hosting vendor. In our case that's AWS, but the same can
    be said for Azure. If you will always operate cluster from the command line (as
    I think you should), this is where the story ends and kops is a winner with a
    very narrow margin.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we can create Docker For AWS cluster using CloudFormation means
    that we can take advantage of it from AWS Console. That translates into UI experience.
    We can use AWS Console UI to create, update, or delete a cluster. We can see the
    events as they progress, explore the resources that were created, roll back to
    the previous version, and so on. By choosing CloudFormation template, Docker decided
    to provide not only command line but also a visual experience.
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I think that UIs are evil and that we should do everything from
    the command line. That being said, I'm fully aware that not everybody feels the
    same. Even if you do choose never to use UI for "real" work, it is very helpful,
    at least at the beginning, as a learning experience of what can and what cannot
    be done, and how all the steps tie together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc704627-7611-4e26-81cf-b8efabc6ceb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14-9: Docker For AWS UI'
  prefs: []
  type: TYPE_NORMAL
- en: It's a tough call. What matters is that both tools are creating reliable clusters.
    Kops is more user-friendly from the command line, but it has no UI. Docker For
    AWS, on the other hand, works as native AWS solution through CloudFormation. That
    gives it the UI, but at the cost of suboptimal command line experience.
  prefs: []
  type: TYPE_NORMAL
- en: You won't have to choose one over the other since the choice will not depend
    on which one you like more, but whether you want to use Docker Swarm or Kubernetes.
  prefs: []
  type: TYPE_NORMAL
