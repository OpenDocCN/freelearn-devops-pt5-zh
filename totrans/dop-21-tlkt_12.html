<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Creating and Managing a Docker Swarm Cluster in Amazon Web Services</h1>
            </header>

            <article>
                
<div class="packt_quote">In fast moving markets, adaptation is significantly more important than optimization.<br/>
                                                                                                         –Larry Constantine</div>
<p>The time has finally come to set up a Swarm cluster that is closer to what we should have in production. The reason I added the word "closer" lies in a few subjects that we'll explore in later chapters. Later on, once we go through a few hosting providers, we'll work on the missing pieces (example: persistent storage).</p>
<p>For now, we'll limit ourselves to the creation of a production-like cluster and exploration of different tools we can choose from.</p>
<p>Since AWS holds by far the biggest share of the hosting market, it is the natural choice as the first provider we'll explore.</p>
<p>I'm sure that AWS does not need much of an introduction. Even if you haven't used it, I'm sure you are aware of its existence and a general gist.</p>
<p><strong>Amazon Web Services</strong> (<strong>AWS</strong>) were created in 2006 with the offering of IT infrastructure services. The type of services AWS offered later on became commonly known as cloud computing. With the cloud, companies and individuals alike no longer need to plan for and procure servers and other IT infrastructure weeks or months in advance. Instead, they can instantly spin up hundreds or even thousands of servers in minutes.<br/>
I will assume that you already have an AWS account. If that's not the case, please head over to Amazon Web Services and sign-up. Even if you already made up a firm decision to use a different cloud computing provider or in-house servers, I highly recommend going through this chapter. You will be introduced to a few tools you might not have in your tool belt and be able to compare AWS with other solutions.</p>
<p>Before we jump into practical exercises, we'll need to install the AWS CLI, get the access keys and decide the region and the zone where we'll run the cluster.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Installing AWS CLI and setting up the environment variables</h1>
            </header>

            <article>
                
<p>The first thing we should do is get the AWS credentials.</p>
<p>Please open the <em>Amazon EC2 Console </em>(<a href="https://console.aws.amazon.com/ec2/">https://console.aws.amazon.com/ec2/</a>), click on your name from the top-right menu, and select <span class="packt_screen">My Security Credentials</span>. You will see the screen with different types of credentials. Expand the <span class="packt_screen">Access Keys</span> <em>(<span class="packt_screen">Access Key ID</span></em> and <em><span class="packt_screen">Secret Access Key</span>)</em> section and click the <span class="packt_screen">Create New Access Key</span> button. Expand the <span class="packt_screen">Show Access Key</span> section to see the keys.</p>
<p>You will not be able to view the keys later on, so this is the only chance you'll have to Download Key File.</p>
<div class="packt_infobox">All the commands from this chapter are available in the <kbd>11-aws.sh</kbd> (<a href="https://gist.github.com/vfarcic/03931d011324431f211c4523941979f8">https://gist.github.com/vfarcic/03931d011324431f211c4523941979f8</a>) Gist.</div>
<p>We'll place the keys as environment variables that will be used by the tools we'll explore in this chapter:</p>
<pre>
<strong><span class="hljs-keyword">export</span> AWS_ACCESS_KEY_ID=[...]<br/><br/><span class="hljs-keyword">export</span> AWS_SECRET_ACCESS_KEY=[...]</strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual values.</p>
<p>We'll need to install AWS <strong>Command Line Interface</strong> (<strong>CLI</strong>) (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>)and gather info about your account.<br/></p>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
I found the most convienent way to get awscli installed on Windows is to use <em>Chocolatey </em>(<a href="https://chocolatey.org/">https://chocolatey.org/</a>). Download and install Chocolatey, then run <kbd>choco install awscli</kbd> from an Administrator Command Prompt. Later in the chapter, Chocolatey will be used to <kbd>install jq</kbd>, <kbd>packer</kbd>, and terraform.</div>
<p>If you haven't already, please open the <em>Installing the AWS Command Line Interface </em>(<a href="http://docs.aws.amazon.com/cli/latest/userguide/installing.html">http://docs.aws.amazon.com/cli/latest/userguide/installing.html</a>) page and follow the installation method best suited for your OS.</p>
<p>Once you're done, we should confirm that the installation was successful by outputting the version:</p>
<pre>
<strong>aws --version</strong>
</pre>
<p>The output (from my laptop) is as follows:</p>
<pre>
<strong>aws-<span class="hljs-keyword">cli</span>/<span class="hljs-number">1.11</span><span class="hljs-number">.15</span> Python/<span class="hljs-number">2.7</span><span class="hljs-number">.10</span> Darwin/<span class="hljs-number">16.0</span><span class="hljs-number">.0</span> botocore/<span class="hljs-number">1.4</span><span class="hljs-number">.72</span></strong>
</pre>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
You might need to reopen your <em>GitBash</em> terminal for the changes to the environment variable <kbd>path</kbd> to take effect.</div>
<p>Now that the CLI is installed, we can get the region and the availability zones our cluster will run in.</p>
<p>Amazon EC2 is hosted in multiple locations worldwide. These locations are composed of regions and availability zones. Each region is a separate geographic area composed of multiple isolated locations known as availability zones. Amazon EC2 provides you the ability to place resources, such as instances, and data in multiple locations.</p>
<p>You can see the currently available regions from the <em>Available Regions</em> (<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions</a>) section of the <em>Regions and Availability Zones</em> (<a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html</a>) page.</p>
<p>Throughout this chapter, I will be using <kbd>us-east-1</kbd> (US East (N. Virginia)) region. Feel free to change it to the region closest to your location.</p>
<p>Please put the region inside the environment variable <kbd>AWS_DEFAULT_REGION</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">export</span> AWS_DEFAULT_REGION=us-east-<span class="hljs-number">1</span></strong>
</pre>
<p>With the region selected, we can decide which availability zone we'll select to run our cluster.</p>
<p>Each region is completely independent and consists of multiple availability zones. Zones within a region are isolated and connected through low-latency links.</p>
<p>As a general rule, you should place all your nodes of a cluster inside one region and benefit from low-latency links. Those nodes should be distributed across multiple availability zones so that failure of one of them does not imply failure of the whole cluster. If you have a need to operate across multiple regions, the best option is to set up multiple clusters (one for each region). Otherwise, if you’d set up a single cluster that spans multiple regions, you might experience latency problems.</p>
<div class="packt_tip"><strong>A note to first time AWS users</strong><br/>
If this is the first time you are executing <kbd>aws</kbd>, you will receive a message asking you to configure credentials. Please run the <kbd>aws configure</kbd> command and follow the instructions. You will be asked for credentials. Use those we generated earlier. Feel free to answer the rest of the questions with the enter key.</div>
<p>Let's use the AWS CLI to see the available zones within the selected region:</p>
<pre>
<strong>aws ec2 describe-availability-zones \<br/>    --region <span class="hljs-variable">$AWS_DEFAULT_REGION</span></strong>
</pre>
<p>Since I selected <kbd>us-east-1</kbd> as the region, the output is as follows:</p>
<pre>
<strong>{</strong><br/><strong>    "AvailabilityZones": [</strong><br/><strong>        {</strong><br/><strong>            "State": "available",</strong><br/><strong>            "RegionName": "us-east-1",</strong><br/><strong>            "Messages": [],</strong><br/><strong>            "ZoneName": "us-east-1a"</strong><br/><strong>        },</strong><br/><strong>        {</strong><br/><strong>            "State": "available",</strong><br/><strong>            "RegionName": "us-east-1",</strong><br/><strong>            "Messages": [],</strong><br/><strong>            "ZoneName": "us-east-1b"</strong><br/><strong>        },</strong>
</pre>
<pre>
<strong>        {</strong><br/><strong>            "State": "available",</strong><br/><strong>            "RegionName": "us-east-1",</strong><br/><strong>            "Messages": [],</strong><br/><strong>            "ZoneName": "us-east-1d"</strong><br/><strong>        },<br/></strong><strong>        {</strong><br/><strong>            "State": "available",</strong><br/><strong>            "RegionName": "us-east-1",</strong><br/><strong>            "Messages": [],</strong><br/><strong>            "ZoneName": "us-east-1e"</strong><br/><strong>        }</strong><br/><strong>   ]</strong><br/><strong>}</strong>
</pre>
<p>As you can see, there are four zones (<kbd>a</kbd>, <kbd>b</kbd>, <kbd>d</kbd>, and <kbd>e</kbd>) available for the <kbd>us-east-1</kbd> region. In your case, depending on the selected region, the output might be different.</p>
<p>Please choose the zones and put them inside environment variables, one for each of the five servers that will form our cluster:</p>
<pre>
<strong>AWS_ZONE[<span class="hljs-number">1</span>]=b<br/><br/>AWS_ZONE[<span class="hljs-number">2</span>]=d<br/><br/>AWS_ZONE[<span class="hljs-number">3</span>]=e<br/><br/>AWS_ZONE[<span class="hljs-number">4</span>]=b<br/><br/>AWS_ZONE[<span class="hljs-number">5</span>]=d</strong>
</pre>
<p>Feel free to choose any combination of availability zones. In my case, I made a decision to distribute the cluster across zones <kbd>b</kbd>, <kbd>d</kbd>, and <kbd>e</kbd>.</p>
<p>Now we are all set with the prerequisites that will allow us to create the first Swarm cluster in AWS. Since we used Docker Machine throughout the most of the book, it will be our first choice.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up a Swarm cluster with Docker Machine and AWS CLI</h1>
            </header>

            <article>
                
<p>We'll continue using the <kbd>vfarcic/cloud-provisioning</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning">https://github.com/vfarcic/cloud-provisioning</a>) repository. It contains configurations and scripts that'll help us out. You already have it cloned. To be on the safe side, we'll pull the latest version:</p>
<pre>
<strong><span class="hljs-built_in">cd</span> cloud-provisioning<br/><br/>git pull</strong>
</pre>
<p>Let's create the first EC2 instance:</p>
<pre>
<strong>docker-machine create \<br/>    --driver amazonec2 \<br/>    --amazonec2-zone <span class="hljs-variable">${AWS_ZONE[1]}</span> \<br/>    --amazonec2-tags <span class="hljs-string">"Type,manager"</span> \<br/>    swarm-<span class="hljs-number">1</span></strong>
</pre>
<p>We specified that the <em>Docker Machine</em> should use the <kbd>amazonec2</kbd> driver to create an instance in the zone we defined as the environment variable <kbd>AWS_ZONE_1</kbd>.</p>
<p>We made a tag with the key <kbd>type</kbd> and the value manager. Tag are mostly for informational purposes.</p>
<p>Finally, we specified the name of the instance to be <kbd>swarm-1</kbd>.</p>
<p>The output is as follows:</p>
<pre>
<strong>Running pre<span class="hljs-attribute">-create</span> checks<span class="hljs-attribute">...</span><br/>Creating machine<span class="hljs-attribute">...</span><br/>(swarm<span class="hljs-subst">-</span><span class="hljs-number">1</span>) Launching instance<span class="hljs-attribute">...</span><br/>Waiting for machine <span class="hljs-keyword">to</span> be running, this may <span class="hljs-keyword">take</span> a few minutes<span class="hljs-attribute">...</span><br/>Detecting operating system of created instance<span class="hljs-attribute">...</span><br/>Waiting for SSH <span class="hljs-keyword">to</span> be available<span class="hljs-attribute">...</span><br/>Detecting the provisioner<span class="hljs-attribute">...</span><br/>Provisioning <span class="hljs-keyword">with</span> ubuntu(systemd)<span class="hljs-attribute">...</span><br/>Installing Docker<span class="hljs-attribute">...</span><br/>Copying certs <span class="hljs-keyword">to</span> the <span class="hljs-built_in">local</span> machine directory<span class="hljs-attribute">...</span><br/>Copying certs <span class="hljs-keyword">to</span> the remote machine<span class="hljs-attribute">...</span><br/>Setting Docker configuration <span class="hljs-keyword">on</span> the remote daemon<span class="hljs-attribute">...</span><br/>Checking connection <span class="hljs-keyword">to</span> Docker<span class="hljs-attribute">...</span><br/>Docker is up <span class="hljs-literal">and</span> running<span class="hljs-subst">!</span><br/><span class="hljs-keyword">To</span> see how <span class="hljs-keyword">to</span> connect your Docker Client <span class="hljs-keyword">to</span> the Docker Engine running <span class="hljs-keyword">on</span> this\ virtual machine, run: docker<span class="hljs-attribute">-machine</span> env swarm<span class="hljs-subst">-</span><span class="hljs-number">1</span></strong>
</pre>
<p>Docker machine launched an AWS EC2 instance, provisioned it with Ubuntu, and installed and configured Docker Engine.</p>
<p>Now we can initialize the cluster. We should use private IPs for all communication between nodes. Unfortunately, the command <kbd>docker-machine ip</kbd> returns only the public IP, so we'll have to resort to a different method to get the private IP.<br/>
We can use the <kbd>aws ec2 describe-instances</kbd> command to retrieve all the information about our EC2 instances. We also filter only running instances by adding <kbd>Name=instance-state-name</kbd>, <kbd>Values=running</kbd>. Doing so excludes instances that are being terminated or that are terminated:</p>
<pre>
<strong>aws ec2 describe-instances \<br/>    --filter <span class="hljs-string">"Name=tag:Name,Values=swarm-1" \<br/></span><span class="hljs-string">    "Name=instance-state-name,Values=running"</span></strong>
</pre>
<p>The <kbd>describe-instances</kbd> command lists all the EC2 instances. We combined it with <kbd>--filter</kbd> to limit the output to the instance tagged with the name <kbd>swarm-1</kbd>.</p>
<p>The relevant sample of the output is as follows:</p>
<pre>
<strong>{<br/><span class="hljs-string">"Reservations"</span>: [<br/>  {<br/><span class="hljs-keyword">   ...</span><br/><span class="hljs-string">   "Instances"</span>: [<br/>    {<br/><span class="hljs-keyword">     ...</span><br/><span class="hljs-string">     "PrivateIpAddress"</span>: <span class="hljs-string">"172.31.51.25"</span>,<br/><span class="hljs-keyword">     ...</span></strong>
</pre>
<p>Even though we got all the information related to the <kbd>swarm-1</kbd> EC2 instance, we still need to limit the output to the <kbd>PrivateIpAddress</kbd> value. We'll use <kbd>jq</kbd> (<a href="https://stedolan.github.io/jq/">https://stedolan.github.io/jq/</a>)to filter the output and get what we need. Please download and install the distribution suited for your OS:</p>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
Using Chocolatey, install <kbd>jq</kbd> from an Administrator Command Prompt via <kbd>choco install jq</kbd>.</div>
<pre>
<strong>MANAGER_IP=$(aws ec2 describe-instances \<br/>    --filter <span class="hljs-string">"Name=tag:Name,Values=swarm-1"</span> \<br/><span class="hljs-string">    "Name=instance-state-name,Values=running"</span> \<br/>    | jq -r <span class="hljs-string">".Reservations[0].Instances[0].PrivateIpAddress"</span>)</strong>
</pre>
<p>We used <kbd>jq</kbd> to retrieve the first element of the Reservations array. Within it, we got the first entry of the Instances, followed with the <kbd>PrivateIpAddress</kbd>. The <kbd>-r</kbd> returns the value in its raw format (in this case without double quotes that surround the IP). The result of the command is stored in the environment variable <kbd>MANAGER_IP</kbd>.<br/>
To be on the safe side, we can echo the value of the newly created variable:</p>
<pre>
<strong><span class="hljs-built_in">echo</span> <span class="hljs-variable">$MANAGER_IP</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>172.31.51.25</strong>
</pre>
<p>Now we can execute the <kbd>swarm init</kbd> command in the same way as we did in the previous chapters:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker swarm init \<br/>    --advertise-addr <span class="hljs-variable">$MANAGER_IP</span></strong>
</pre>
<p>Let's confirm that the cluster is indeed initialized:</p>
<pre>
<strong>docker node ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS<br/>swarm-<span class="hljs-number">1</span>   Ready   <span class="hljs-keyword">Active</span>        Leader</strong>
</pre>
<p>Apart from creating an EC2 instance, <kbd>docker-machine</kbd> created a security group as well.</p>
<p>A security group acts as a virtual firewall that controls the traffic. When you launch an instance, you associate one or more security groups with the instance. You add rules to each security group that allows traffic to or from its associated instances.</p>
<p>At the time of this writing, Docker Machine was not yet adapted to support Swarm Mode out of the box. As a result, it created an AWS security group named <kbd>docker-machine</kbd> and opened only the ingress (inbound) ports <kbd>22</kbd> and <kbd>2376</kbd>. Egress (output) is opened for all ports.</p>
<p>For Swarm Mode to function correctly, the ingress ports that should be opened are as follows:</p>
<ul>
<li>TCP port 2377 for cluster management communications</li>
<li>TCP and UDP port 7946 for communication among nodes</li>
<li>TCP and UDP port 4789 for overlay network traffic</li>
</ul>
<p>To modify the security group, we need to get its ID. We can see the info of a security group with the <kbd>aws ec2 describe-security-groups</kbd> command:</p>
<pre>
<strong>aws ec2 describe-security-groups \<br/>    --filter <span class="hljs-string">"Name=group-name,Values=docker-machine"</span></strong>
</pre>
<p>The part of the output is as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/><span class="hljs-string">           "GroupName"</span>: <span class="hljs-string">"docker-machine"</span>,<br/><span class="hljs-string">           "VpcId"</span>: <span class="hljs-string">"vpc-7bbc391c"</span>,<br/><span class="hljs-string">           "OwnerId"</span>: <span class="hljs-string">"036548781187"</span>,<br/><span class="hljs-string">           "GroupId"</span>: <span class="hljs-string">"sg-f57bf388"</span><br/>        }<br/>    ]<br/>}</strong>
</pre>
<p>The command that will assign the ID to the <kbd>SECURITY_GROUP_ID</kbd> environment variable is as follows:</p>
<pre>
<strong>SECURITY_GROUP_ID=$(aws ec2 \<br/>    describe-security-groups \<br/>    --filter \<br/><span class="hljs-string">    "Name=group-name,Values=docker-machine"</span> |\ <br/>    jq -r <span class="hljs-string">'.SecurityGroups[0].GroupId'</span>)</strong>
</pre>
<p>We requested the information about the security group <kbd>docker-machine</kbd> and filtered the JSON output to get the <kbd>GroupId</kbd> key located in the first element of the <kbd>SecurityGroups</kbd> array.</p>
<p>Now we can use the <kbd>aws ec2 authorize-security-group-ingress</kbd> command to open TCP ports <kbd>2377</kbd>, <kbd>7946</kbd>, and <kbd>4789</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-number">2377</span> <span class="hljs-number">7946</span> <span class="hljs-number">4789</span>; <span class="hljs-keyword">do \</span><br/>    aws ec2 authorize-security-group-ingress \<br/>        --group-id <span class="hljs-variable">$SECURITY_GROUP_ID</span> \<br/>        --protocol tcp \<br/>        --port <span class="hljs-variable">$p</span> \<br/>        --source-group <span class="hljs-variable">$SECURITY_GROUP_ID</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>We should execute a similar command to open UDP ports <kbd>7946</kbd> and <kbd>4789</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-number">7946</span> <span class="hljs-number">4789</span>; <span class="hljs-keyword">do \</span><br/>    aws ec2 authorize-security-group-ingress \<br/>        --group-id <span class="hljs-variable">$SECURITY_GROUP_ID</span> \<br/>        --protocol udp \<br/>        --port <span class="hljs-variable">$p</span> \<br/>        --source-group <span class="hljs-variable">$SECURITY_GROUP_ID</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>Please note that, in all the cases, we specified that the <kbd>source-group</kbd> should be the same as the security group. That means that the ports will be opened only to instances that belong to the same group. In other words, those ports will not be available to the public. Since they will be used only for internal communication within the cluster, there is no reason to put our security at risk by exposing those ports further.</p>
<p>Please repeat the <kbd>aws ec2 describe-security-groups</kbd> command to confirm that the ports were indeed opened:</p>
<pre>
<strong>aws ec2 describe-security-groups \<br/>    --filter \<br/><span class="hljs-string">    "Name=group-name,Values=docker-machine"</span></strong>
</pre>
<p>Now we can add more nodes to the cluster. We'll start by creating two new instances and joining them as managers:</p>
<pre>
<strong>MANAGER_TOKEN=$(docker swarm join-token -q manager)<br/><br/><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>; <span class="hljs-keyword">do</span><br/>    docker-machine create \<br/>        --driver amazonec2 \<br/>        --amazonec2-zone <span class="hljs-variable">${AWS_ZONE[$i]}</span> \<br/>        --amazonec2-tags <span class="hljs-string">"Type,manager"</span> \<br/>        swarm-<span class="hljs-variable">$i</span><br/><br/>    IP=$(aws ec2 describe-instances \<br/>        --filter <span class="hljs-string">"Name=tag:Name,Values=swarm-<span class="hljs-variable">$i</span>"</span> \<br/><span class="hljs-string">        "Name=instance-state-name,Values=running"</span> \<br/>        | jq -r <span class="hljs-string">".Reservations[0].Instances[0].PrivateIpAddress"</span>)<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-variable">$i</span>)<br/><br/>    docker swarm join \<br/>        --token <span class="hljs-variable">$MANAGER_TOKEN</span> \<br/>        --advertise-addr <span class="hljs-variable">$IP \<br/></span><span class="hljs-variable">        $MANAGER_IP</span>:<span class="hljs-number">2377</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>There's no need to explain the commands we just executed since they are the combination of those we used before.</p>
<p>We'll add a few worker nodes as well:</p>
<pre>
<strong>WORKER_TOKEN=$(docker swarm join-token -q worker)<br/><br/><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>; <span class="hljs-keyword">do</span><br/>  docker-machine create \<br/>    --driver amazonec2 \<br/>    --amazonec2-zone <span class="hljs-variable">${AWS_ZONE[$i]}</span> \<br/>    --amazonec2-tags <span class="hljs-string">"type,worker"</span> \<br/>    swarm-<span class="hljs-variable">$i</span><br/><br/>  IP=$(aws ec2 describe-instances \<br/>    --filter <span class="hljs-string">"Name=tag:Name,Values=swarm-<span class="hljs-variable">$i</span>"</span> \<br/><span class="hljs-string">    "Name=instance-state-name,Values=running"</span> \<br/>    | jq -r <span class="hljs-string">".Reservations[0].Instances[0].PrivateIpAddress"</span>)<br/><br/><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-variable">$i</span>)<br/><br/>  docker swarm join \<br/>    --token <span class="hljs-variable">$WORKER_TOKEN</span> \<br/>    --advertise-addr <span class="hljs-variable">$IP</span> \<br/><span class="hljs-variable">    $MANAGER_IP</span>:<span class="hljs-number">2377</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>Let's confirm that all five nodes are indeed forming the cluster:</p>
<pre>
<strong><span class="hljs-built_in">eval</span> $(docker-machine env swarm-<span class="hljs-number">1</span>)<br/><br/>docker node ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS<br/>swarm-<span class="hljs-number">4</span>   Ready   <span class="hljs-keyword">Active</span><br/>swarm-<span class="hljs-number">2</span>   Ready   <span class="hljs-keyword">Active</span>        Reachable<br/>swarm-<span class="hljs-number">3</span>   Ready   <span class="hljs-keyword">Active</span>        Reachable<br/>swarm-<span class="hljs-number">5</span>   Ready   <span class="hljs-keyword">Active</span><br/>swarm-<span class="hljs-number">1</span>   Ready   <span class="hljs-keyword">Active</span>        Leader</strong>
</pre>
<p>That's it. Our cluster is ready. The only thing left is to deploy a few services and confirm that the cluster is working as expected.</p>
<p>Since we already created the services quite a few times, we'll speed up the process with the <kbd>vfarcic/docker-flow-proxy/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml</a>) and <kbd>vfarcic/go-demo/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml</a>) Compose stacks. They'll create the <kbd>proxy</kbd>, <kbd>swarm-listener</kbd>, <kbd>go-demo-db</kbd>, and <kbd>go-demo</kbd> services:</p>
<pre>
<strong>docker-machine ssh swarm-<span class="hljs-number">1</span><br/><br/><span class="hljs-built_in">sudo</span> docker network create --driver overlay proxy<br/><br/>curl -o proxy-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/docker-flow-proxy/master/docker-compose-stack.yml <br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c proxy-stack.yml proxy<br/><br/>curl -o go-demo-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/go-demo/master/docker-compose-stack.yml<br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c go-demo-stack.yml go-demo<br/><br/><span class="hljs-keyword">exit</span><br/><br/>docker service ls</strong>
</pre>
<p>Non-Windows users do not need to enter the <kbd>swarm-1</kbd> machine and can accomplish the same result by deploying the stacks directly from their laptops.</p>
<p>It'll take a few moments until all the images are downloaded. After a while, the output of the <kbd>service ls</kbd> command should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME                 MODE       REPLICA IMAGE<br/>go<span class="hljs-attribute">-demo_db</span>           replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     mongo:latest<br/>go<span class="hljs-attribute">-demo_main</span>         replicated <span class="hljs-number">3</span>/<span class="hljs-number">3</span>     vfarcic/go<span class="hljs-attribute">-demo</span>:latest<br/>proxy_swarm<span class="hljs-attribute">-listener</span> replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-swarm</span><span class="hljs-attribute">listener</span>:latest<br/>proxy_proxy          replicated <span class="hljs-number">2</span>/<span class="hljs-number">2</span>     vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-proxy</span>:latest</strong>
</pre>
<p>Let's confirm that the <kbd>go-demo</kbd> service is accessible:</p>
<pre>
<strong>curl <span class="hljs-string">"<span class="hljs-variable">$(docker-machine ip swarm-1)</span>/demo/hello"</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>curl: (<span class="hljs-number">7</span>) Failed <span class="hljs-keyword">to</span> connect <span class="hljs-keyword">to</span> <span class="hljs-number">54.157</span><span class="hljs-number">.196</span><span class="hljs-number">.113</span> <span class="hljs-keyword">port</span> <span class="hljs-number">80</span>: Operation timed <span class="hljs-keyword">out</span></strong>
</pre>
<p>That's embarrassing. Even though all the services are running and we used the same commands as in the previous chapters, we cannot access the <kbd>proxy</kbd> and, through it, the <kbd>go-demo</kbd> service.</p>
<p>The explanation is quite simple. We never opened ports <kbd>80</kbd> and <kbd>443</kbd>. By default, all incoming traffic to AWS EC2 instances is closed, and we opened only the ports required for Swarm to operate properly. They are open inside EC2 instances attached to the <kbd>docker-machine security</kbd> group, but not outside our AWS VPC.</p>
<p>We'll use the <kbd>aws ec2 authorize-security-group-ingress</kbd> command to open the ports <kbd>80</kbd> and <kbd>443</kbd>. This time we'll specify <kbd>cidr</kbd> instead <kbd>source-group</kbd> as the source:</p>
<pre>
<strong><span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-number">80</span> <span class="hljs-number">443</span>; <span class="hljs-keyword">do</span><br/>    aws ec2 authorize-security-group-ingress \<br/>        --group-id <span class="hljs-variable">$SECURITY_GROUP_ID</span> \<br/>        --protocol tcp \<br/>        --port <span class="hljs-variable">$p</span> \<br/>        --cidr <span class="hljs-string">"0.0.0.0/0"</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>The <kbd>aws ec2 authorize-security-group-ingress</kbd> command was executed twice; once for port <kbd>80</kbd> and the second time for <kbd>443</kbd>.</p>
<p>Let's send the request one more time:</p>
<pre>
<strong>curl <span class="hljs-string">"<span class="hljs-variable">$(docker-machine ip swarm-1)</span>/demo/hello"</span></strong>
</pre>
<p>This time the output is as expected. We got our response:</p>
<pre>
<strong><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></strong>
</pre>
<p>We set up the whole Swarm cluster in AWS using Docker Machine and AWS CLI. Is that everything we need? That depends on the requirements we might define for our cluster. We should probably add a few Elastic IP addresses.</p>
<p>An Elastic IP address is a static IP address designed for dynamic cloud computing. It is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. An Elastic IP address is a public IP address which is reachable from the Internet. If your instance does not have a public IP address, you can associate an Elastic IP address with it to enable communication with the Internet; for example, to connect to your instance from your local computer.</p>
<p>In other words, we should probably set at least two Elastic IPs and map them to two of the EC2 instances in the cluster. Those two (or more) IPs would be set as our DNS records. That way, when an instance fails, and we replace it with a new one, we can remap the Elastic IP without affecting our users.</p>
<p>There are quite a few other improvements we could do. However, that would put us in an awkward position. We would be using a tool that is not meant for setting up a complicated cluster.</p>
<p>The creation of VMs was quite slow. Docker Machine spent too much time provisioning it with Ubuntu and installing Docker Engine. We can reduce that time by creating our own <strong>Amazon Machine Image</strong> (<strong>AMI</strong>) with Docker Engine pre-installed. However, with such an action, the main reason for using Docker Machine would be gone. Its primary usefulness is simplicity. Once we start complicating the setup with other AWS resources, we'll realize that the simplicity is being replaced with too many ad-hoc commands.</p>
<p>Running <kbd>docker-machine</kbd> and <kbd>aws</kbd> commands works great when we are dealing with a small cluster, especially when we want to create something fast and potentially not very durable. The biggest problem is that everything we've done so far has been ad-hoc commands. Chances are that we would not be able to reproduce the same steps the second time. Our infrastructure is not documented, so our team would not know what constitutes our cluster.</p>
<p>My recommendation is to use <kbd>docker-machine</kbd> and <kbd>aws</kbd> as a quick and dirty way to create a cluster mostly for demo purposes. It can be useful for production as well, as long as the cluster is relatively small.</p>
<p>We should look at alternatives if we'd like to set up a complicated, bigger, and potentially more permanent solution.</p>
<p>Let us delete the cluster we created and explore the alternatives with a clean slate:</p>
<pre>
<strong><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span>; <span class="hljs-keyword">do</span><br/>    docker-machine rm <span class="hljs-operator">-f</span> swarm-<span class="hljs-variable">$i</span><br/><span class="hljs-keyword">done</span></strong>
</pre>
<p>The only thing left is to remove the security group <kbd>docker-machine</kbd> created:</p>
<pre>
<strong>aws ec2 delete-security-group \<br/>    --group-id <span class="hljs-variable">$SECURITY_GROUP_ID</span></strong>
</pre>
<p>The last command might fail if the instances are not yet terminated. If that's the case, please wait a few moments and repeat the command.</p>
<p>Let us move on and explore <em>Docker for AWS</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up a Swarm cluster with Docker for AWS</h1>
            </header>

            <article>
                
<p>Before we create a Swarm cluster using <em>Docker for AWS</em>, we'll need to generate a Key Pair that we'll use to SSH into the EC2 instances.</p>
<p>To create a new <kbd>key-pair</kbd>, please execute the command that follows:</p>
<pre>
<strong>aws ec2 create-key-pair \<br/>    --key-name devops21 \<br/>    | jq -r <span class="hljs-string">'.KeyMaterial'</span> &gt;devops21.pem</strong>
</pre>
<p>We executed <kbd>aws ec2 create-key-pair</kbd> command and passed <kbd>devops21</kbd> as the name. The output was filtered with <kbd>jq</kbd> so that only the actual value is returned. Finally, we sent the output to the <kbd>devops21.pem</kbd> file.</p>
<p>If someone gets a hold of your key file, your instances would be exposed. Therefore, we should move the key somewhere safe.</p>
<p>A common location for SSH keys on Linux/OSX systems is <kbd>$HOME/.ssh</kbd>. If you are a Windows user, feel free to change the command that follows to any destination you think is appropriate:</p>
<pre>
<strong>mv devops21.pem <span class="hljs-variable">$HOME</span>/.ssh/devops21.pem</strong>
</pre>
<p>We should also change permissions by giving the current user only the read access and removing all permissions from other users or groups. Feel free to skip the command that follows if you are a Windows user:</p>
<pre>
<strong>chmod <span class="hljs-number">400</span> <span class="hljs-variable">$HOME</span>/.ssh/devops21.pem</strong>
</pre>
<p>Finally, we'll put the path to the key inside the environment variable <kbd>KEY_PATH</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">export</span> KEY_PATH=<span class="hljs-variable">$HOME</span>/.ssh/devops21.pem</strong>
</pre>
<p>Now we are ready to create a Swarm stack using <em>Docker for AWS</em>.</p>
<p>Please open the <em>Docker for AWS Release Notes </em>(<a href="https://docs.docker.com/docker-for-aws/release-notes/">https://docs.docker.com/docker-for-aws/release-notes/</a>) page and click the <span class="packt_screen">Deploy Docker Community Edition for AWS</span> button.</p>
<p>After you log into the <em>AWS Console</em>, you will be presented with the <span class="packt_screen">Select Template</span> screen. It is a generic <span class="packt_screen">CloudFormation</span> screen with the <span class="packt_screen">Docker for AWS</span> template already selected:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/docker-for-aws-select-template.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-1: Docker For AWS Select Template screen</div>
<p>There's not much to do here, so please click the <span class="packt_screen">Next</span> button.</p>
<p>The next screen allows us to specify details of the stack we are about to launch. The fields should be self-explanatory. The only modification we'll make is the reduction of Swarm workers from <em>5</em> to <em>1</em>. The exercises in this section won't need more than four servers, so three managers and one worker should suffice. We'll leave instance types with their default values <kbd>t2.micro</kbd>. By creating only four micro nodes, the whole exercise should have almost negligible cost, and you won't be able to complain to your friends that you went bankrupt because of me. The total cost will probably be smaller than the price of a can of soda or a cup of coffee you're drinking while reading this.</p>
<p>The <span class="packt_screen">Which SSH key to use?</span> field should hold the <kbd>devops21</kbd> key we just created. Please select it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/docker-for-aws-specify-details.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-2: Docker For AWS Specify Details screen</div>
<p>Click the <span class="packt_screen">Next</span> button.</p>
<p>We won't change anything from the <span class="packt_screen">Options</span> screen. Later on, when you get comfortable with <em>Docker for AWS</em>, you might want to go back to this screen and fiddle with the additional options. For now, we'll just ignore its existence:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/docker-for-aws-options.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-3: Docker For AWS Options screen</div>
<p>Click the <span class="packt_screen">Next</span> button.</p>
<p>We reached the last screen. Feel free to review the information about the stack. Once you're done, click the <span class="packt_screen">I acknowledge that AWS CloudFormation might create IAM resources</span><em>.</em> checkbox followed by the <span class="packt_screen">Create</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="416" src="assets/docker-for-aws-review.png" width="445"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-4: Docker For AWS Review Screen</div>
<p>You will see the screen that allows you to create a new stack. Please click the <span class="packt_screen">refresh</span> button located in the top-right corner. You'll see the <em>Docker</em> stack with the status <kbd>CREATE_IN_PROGRESS</kbd>.</p>
<p>It will take a while until all the resources are created. If you'd like to see the progress, please select the <em>Docker</em> stack and click the <span class="packt_screen">restore</span> button located in the bottom-right part of the screen. You'll see the list of all the events generated by the <em>Docker for AWS</em> template. Feel free to explore the content of the tabs while waiting for the stack creation to finish:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="277" src="assets/docker-for-aws-stack-status.png" width="567"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-5: Docker For AWS stack status screen</div>
<p>We can proceed once the <em>Docker</em> stack status is <kbd>CREATE_COMPLETE</kbd>.</p>
<p>Our cluster is ready. We can enter one of the manager instances and explore the cluster in more detail.</p>
<p>To find information about Swarm manager instances, please click the <span class="packt_screen">Outputs</span> tab:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="279" src="assets/docker-for-aws-stack-outputs.png" width="671"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-6: Docker For AWS Stack outputs screen</div>
<p>You'll see two rows.</p>
<p>We'll store the value of the <span class="packt_screen">DefaultDNSTarget</span> in an environment variable. It'll come in handy soon:</p>
<pre>
<strong>DNS=[...]</strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual DefaultDNSTarget value.</p>
<p>If this were a "real" production cluster, you'd use it to update your DNS entries. It is the public entry to your system.</p>
<p>Click the link next to the <span class="packt_screen">Managers</span> column. You will be presented with the <span class="packt_screen">EC2 Instances</span> screen that contains the results filtered by manager nodes. The workers are hidden:</p>
<p><img class="image-border" src="assets/docker-for-aws-managers.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-7: Docker For AWS EC2 instances filtered by manager nodes</div>
<p>Select one of the managers and find its <span class="packt_screen">Public IP</span>. As with the DNS, we'll store it as an environment variable:</p>
<pre>
<strong>MANAGER_IP=[...]</strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual Public IP value.</p>
<p>We are, finally, ready to SSH into one of the managers and explore the cluster we just created:</p>
<pre>
<strong>ssh -i <span class="hljs-variable">$KEY_PATH</span> docker@<span class="hljs-variable">$MANAGER_IP</span></strong>
</pre>
<p>Once inside the server, you will be presented with a welcome message. The OS specifically made for this stack is very minimalistic, and the message reflects that:</p>
<pre>
<strong>Welcome <span class="hljs-keyword">to</span> Docker!<br/>~ $</strong>
</pre>
<p>As usual, we'll start by listing the nodes that form the cluster:</p>
<pre>
<strong>docker node ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME                     STATUS  AVAILABILITY  MANAGER STATUS<br/>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">17</span>-<span class="hljs-number">154.</span>ec2.<span class="hljs-keyword">internal</span>  Ready   Active        Reachable<br/>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">215.</span>ec2.<span class="hljs-keyword">internal</span>  Ready   Active        Reachable<br/>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">31</span>-<span class="hljs-number">44.</span>ec2.<span class="hljs-keyword">internal</span>   Ready   Active<br/>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">214.</span>ec2.<span class="hljs-keyword">internal</span>  Ready   Active        Leader</strong>
</pre>
<p>The only thing left is to create a few services that will confirm that the cluster works as expected.</p>
<p>We'll deploy the same <kbd>vfarcic/docker-flow-proxy/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml</a>) and <kbd>vfarcic/go-demo/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.yml</a>) stacks we used with the cluster we created with Docker Machine:</p>
<pre>
<strong><span class="hljs-built_in">sudo</span> docker network create --driver overlay proxy<br/><br/>curl -o proxy-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/docker-flow-proxy/master/docker-compose-stack.yml<br/><br/>docker stack deploy \<br/>    -c proxy-stack.yml proxy<br/><br/>curl -o go-demo-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/go-demo/master/docker-compose-stack.yml<br/><br/>docker stack deploy \<br/>    -c go-demo-stack.yml go-demo</strong>
</pre>
<p>We downloaded the Compose file and deployed the stacks.</p>
<p>Let's confirm that the services are indeed up and running:</p>
<pre>
<strong>docker service ls</strong>
</pre>
<p>After a while, the output should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME                 MODE       REPLICAS <br/>proxy_proxy          replicated <span class="hljs-number">2</span>/<span class="hljs-number">2</span>      <br/>go<span class="hljs-attribute">-demo_main</span>         replicated <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      <br/>proxy_swarm<span class="hljs-attribute">-listener</span> replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      <br/>go<span class="hljs-attribute">-demo_db</span>           replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      <br/>----------------------------------------------<br/>IMAGE<br/>vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-proxy</span>:latest<br/>vfarcic/go<span class="hljs-attribute">-demo</span>:latest<br/>vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-swarm</span><span class="hljs-attribute">-listener</span>:latest<br/>mongo:latest<br/></strong>
</pre>
<p>Let's get out of the server and confirm that the <kbd>go-demo</kbd> service is accessible to the public:</p>
<pre>
<strong><span class="hljs-keyword">exit</span><br/><br/>curl <span class="hljs-variable">$DNS</span>/demo/hello</strong>
</pre>
<p>As expected, we received the response confirming that the cluster is operational and accessible:</p>
<pre>
<strong><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></strong>
</pre>
<p>What happens if our servers become too crowded and we need to expand the capacity? How can we increase (or decrease) the number of nodes that form our cluster? The answer lies in AWS <span class="packt_screen">Auto Scaling Groups</span>. Please click the <span class="packt_screen">Auto Scaling Groups</span> link from the <span class="packt_screen">AUTO SCALING</span> group in the left-hand menu of the EC2 console and select the row with the group name that starts with <kbd>Docker-NodeAsg</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/docker-for-aws-auto-scaling-groups.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-8: Docker For AWS Auto Scaling Groups</div>
<p>To scale or de-scale the number of nodes, all we have to do is click the <span class="packt_screen">Edit</span> button from the <span class="packt_screen">Actions</span> menu, change the value of the <span class="packt_screen">Desired</span> field from <kbd>1</kbd> to <kbd><em>2</em></kbd>, and click the <span class="packt_screen">Save</span> button. The number of <span class="packt_screen">Desired</span> instances will immediately change to <kbd>2</kbd>. However, it might take a while until the actual number of instances aligns with the desire. Let's go back to one of the manager servers and confirm that the desire we expressed is indeed fulfilled:</p>
<pre>
<strong>ssh -i <span class="hljs-variable">$KEY_PATH</span> docker@<span class="hljs-variable">$MANAGER_IP</span><br/><br/>docker node ls</strong>
</pre>
<p>It might take a while until the new instance is created and joined to the cluster. The end result should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME                     STATUS  AVAILABILITY  MANAGER STATUS</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">17</span>-<span class="hljs-number">154.</span>ec2.<span class="hljs-keyword">internal</span>  Ready   Active        Reachable</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">215.</span>ec2.<span class="hljs-keyword">internal</span>  Ready   Active        Reachable</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">31</span>-<span class="hljs-number">44.</span>ec2.<span class="hljs-keyword">internal</span>   Ready   Active</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">214.</span>ec2.<span class="hljs-keyword">internal</span>  Ready   Active        Leader</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">11</span>-<span class="hljs-number">174.</span>ec2.<span class="hljs-keyword">internal</span>  Ready   Active</strong>
</pre>
<p>What happens in case one of the servers fails? After all, everything fails sooner or later. We can test that by removing one of the nodes.</p>
<p>Please click the <span class="packt_screen">Instances</span> link from the left-hand menu of the EC2 console, select one of the <kbd>Docker-worker</kbd> nodes, click <span class="packt_screen">Actions</span>, and change the <span class="packt_screen">Instance State to Terminate</span>. Confirm the termination by clicking the <span class="packt_screen">Yes</span><em>,</em> <span class="packt_screen">Terminate</span> button:</p>
<pre>
<strong>docker node ls</strong>
</pre>
<p>After a while, the output of the <kbd>node ls</kbd> command should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME                     STATUS  AVAILABILITY  MANAGER STATUS</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">17</span>-<span class="hljs-number">154.</span>ec2.internal  Ready   <span class="hljs-keyword">Active</span>        Reachable</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">215.</span>ec2.internal  Ready   <span class="hljs-keyword">Active</span>        Reachable</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">31</span>-<span class="hljs-number">44.</span>ec2.internal   Ready   <span class="hljs-keyword">Active</span></strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">214.</span>ec2.internal  Ready   <span class="hljs-keyword">Active</span>        Leader</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">11</span>-<span class="hljs-number">174.</span>ec2.internal  <span class="hljs-keyword">Down</span>    <span class="hljs-keyword">Active</span></strong>
</pre>
<p>Once the auto scaling group realizes that the node is down, it'll start the process of creating a new one and joining it to the cluster:</p>
<pre>
<strong>docker node ls</strong>
</pre>
<p>Before long, the output of the <kbd>node ls</kbd> command should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME                     STATUS  AVAILABILITY  MANAGER STATUS</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">17</span>-<span class="hljs-number">154.</span>ec2.internal  Ready   <span class="hljs-keyword">Active</span>        Reachable</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">215.</span>ec2.internal  Ready   <span class="hljs-keyword">Active</span>        Reachable</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">2</span>-<span class="hljs-number">22.</span>ec2.internal    Ready   <span class="hljs-keyword">Active</span></strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">31</span>-<span class="hljs-number">44.</span>ec2.internal   Ready   <span class="hljs-keyword">Active</span></strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">15</span>-<span class="hljs-number">214.</span>ec2.internal  Ready   <span class="hljs-keyword">Active</span>        Leader</strong><br/><strong>ip-<span class="hljs-number">10</span>-<span class="hljs-number">0</span>-<span class="hljs-number">11</span>-<span class="hljs-number">174.</span>ec2.internal  <span class="hljs-keyword">Down</span>    <span class="hljs-keyword">Active</span></strong>
</pre>
<p>The server we terminated is still marked as <kbd>Down</kbd>, and a new one is created and added to the cluster in its place.</p>
<p>There's much more to the <em>Docker for AWS</em> stack than we explored. I hope that what you learned during this brief exploration gave you enough base information to expand the knowledge by yourself.</p>
<p>Instead of exploring more details about the stack, we'll see how we can accomplish the same result without the UI. By this time you should know me well enough to understand that I prefer automatable and reproducible ways of running tasks. I broke my "no UIs unless necessary" rule only to give you a better understanding how the <em>Docker for AWS</em> stack works. A fully automated way to do the same is coming up next.</p>
<p>Before we proceed, we'll delete the stack and, with it, remove the cluster. That will be the last time you'll see any UI in this chapter.</p>
<p>Please click the <span class="packt_screen">Services</span> link from the top menu, followed by the <span class="packt_screen">CloudFormation</span> link. Select the Docker stack, and click the <span class="packt_screen">Delete Stack</span> option from the <span class="packt_screen">Actions</span> menu:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="273" src="assets/docker-for-aws-delete-stack.png" width="520"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-9: Docker For AWS Delete Stack screen</div>
<p>Confirm the destruction by clicking the <span class="packt_screen">Yes</span><em>,</em> <span class="packt_screen">Delete</span> button that appears after you click <span class="packt_screen">Delete Stack</span>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Automatically setting up a Swarm cluster with Docker for AWS</h1>
            </header>

            <article>
                
<p>Creating a <em>Docker for AWS</em> stack from the UI was a great exercise. It helped us understand better how things work. However, our mission is to automate as many processes as possible. With automation, we gain speed, reliability, and higher quality. When we run some manual tasks, like going through a UI and selecting different options, we are increasing the chance that something will go wrong due to human error. We are slow. We are much slower than machines when we need to execute repeatable steps.</p>
<p>Due to my mistrust in manual operations of repeatable tasks, it's only natural to seek a more automated way to create a <em>Docker for AWS</em> stack. All we did through the AWS console was to fill in a few fields which, in the background, generate parameters which are later used to execute a <kbd>CloudFormation</kbd> process. We can do the same without a UI.</p>
<p>We'll start by defining a few environment variables. They will be the same as those you already created in this chapter. Feel free to skip the next set of commands if you still have the same terminal session open:</p>
<pre>
<strong><span class="hljs-keyword">export</span> AWS_DEFAULT_REGION=us-east-<span class="hljs-number">1</span><br/><br/><span class="hljs-keyword">export</span> AWS_ACCESS_KEY_ID=[...]<br/><br/><span class="hljs-keyword">export</span> AWS_SECRET_ACCESS_KEY=[...]</strong>
</pre>
<p>Please change <kbd>us-east-1</kbd> to the region of your choice and replace <kbd>[...]</kbd> with the actual values.</p>
<p>If you recall the first screen that allowed us to select a template, you'll remember that there was a field with pre-populated URL of a <span class="packt_screen">CloudFormation</span> template. At the time of this writing, the template is <kbd>Docker.tmpl</kbd> (<a href="https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl">https://editions-us-east-1.s3.amazonaws.com/aws/stable/Docker.tmpl</a>) Please note that the address differs from one region to another. I'll be using <kbd>us-east-1</kbd> edition.</p>
<p>We can inspect the template by retrieving its content with <kbd>curl</kbd>:</p>
<pre>
<strong>curl https://editions-us-east-<span class="hljs-number">1</span>.s3.amazonaws.com/aws/stable/Docker.tmpl</strong>
</pre>
<p>Please spend some time exploring the output. Even if you are not familiar with <kbd>CloudFormation</kbd> syntax, you should recognize the AWS resources.</p>
<p>The part of the template that we are most interested in is Metadata:</p>
<pre>
<strong>curl https://editions-us-east-<span class="hljs-number">1</span>.s3.amazonaws.com/aws/stable/ \<br/>Docker.tmpl  \<br/>    | jq <span class="hljs-string">'.Metadata'</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>{<br/>  "<span class="hljs-attribute">AWS::CloudFormation::Interface</span>": <span class="hljs-value">{<br/>    "<span class="hljs-attribute">ParameterGroups</span>": <span class="hljs-value">[<br/>      {<br/>        "<span class="hljs-attribute">Label</span>": <span class="hljs-value">{<br/>          "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Swarm Size"</span><br/></span>         }</span>,<br/>        "<span class="hljs-attribute">Parameters</span>": <span class="hljs-value">[<br/><span class="hljs-string">         "ManagerSize"</span>,<br/><span class="hljs-string">         "ClusterSize"</span><br/>        ]<br/></span>      },<br/>    {<br/>        "<span class="hljs-attribute">Label</span>": <span class="hljs-value">{<br/>        "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Swarm Properties"</span><br/></span>        }</span>,<br/>        "<span class="hljs-attribute">Parameters</span>": <span class="hljs-value">[<br/><span class="hljs-string">         "KeyName"</span>,<br/><span class="hljs-string">         "EnableSystemPrune"</span>,<br/><span class="hljs-string">         "EnableCloudWatchLogs"</span><br/>        ]<br/></span>    },<br/>    {<br/>        "<span class="hljs-attribute">Label</span>": <span class="hljs-value">{<br/>         "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Swarm Manager Properties"</span><br/></span>      }</span>,<br/>        "<span class="hljs-attribute">Parameters</span>": <span class="hljs-value">[<br/><span class="hljs-string">         "ManagerInstanceType"</span>,<br/><span class="hljs-string">         "ManagerDiskSize"</span>,<br/><span class="hljs-string">         "ManagerDiskType"</span><br/>        ]<br/></span>    },<br/>    {<br/>        "<span class="hljs-attribute">Label</span>": <span class="hljs-value">{<br/>         "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Swarm Worker Properties"</span><br/></span>        }</span>,<br/>        "<span class="hljs-attribute">Parameters</span>": <span class="hljs-value">[<br/><span class="hljs-string">         "InstanceType"</span>,<br/><span class="hljs-string">         "WorkerDiskSize"</span>,<br/><span class="hljs-string">         "WorkerDiskType"</span><br/>        ]<br/></span>    }<br/>  ]</span>,<br/>        "<span class="hljs-attribute">ParameterLabels</span>": <span class="hljs-value">{<br/>         "<span class="hljs-attribute">ClusterSize</span>": <span class="hljs-value">{<br/>          "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Number of Swarm worker nodes?"</span><br/></span>        }</span>,<br/>      "<span class="hljs-attribute">EnableCloudWatchLogs</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Use Cloudwatch for container logging?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">EnableSystemPrune</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Enable daily resource cleanup?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">InstanceType</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Agent worker instance type?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">KeyName</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Which SSH key to use?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">ManagerDiskSize</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Manager ephemeral storage volume size?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">ManagerDiskType</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Manager ephemeral storage volume type"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">ManagerInstanceType</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Swarm manager instance type?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">ManagerSize</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Number of Swarm managers?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">WorkerDiskSize</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Worker ephemeral storage volume size?"</span><br/></span>      }</span>,<br/>      "<span class="hljs-attribute">WorkerDiskType</span>": <span class="hljs-value">{<br/>       "<span class="hljs-attribute">default</span>": <span class="hljs-value"><span class="hljs-string">"Worker ephemeral storage volume type"</span><br/></span>      }<br/></span>    }<br/></span>  }<br/></span>}</strong>
</pre>
<p>We can use the <kbd>ParameterLabels</kbd> to customize the result of the template.</p>
<p>The command that would create the same stack like the one we generated using the AWS console is as follows:</p>
<pre>
<strong>aws cloudformation create-stack \<br/>    --template-url https://editions-us-east \<br/>    -<span class="hljs-number">1</span>.s3.amazonaws.com/aws/stable/Docker.tmpl \<br/>    --stack-name swarm \<br/>    --capabilities CAPABILITY_IAM \<br/>    --parameters \<br/>    ParameterKey=KeyName,ParameterValue=devops21 \<br/>    ParameterKey=InstanceType,ParameterValue=t2.micro \<br/>    ParameterKey=ManagerInstanceType,ParameterValue=t2.micro \<br/>    ParameterKey=ManagerSize,ParameterValue=<span class="hljs-number">3</span> \<br/>    ParameterKey=ClusterSize,ParameterValue=<span class="hljs-number">1</span></strong>
</pre>
<p>The command should be self-explanatory. We used <kbd>aws</kbd> to create a <kbd>CloudFormation</kbd> stack with all the required parameters.</p>
<p>We can monitor the status of the stack resources by executing the <kbd>cloudformation describe-stack-resources</kbd> command:</p>
<pre>
<strong>aws cloudformation describe-stack-resources \<br/>    --stack-name swarm</strong>
</pre>
<p>After a while, three manager instances should be created:</p>
<pre>
<strong>aws ec2 describe-instances \<br/>    --filters <span class="hljs-string">"Name=tag:Name,Values=swarm-Manager"</span> \<br/><span class="hljs-string">    "Name=instance-state-name,Values=running"</span></strong>
</pre>
<p>Now we can enter one of the managers and start creating services. I'll skip the examples that create services and validate that they are working correctly. The end-result is the same cluster as the one we created previously with AWS console.</p>
<p>Feel free to explore the cluster on your own and <kbd>delete</kbd> the stack once you're finished:</p>
<pre>
<strong>aws cloudformation delete-stack \<br/>    --stack-name swarm</strong>
</pre>
<p>For anything but small clusters, <em>Docker for AWS</em> is a much better option than using the combination with <kbd>docker-machine</kbd> and <kbd>aws</kbd> commands. It is a more robust and reliable solution. However, it comes with its downsides.</p>
<p><em>Docker for AWS</em> is still young and prone to frequent changes. Moreover, it is still so new that the documentation is close to non-existent.</p>
<p>By being an out-of-the-box solution, it is very easy to use and requires little effort. That is both a blessing and a curse. It is a good choice if what you need is, more or less, what <em>Docker for AWS</em> offers. However, if your needs are different, you might experience quite a few problems when trying to adapt the template to your needs. The solution is based on a custom OS, <kbd>CloudFormation</kbd> template, and containers built specifically for this purpose. Changing anything but the template is highly discouraged.</p>
<p>Overall, I think that <em>Docker for AWS</em> has a very bright future and is, in most cases, a better solution than <kbd>docker-machine</kbd>. If those two were the only choices, my vote would be to use <em>Docker for AWS</em>. Fortunately, there are many other options we can choose from; much more than could fit into a single chapter. You might be reading the printed edition of the book, and I don't feel comfortable sacrificing too many trees. Therefore, I'll present only one more set of tools we can use to create a Swarm (or any other type of) cluster.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting up a Swarm cluster with Packer and Terraform</h1>
            </header>

            <article>
                
<p>This time we'll use a set of tools completely unrelated to Docker. It'll be <em>Packer</em>(<a href="https://www.packer.io/">https://www.packer.io/</a>)and <em>Terraform </em>(<a href="https://www.terraform.io/">https://www.terraform.io/</a>). Both are coming from <em>HashiCorp </em>(<a href="https://www.hashicorp.com/">https://www.hashicorp.com/</a>).</p>
<div class="packt_tip"><strong>A note to Windows users</strong><br/>
Using Chocolatey, install packer from an administrator command prompt via <kbd>choco install packer</kbd>. For terraform, execute <kbd>choco install terraform</kbd> in an administrator command prompt.</div>
<p>Packer allows us to create machine images. With Terraform we can create, change, and improve cluster infrastructure. Both tools support almost all the major providers. They can be used with Amazon EC2, CloudStack, DigitalOcean, Google Compute Engine (GCE), Microsoft Azure, VMware, VirtualBox, and quite a few others. The ability to be infrastructure agnostic allows us to avoid vendor lock-in. With a minimal change in configuration, we can easily transfer our cluster from one provider to another. Swarm is designed to work seamlessly no matter which hosting provider we use, as long as the infrastructure is properly defined. With Packer and Terraform we can define infrastructure in such a way that transitioning from one to another is as painless as possible.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using Packer to create Amazon Machine Images</h1>
            </header>

            <article>
                
<p>The <kbd>vfarcic/cloud-provisioning</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning">https://github.com/vfarcic/cloud-provisioning</a>) repository already has the Packer and Terraform configurations we'll use. They are located in the <kbd>terraform/aws</kbd> directory:</p>
<pre>
<strong><span class="hljs-built_in">cd</span> terraform/aws</strong>
</pre>
<p>The first step is to use Packer <span>to</span> create an <strong>Amazon Machine Image</strong> (<strong>AMI</strong>). To do that, we'll need AWS access keys set as environment variables. They will be the same as those you already created in this chapter. Feel free to skip the next set of commands if you still have the same terminal session open:</p>
<pre>
<strong><span class="hljs-keyword">export</span> AWS_ACCESS_KEY_ID=[...]<br/><br/><span class="hljs-keyword">export</span> AWS_SECRET_ACCESS_KEY=[...]<br/><br/><span class="hljs-keyword">export</span> AWS_DEFAULT_REGION=us-east-<span class="hljs-number">1</span></strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual values.</p>
<p>We'll instantiate all Swarm nodes from the same AMI. It'll be based on Ubuntu and have the latest Docker engine installed.</p>
<p>The JSON definition of the image we are about to build is in <kbd>terraform/aws/packer-ubuntu-docker.json</kbd>   (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/packer-ubuntu-docker.json">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/packer-ubuntu-docker.json</a>):</p>
<pre>
<strong>cat packer-ubuntu-docker.json</strong>
</pre>
<p>The configuration consists of two sections: <kbd>builders</kbd> and <kbd>provisioners</kbd>:</p>
<pre>
<strong>{<br/><span class="hljs-string">   "builders"</span>: [{<br/><span class="hljs-keyword">   ...</span><br/>  }],<br/><span class="hljs-string">   "provisioners"</span>: [{<br/><span class="hljs-keyword">   ...</span><br/>  }]<br/>}</strong>
</pre>
<p>The builders section defines all the information Packer needs to build an image. The <kbd>provisioners</kbd> section describes the commands that will be used to install and configure software for the machines created by builders. The only required section is builders.</p>
<p>Builders are responsible for creating machines and generating images from them for various platforms. For example, there are separate builders for EC2, VMware, VirtualBox, and so on. Packer comes with many builders by default, and can also be extended to add new builders.</p>
<p>The <kbd>builders</kbd> section we'll use is as follows:</p>
<pre>
<strong><span class="hljs-string">"builders"</span>: [{<br/><span class="hljs-string">  "type"</span>: <span class="hljs-string">"amazon-ebs"</span>,<br/><span class="hljs-string">  "region"</span>: <span class="hljs-string">"us-east-1"</span>,<br/><span class="hljs-string">  "source_ami_filter"</span>: {<br/><span class="hljs-string">    "filters"</span>: {<br/><span class="hljs-string">      "virtualization-type"</span>: <span class="hljs-string">"hvm"</span>,<br/><span class="hljs-string">      "name"</span>: <span class="hljs-string">"*ubuntu-xenial-16.04-amd64-server-*"</span>,<br/><span class="hljs-string">      "root-device-type"</span>: <span class="hljs-string">"ebs"<br/></span>    },<br/><span class="hljs-string">    "owners"</span>: [<span class="hljs-string">"099720109477"</span>],<br/><span class="hljs-string">    "most_recent"</span>: <span class="hljs-literal">true<br/></span>  },<br/><span class="hljs-string">  "instance_type"</span>: <span class="hljs-string">"t2.micro"</span>,<br/><span class="hljs-string">  "ssh_username"</span>: <span class="hljs-string">"ubuntu"</span>,<br/><span class="hljs-string">  "ami_name"</span>: <span class="hljs-string">"devops21"</span>,<br/><span class="hljs-string">  "force_deregister"</span>: <span class="hljs-literal">true <br/></span>}],</strong>
</pre>
<p>Each type of builder has specific arguments that can be used. We specified that the <kbd>type</kbd> is <kbd>amazon-ebs</kbd>. Besides <kbd>amazon-ebs</kbd>, we can also use <kbd>amazon-instance</kbd> and <kbd>amazon-chroot</kbd> builders for AMIs. In most cases, <kbd>amazon-ebs</kbd> is what we should use. Please visit the <em>Amazon AMI Builder </em>(<a href="https://www.packer.io/docs/builders/amazon.html">https://www.packer.io/docs/builders/amazon.html</a>) page for more info.</p>
<p>Please note that, when using <kbd>amazon-ebs</kbd> type, we have to provide AWS keys. We could have specified them through the <span class="packt_screen">access_key</span> and <span class="packt_screen">secret_key</span> fields. However, there is an alternative. If those fields are not specified, Packer will try to get the values from environment variables <kbd>AWS_ACCESS_KEY_ID</kbd> and <kbd>AWS_SECRET_ACCESS_KEY</kbd>. Since we already exported them, there's no need to repeat ourselves by setting them inside the Packer configuration. Moreover, those keys should be secret. Putting them inside the config would risk exposure.</p>
<p>The region is critical since an AMI can be created only within one region. If we wanted to share the same machine across multiple regions, each would need to be specified as a separate builder.</p>
<p>We could have specified <kbd>source_ami</kbd> with the ID of the initial AMI that would serve as a base for the newly created machine. However, since AMIs are unique to a particular region, that would make it unusable if we decide to change the region. Instead, we took a different approach and specified the <kbd>source_ami_filter</kbd> which will populate the s<kbd>ource_ami</kbd> field. It'll filter AMIs and find an Ubuntu <kbd>16.04</kbd> image with <kbd>hvm</kbd> virtualization type with Root Device Type set to <kbd>ebs</kbd>. The owners field will limit the results to a trusted AMI provider. Since the filter would fail if more than one AMI is returned, the <kbd>most_recent</kbd> field will limit the result by selecting the newest image.</p>
<p>The <kbd>instance_type</kbd> field defines which EC2 instance type will be used to build the AMI. Please note that this will not prevent us from instantiating VMs based on this image using any other instance type supported, in this case, by Ubuntu.</p>
<p>Unlike other fields we used, the <kbd>ssh_username</kbd> is not specific to the <kbd>amazon-ebs</kbd> builder. It specifies the user Packer will use while creating the image. Just like the instance type, it will not prevent us from specifying any other user when instantiating VMs based on this image.</p>
<p>The <kbd>ami_name</kbd> field is the name we'll give to this AMI.</p>
<p>In case we already created an AMI with the same, the <kbd>force_deregister</kbd> field will remove it before creating the new one.</p>
<p>Please visit the <em>AMI Builder (EBS Backed)</em> (<a href="https://www.packer.io/docs/builders/amazon-ebs.html">https://www.packer.io/docs/builders/amazon-ebs.html</a>) page for more information.</p>
<p>The second section is provisioners. It contains an array of all the provisioners that Packer should use to install and configure software within running machines before turning them into Machine Images.</p>
<p>There are quite a few provisioner types we can use. If you read <em>The DevOps 2.0 Toolkit</em>, you know that I advocated Ansible as the provisioner of choice. Should we use it here as well? In most cases, when building images meant to run Docker containers, I opt for a simple shell. The reasons for a change from Ansible to Shell lies in the different objectives provisioners should fulfill when running on live servers, as opposed to building images.</p>
<p>Unlike Shell, Ansible (and most other provisioners) are idempotent. They verify the actual state and execute one action or another depending on what should be done for the desired state to be fulfilled. That's a great approach since we can run Ansible as many times as we want and the result will always be the same. For example, if we specify that we want to have JDK 8, Ansible will SSH into a destination server, discover that the JDK is not present and install it. The next time we run it, it'll discover that the JDK is already there and do nothing.<br/>
Such an approach allows us to run Ansible playbooks as often as we want and it'll always result in JDK being installed. If we'd try to accomplish the same through a Shell script, we'd need to write lengthy <kbd>if/else</kbd> statements. If JDK is installed, do nothing, if it's not installed, install it, if it's installed, but the version is not correct, upgrade it, and so on.</p>
<p>So, why not use it with Packer? The answer is simple. We do not need idempotency since we'll run it only once while creating an image. We won't use it on running instances. Do you remember the "pets vs cattle" discussion? Our VMs will be instantiated from an image that already has everything we need. If the state of that VM changes, we'll terminate it and create a new one.<br/>
<br/>
If we need to do an upgrade or install additional software, we won't do it inside the running instance, but create a new image, destroy running instances, and instantiate new ones based on the updated image.</p>
<p>Is idempotency the only reason we would use Ansible? Definitely not! It is a very handy tool when we need to define a complicated server setup. However, in our case the setup is simple. We need Docker Engine and not much more. Almost everything will be running inside containers. Writing a few Shell commands to install Docker is easier and faster than defining Ansible playbooks. It would probably take the same number of commands to install Ansible as to install Docker.</p>
<p>To make a Long story short, we'll use <kbd>shell</kbd> as our provisioner of choice for building AMIs.</p>
<p>The <kbd>provisioners</kbd> section we'll use is as follows:</p>
<pre>
<strong>"provisioners": [{<br/>  "type": "shell",<br/>  "inline": [<br/>    "sleep 15",<br/>    "sudo apt-get <span class="hljs-operator"><span class="hljs-keyword">update</span><span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">get</span> install -y apt-transport-https ca-certificates \<br/>nfs-common<span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">key</span> adv --keyserver hkp://ha.pool.sks-keyservers.net: \<br/><span class="hljs-number">80</span> --recv-keys <span class="hljs-number">58118E89</span>F3A912897C070ADBF76221572C52609D<span class="hljs-string">",<br/>    "</span>echo <span class="hljs-string">'deb https://apt.dockerproject.org/repo ubuntu-xenial \<br/>main'</span> | sudo tee /etc/apt/sources.list.d/docker.list<span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">get</span> <span class="hljs-keyword">update</span><span class="hljs-string">",<br/>    "</span>sudo apt-<span class="hljs-keyword">get</span> install -y docker-engine<span class="hljs-string">",<br/>    "</span>sudo usermod -aG docker ubuntu<span class="hljs-string">"<br/>  ]<br/>}]</span></span></strong>
</pre>
<p>The <kbd>shell type</kbd> is followed by a set of commands. They are the same as the commands we can find in the <em>Install Docker on Ubuntu</em> (<a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/">https://docs.docker.com/engine/installation/linux/ubuntulinux/</a>) page.</p>
<p>Now that we have a general idea how Packer configuration works, we can proceed and build an image:</p>
<pre>
<strong>packer build -machine-readable \<br/>    packer-ubuntu-docker.json \<br/>    | tee packer-ubuntu-docker.log</strong>
</pre>
<p>We ran the <kbd>packer</kbd> build of the <kbd>packer-ubuntu-docker.json</kbd> with the <kbd>machine-readable</kbd> output sent to the <kbd>packer-ubuntu-docker.log</kbd> file. Machine readable output will allow us to parse it easily and retrieve the ID of the AMI we just created.</p>
<p>The final lines of the output are as follows:</p>
<pre>
<strong>...<br/><span class="hljs-number">1480105510</span>,,ui,say,Build 'amazon-ebs' finished.<br/><span class="hljs-number">1480105510</span>,,ui,say,\n==&gt; Builds finished. The artifacts of successful builds are:<br/><span class="hljs-number">1480105510</span>,amazon-ebs,artifact-count,<span class="hljs-number">1</span><br/><span class="hljs-number">1480105510</span>,amazon-ebs,artifact,<span class="hljs-number">0</span>,builder-id,mitchellh.amazonebs<br/><span class="hljs-number">1480105510</span>,amazon-ebs,artifact,<span class="hljs-number">0</span>,id,us-east-<span class="hljs-number">1</span>:ami-<span class="hljs-number">02</span>ebd915<br/><span class="hljs-number">1480105510</span>,amazon-ebs,artifact,<span class="hljs-number">0</span>,string,AMIs were \<br/>created: \n\nus-east-<span class="hljs-number">1</span>: ami-<span class="hljs-number">02</span>ebd915<br/><span class="hljs-number">1480105510</span>,amazon-ebs,artifact,<span class="hljs-number">0</span>,files-count,<span class="hljs-number">0</span><br/><span class="hljs-number">1480105510</span>,amazon-ebs,artifact,<span class="hljs-number">0</span>,end<br/><span class="hljs-number">1480105510</span>,,ui,say,--&gt; amazon-ebs: AMIs were created: \n\nus-east-<span class="hljs-number">1</span>: ami-<span class="hljs-number">02</span>ebd915</strong>
</pre>
<p>Apart from the confirmation that the build was successful, the relevant part of the output is the line id, <kbd>us-east-1:ami-02ebd915</kbd>. It contains the AMI ID we'll need to instantiate VMs based on the image.</p>
<p>You might want to store the <kbd>packer-ubuntu-docker.log</kbd> in your code repository in case you need to get the ID from a different server.</p>
<p>The flow of the process we executed can be described through <em>figure 11-10:</em></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" height="234" src="assets/cloud-architecture-images.png" width="367"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-10: The flow of the Packer process</div>
<p>Now we are ready to create a Swarm cluster with VMs based on the image we built.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using Terraform to create a Swarm cluster in AWS</h1>
            </header>

            <article>
                
<p>We'll start by redefining the environment variables we used with Packer in case you start this section in a new terminal session:</p>
<pre>
<strong><span class="hljs-built_in">cd</span> terraform/aws<br/><br/><span class="hljs-keyword">export</span> AWS_ACCESS_KEY_ID=[...]<br/><br/><span class="hljs-keyword">export</span> AWS_SECRET_ACCESS_KEY=[...]<br/><br/><span class="hljs-keyword">export</span> AWS_DEFAULT_REGION=us-east-<span class="hljs-number">1</span></strong>
</pre>
<p>Please replace <kbd>[...]</kbd> with the actual values.</p>
<p>Terraform does not force us to have any particular file structure. We can define everything in a single file. However, that does not mean that we should. Terraform configs can get big, and separation of logical sections into separate files is often a good idea. In our case, we'll have three <kbd>tf</kbd> files. The <kbd>terraform/aws/variables.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf</a>) holds all the variables. </p>
<p>If we need to change any parameter, we'll know where to find it. The <kbd>terraform/aws/common.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf</a>) file contains definitions of the elements that might be potentially reusable on other occasions. Finally, the <kbd>terraform/aws/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf</a>) file has the <kbd>Swarm-specific</kbd> resources.</p>
<p>We'll explore each of the Terraform configuration files separately.</p>
<p>The content of the <kbd>terraform/aws/variables.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/variables.tf</a>) file is as follows:</p>
<pre>
<strong><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_manager_token"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = ""</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_worker_token"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = ""</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_ami_id"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = "unknown"</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_manager_ip"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = ""</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_managers"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = 3</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_workers"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = 2</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_instance_type"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = "t2.micro"</span><br/>}<br/><span class="hljs-title">variable</span> <span class="hljs-string">"swarm_init"</span> {<br/><span class="hljs-default"><span class="hljs-keyword">  default</span> = false</span><br/>}</strong>
</pre>
<p>The <kbd>swarm_manager_token</kbd> and <kbd>swarm_worker_token</kbd> will be required to join the nodes to the cluster. The <kbd>swarm_ami_id</kbd> will hold the ID of the image we created with Packer. The <kbd>swarm_manager_ip</kbd> variable is the IP of one of the managers that we'll need to provide for the nodes to join the cluster. The <kbd>swarm_managers</kbd> and <kbd>swarm_workers</kbd> define how many nodes we want of each. The <kbd>swarm_instance_typ</kbd>e is the type of the instance we want to create. If defaults to the smallest and cheapest (often free) instance. Feel free to change it to a more potent type later on if you start using this Terraform config to create a "real" cluster.<br/>
&gt;Finally, the <kbd>swarm_init</kbd> variable allows us to specify whether this is the first run and the node should initialize the cluster. We'll see its usage very soon.</p>
<p>The content of the <kbd>terraform/aws/common.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/common.tf</a>) file is as follows:</p>
<pre>
<strong>resource <span class="hljs-string">"aws_security_group"</span> <span class="hljs-string">"docker"</span> {<br/>  name = <span class="hljs-string">"docker"</span><br/>  ingress {<br/>    from_port   = <span class="hljs-number">22</span><br/>    to_port     = <span class="hljs-number">22</span><br/>    protocol    = <span class="hljs-string">"tcp"</span><br/>    cidr_blocks = [<span class="hljs-string">"0.0.0.0/0"</span>]<br/>  }<br/>  ingress {<br/>    from_port = <span class="hljs-number">80</span><br/>    to_port   = <span class="hljs-number">80</span><br/>    protocol  = <span class="hljs-string">"tcp"</span><br/>    cidr_blocks = [<span class="hljs-string">"0.0.0.0/0"</span>]<br/>  }<br/>  ingress {<br/>    from_port = <span class="hljs-number">443</span><br/>    to_port   = <span class="hljs-number">443</span><br/>    protocol  = <span class="hljs-string">"tcp"</span><br/>    cidr_blocks = [<span class="hljs-string">"0.0.0.0/0"</span>]<br/>  }<br/>  ingress {<br/>    from_port = <span class="hljs-number">2377</span><br/>    to_port   = <span class="hljs-number">2377</span><br/>    protocol  = <span class="hljs-string">"tcp"</span><br/><span class="hljs-keyword">    self</span>      = <span class="hljs-keyword">true</span><br/>  }<br/>  ingress {<br/>    from_port = <span class="hljs-number">7946</span><br/>    to_port   = <span class="hljs-number">7946</span><br/>    protocol  = <span class="hljs-string">"tcp"</span><br/><span class="hljs-keyword">    self</span>      = <span class="hljs-keyword">true</span><br/>  }<br/>  ingress {<br/>    from_port = <span class="hljs-number">7946</span><br/>    to_port   = <span class="hljs-number">7946</span><br/>    protocol  = <span class="hljs-string">"udp"</span><br/><span class="hljs-keyword">    self</span>      = <span class="hljs-keyword">true</span><br/>  }<br/>  ingress {<br/>    from_port = <span class="hljs-number">4789</span><br/>    to_port   = <span class="hljs-number">4789</span><br/>    protocol  = <span class="hljs-string">"tcp"</span><br/><span class="hljs-keyword">    self</span>      = <span class="hljs-keyword">true</span><br/>  }<br/>  ingress {<br/>    from_port = <span class="hljs-number">4789</span><br/>    to_port   = <span class="hljs-number">4789</span><br/>    protocol  = <span class="hljs-string">"udp"</span><br/><span class="hljs-keyword">    self</span>      = <span class="hljs-keyword">true</span><br/>  }<br/>  egress {<br/>    from_port   = <span class="hljs-number">0</span><br/>    to_port     = <span class="hljs-number">0</span><br/>    protocol    = <span class="hljs-string">"-1"</span><br/>    cidr_blocks = [<span class="hljs-string">"0.0.0.0/0"</span>]<br/>  }<br/>}</strong>
</pre>
<p>Each resource is defined with a type (example: <kbd>aws_security_group</kbd>) and a name (example: <kbd>docker</kbd>). The type determines which resource should be created and must be one of those currently supported.</p>
<p>The first resource <kbd>aws_security_group</kbd> contains the list of all ingress ports that should be opened. Port <kbd>22</kbd> is required for SSH. Ports <kbd>80</kbd> and <kbd>443</kbd> will be used for HTTP and HTTPS access to the <kbd>proxy</kbd>. The rest of the ports will be used for Swarms internal communication. TCP port <kbd>2377</kbd> is for cluster management communications, TCP and UDP port <kbd>7946</kbd> for communication among nodes, and TCP and UDP port <kbd>4789</kbd> for overlay network traffic. Those are the same ports we had to open when we created the cluster using Docker Machine. Please note that all but ports <kbd>22</kbd>, <kbd>80</kbd> and <kbd>443</kbd> are assigned to self. That means that they will be available only to other servers that belong to the same group. Any outside access will be blocked.</p>
<p>The last entry in the <kbd>aws_security_group</kbd> is <kbd>egress</kbd> allowing communication from the cluster to the outside world without any restrictions.</p>
<p>Please consult the <kbd>AWS_SECURITY_GROUP</kbd> (<a href="https://www.terraform.io/docs/providers/aws/d/security_group.html">https://www.terraform.io/docs/providers/aws/d/security_group.html</a>) page for more info.</p>
<p>Now comes the "real deal". The <kbd>terraform/aws/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf</a>) file contains the definition of all the instances we'll create. Since the content of this file is a bit bigger than the others, we'll examine each resource separately.</p>
<p>The first resource in line is the <kbd>aws_instance</kbd> type named <kbd>swarm-manager</kbd>. Its purpose is to create Swarm manager nodes:</p>
<pre>
<strong>resource <span class="hljs-string">"aws_instance"</span> <span class="hljs-string">"swarm-manager"</span> {<br/>  ami = <span class="hljs-string">"<span class="hljs-variable">${var.swarm_ami_id}</span>"</span><br/>  instance_<span class="hljs-built_in">type</span> = <span class="hljs-string">"<span class="hljs-variable">${var.swarm_instance_type}</span>"</span><br/>  count = <span class="hljs-string">"<span class="hljs-variable">${var.swarm_managers}</span>"</span><br/>  tags {<br/>    Name = <span class="hljs-string">"swarm-manager"</span><br/>  }<br/>  vpc_security_group_ids = [<br/><span class="hljs-string">    "<span class="hljs-variable">${aws_security_group.docker.id}</span>"</span><br/>  ]<br/>  key_name = <span class="hljs-string">"devops21"</span><br/>  connection {<br/>    user = <span class="hljs-string">"ubuntu"</span><br/>    private_key = <span class="hljs-string">"<span class="hljs-variable">${file("devops21.pem")}</span>"</span><br/>  }<br/>  provisioner <span class="hljs-string">"remote-exec"</span> {<br/>    inline = [<br/><span class="hljs-string">      "if <span class="hljs-variable">${var.swarm_init}</span>; then docker swarm init \<br/>--advertise-addr <span class="hljs-variable">${self.private_ip}</span>; fi"</span>,<br/><span class="hljs-string">      "if ! <span class="hljs-variable">${var.swarm_init}</span>; then docker swarm join \<br/>--token <span class="hljs-variable">${var.swarm_manager_token}</span> --advertise-addr \<br/><span class="hljs-variable">${self.private_ip}</span> <span class="hljs-variable">${var.swarm_manager_ip}</span>:2377; fi"</span><br/>    ]<br/>  }<br/>}</strong>
</pre>
<p>The resource contains the <kbd>ami</kbd> that references the image we created with Packer. The actual value is a variable that we'll define at runtime. The <kbd>instance_type</kbd> specifies the type of the instance we want to create. The default value is fetched from the variable <kbd>swarm_instance_type</kbd>. By default, it is set to <kbd>t2.micro</kbd>. Just as any other variable, it can be overwritten at runtime.</p>
<p>The count field defines how many managers we want to create. The first time we run <kbd>terraform</kbd>, the value should be 1 since we want to start with one manager that will initialize the cluster. Afterward, the value should be whatever is defined in variables. We'll see the use case of both combinations soon.</p>
<p>The tags are for informational purposes only.</p>
<p>The <kbd>vpc_security_group_id</kbd>s field contains the list of all groups we want to attach to the server. In our case, we are using only the group docker defined in <kbd>terraform/aws/common.tf</kbd>.</p>
<p>The <kbd>key_name</kbd> is the name of the key we have stored in AWS. We created the <kbd>devops21</kbd> key at the beginning of the chapter. Please double check that you did not remove it. Without it, you won't be able to SSH into the machine.</p>
<p>The connection field defines the SSH connection details. The user will be <kbd>ubuntu</kbd>. Instead of a password, we'll use the <kbd>devops21.pem</kbd> key.</p>
<p>Finally, the provisioner is defined. The idea is to do as much provisioning as possible during the creation of the images. That way, instances are created much faster since the only action is to create a VM out of an image. However, there is often a part of provisioning that cannot be done when creating an image. The <kbd>swarm init</kbd> command is one of those. We cannot initialize the first Swarm node until we get the IP of the server. In other words, the server needs to be running (and therefore has an IP) before the <kbd>swarm init command</kbd> is executed.</p>
<p>Since the first node has to initialize the cluster while any other should join, we're using if statements to distinguish one case from the other. If the variable <kbd>swarm_init</kbd> is true, the docker swarm init command will be executed. On the other hand, if the variable <kbd>swarm_init</kbd> is set to false, the command will be docker swarm join. In that case, we are using another variable <kbd>swarm_manager_ip</kbd> to tell the node which manager to use to join the cluster.</p>
<p>Please note that the IP is obtained using the special syntax <kbd>self.private_ip</kbd>. We are referencing oneself and getting the <kbd>private_ip</kbd>. There are many other attributes we can get from a resource.</p>
<p>Please consult the <em>AWS_INSTANCE</em> (<a href="https://www.terraform.io/docs/providers/aws/r/instance.html">https://www.terraform.io/docs/providers/aws/r/instance.html</a><em>)</em> page for more info.</p>
<p>Let's take a look at the <kbd>aws_instance</kbd> resource named <kbd>swarm-worker</kbd>:</p>
<pre>
<strong>resource <span class="hljs-string">"aws_instance"</span> <span class="hljs-string">"swarm-worker"</span> {<br/>  count = <span class="hljs-string">"<span class="hljs-variable">${var.swarm_workers}</span>"</span><br/>  ami = <span class="hljs-string">"<span class="hljs-variable">${var.swarm_ami_id}</span>"</span><br/>  instance_<span class="hljs-built_in">type</span> = <span class="hljs-string">"<span class="hljs-variable">${var.swarm_instance_type}</span>"</span><br/>  tags {<br/>    Name = <span class="hljs-string">"swarm-worker"</span><br/>  }<br/>  vpc_security_group_ids = [<br/><span class="hljs-string">    "<span class="hljs-variable">${aws_security_group.docker.id}</span>"</span><br/>  ]<br/>  key_name = <span class="hljs-string">"devops21"</span><br/>  connection {<br/>    user = <span class="hljs-string">"ubuntu"</span><br/>    private_key = <span class="hljs-string">"<span class="hljs-variable">${file("devops21.pem")}</span>"</span><br/>  }<br/>  provisioner <span class="hljs-string">"remote-exec"</span> {<br/>    inline = [<br/><span class="hljs-string">    "docker swarm join --token <span class="hljs-variable">${var.swarm_worker_token}</span> \<br/>--advertise-addr <span class="hljs-variable">${self.private_ip}</span> <span class="hljs-variable">${var.swarm_manager_ip}</span>:2377"</span><br/>    ]<br/>  }<br/>}</strong>
</pre>
<p>The <kbd>swarm-worker</kbd> resource is almost identical to <kbd>swarm-manager</kbd>. The only difference is in the count field that uses the <kbd>swarm_workers</kbd> variable and the provisioner. Since a worker cannot initialize a cluster, there was no need for if statements, so the only command we want to execute is <kbd>docker swarm join</kbd></p>
<p>Terraform uses a naming convention that allows us to specify values as environment variables by adding the <kbd>TF_VAR_ prefix</kbd>. For example, we can specify the value of the variable <kbd>swarm_ami_id</kbd> by setting the environment variable <kbd>TF_VAR_swarm_ami_id</kbd>. The alternative is to use the <kbd>-var</kbd> argument. I prefer environment variables since they allow me to specify them once instead of adding <kbd>-var</kbd> to every command.</p>
<p>The last part of the <kbd>terraform/aws/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf</a>) specification are outputs.</p>
<p>When building potentially complex infrastructure, Terraform stores hundreds or thousands of attribute values for all resources. But, as a user, we may only be interested in a few values of importance, such as manager IPs. Outputs are a way to tell Terraform what data is relevant. This data is outputted when apply is called, and can be queried using the <kbd>terraform output</kbd> command.</p>
<p>The outputs we defined are as follows:</p>
<pre>
<strong>output <span class="hljs-string">"swarm_manager_1_public_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${aws_instance.swarm-manager.0.public_ip}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_1_private_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${aws_instance.swarm-manager.0.private_ip}</span>"</span><br/>}<br/>output <span class="hljs-string">"swarm_manager_2_public_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${aws_instance.swarm-manager.1.public_ip}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_2_private_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${aws_instance.swarm-manager.1.private_ip}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_3_public_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${aws_instance.swarm-manager.2.public_ip}</span>"</span><br/>}<br/><br/>output <span class="hljs-string">"swarm_manager_3_private_ip"</span> {<br/>  value = <span class="hljs-string">"<span class="hljs-variable">${aws_instance.swarm-manager.2.private_ip}</span>"</span><br/>}</strong>
</pre>
<p>They are public and private IPs of the managers. Since there are only a few (if any) reasons to know worker IPs, we did not define them as outputs. Please consult the <em>Output Configuration</em> (<a href="https://www.terraform.io/docs/configuration/outputs.html">https://www.terraform.io/docs/configuration/outputs.html</a>) page for more info. Since we'll use the AMI we created with Packer, we need to retrieve the ID from the <kbd>packer-ubuntu-docker.log</kbd>. The command that follows parses the output and extracts the ID:</p>
<pre>
<strong><span class="hljs-keyword">export</span> TF_VAR_swarm_ami_id=$( \<br/>    grep <span class="hljs-string">'artifact,0,id'</span> \<br/>    packer-ubuntu-docker.log \<br/>    | cut <span class="hljs-operator">-d</span>: <span class="hljs-operator">-f</span>2)</strong>
</pre>
<p>Before we create our cluster and the infrastructure around it, we should ask Terraform to show us the execution plan:</p>
<pre>
<strong>terraform plan</strong>
</pre>
<p>The result is an extensive list of resources and their properties. Since the output is too big to be printed, I'll limit the output only to the resource types and names:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>+ aws_instance.swarm-manager.0<br/><span class="hljs-keyword">...</span><br/>+ aws_instance.swarm-manager.1<br/><span class="hljs-keyword">...</span><br/>+ aws_instance.swarm-manager.2<br/><span class="hljs-keyword">...</span><br/>+ aws_instance.swarm-worker.0<br/><span class="hljs-keyword">...</span><br/>+ aws_instance.swarm-worker.1<br/><span class="hljs-keyword">...</span><br/>+ aws_security_group.docker<br/><span class="hljs-keyword">...</span><br/>Plan: <span class="hljs-number">6</span> to add, <span class="hljs-number">0</span> to change, <span class="hljs-number">0</span> to destroy.</strong>
</pre>
<p>Since this is the first execution, all the resources would be created if we were to execute terraform apply. We would get five EC2 instances; three managers and two workers. That would be accompanied by one security group.</p>
<p>If you see the complete output, you'll notice that some of the property values are set to <kbd>&lt;computed&gt;</kbd>. That means that Terraform cannot know what will be the actual values until it creates the resources. A good example are IPs. They do not exist until the EC2 instance is created.</p>
<p>We can also output the plan using the <kbd>graph</kbd> command:</p>
<pre>
<strong>terraform graph</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-title">digraph</span> {<br/>    compound = <span class="hljs-string">"true"</span><br/>    newrank = <span class="hljs-string">"true"<br/></span>    subgraph <span class="hljs-string">"root"</span> {<br/><span class="hljs-string">      "[root] aws_instance.swarm-manager"</span> [label = \<br/><span class="hljs-string">"aws_instance.swarm-manager"</span>,shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">      "[root] aws_instance.swarm-worker"</span> [label = \<br/><span class="hljs-string">"aws_instance.swarm-worker"</span>, shape= <span class="hljs-string">"box"</span>] <br/><span class="hljs-string">      "[root] aws_security_group.docker"</span> [label = \<br/><span class="hljs-string">"aws_security_group.docker"</span>, shape = <span class="hljs-string">"box"</span>]<br/><span class="hljs-string">     "[root] provider.aws"</span> [label = <span class="hljs-string">"provider.aws"</span>, shape = \<br/><span class="hljs-string">"diamond"</span>]<br/><span class="hljs-string">     "[root] aws_instance.swarm-man ager"</span> -&gt; <span class="hljs-string">"[root] \<br/>aws_security_group.docker"</span><br/><span class="hljs-string">     "[root] aws_instance.swarm-manager"</span> -&gt; <span class="hljs-string">"[root] provider.aws" \</span><br/><span class="hljs-string">     "[root] aws_instance.swarm-worker"</span> -&gt; <span class="hljs-string">"[root] \<br/>aws_security_group.docker"</span><br/><span class="hljs-string">     "[root] aws_instance.swarm-worker"</span> -&gt; <span class="hljs-string">"[root] provider.aws" \</span><br/><span class="hljs-string">     "[root] aws_security_group.docker"</span> -&gt; <span class="hljs-string">"[root] provider.aws" \</span><br/>    }<br/>}</strong>
</pre>
<p>That, in itself, is not very useful.</p>
<p>The <kbd>graph</kbd> command is used to generate a visual representation of either a configuration or an execution plan. The output is in the DOT format, which can be used by GraphViz to make graphs.</p>
<p>Please open <em>Graphviz Download</em> (<a href="http://www.graphviz.org/Download..php">http://www.graphviz.org/Download..php</a>) page and download and install the distribution compatible with your OS.</p>
<p>Now we can combine the <kbd>graph</kbd> command with dot:</p>
<pre>
<strong>terraform graph | dot -Tpng &gt; graph.png</strong>
</pre>
<p>The output should be the same as in <em>Figure 11-11:</em></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="189" src="assets/terraform-graph.png" width="401"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-11: The image generated by Graphviz from the output of the terraform graph command</div>
<p>Visualization of the plan allows us to see the dependencies between different resources. In our case, all resources will use the <kbd>aws</kbd> provider. Both instance types will depend on the security group docker.</p>
<p>When dependencies are defined, we don't need to explicitly specify all the resources we need.</p>
<p>As an example, let's take a look at the plan Terraform will generate when we limit it to only one Swarm manager node, so that we can initialize the cluster:</p>
<pre>
<strong>terraform plan \<br/>    -target aws_instance.swarm-manager \<br/>    -var swarm_init=<span class="hljs-literal">true</span> \<br/>    -var swarm_managers=<span class="hljs-number">1</span></strong>
</pre>
<p>The runtime variables <kbd>swarm_init</kbd> and <kbd>swarm_managers</kbd> will be used to tell Terraform that we want to initialize the cluster with one manager. The plan command takes those variables into account and outputs the execution plan.</p>
<p>The output, limited only to resource types and names, is as follows:</p>
<pre>
<strong><span class="hljs-addition">+ aws_instance.swarm-manager</span><br/><span class="hljs-addition">+ aws_security_group.docker</span></strong>
</pre>
<p>Even though we specified that we only want the plan for the <kbd>swarm-manager</kbd> resource, Terraform noticed that it depends on the security group <kbd>docker</kbd>, and included it in the execution plan.</p>
<p>The only thing missing before we start creating the AWS resources is to copy the SSH key <kbd>devops21.pem</kbd> to the current directory. The configuration expects it to be there:</p>
<pre>
<strong><span class="hljs-keyword">export</span> KEY_PATH=<span class="hljs-variable">$HOME</span>/.ssh/devops21.pem<br/><br/>cp <span class="hljs-variable">$KEY_PATH</span> devops21.pem</strong>
</pre>
<p>Please change the <kbd>KEY_PATH</kbd> value to the correct path before copying it.</p>
<p>We'll start small and create only one manager instance that will initialize the cluster. As we saw from the plan, it depends on the security group, so Terraform will create it as well.</p>
<pre>
<strong>terraform apply \<br/>    -target aws_instance.swarm-manager \<br/>    -var swarm_init=<span class="hljs-literal">true</span> \<br/>    -var swarm_managers=<span class="hljs-number">1</span></strong>
</pre>
<p>The output is too big to be presented in the book. If you look at it from your terminal, you'll notice that the security group is created first since <kbd>swarm-manager</kbd> depends on it. Please note that we did not specify the dependency explicitly. However, since the resource has it specified in the <kbd>vpc_security_group_ids</kbd> field, Terraform understood that it is the dependency.</p>
<p>Once the <kbd>swarm-manager</kbd> instance is created, Terraform waited until SSH access is available. After it had managed to connect to the new instance, it executed <kbd>provisioning</kbd> commands that initialized the cluster.</p>
<p>The final lines of the output are as follows:</p>
<pre>
<strong>Apply complete! Resources: <span class="hljs-number">2</span> added, <span class="hljs-number">0</span> changed, <span class="hljs-number">0</span> destroyed.<br/><br/>The state <span class="hljs-operator">of</span> your infrastructure has been saved <span class="hljs-built_in">to</span> <span class="hljs-operator">the</span> path<br/>below. This state is required <span class="hljs-built_in">to</span> modify <span class="hljs-operator">and</span> destroy your<br/>infrastructure, so keep <span class="hljs-keyword">it</span> safe. To inspect <span class="hljs-operator">the</span> complete state<br/>use <span class="hljs-operator">the</span> `terraform show` <span class="hljs-command"><span class="hljs-keyword">command</span>.</span><br/><br/>State path: terraform.tfstate<br/><br/>Outputs:<br/><br/>swarm_manager_1_private_ip = <span class="hljs-number">172.31</span><span class="hljs-number">.49</span><span class="hljs-number">.214</span><br/>swarm_manager_1_public_ip = <span class="hljs-number">52.23</span><span class="hljs-number">.252</span><span class="hljs-number">.207</span></strong>
</pre>
<p>The outputs are defined at the bottom of the <kbd>terraform/aws/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf</a>) file. Please note that not all outputs are listed but only those of the resources that were created.</p>
<p>We can use the public IP of the newly created EC2 instance and SSH into it.</p>
<p>You might be inclined to copy the IP. There's no need for that. Terraform has a command that can be used to retrieve any information we defined as the output.</p>
<p>The command that retrieves the public IP of the first, and currently the only manager is as follows:</p>
<pre>
<strong>terraform output swarm_manager_1_public_ip</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>52.23.252.207</strong>
</pre>
<p>We can leverage the <kbd>output</kbd> command to construct <kbd>SSH</kbd> commands. As an example, the command that follows will SSH into the machine and retrieve the list of Swarm nodes:</p>
<pre>
<strong>ssh -i devops21.pem \<br/>    ubuntu@$(terraform output \<br/>    swarm_manager_1_public_ip) \<br/>    docker node ls</strong>
</pre>
<p>The output is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME         STATUS AVAILABILITY MANAGER STATUS<br/>ip-<span class="hljs-number">172</span>-<span class="hljs-number">31</span>-<span class="hljs-number">49</span>-<span class="hljs-number">214</span> Ready  <span class="hljs-keyword">Active</span>       Leader</strong>
</pre>
<p>From now on, we won't be limited to a single manager node that initialized the cluster. We can create all the rest of the nodes. However, before we do that, we need to discover the <kbd>manager</kbd> and <kbd>worker</kbd> tokens. For security reasons, it is better that they are not stored anywhere, so we'll create environment variables:</p>
<pre>
<strong><span class="hljs-keyword">export</span> TF_VAR_swarm_manager_token=$(ssh \<br/>    -i devops21.pem \<br/>    ubuntu@$(terraform output \<br/>    swarm_manager_1_public_ip) \<br/>    docker swarm join-token -q manager)<br/><br/><span class="hljs-keyword">export</span> TF_VAR_swarm_worker_token=$(ssh \<br/>    -i devops21.pem \<br/>    ubuntu@$(terraform output \<br/>    swarm_manager_1_public_ip) \<br/>    docker swarm join-token -q worker)</strong>
</pre>
<p>We'll also need to set the environment variable <kbd>swarm_manager_ip</kbd>:</p>
<pre>
<strong><span class="hljs-keyword">export</span> TF_VAR_swarm_manager_ip=$(terraform \<br/>    output swarm_manager_1_private_ip)</strong>
</pre>
<p>Even though we could use <kbd>aws_instance.swarm-manager.0.private_ip</kbd> inside the <kbd>terraform/aws/swarm.tf</kbd> (<a href="https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf">https://github.com/vfarcic/cloud-provisioning/blob/master/terraform/aws/swarm.tf</a>), it is a good idea to have it defined as an environment variable. That way, if the first manager fails, we can easily change it to <kbd>swarm_manager_2_private_ip</kbd> without modifying the <kbd>tf</kbd> files.</p>
<p>Now, let us see the plan for the creation of all the missing resources:</p>
<pre>
<strong>terraform plan</strong>
</pre>
<p>The was no need to specify targets since, this time, we want to create all the resources that are missing.</p>
<p>The last line of the output is as follows:</p>
<pre>
<strong><span class="hljs-keyword">...</span><br/>Plan: <span class="hljs-number">4</span> to add, <span class="hljs-number">0</span> to change, <span class="hljs-number">0</span> to destroy.</strong>
</pre>
<p>We can see that the plan is to create four new resources. Since we already have one manager running and specified that the desired number is three, two additional managers will be created together with two workers.</p>
<p>Let's apply the execution plan:</p>
<pre>
<strong>terraform apply</strong>
</pre>
<p>The last lines of the output are as follows:</p>
<pre>
<strong>...<br/>Apply complete! Resources: <span class="hljs-number">4</span> added, <span class="hljs-number">0</span> changed, <span class="hljs-number">4</span> destroyed.<br/><br/>The state <span class="hljs-operator">of</span> your infrastructure has been saved <span class="hljs-built_in">to</span> <span class="hljs-operator">the</span> path<br/>below. This state is required <span class="hljs-built_in">to</span> modify <span class="hljs-operator">and</span> destroy your<br/>infrastructure, so keep <span class="hljs-keyword">it</span> safe. To inspect <span class="hljs-operator">the</span> complete state<br/>use <span class="hljs-operator">the</span> `terraform show` <span class="hljs-command"><span class="hljs-keyword">command</span>.</span><br/><br/>State path: terraform.tfstate<br/><br/>Outputs:<br/><br/>swarm_manager_1_private_ip = <span class="hljs-number">172.31</span><span class="hljs-number">.49</span><span class="hljs-number">.214</span><br/>swarm_manager_1_public_ip = <span class="hljs-number">52.23</span><span class="hljs-number">.252</span><span class="hljs-number">.207</span><br/>swarm_manager_2_private_ip = <span class="hljs-number">172.31</span><span class="hljs-number">.61</span><span class="hljs-number">.11</span><br/>swarm_manager_2_public_ip = <span class="hljs-number">52.90</span><span class="hljs-number">.245</span><span class="hljs-number">.134</span><br/>swarm_manager_3_private_ip = <span class="hljs-number">172.31</span><span class="hljs-number">.49</span><span class="hljs-number">.221</span><br/>swarm_manager_3_public_ip = <span class="hljs-number">54.85</span><span class="hljs-number">.49</span><span class="hljs-number">.136</span></strong>
</pre>
<p>All four resources were created, and we got the output of the manager public and private IPs.</p>
<p>Let's enter into one of the managers and confirm that the cluster indeed works:</p>
<pre>
<strong>ssh -i devops21.pem \<br/>    ubuntu@$(terraform \<br/>    output swarm_manager_1_public_ip)<br/><br/>docker node ls</strong>
</pre>
<p>The output of the <kbd>node ls</kbd> command is as follows (IDs are removed for brevity):</p>
<pre>
<strong>HOSTNAME          STATUS  AVAILABILITY  MANAGER STATUS<br/>ip-<span class="hljs-number">172</span>-<span class="hljs-number">31</span>-<span class="hljs-number">61</span>-<span class="hljs-number">11</span>   Ready   <span class="hljs-keyword">Active</span>        Reachable<br/>ip-<span class="hljs-number">172</span>-<span class="hljs-number">31</span>-<span class="hljs-number">49</span>-<span class="hljs-number">221</span>  Ready   <span class="hljs-keyword">Active</span>        Reachable<br/>ip-<span class="hljs-number">172</span>-<span class="hljs-number">31</span>-<span class="hljs-number">50</span>-<span class="hljs-number">78</span>   Ready   <span class="hljs-keyword">Active</span><br/>ip-<span class="hljs-number">172</span>-<span class="hljs-number">31</span>-<span class="hljs-number">49</span>-<span class="hljs-number">214</span>  Ready   <span class="hljs-keyword">Active</span>        Leader<br/>ip-<span class="hljs-number">172</span>-<span class="hljs-number">31</span>-<span class="hljs-number">49</span>-<span class="hljs-number">41</span>   Ready   <span class="hljs-keyword">Active</span></strong>
</pre>
<p>All the nodes are present, and the cluster seems to be working.</p>
<p>To be fully confident that everything works as expected, we'll deploy a few services. Those will be the same services we were creating throughout the book, so we'll save ourselves some time and deploy the <kbd>vfarcic/docker-flow-proxy/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml">https://github.com/vfarcic/docker-flow-proxy/blob/master/docker-compose-stack.yml</a>) and <kbd>vfarcic/go-demo/docker-compose-stack.yml</kbd> (<a href="https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.ym">https://github.com/vfarcic/go-demo/blob/master/docker-compose-stack.ym</a>l) stacks:</p>
<pre>
<strong><span class="hljs-built_in">sudo</span> docker network create --driver overlay proxy<br/><br/>curl -o proxy-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/docker-flow-proxy/master/docker-compose-stack.yml<br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c proxy-stack.yml proxy<br/><br/>curl -o go-demo-stack.yml \<br/>    https://raw.githubusercontent.com/ \<br/>vfarcic/go-demo/master/docker-compose-stack.yml<br/><br/><span class="hljs-built_in">sudo</span> docker stack deploy \<br/>    -c go-demo-stack.yml go-demo<br/><br/>docker service ls</strong>
</pre>
<p>We downloaded the script from the repository, gave it executable permissions, and executed it. At the end, we listed all the services.</p>
<p>After a while, the output of the <kbd>service ls</kbd> command should be as follows (IDs are removed for brevity):</p>
<pre>
<strong>NAME                 MODE       REPLICAS <br/>go<span class="hljs-attribute">-demo_db</span>           replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      <br/>proxy_swarm<span class="hljs-attribute">-listener</span> replicated <span class="hljs-number">1</span>/<span class="hljs-number">1</span>      <br/>proxy_proxy          replicated <span class="hljs-number">2</span>/<span class="hljs-number">2</span>      <br/>go<span class="hljs-attribute">-demo_main</span>         replicated <span class="hljs-number">3</span>/<span class="hljs-number">3</span>      <br/>-------------------------------------------------<br/>IMAGE<br/>mongo:latest<br/>vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-swarm</span><span class="hljs-attribute">-listener</span>:latest<br/>vfarcic/docker<span class="hljs-attribute">-flow</span><span class="hljs-attribute">-proxy</span>:latest<br/>vfarcic/go<span class="hljs-attribute">-demo</span>:latest<br/></strong>
</pre>
<p>Finally, let's send a request to the <kbd>go-demo</kbd> service through the <kbd>proxy</kbd>. If it returns the correct response, we'll know that everything works correctly:</p>
<pre>
<strong>curl localhost/demo/hello</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></strong>
</pre>
<p>It works!</p>
<p>Are we finished? We probably are. As a last check, let's validate that the <kbd>proxy</kbd> is accessible from outside the security group. We can confirm that by exiting the server and sending a request from our laptop:</p>
<pre>
<strong><span class="hljs-keyword">exit</span><br/><br/>curl $(terraform output \<br/>    swarm_manager_1_public_ip)/demo/hello</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-function_or_atom">hello</span>, <span class="hljs-function_or_atom">world</span><span class="hljs-exclamation_mark">!</span></strong>
</pre>
<p>Let's see what happens if we simulate a failure of an instance.</p>
<p>We'll delete an instance using AWS CLI. We could use Terraform to remove an instance. However, removing it with the AWS CLI will more closely simulate an unexpected failure of a node. To remove an instance, we need to find its ID. We can do that with the <kbd>terraform show</kbd> command. Let's say that we want to remove the second worker. The command to find all its information is as follows:</p>
<pre>
<strong>terraform state show <span class="hljs-string">"aws_instance.swarm-worker[1]"</span></strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong><span class="hljs-constant">id</span>                                        = i-6a3a1964<br/><span class="hljs-constant">ami</span>                                       = ami-02ebd915<br/><span class="hljs-constant">associate_public_ip_address</span>               = true<br/><span class="hljs-constant">availability_zone</span>                         = us-east-1b<br/><span class="hljs-constant">disable_api_termination</span>                   = false<br/><span class="hljs-constant">ebs_block_device</span>.#                        = 0<br/><span class="hljs-constant">ebs_optimized</span>                             = false<br/><span class="hljs-constant">ephemeral_block_device</span>.#                  = 0<br/><span class="hljs-constant">iam_instance_profile</span>                      =<br/><span class="hljs-constant">instance_state</span>                            = running<br/><span class="hljs-constant">instance_type</span>                             = t2.micro<br/><span class="hljs-constant">key_name</span>                                  = devops21<br/><span class="hljs-constant">monitoring</span>                                = false<br/><span class="hljs-constant">network_interface_id</span>                      = eni-322fd9cc<br/><span class="hljs-constant">private_dns</span>                               = ip-172-31-56-227.ec2.internal<br/><span class="hljs-constant">private_ip</span>                                = 172.31.56.227<br/><span class="hljs-constant">public_dns</span>                                = ec2-54-174-83-184.compute-1.amazonaws.com<br/><span class="hljs-constant">public_ip</span>                                 = 54.174.83.184<br/><span class="hljs-constant">root_block_device</span>.#                       = 1<br/>root_block_device.0.delete_on_termination = true<br/>root_block_device.0.iops                  = 100<br/>root_block_device.0.volume_size           = 8<br/>root_block_device.0.volume_type           = gp2<br/><span class="hljs-constant">security_groups</span>.#                         = 0<br/><span class="hljs-constant">source_dest_check</span>                         = true<br/><span class="hljs-constant">subnet_id</span>                                 = subnet-e71631cd<br/><span class="hljs-constant">tags</span>.%                                    = 1<br/>tags.Name                                 = swarm-worker<br/><span class="hljs-constant">tenancy</span>                                   = default<br/><span class="hljs-constant">vpc_security_group_ids</span>.#                  = 1<br/>vpc_security_group_ids.937984769          = sg-288e1555</strong>
</pre>
<p>Among other pieces of data, we got the ID. In my case, it is <kbd>i-6a3a1964</kbd>.Before running the command that follows, please change the ID to the one you got from the <kbd>terraform state show</kbd> command:</p>
<pre>
<strong>aws ec2 terminate-instances \<br/>    --instance-ids i-<span class="hljs-number">6</span>a3a1964</strong>
</pre>
<p>The output is as follows:</p>
<pre>
<strong>{<br/>    "<span class="hljs-attribute">TerminatingInstances</span>": <span class="hljs-value">[<br/>        {<br/>            "<span class="hljs-attribute">InstanceId</span>": <span class="hljs-value"><span class="hljs-string">"i-6a3a1964"</span></span>,<br/>            "<span class="hljs-attribute">CurrentState</span>": <span class="hljs-value">{<br/>                "<span class="hljs-attribute">Code</span>": <span class="hljs-value"><span class="hljs-number">32</span></span>,<br/>                "<span class="hljs-attribute">Name</span>": <span class="hljs-value"><span class="hljs-string">"shutting-down"</span><br/></span>            }</span>,<br/>            "<span class="hljs-attribute">PreviousState</span>": <span class="hljs-value">{<br/>                "<span class="hljs-attribute">Code</span>": <span class="hljs-value"><span class="hljs-number">16</span></span>,<br/>                "<span class="hljs-attribute">Name</span>": <span class="hljs-value"><span class="hljs-string">"running"</span><br/></span>            }<br/></span>        }<br/>    ]<br/></span>}</strong>
</pre>
<p>AWS changed the state of the instance from <kbd>running</kbd> to <kbd>shutting-down</kbd>.</p>
<p>Let's run the <kbd>terraform plan</kbd> command one more time:</p>
<pre>
<strong>terraform plan</strong>
</pre>
<p>The last line of the output is as follows:</p>
<pre>
<strong>Plan: <span class="hljs-number">1</span> <span class="hljs-built_in">to</span> <span class="hljs-built_in">add</span>, <span class="hljs-number">0</span> <span class="hljs-built_in">to</span> change, <span class="hljs-number">0</span> <span class="hljs-built_in">to</span> destroy.</strong>
</pre>
<p>Terraform deduced that one resource <kbd>swarm-worker.1</kbd> needs to be added to reconcile the discrepancy between the state it has stored locally and the actual state of the cluster.</p>
<p>All we have to do to restore the cluster to the desirable state is to run <kbd>terraform apply</kbd>:</p>
<pre>
<strong>terraform apply</strong>
</pre>
<p>The last lines of the output are as follows:</p>
<pre>
<strong>...<br/>Apply complete! Resources: <span class="hljs-number">1</span> added, <span class="hljs-number">0</span> changed, <span class="hljs-number">0</span> destroyed.<br/><br/>The state <span class="hljs-operator">of</span> your infrastructure has been saved <span class="hljs-built_in">to</span> <span class="hljs-operator">the</span> path<br/>below. This state is required <span class="hljs-built_in">to</span> modify <span class="hljs-operator">and</span> destroy your<br/>infrastructure, so keep <span class="hljs-keyword">it</span> safe. To inspect <span class="hljs-operator">the</span> complete state<br/>use <span class="hljs-operator">the</span> `terraform show` <span class="hljs-command"><span class="hljs-keyword">command</span>.</span><br/><br/>State path: terraform.tfstate<br/><br/>Outputs:<br/><br/>swarm_manager_1_private_ip = <span class="hljs-number">172.31</span><span class="hljs-number">.60</span><span class="hljs-number">.117</span><br/>swarm_manager_1_public_ip = <span class="hljs-number">52.91</span><span class="hljs-number">.201</span><span class="hljs-number">.148</span><br/>swarm_manager_2_private_ip = <span class="hljs-number">172.31</span><span class="hljs-number">.57</span><span class="hljs-number">.177</span><br/>swarm_manager_2_public_ip = <span class="hljs-number">54.91</span><span class="hljs-number">.90</span><span class="hljs-number">.33</span><br/>swarm_manager_3_private_ip = <span class="hljs-number">172.31</span><span class="hljs-number">.48</span><span class="hljs-number">.226</span><br/>swarm_manager_3_public_ip = <span class="hljs-number">54.209</span><span class="hljs-number">.238</span><span class="hljs-number">.50</span></strong>
</pre>
<p>We can see that one resource was added. The terminated worker has been recreated, and the cluster continues operating at its full capacity.</p>
<p>The state of the cluster is stored in the <kbd>terraform.tfstate</kbd>  file. If you are not running it always from the same computer, you might want to store that file in your repository together with the rest of configuration files. The alternative is to use <em>Remote State </em>(<a href="https://www.terraform.io/docs/state/remote/index.html">https://www.terraform.io/docs/state/remote/index.html</a>)and, for example, store it in Consul.</p>
<p>Changing the desired state of the cluster is easy as well. All we have to to is add more resources and rerun <kbd>terraform apply</kbd>.</p>
<p>We are finished with the brief introduction to Terraform for AWS.</p>
<p>The flow of the process we executed can be described through <em>figure 11-12:</em></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="243" src="assets/cloud-architecture-instances.png" width="423"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11-12: The flow of the Terraform process</div>
<p>Let's destroy what we did before we compare the different approaches we took to create and manage a Swarm cluster in AWS:</p>
<pre>
<strong>terraform destroy -force</strong>
</pre>
<p>The cluster is gone as if it never existed, saving us from unnecessary expenses.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Choosing the right tools to create and manage Swarm clusters in AWS</h1>
            </header>

            <article>
                
<p>We tried three different combinations to create a Swarm cluster in AWS. We used <em>Docker Machine</em> with the <em>AWS CLI</em>, <em>Docker for AWS</em> with a CloudFormation template, and <em>Packer</em> with <em>Terraform</em>. That is, by no means, the final list of the tools we can use. The time is limited, and I promised myself that this book will be shorter than <em>War and Peace</em> so I had to draw the line somewhere. Those three combinations are, in my opinion, the best candidates as your tools of choice. Even if you do choose something else, this chapter, hopefully, gave you an insight into the direction you might want to take.</p>
<p>Most likely you won't use all three combinations so the million dollar question is which one should it be?</p>
<p>Only you can answer that question. Now you have the practical experience that should be combined with the knowledge of what you want to accomplish. Each use case is different, and no combination would be the best fit for everyone.</p>
<p>Nevertheless, I will provide a brief overview and some of the use-cases that might be a good fit for each combination.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">To Docker Machine or not to Docker Machine?</h1>
            </header>

            <article>
                
<p>Docker Machine is the weakest solution we explored. It is based on ad-hoc commands and provides little more than a way to create an EC2 instance and install Docker Engine. It uses <em>Ubuntu 15.10</em> as the base AMI. Not only that it is old but is a temporary release. If we choose to use Ubuntu, the correct choice is <em>16.04</em>  <strong>Long Term Support</strong> (<strong>LTS</strong>).</p>
<p>Moreover, Docker Machine still does not support Swarm Mode so we need to manually open the port before executing <kbd>docker swarm init</kbd> and <kbd>docker swarm join</kbd> commands. To do that, we need to combine Docker Machine with AWS Console, AWS CLI, or CloudFormation.</p>
<p>If Docker Machine would, at least, provide the minimum setup for Swarm Mode (as it did with the old Standalone Swarm), it could be a good choice for a small cluster.</p>
<p>As it is now, the only true benefit Docker Machine provides when working with a Swarm cluster in AWS is Docker Engine installation on a remote node and the ability to use the <kbd>docker-machine env</kbd> command to make our local Docker client seamlessly communicate with the remote cluster. Docker Engine installation is simple so that alone is not enough. On the other hand, the <kbd>docker-machine env</kbd> command should not be used in a production environment. Both benefits are too weak.</p>
<p>Many of the current problems with Docker Machine can be fixed with some extra arguments (example: <kbd>--amazonec2-ami</kbd>) and in combination with other tools. However, that only diminishes the primary benefit behind Docker Machine. It was supposed to be simple and work out of the box. That was partly true before <em>Docker 1.12</em>. Now, at least in AWS, it is lagging behind.</p>
<p>Does that mean we should discard Docker Machine when working with AWS? Not always. It is still useful when we want to create an ad-hoc cluster for demo purposes or maybe experiment with some new features. Also, if you don't want to spend time learning other tools and just want something you're familiar with, Docker Machine might be the right choice. I doubt that's your case.<br/>
The fact that you reached this far in this book tells me that you do want to explore better ways of managing a cluster.</p>
<p>The final recommendation is to keep Docker Machine as the tool of choice when you want to simulate a Swarm cluster locally as we did before this chapter. There are better choices for AWS.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">To Docker for AWS or not to Docker for AWS?</h1>
            </header>

            <article>
                
<p><em>Docker for AWS</em> (<a href="https://docs.docker.com/docker-for-aws/release-notes/">https://docs.docker.com/docker-for-aws/release-notes/</a>) is opposite from Docker Machine. It is a complete solution for your Swarm cluster. While Docker Machine does not do much more than to create EC2 instances and install Docker Engine, <em>Docker for AWS</em> sets up many of the things we might have a hard time setting up ourselves. Autoscaling groups, VPCs, subnets, and ELB are only a few of the things we get with it.</p>
<p>There is almost nothing we need to do to create and manage a Swarm cluster with <em>Docker for AWS</em>. Choose how many managers and how many workers you need, click the <span class="packt_screen">Create Stack</span> button, and wait a few minutes. That's all there is to it.</p>
<p>There's even more. <em>Docker for AWS</em> comes with a new OS specifically designed to run containers.</p>
<p>Does so much praise for <em>Docker for AWS</em> inevitably mean it's the best choice? Not necessarily. It depends on your needs and the use case. If what <em>Docker for AWS</em> offers is what you need, the choice is simple. Go for it. On the other hand, if you'd like to change some of its aspects or add things that are not included, you might have a hard time. It is not easy to modify or extend it.</p>
<p>As an example, <em>Docker for AWS</em> will output all the logs to <em>Amazon CloudWatch</em> (<a href="https://aws.amazon.com/cloudwatch/">https://aws.amazon.com/cloudwatch/</a>). That's great as long as CloudWatch is where you want to have your logs. On the other hand, if you prefer the ELK stack, DataDog, or any other logging solution, you will discover that changing the default setup is not that trivial.</p>
<p>Let's see another example. What if you'd like to add persistent storage. You might mount an EFS volume on all servers, but that's not an optimum solution. You might want to experiment with RexRay or Flocker. If that's the case, you will discover that, again, it's not that simple to extend the system. You'd probably end up modifying the CloudFormation template and risk not being able to upgrade to a new <em>Docker for AWS</em> version.</p>
<p>Did I mention that <em>Docker for AWS</em> is still young? At the time of this writing, it is, more or less, stable, but it still has its problems. More than problems, it lacks some features like, for example, persistent storage. All this negativity does not mean that you should give up on <em>Docker for AWS</em>. It is a great solution that will only become better with time.</p>
<p>The final recommendation is to use <em>Docker for AWS</em> if it provides (almost) everything you need or if you do not want to start working on your solution from scratch. The biggest show stopper would be if you already have a set of requirements that need to be fulfilled no matter the tool you'll use.</p>
<p>If you decide to host your cluster in AWS and you do not want to spend time learning how all its services work, read no more. <em>Docker for AWS</em> is what you need. It saves you from learning about security groups, VPCs, elastic IPs, and a myriad of other services that you might, or might not need.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">To Terraform or not to Terraform?</h1>
            </header>

            <article>
                
<p>Terraform, when combined with Packer, is an excellent choice. HashiCorp managed to make yet another tool that changes the way we configure and provision servers.</p>
<p>Configuration management tools have as their primary objective the task of making a server always be in the desired state. If a web server stops, it will be started again. If a configuration file is changed, it will be restored. No matter what happens to a server, its desired state will be restored. Except, when there is no fix to the issue. If a hard disk fails, there's nothing configuration management can do.</p>
<p>The problem with configuration management tools is that they were designed to work with physical, not virtual servers. Why would we fix a faulty virtual server when we can create a new one in a matter of seconds? Terraform understands how cloud computing works better than anyone and embraces the idea that our servers are not pets anymore. They are cattle. It'll make sure that all your resources are available. When something is wrong on a server, it will not try to fix it. Instead, it will destroy it and create a new one based on the image we choose.</p>
<p>Does that mean that there is no place for Puppet, Chef, Ansible, and other similar tools? Are they obsolete when operating in the cloud? Some are more outdated than others. Puppet and Chef are designed to run an agent on each server continuously monitoring its state and modifying it if things go astray. There is no place for such tools when we start treating our servers as cattle. Ansible is in a bit better position since it is more useful than others as a tool designed to configure a server instead to monitor it. As such, it could be very helpful when creating images.</p>
<p>We can combine Ansible with Packer. Packer would create a new VM, Ansible would provision that VM with everything we need, and leave it to Packer to create an image out of it. If the server setup is complicated, that makes a lot of sense. The question is how complex a server setup should be? With AWS, many of the resources that would traditionally run on a server are now services. We do not set up a firewall on each server but use VPC and security group services. We don't create a lot of system users since we do not log into a machine to deploy software. Swarm does that for us. We do not install web servers and runtime dependencies anymore. They are inside containers. Is there a true benefit from using configuration management tools to install a few things into VMs that will be converted into images? More often than not, the answer is no. The few things we need can be just as easily installed and configured with a few Shell commands. Configuration management of our cattle can, and often should, be done with bash.</p>
<p>I might have been too harsh. Ansible is still a great tool if you know when to use it and for what purpose. If you prefer it over bash to install and configure a server before it becomes an image, go for it. If you try to use it to control your nodes and create AWS resources, you're on a wrong path. Terraform does that much better. If you think that it is better to provision a running node instead instantiating images that already have everything inside, you must have much more patience than I do.</p>
<p>Now that we established my bias towards tools that are designed from the ground up to work with cloud (as opposed to on-premise physical servers), you might be wondering whether to use CloudFormation instead of Terraform.</p>
<p>The major problem with CloudFormation is that it is designed to lock you into AWS. It manages Amazon services and not much more. Personally, I think that vendor lock-in is unacceptable if there is a good alternative. If you are already using AWS services to their fullest, feel free to disregard my opinion on that subject. I prefer the freedom of choice. I'm usually trying to design systems that have as little dependency on the provider as it makes sense. I'll use a service in AWS if it's truly better or easier to setup than some other. In some cases, that's true, while in many others it isn't. AWS VPCs and security groups are a good example of services that provide a lot of value. I don't see a reason not to use them, especially since they are easy to replace if I move to a different provider.<br/>
<br/>
CloudWatch would be an opposite example. ELK is a better solution than CloudWatch, it's free, and it can be ported to any provider. The same can be said, for example, for ELB. It is mostly obsolete with Docker Networking. If you need a proxy, choose HAProxy or nginx.</p>
<p>For you, the vendor lock-in argument might be irrelevant. You might have chosen AWS and will stick with it for some time to come. Fair enough. However, Terraform's ability to work with a myriad of hosting providers is not the only advantage it has.<br/>
<br/>
When compared with CloudFormation, its configuration is easier to understand, it works well with other types of resources like <em>DNSimple </em>(<a href="https://www.terraform.io/docs/providers/dnsimple/">https://www.terraform.io/docs/providers/dnsimple/</a>), and its ability to display a plan before applying it can save us from a lot of painful errors. When combined with Packer, it is, in my opinion, the best combination for managing a cloud infrastructure.</p>
<p>Let's get back to the original discussion. Should we use <em>Docker for AWS</em> or <em>Terraform</em> with <em>Packer</em>?</p>
<p>Unlike Docker Machine that was easy to reject for most cases, the dilemma whether to use <em>Terraform</em> or <em>Docker for AWS</em> is harder to resolve. It might take you a while to reach with Terraform the state where the cluster has everything you need. It is not an out-of-the-box solution. You have to write the configs yourself. If you are experienced with AWS, such a feat should not cause much trouble. On the other hand, if AWS is not your strongest virtue, it might take you quite a while to define everything.</p>
<p>Still, I would discard learning AWS as the reason to choose one over the other. Even if you go with an out-of-the-box solution like <em>Docker for AWS</em>, you should still know AWS. Otherwise, you're running a risk of failing to react to infrastructure problems when they come. Don't think that anything can save you from understanding AWS intricacies. The question is only whether you'll learn the details before or after you create your cluster.</p>
<p>The final recommendation is to use Terraform with Packer if you want to have a control of all the pieces that constitute your cluster or if you already have a set of rules that need to be followed. Be ready to spend some time tuning the configs until you reach the optimum setup. Unlike with Docker for AWS, you will not have a definition of a fully functioning cluster in an hour. If that's what you want, choose Docker for AWS. On the other hand, when you do configure Terraform to do everything you need, the result will be beautiful.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The final verdict</h1>
            </header>

            <article>
                
<p>What should we use? How do we make a decision? Fully functioning cluster made by people who know what they're doing <em>Docker for AWS</em> versus fully operational cluster made by you Terraform. Docker for AWS versus whatever you'd like to label your own solution Terraform. More things than you need (Docker for AWS) versus just the resources you want Terraform.</p>
<p>Making the choice is hard. <em>Docker for AWS</em> is still too young and might be an immature solution. Docker folks will continue developing it and it will almost certainly become much better in the not so distant future. Terraform gives you freedom at a price.</p>
<p>Personally, I will closely watch the improvements in <em>Docker for AWS</em> and reserve the right to make the verdict later. Until that time, I am slightly more inclined towards Terraform. I like building things. It's a very narrow victory that should be revisited soon.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>