<html><head></head><body>
<div class="calibre6">
<h2 id="leanpub-auto-alerting-humans" class="calibre15">Alerting Humans</h2>

<p class="calibre3">While Prometheus alerts are great by themselves, they are not very useful unless you’re planning to spend all your time in front of the <em class="calibre21">alerts</em> screen. There are much better things to stare. For example, you can watch Netflix instead. It is much more entertaining than watching Prometheus screen. However, before you start watching Netflix during your working hours, we need to find a way so that you do get notified when an alert is fired.</p>

<p class="calibre3">Before we proceed, I must stress that alerts to humans (operators and sysadmins) are the last resort. We should receive them only if the system was not capable of fixing the problem. However, in the beginning, we do not have a self-healing system. The approach we’ll take is to send each alert to a human. That’s a quick fix. From there on, we’ll try to build the system that will receive those alerts instead us. It will be per use-case. We’ll create a system that sends us all alerts and then we’ll start exploring each of the scenarios. If we can make the system accept that alert and self-heal, we’ll stop sending it to a human. On the other hand, if we cannot add that scenario into the system, it will continue alerting us. In other words, all alerts will be sent to humans except those integrated into the self-healing system we’ll build.</p>

<p class="calibre3">Where should we send alert messages? Slack is probably a good candidate to start. Even if you do not use Slack, the principles we’ll explore will be the same no matter whether your end-point will be email, HangOuts, Messenger, HipChat, SMS, or pigeon couriers. As long as that end-point has an API, we should be able to utilize it. That might be easier for some than others. Pigeon couriers might not yet have an API.</p>

<h3 id="leanpub-auto-creating-the-cluster-and-deploying-services-2" class="calibre20">Creating The Cluster And Deploying Services</h3>

<p class="calibre3">We’ll start by recreating the cluster and deploying the stacks that we used in the previous chapter.</p>

<aside class="information">
    <p class="calibre3">All the commands from this chapter are available in the <a href="https://gist.github.com/vfarcic/2cdd86977b22288313345a2ca0416fe9">06-alert-humans.sh</a> Gist.</p>

</aside>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>chmod +x scripts/dm-swarm-06.sh
<code class="lineno">2 </code>
<code class="lineno">3 </code>./scripts/dm-swarm-06.sh
<code class="lineno">4 </code>
<code class="lineno">5 </code><code class="nb">eval</code> <code class="k">$(</code>docker-machine env swarm-1<code class="k">)</code>
</pre></div>

</figure>

<p class="calibre3">We executed the <code class="calibre19">dm-swarm-06.sh</code> script which, in turn, created a Swarm cluster composed of Docker Machines, created the networks and deployed the stacks. Now we should wait a few moments until all the services in the <code class="calibre19">monitor</code> stack are up and running. Please use <code class="calibre19">docker stack ps monitor</code> command to confirm that the status of all the services in the stack is <em class="calibre21">Running</em>.</p>

<p class="calibre3">Finally, we’ll confirm that everything is deployed correctly by opening Prometheus in a browser.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor"</code>
</pre></div>

</figure>

<p class="calibre3">Now the state of our cluster is the same as it was at the end of the previous chapter and we can proceed towards deploying exporters.</p>

<h3 id="leanpub-auto-setting-up-alertmanager" class="calibre20">Setting Up Alertmanager</h3>

<p class="calibre3">Since we are already using Prometheus, it makes sense to deploy Prometheus’ companion <em class="calibre21">Alertmanager</em>. It will receive alerts, filter and forward them to the end-points we’ll define. Slack will be the first.</p>

<p class="calibre3">Alertmanager Docker image expect us to define a configuration file that defines routes, receivers, and a few other things. One possible configuration can be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="calibre19">route</code><code class="o">:</code>
<code class="lineno"> 2 </code>  <code class="calibre19">receiver</code><code class="o">:</code> <code class="s">"slack"</code>
<code class="lineno"> 3 </code>  <code class="calibre19">repeat_interval</code><code class="o">:</code> <code class="o">1</code><code class="calibre19">h</code>
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code><code class="calibre19">receivers</code><code class="o">:</code>
<code class="lineno"> 6 </code>    <code class="o">-</code> <code class="calibre19">name</code><code class="o">:</code> <code class="s">"slack"</code>
<code class="lineno"> 7 </code>      <code class="calibre19">slack_configs</code><code class="o">:</code>
<code class="lineno"> 8 </code>          <code class="o">-</code> <code class="calibre19">send_resolved</code><code class="o">:</code> <code class="k">true</code>
<code class="lineno"> 9 </code>            <code class="calibre19">text</code><code class="o">:</code> <code class="s">"Something horrible happened! Run for your lives!"</code>
<code class="lineno">10 </code>            <code class="calibre19">api_url</code><code class="o">:</code> <code class="s">"https://hooks.slack.com/services/T308SC7HD/B59ER97SS/S0Kvv\</code>
<code class="lineno">11 </code><code class="s">yStVnIt3ZWpIaLnqLCu"</code>
</pre></div>

</figure>

<p class="calibre3">The configuration defines the <code class="calibre19">route</code> with <code class="calibre19">slack</code> as the receiver of the alerts. In the <code class="calibre19">receivers</code> section, we specified that we want resolved notifications (besides alerts), creative text, and the Slack API URL. As a result, alerts will be posted to the <em class="calibre21">df-monitor-tests</em> channel in DevOps20 team slack. Please sign up through the <a href="http://slack.devops20toolkit.com/">DevOps20 Registration page</a> and make sure you join the <em class="calibre21">df-monitor-tests</em> channel. This configuration should be more than enough for demo purposes.</p>

<p class="calibre3">Please consult the <a href="https://prometheus.io/docs/alerting/configuration/">alerting documentation</a> for more information about <em class="calibre21">Alertmanager</em> configuration options.</p>

<p class="calibre3">Next, we’ll take a quick look at the <a href="https://github.com/vfarcic/docker-flow-monitor/blob/master/stacks/alert-manager-slack.yml">alert-manager-slack.yml</a> stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>version: "3"
<code class="lineno"> 2 </code>
<code class="lineno"> 3 </code>services:
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code>  alert-manager:
<code class="lineno"> 6 </code>    image: prom/alertmanager
<code class="lineno"> 7 </code>    ports:
<code class="lineno"> 8 </code>      - 9093:9093
<code class="lineno"> 9 </code>    networks:
<code class="lineno">10 </code>      - monitor
<code class="lineno">11 </code>    secrets:
<code class="lineno">12 </code>      - alert_manager_config
<code class="lineno">13 </code>    command: -config.file=/run/secrets/alert_manager_config -storage.path=/alert\
<code class="lineno">14 </code>manager
<code class="lineno">15 </code>
<code class="lineno">16 </code>networks:
<code class="lineno">17 </code>  monitor:
<code class="lineno">18 </code>    external: true
<code class="lineno">19 </code>
<code class="lineno">20 </code>secrets:
<code class="lineno">21 </code>  alert_manager_config:
<code class="lineno">22 </code>    external: true
</pre></div>

</figure>

<p class="calibre3">The stack is very straightforward. The only thing worth noting is that we are exposing port <code class="calibre19">9093</code> only for demo purposes. Later on, when we integrate it with <em class="calibre21">Docker Flow Monitor</em>, they will communicate through the <code class="calibre19">monitor</code> network without the need to expose any ports. We need the port <code class="calibre19">9093</code> to demonstrate manual triggering of alerts through <em class="calibre21">Alertmanager</em>. We’ll get rid of it later on.</p>

<p class="calibre3">If you take a look at the <code class="calibre19">command</code>, you’ll notice that it specifies the configuration file that resides in the <code class="calibre19">/run/secrets/</code> directory. It is an in-memory file system where Docker stores secrets. We defined <code class="calibre19">alert_manager_config</code> as the external secret. Please visit <a href="https://prometheus.io/docs/alerting/rules/">Alerting Rules</a> for more information.</p>

<p class="calibre3">Let’s create the secret.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">echo</code> <code class="s">'route:</code>
<code class="lineno"> 2 </code><code class="s">  receiver: "slack"</code>
<code class="lineno"> 3 </code><code class="s">  repeat_interval: 1h</code>
<code class="lineno"> 4 </code>
<code class="lineno"> 5 </code><code class="s">receivers:</code>
<code class="lineno"> 6 </code><code class="s">  - name: "slack"</code>
<code class="lineno"> 7 </code><code class="s">    slack_configs:</code>
<code class="lineno"> 8 </code><code class="s">      - send_resolved: true</code>
<code class="lineno"> 9 </code><code class="s">        text: "Something horrible happened! Run for your lives!"</code>
<code class="lineno">10 </code><code class="s">        api_url: "https://hooks.slack.com/services/T308SC7HD/B59ER97SS/S0KvvyStV\</code>
<code class="lineno">11 </code><code class="s">nIt3ZWpIaLnqLCu"</code>
<code class="lineno">12 </code><code class="s">'</code> <code class="calibre19">|</code> docker secret create alert_manager_config -
</pre></div>

</figure>

<p class="calibre3">Now that the secret with the <em class="calibre21">Alertmanager</em> configuration is created, we can deploy the <code class="calibre19">alert-manager-slack.yml</code> stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack deploy <code class="se">\</code>
<code class="lineno">2 </code>    -c stacks/alert-manager-slack.yml <code class="se">\</code>
<code class="lineno">3 </code>    alert-manager
</pre></div>

</figure>

<p class="calibre3">Please wait a few moments until the service is deployed. You can monitor the status through the <code class="calibre19">docker stack ps alert-manager</code> command.</p>

<p class="calibre3">Now we can send a manual request to the <em class="calibre21">Alertmanager</em>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>curl -H <code class="s">"Content-Type: application/json"</code> <code class="se">\</code>
<code class="lineno">2 </code>    -d <code class="s">'[{"labels":{"alertname":"My Fancy Alert"}}]'</code> <code class="se">\</code>
<code class="lineno">3 </code>    <code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code>:9093/api/v1/alerts
</pre></div>

</figure>

<p class="calibre3">Before you execute the request, please change the <em class="calibre21">My Fancy Alert</em> name to something else. That way you’ll be able to recognize your alert from those submitted by other readers.</p>

<p class="calibre3">The output should be as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>{
<code class="lineno">2 </code>  "status": "success"
<code class="lineno">3 </code>}
</pre></div>

</figure>

<p class="calibre3">Please open <em class="calibre21">df-monitor-tests</em> channel in <em class="calibre21">DevOps20</em> Slack team and observe that a new notification was posted.</p>

<p class="calibre3">Now that we confirmed that <code class="calibre19">alert-manager</code> works when triggered manually, we’ll remove the stack and deploy the version integrated with <em class="calibre21">Docker Flow Monitor</em>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker stack rm alert-manager
</pre></div>

</figure>

<p class="calibre3">We’ll deploy the <code class="calibre19">docker-flow-monitor-slack.yml</code> stack. It contains <code class="calibre19">monitor</code> and <code class="calibre19">swarm-listener</code> services we’re already familiar with and adds <code class="calibre19">alert-manager</code>. The only change to the <code class="calibre19">monitor</code> service is the addition of the environment variable <code class="calibre19">ARG_ALERTMANAGER_URL=http://alert-manager:9093</code>. It defines the address and the port of the <code class="calibre19">alert-manager</code>.</p>

<p class="calibre3">The definition of the <code class="calibre19">alert-manager</code> service is as follows.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code>  monitor:
<code class="lineno"> 2 </code>    image: vfarcic/docker-flow-monitor
<code class="lineno"> 3 </code>    environment:
<code class="lineno"> 4 </code>      ...
<code class="lineno"> 5 </code>      - ARG_ALERTMANAGER_URL=http://alert-manager:9093
<code class="lineno"> 6 </code>    ...
<code class="lineno"> 7 </code>
<code class="lineno"> 8 </code>  alert-manager:
<code class="lineno"> 9 </code>    image: prom/alertmanager
<code class="lineno">10 </code>    networks:
<code class="lineno">11 </code>      - monitor
<code class="lineno">12 </code>    secrets:
<code class="lineno">13 </code>      - alert_manager_config
<code class="lineno">14 </code>    command: -config.file=/run/secrets/alert_manager_config -storage.path=/alert\
<code class="lineno">15 </code>manager
<code class="lineno">16 </code>...
</pre></div>

</figure>

<p class="calibre3">We added the environment variable <code class="calibre19">ARG_ALERTMANAGER_URL</code> to the <code class="calibre19">monitor</code> service. Prometheus will use it as the address to which to send alerts. Since both services are connected through the same <code class="calibre19">monitor</code> network, all we had to specify is the name of the service and the internal port.</p>

<p class="calibre3">The <code class="calibre19">alert-manager</code> service is the same as the one we deployed earlier except that the ports are removed. There’s no need to publish them when services communicate through Overlay network.</p>

<p class="calibre3">Let’s deploy the new stack.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">DOMAIN</code><code class="o">=</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code> <code class="se">\</code>
<code class="lineno">2 </code>    docker stack deploy <code class="se">\</code>
<code class="lineno">3 </code>    -c stacks/docker-flow-monitor-slack.yml <code class="se">\</code>
<code class="lineno">4 </code>    monitor
</pre></div>

</figure>

<p class="calibre3">We should confirm that the <code class="calibre19">alert-manager</code> is correctly configured through the environment variable <code class="calibre19">ARG_ALERTMANAGER_URL</code>.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/flags"</code>
</pre></div>

</figure>

<p class="calibre3">As you can see from the <em class="calibre21">flags</em> screen, the <em class="calibre21">alertmanager.url</em> is now part of the Prometheus configuration. Since both are connected through the same network (<code class="calibre19">monitor</code>), the address is the name of the service.</p>


<figure class="image">
  <img src="../images/00034.jpeg" alt="Figure 6-1: Prometheus flags screen with values passed through environment variables" class="calibre17"/>
  <figcaption class="calibre18">Figure 6-1: Prometheus flags screen with values passed through environment variables</figcaption>
</figure>


<p class="calibre3">Let us generate an alert.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add com.df.alertIf<code class="o">=</code>@service_mem_limit:0.1 <code class="se">\</code>
<code class="lineno">3 </code>    go-demo_main
</pre></div>

</figure>

<p class="calibre3">We updated the <code class="calibre19">main</code> service from the <code class="calibre19">go-demo</code> stack by adding the <code class="calibre19">alertIf</code> label. It defines <code class="calibre19">mem_limit</code> alert that will be triggered if the service exceeds 10% of the memory limit. In other words, it will almost certainly fire the alert.</p>

<p class="calibre3">Let’s open the alerts screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">As you can see, the alert is red (if it isn’t, wait a few moments and refresh your screen). Since we configured <em class="calibre21">Alertmanager</em>, the alert was already sent to it and, from there, forwarded to Slack. Please open the <em class="calibre21">df-monitor-tests</em> channel in the <em class="calibre21">DevOps20</em> Slack team and observe that a new notification was posted.</p>


<figure class="image">
  <img src="../images/00035.jpeg" alt="Figure 6-2: Slack message generated by Alertmanager" class="calibre17"/>
  <figcaption class="calibre18">Figure 6-2: Slack message generated by Alertmanager</figcaption>
</figure>


<p class="calibre3">As you can see, the message is not very well defined. The title is anything but understandable, the text of the message is the same no matter which alert was fired, and the link does not lead back to Prometheus but to the internal address. We’ll fix all those problems soon. For now, the important thing is that we managed to send alert to Slack.</p>

<p class="calibre3">The flow of the events is described through figure 6-3.</p>


<figure class="image1">
  <img src="../images/00036.jpeg" alt="Figure 6-3: The flow of the events that results in a Slack message being created" class="calibre17"/>
  <figcaption class="calibre18">Figure 6-3: The flow of the events that results in a Slack message being created</figcaption>
</figure>


<p class="calibre3">We’ll restore the <code class="calibre19">go-demo</code> alert to its original state (used memory over 80%).</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add com.df.alertIf<code class="o">=</code>@service_mem_limit:0.8 <code class="se">\</code>
<code class="lineno">3 </code>    go-demo_main
</pre></div>

</figure>

<p class="calibre3">A few moments later, we can observe that the alert is green again.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">If the alert is still not green, please wait for a while and refresh the screen.</p>

<p class="calibre3">Since we specified <code class="calibre19">send_resolved: true</code> in the <code class="calibre19">alert-manager</code> config, we got another notification. This time, the message states that the issue is resolved.</p>

<p class="calibre3">The only thing left is to create your own <em class="calibre21">Alertmanager</em> configuration. You’ll need Webhook URL if you choose to send alerts to your team’s Slack. The instructions for obtaining it are as follows.</p>

<p class="calibre3">Please login to your Team Slack channel, open the settings menu by clicking the team name, and select <em class="calibre21">Apps &amp; integrations</em>.</p>


<figure class="image">
  <img src="../images/00037.jpeg" alt="Figure 6-4: Team setting Slack menu" class="calibre17"/>
  <figcaption class="calibre18">Figure 6-4: Team setting Slack menu</figcaption>
</figure>


<p class="calibre3">You will be presented with the <em class="calibre21">App Directory</em> screen. Click the <em class="calibre21">Manage</em> link located in the top-right corner of the screen followed by the <em class="calibre21">Custom Integrations</em> item in the left-hand menu. Select <em class="calibre21">Incoming WebHooks</em> and click the <em class="calibre21">Add Configuration</em> button. Choose the channel where alerts will be posted and click the <em class="calibre21">Add Incoming WebHooks integration</em> button. Copy the <em class="calibre21">Webhook URL</em>. You’ll need it when you customize the solution to your needs.</p>

<p class="calibre3">Now that you know how to get the <em class="calibre21">Webhook URL</em>, feel free to replace the one from the examples that follow. That does not mean that you cannot run them as they are. You’re free to use <em class="calibre21">DevOps20</em> team Slack if that suits you better.</p>

<h3 id="leanpub-auto-using-templates-in-alertmanager-configuration" class="calibre20">Using Templates In Alertmanager Configuration</h3>

<p class="calibre3">Defining <em class="calibre21">Alertmanager</em> configuration using static text is not very useful if we’re running more than one service. Instead, we should employ templates that will help us customize messages. While we’re at it, we can also fix the broken link from the message and customize the title.</p>

<p class="calibre3">Before we proceed, let us remove the <code class="calibre19">monitor_alert-manager</code> service and the <code class="calibre19">alert_manager_config</code> secret. That will allow us to deploy it again with better-defined messages.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service rm monitor_alert-manager
<code class="lineno">2 </code>
<code class="lineno">3 </code>docker secret rm alert_manager_config
</pre></div>

</figure>

<p class="calibre3">We’ll create a new secret with the complete <em class="calibre21">Alertmanager</em> configuration.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno"> 1 </code><code class="nb">echo</code> <code class="s">"route:</code>
<code class="lineno"> 2 </code><code class="s">  group_by: [service]</code>
<code class="lineno"> 3 </code><code class="s">  receiver: 'slack'</code>
<code class="lineno"> 4 </code><code class="s">  repeat_interval: 1h</code>
<code class="lineno"> 5 </code>
<code class="lineno"> 6 </code><code class="s">receivers:</code>
<code class="lineno"> 7 </code><code class="s">  - name: 'slack'</code>
<code class="lineno"> 8 </code><code class="s">    slack_configs:</code>
<code class="lineno"> 9 </code><code class="s">      - send_resolved: true</code>
<code class="lineno">10 </code><code class="s">        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is \</code>
<code class="lineno">11 </code><code class="s">in danger!'</code>
<code class="lineno">12 </code><code class="s">        title_link: 'http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts'</code>
<code class="lineno">13 </code><code class="s">        text: '{{ .CommonAnnotations.summary}}'</code>
<code class="lineno">14 </code><code class="s">        api_url: 'https://hooks.slack.com/services/T308SC7HD/B59ER97SS/S0KvvyStV\</code>
<code class="lineno">15 </code><code class="s">nIt3ZWpIaLnqLCu'</code>
<code class="lineno">16 </code><code class="s">"</code> <code class="calibre19">|</code> docker secret create alert_manager_config -
</pre></div>

</figure>

<p class="calibre3">Previously, we specified only <code class="calibre19">text</code> and <code class="calibre19">api_url</code> and let <em class="calibre21">Alertmanager</em> fill in the blanks. This time, we added <code class="calibre19">title</code> and <code class="calibre19">title_link</code> to the mix.</p>

<p class="calibre3">We used <code class="calibre19">{{ .GroupLabels.service }}</code> to specify the name of the service inside the <code class="calibre19">title</code>. Group labels are defined in the <code class="calibre19">route</code> section. Even though we could use “normal” labels, group labels are easier since they are unique for all the alerts coming from, in this case, the same service. The title is prefixed with the alert <code class="calibre19">status</code> in the upper case. That should give us clear indication whether the alert is fired or resolved.</p>

<p class="calibre3">The previous configuration produced a link that did not work. That was to be expected since the communication goes through internal networking. This time, we made sure that the <code class="calibre19">title_link</code> is correct and points to one of the servers in the cluster.</p>

<p class="calibre3">Finally, the <code class="calibre19">text</code> is the same as the alert summary defined as one of the alert <code class="calibre19">ANNOTATIONS</code>.</p>

<p class="calibre3">Please visit <a href="https://prometheus.io/docs/alerting/notifications/">Notification Template Reference</a> for more info about the templates that can be used when configuring Alertmanager.</p>

<p class="calibre3">If everything works as expected, the new Alertmanager config will result in clearer messages customized for each service.</p>

<p class="calibre3">Let us deploy the stack and, with it, <code class="calibre19">alert-manager</code> with the new configuration.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code><code class="nv">DOMAIN</code><code class="o">=</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code> <code class="se">\</code>
<code class="lineno">2 </code>    docker stack deploy <code class="se">\</code>
<code class="lineno">3 </code>    -c stacks/docker-flow-monitor-slack.yml <code class="se">\</code>
<code class="lineno">4 </code>    monitor
</pre></div>

</figure>

<p class="calibre3">We’ll test the alert in the same way as before by decreasing the threshold.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add com.df.alertIf<code class="o">=</code>@service_mem_limit:0.1 <code class="se">\</code>
<code class="lineno">3 </code>    go-demo_main
</pre></div>

</figure>

<p class="calibre3">A few moments later, Prometheus will change the state of the alert to pending and, a while later to firing. We can observe those changes by opening the <em class="calibre21">Alerts</em> screen.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>open <code class="s">"http://</code><code class="k">$(</code>docker-machine ip swarm-1<code class="k">)</code><code class="s">/monitor/alerts"</code>
</pre></div>

</figure>

<p class="calibre3">If you open Slack channel <em class="calibre21">#df-monitor-tests</em> you’ll notice that the message is much better this time.</p>

<p class="calibre3">The only thing left is to confirm that we’re receiving the correct message when an issue is resolved. We’ll change the alert threshold back to 80%.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker service update <code class="se">\</code>
<code class="lineno">2 </code>    --label-add com.df.alertIf<code class="o">=</code>@service_mem_limit:0.8 <code class="se">\</code>
<code class="lineno">3 </code>    go-demo_main
</pre></div>

</figure>

<p class="calibre3">After a while, Prometheus will change the alert status to resolved, send a notification to Alertmanager which, in turn, will communicate the news to Slack. The result will be the <em class="calibre21">RESOLVED</em> message.</p>


<figure class="image">
  <img src="../images/00038.jpeg" alt="Figure 6-5: Customized Slack alert messages" class="calibre17"/>
  <figcaption class="calibre18">Figure 6-5: Customized Slack alert messages</figcaption>
</figure>


<h3 id="leanpub-auto-what-now-5" class="calibre20">What Now?</h3>

<p class="calibre3">We’re done. We have a system that will alert us whenever there’s something wrong inside the cluster. The next step is to start alerting the system so that it can self-heal and leave our Slack channel only for emergencies that cannot be auto-fixed.</p>

<p class="calibre3">The next chapter will explore the options for alerting the system. For now, the time has come for both us and our laptops to take a rest.</p>

<figure class="code">
<div class="highlight"><pre class="calibre24"><code class="calibre19"/><code class="lineno">1 </code>docker-machine rm -f <code class="se">\</code>
<code class="lineno">2 </code>    swarm-1 swarm-2 swarm-3
</pre></div>

</figure>



</div>
</body></html>