- en: Chapter 5. Configuring Service Discovery and Docker Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：配置服务发现与 Docker 网络
- en: In this chapter, we will be looking at two very important topics when working
    with containers. First, we will be looking at what is service discovery, why do
    we need it, and the different types of service discovery. The second topic we
    will cover is Docker networking. There are many ways to run container networks.
    There are some great technologies out there such as the CoreOS project flannel
    ([https://coreos.com/flannel/docs/latest/](https://coreos.com/flannel/docs/latest/)).
    There is also Weave from Weave Works ([http://weave.works/](http://weave.works/)),
    but we are going to use the native Docker networking stack released in engine
    version 1.9.1.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在使用容器时的两个非常重要的话题。首先，我们将探讨什么是服务发现，为什么我们需要它，以及不同类型的服务发现。第二个话题是 Docker
    网络。运行容器网络有很多种方式。这里有一些很棒的技术，比如 CoreOS 项目的 flannel（[https://coreos.com/flannel/docs/latest/](https://coreos.com/flannel/docs/latest/)）。还有
    Weave Works 的 Weave（[http://weave.works/](http://weave.works/)），但是我们将使用 Docker
    引擎 1.9.1 版本中发布的原生 Docker 网络栈。
- en: Service discovery
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务发现
- en: This is a fairly important topic in the world of containers, when we start to
    move into multinode applications and Docker schedulers. The question is what is
    service discovery? Is it limited to containers? What are the types of service
    discovery for us to make smart design choices in our Puppet modules.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这是容器世界中一个相当重要的话题，特别是当我们开始进入多节点应用程序和 Docker 调度器时。问题是，什么是服务发现？它仅限于容器吗？在我们的 Puppet
    模块中，哪些服务发现类型可以帮助我们做出明智的设计选择？
- en: The theory
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论
- en: Service discovery is essential when we start to work with multinode applications,
    as it allows our applications to talk to each other as they move from node to
    node. So, as you can see in the world of containers, this is fairly important.
    We have a few choices when we choose a service discovery backend. The two big
    names in this space are **etcd** ([https://coreos.com/etcd/](https://coreos.com/etcd/)),
    which again is from CoreOS, and **Consul** from HashiCorp ([https://www.consul.io/](https://www.consul.io/)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始处理多节点应用程序时，服务发现至关重要，因为它使得我们的应用程序可以在节点之间移动时相互通信。因此，正如你所看到的，在容器的世界里，这个问题非常重要。当选择一个服务发现后端时，我们有几个选择。这个领域中的两个大名鼎鼎的名字是**etcd**（[https://coreos.com/etcd/](https://coreos.com/etcd/)），它同样来自
    CoreOS，以及来自 HashiCorp 的**Consul**（[https://www.consul.io/](https://www.consul.io/)）。
- en: You might remember that we have already written a `consul` module. So for this
    chapter, we are going to choose the same, as we already have the written code.
    First, let's look at the architecture of Consul so we can understand how the backend
    works, how it handles failure, and what option do we get with our configuration
    of Consul.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们已经编写了一个 `consul` 模块。因此，在本章中，我们将选择相同的模块，因为我们已经有了现成的代码。首先，让我们来看一下 Consul
    的架构，以便我们理解后端是如何工作的，它如何处理故障，以及通过配置 Consul，我们可以获得哪些选项。
- en: 'So, let''s talk about how Consul works. In Consul, we have two types of configuration
    that we can give to a server. First is a server role and the second is an agent
    role. Although the two interact, they serve different purposes. Lets dive into
    the server role first. The server''s role is to participate in the RAFT quorum;
    this is to maintain the state of the cluster. In Consul, we have the idea of data
    centers. What is a data centre, you may ask? It is a group of logical servers
    and agents. For example, if you are an AWS, a data center could be an AZ or even
    a VPC. Consul allows connectivity between data centers; it is the role of the
    sever to look after the communications between data centers. Consul uses the gossip
    protocol to achieve this. The server also holds the key/value store and replicates
    it between the servers using the serf protocol. Let''s look at a diagram of what
    we discussed:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来谈谈 Consul 是如何工作的。在 Consul 中，我们可以为服务器提供两种类型的配置。第一种是服务器角色，第二种是代理角色。虽然这两者有交互作用，但它们服务于不同的目的。让我们先深入了解服务器角色。服务器的角色是参与
    RAFT 法定人数；这就是维护集群状态的任务。在 Consul 中，我们有数据中心的概念。你可能会问，什么是数据中心？它是由一组逻辑服务器和代理组成的。例如，如果你在
    AWS 上，数据中心可以是一个可用区（AZ）或甚至一个虚拟私有云（VPC）。Consul 允许数据中心之间进行连接；服务器的角色是负责数据中心之间的通信。Consul
    使用传播协议（gossip protocol）来实现这一点。服务器还持有键/值存储，并使用 Serf 协议在服务器之间复制它。让我们来看一下我们讨论过的图示：
- en: '![The theory](img/B05201_05_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B05201_05_01.jpg)'
- en: The agent's role is to report to the server about the state of the machine and
    any health checks that may be assigned to it. Again, Consul will use the serf
    protocol to pass the communication.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的角色是向服务器报告机器的状态以及可能分配给它的健康检查。同样，Consul 将使用 serf 协议来传递通信。
- en: Now that we have an understanding of what Consul is doing behind the scenes,
    let's look at the features that it has that we can take advantage of in our Puppet
    modules. The first feature we will take advantage of is DNS service discovery.
    In the container world, this is pretty important. As our containers move from
    node to node, we need to know how to connect to them. DNS service discovery solves
    this very neatly. So, let's look at an example to understand this.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 Consul 在后台所做的工作，让我们来看一下它有哪些功能，可以在我们的 Puppet 模块中利用。我们将首先利用的功能是 DNS
    服务发现。在容器环境中，这是非常重要的。随着我们的容器在节点间迁移，我们需要知道如何连接到它们。DNS 服务发现非常巧妙地解决了这个问题。那么，让我们来看一个示例来理解这一点。
- en: 'In this example, we have a **mario** service and we have **Docker swarm cluster**
    of three nodes. When we hit the Docker API and swarm schedules the container,
    we don''t know which of the three machines **mario** will end up on. But we have
    other services that will need to find **mario** as soon as he is up. If we tell
    the other services that **mario** is actually at **mario.service.consul**, no
    matter what node the container comes up on, it will resolve **mario.service.consul**
    to the right address. Refer to the following diagram to understand this in detail:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们有一个 **mario** 服务，并且有一个包含三个节点的 **Docker swarm 集群**。当我们访问 Docker API
    并由 swarm 调度容器时，我们并不知道 **mario** 会在哪个节点上启动。但是，我们有其他服务需要在 **mario** 启动后尽快找到它。如果我们告诉其他服务，**mario**
    实际上位于 **mario.service.consul**，那么无论容器在哪个节点上启动，它都会将 **mario.service.consul** 解析到正确的地址。请参见以下图示，了解更多细节：
- en: '![The theory](img/B05201_05_02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![理论](img/B05201_05_02.jpg)'
- en: In this case, if we were to ping **mario.service.consul**, we would get **192.168.100.11**.
    Pending our scheduling configuration in swarm, if **Server b** fails, **mario.service.consul**
    could end up on **Server d**. So, the response to **mario.service.consul** would
    now come from **192.168.100.13**. This would take no human intervention and would
    be seamless to the applications. That is all the theory we will see for service
    discovery in this chapter; there is more that we will cover in the later chapters.
    Now, let's get to writing some code.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果我们 ping **mario.service.consul**，我们会得到 **192.168.100.11**。根据我们在 swarm
    中的调度配置，如果 **Server b** 失败，**mario.service.consul** 可能会转移到 **Server d**。因此，**mario.service.consul**
    的响应现在将来自 **192.168.100.13**。这一切无需人工干预，对应用程序来说是无缝的。这就是我们在本章中看到的有关服务发现的所有理论；后续章节中我们会涵盖更多内容。现在，让我们开始编写一些代码。
- en: The service discovery module
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现模块
- en: In this module, we are going to write a module that uses consul as our DNS service
    discovery backend. As we already have a consul module, we won't start from scratch
    but add new features to the existing module. We will again write the module with
    manifests and Docker Compose. So, let's start with the manifests.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本模块中，我们将编写一个使用 consul 作为 DNS 服务发现后端的模块。由于我们已经有了一个 consul 模块，因此我们不会从头开始，而是向现有模块中添加新功能。我们将再次使用清单和
    Docker Compose 来编写模块。那么，让我们从清单开始。
- en: 'Our folder structure should look like this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文件夹结构应该如下所示：
- en: '![The service discovery module](img/B05201_05_03.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_03.jpg)'
- en: 'Let''s jump straight to `install.pp`. Without making any changes, it should
    look as shown in the following screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接跳到 `install.pp`。在不做任何更改的情况下，它应该看起来像下面的截图：
- en: '![The service discovery module](img/B05201_05_04.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_04.jpg)'
- en: 'Now, we are going to add one extra container that is going to be part of the
    plumbing for our DNS service discovery solution. We will need something to register
    our containers with Consul as they spawn. For this, we will use a golang application
    called **registrator** ([https://github.com/gliderlabs/registrator](https://github.com/gliderlabs/registrator)).
    This is a fantastic app. I have been using it for over a year, and it has been
    faultless. So, let''s make changes to our `params.pp` file to allow the new container.
    At the moment, `params.pp` looks like the one shown in the following screenshot:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要添加一个额外的容器，它将作为我们 DNS 服务发现解决方案的一部分。我们需要一个东西来在容器启动时将其注册到 Consul。为此，我们将使用一个名为
    **registrator** 的 Golang 应用程序（[https://github.com/gliderlabs/registrator](https://github.com/gliderlabs/registrator)）。这是一个非常棒的应用，我已经使用它超过一年，效果非常稳定。所以，让我们修改
    `params.pp` 文件，允许新容器的加入。目前，`params.pp` 文件的内容如下所示：
- en: '![The service discovery module](img/B05201_05_05.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_05.jpg)'
- en: 'The first thing that we will do is make changes to the `docker_image` and `container_hostname`
    parameters. As we already have the convention of `consul_xxx`, we can carry on
    with that:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是修改 `docker_image` 和 `container_hostname` 参数。由于我们已经使用了 `consul_xxx`
    的命名惯例，所以可以继续使用：
- en: '![The service discovery module](img/B05201_05_06.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_06.jpg)'
- en: 'Now, let''s add the parameters for registrator:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来添加 registrator 的参数：
- en: '![The service discovery module](img/B05201_05_07.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_07.jpg)'
- en: As you can see, we have added the parameter for the image as `$reg_docker_image
    = 'gliderlabs/registrator'` and the parameter for the hostname as `$reg_container_hostname
    = 'registrator'`. We have told the container to listen to the host's `$reg_net
    = 'host'` network. The next parameter will need some explaining. The registrator
    maps the Unix socket that the Docker daemon is bound to into its Unix socket.
    It does this to listen to any new services that get spawned and need to be registered
    in consul for discovery. As you can see, we do this with `$reg_volume = ['/var/run/docker.sock:/tmp/docker.sock']`.
    The last parameter tells registrator where to find `consul`. We are going to set
    that with `$reg_command = "consul://$::ipaddress_enp0s8:8500"`. Now, let's move
    over to our `init.pp` file.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经为镜像添加了 `$reg_docker_image = 'gliderlabs/registrator'` 参数，并为主机名添加了 `$reg_container_hostname
    = 'registrator'` 参数。我们告诉容器监听主机的 `$reg_net = 'host'` 网络。接下来的参数需要一些解释。registrator
    将 Docker 守护进程绑定的 Unix 套接字映射到它自己的 Unix 套接字。这是为了监听任何新启动的服务，并将其注册到 Consul 以便进行发现。如你所见，我们使用
    `$reg_volume = ['/var/run/docker.sock:/tmp/docker.sock']` 来实现这一点。最后一个参数告诉 registrator
    如何找到 `consul`。我们将通过 `$reg_command = "consul://$::ipaddress_enp0s8:8500"` 来设置这个参数。现在，让我们转到
    `init.pp` 文件。
- en: 'Our `init.pp` file should look as shown in the following screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `init.pp` 文件应该如下所示：
- en: '![The service discovery module](img/B05201_05_08.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_08.jpg)'
- en: 'Let''s add our new parameters, as shown in the following screenshot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加新的参数，如下图所示：
- en: '![The service discovery module](img/B05201_05_09.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_09.jpg)'
- en: 'Now that we have all our parameters set up, we can go to our `install.pp` file
    to add our code in order to install registrator:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了所有参数，可以进入 `install.pp` 文件，添加代码来安装 registrator：
- en: '![The service discovery module](img/B05201_05_10.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_10.jpg)'
- en: 'As you can see in the preceding screenshot, we have added a new block of code
    at the bottom of our file. It''s similar to our code that configures Consul; however,
    there are a few different parameters. We covered those earlier, so let''s not
    repeat ourselves. Now that we''ve made a fair chunk of changes to our module,
    we should run it in Vagrant to check whether we have any issues. Before we can
    run Vagrant, we need to change our `servers.yaml` file in the root of our Vagrant
    repo so that it allows us to hit the Consul URL on port `8500`. We do this with
    the following change to the code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面截图所示，我们在文件底部添加了一段新的代码块。这与我们配置 Consul 的代码类似；不过，有一些不同的参数。我们之前已经讲解过这些内容，所以不再重复。既然我们对模块做了一些修改，我们应该在
    Vagrant 中运行它，检查是否存在任何问题。在我们运行 Vagrant 之前，我们需要更改 Vagrant 仓库根目录下的 `servers.yaml`
    文件，以便让我们可以访问 Consul 的 `8500` 端口。我们可以通过以下代码更改来实现这一点：
- en: '![The service discovery module](img/B05201_05_11.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_11.jpg)'
- en: 'Now, let''s open our terminal and change the directory to the root of our Vagrant
    repo. From there, we will just issue the `vagrant up` command. The output from
    our terminal should look as shown in the following screenshot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开终端并将目录切换到 Vagrant 仓库的根目录。从那里，我们只需要执行 `vagrant up` 命令。终端的输出应如下面的截图所示：
- en: '![The service discovery module](img/B05201_05_12.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_12.jpg)'
- en: 'After this, let''s open our browser and go to `127.0.0.1:8500`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，让我们打开浏览器，访问 `127.0.0.1:8500`：
- en: '![The service discovery module](img/B05201_05_13.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_13.jpg)'
- en: You will notice now that there are a lot more services listed in the Consul
    web UI than when we ran the module in the last chapter. This is because now, registrator
    is listening on the Unix socket, and any container with a port mapped to the host
    will be registered. So the good news is that our module is working. Now, let's
    add an application to the module.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，现在在 Consul 网页界面中列出了更多的服务，而不是我们在上一章运行模块时看到的那些。这是因为现在，registrator 正在监听 Unix
    套接字，并且任何与主机端口映射的容器都会被注册。所以，好消息是我们的模块正在工作。接下来，让我们往模块中添加一个应用程序。
- en: 'The easiest way to do this is to add another container module to our node.
    So, let''s add our `bitbucket` module. We do this by adding the class to our `default.pp`
    file that lives in our `manifests` directory:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是向我们的节点添加另一个容器模块。接下来，让我们添加我们的 `bitbucket` 模块。我们通过向 `manifests` 目录中的 `default.pp`
    文件添加类来实现：
- en: '![The service discovery module](img/B05201_05_14.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_14.jpg)'
- en: 'We will also need to make some quick modifications to the `bitbucket` module
    so that we don''t get duplicate declaration errors. Note that this is not something
    you would do in production. But it is good enough for our test lab. We need to
    comment out the top block of code as shown in the following screenshot:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要对 `bitbucket` 模块进行一些快速修改，以避免重复声明错误。请注意，这不是你在生产环境中会做的事。但对我们测试实验室来说，这已经足够了。我们需要注释掉如下面截图所示的顶部代码块：
- en: '![The service discovery module](img/B05201_05_15.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_15.jpg)'
- en: 'We can even comment out the code as shown in the following screenshot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以注释掉如下面截图所示的代码：
- en: '![The service discovery module](img/B05201_05_16.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_16.jpg)'
- en: This depends on whether you used the `manifest` module or the `compose` module.
    I used the `compose` module.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于你是否使用了 `manifest` 模块或 `compose` 模块。我使用的是 `compose` 模块。
- en: 'So, let''s go back to our terminal in the root of our Vagrant repo and issue
    the `vagrant provision` command. The output of the terminal should look as shown
    in the following screenshot:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们回到终端并在 Vagrant 仓库的根目录下执行 `vagrant provision` 命令。终端的输出应如下面的截图所示：
- en: '![The service discovery module](img/B05201_05_17.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_17.jpg)'
- en: 'Now, let''s look at our browser again. We can see that our `bitbucket` services
    have been registered, as shown in this screenshot:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再看看浏览器。我们可以看到我们的 `bitbucket` 服务已经注册，如下图所示：
- en: '![The service discovery module](img/B05201_05_18.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_18.jpg)'
- en: 'We have the service discovery working; however, we still need to add another
    class to our module for DNS service discovery. So, let''s go back to our `consul`
    module. We will add a new file called `package.pp`. In this file, we will install
    the bind package and add two templates, one to configure `named.conf` and the
    other to configure `consul.conf` in the directory named `/etc/`. Let''s start
    coding. The first thing we will need to do is create our `package.pp` file in
    the `manifests` directory of our module:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使服务发现功能正常工作；但是，我们仍然需要在我们的模块中添加另一个类，用于 DNS 服务发现。接下来，让我们回到我们的 `consul` 模块。我们将添加一个名为
    `package.pp` 的新文件。在这个文件中，我们将安装 bind 包并添加两个模板，一个用于配置 `named.conf`，另一个用于配置 `/etc/`
    目录下的 `consul.conf`。让我们开始编码。首先，我们需要做的是在模块的 `manifests` 目录中创建我们的 `package.pp` 文件：
- en: '![The service discovery module](img/B05201_05_19.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_19.jpg)'
- en: 'We will add the following code to the file:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向文件中添加以下代码：
- en: '![The service discovery module](img/B05201_05_20.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_20.jpg)'
- en: 'Now, let''s create a `templates` folder. In this example, we are not parameterizing
    the files, and in a production instance, you would. That''s why we are using the
    `templates` folder and not files:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个 `templates` 文件夹。在这个示例中，我们没有对文件进行参数化，而在生产环境中你会这么做。这就是我们使用 `templates`
    文件夹而不是文件的原因：
- en: '![The service discovery module](img/B05201_05_21.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_21.jpg)'
- en: 'Now, let''s create a file called `named.conf.erb` and add the following code
    to it:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个名为`named.conf.erb`的文件，并添加以下代码：
- en: '![The service discovery module](img/B05201_05_22.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_22.jpg)'
- en: 'The code is just setting our DNS resolver to listen to `127.0.0.1`. Remember
    that we have set port forwarding on our Consul container to forward port `53`.
    That is how the host will connect to the container. Lastly, it will call our next
    template file `/etc/named/consul.conf`. Let''s create that now:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码仅仅是将我们的DNS解析器设置为监听`127.0.0.1`。记住，我们已经在Consul容器上设置了端口转发，将端口`53`转发。这就是主机如何连接到容器的方式。最后，它会调用我们下一个模板文件`/etc/named/consul.conf`。让我们现在创建它：
- en: '![The service discovery module](img/B05201_05_23.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_23.jpg)'
- en: 'The code that we will add is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将添加的代码如下：
- en: '![The service discovery module](img/B05201_05_24.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_24.jpg)'
- en: 'You will note that we are forwarding port `8600`, which is the port that Consul
    uses for its DNS traffic, and removing port `53`. As TCP bind will use port `53`,
    we will forward the request to `8600`, as shown in the following piece of code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们正在转发端口`8600`，这是Consul用于DNS流量的端口，并且移除了端口`53`。由于TCP绑定将使用端口`53`，我们将请求转发到`8600`，如下面的代码所示：
- en: '![The service discovery module](img/B05201_05_25.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_25.jpg)'
- en: 'We need to make one more change before we can run Puppet. We need to add the
    new code of `package.pp` to our `init.pp` file. We can do so like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行Puppet之前，我们需要再做一次更改。我们需要将`package.pp`的新代码添加到`init.pp`文件中。我们可以这样做：
- en: '![The service discovery module](img/B05201_05_26.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_26.jpg)'
- en: 'Now, we can run our module. Let''s go to the terminal and change to root of
    our Vagrant repo. We will issue the `vagrant up` command and if you already have
    a box running, just issue the `vagrant destroy -f && vagrant up` command. Now,
    let''s check the web UI (`127.0.0.1:8500`):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行我们的模块了。让我们打开终端并切换到我们的Vagrant仓库的根目录。我们将输入`vagrant up`命令，如果你已经有一个盒子在运行，只需输入`vagrant
    destroy -f && vagrant up`命令。现在，让我们检查Web UI（`127.0.0.1:8500`）：
- en: '![The service discovery module](img/B05201_05_28.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_28.jpg)'
- en: 'As you can see in the preceding screenshot, we have a new service registered
    on port `8600` (`consul-8600`). Now, we need to make sure that our machines are
    listening to the right DNS servers on their interfaces. We are going to do this
    in `servers.yaml`, as I would usually add this configuration to my user data in
    AWS. You could very well control this with Puppet. So, in future, you can decide
    the right place for the configuration of your environment. The line we are going
    to add is `- { shell: ''echo -e "PEERDNS=no\nDNS1=127.0.0.1\nDNS2=8.8.8.8">>/etc/sysconfig/network-scripts/ifcfg-enp0s3
    && systemctl restart network''}`. We will add it as shown in the following screenshot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '正如你在上面的截图中看到的，我们在端口`8600`上注册了一个新服务（`consul-8600`）。现在，我们需要确保我们的机器在它们的接口上监听正确的DNS服务器。我们将通过`servers.yaml`来完成这一操作，正如我通常会将此配置添加到AWS的用户数据中。你也可以使用Puppet来控制此配置。因此，未来你可以根据自己的环境决定配置的位置。我们要添加的行是`-
    { shell: ''echo -e "PEERDNS=no\nDNS1=127.0.0.1\nDNS2=8.8.8.8">>/etc/sysconfig/network-scripts/ifcfg-enp0s3
    && systemctl restart network''}`。我们将按照以下截图所示的方式添加：'
- en: '![The service discovery module](img/B05201_05_29.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_29.jpg)'
- en: 'Now, let''s go to our terminal and issue the `vagrant up` command. If you have
    a box already running then issue the `vagrant destroy -f && vagrant up` command.
    The terminal output should look like the one shown in the following screenshot:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开终端并输入`vagrant up`命令。如果你已经有一个盒子在运行，则输入`vagrant destroy -f && vagrant
    up`命令。终端输出应该如下所示：
- en: '![The service discovery module](img/B05201_05_30.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_30.jpg)'
- en: 'We can then log in to our vagrant box using `vagrant ssh` and test whether
    our DNS setup works. We can do this by selecting a service and trying to ping
    it. We are going to choose our `ping bitbucket-server-7990` service by entering
    the `ping bitbucket-server-7990.service.consul` command, and we should get the
    following results:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`vagrant ssh`登录到我们的vagrant盒子，测试我们的DNS设置是否有效。我们可以通过选择一个服务并尝试ping它来进行测试。我们将选择`ping
    bitbucket-server-7990`服务，输入`ping bitbucket-server-7990.service.consul`命令，应该得到如下结果：
- en: '![The service discovery module](img/B05201_05_31.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_31.jpg)'
- en: As you can see in the preceding screenshot, it returns the echo response as
    the loopback, as the service is running locally on this host. If we were external
    of the host, it would return the IP of the host that is running the service. Now,
    we run our container schedulers, such as Docker swarm, that have multiple host.
    We now know how service discovery works.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，它返回的是回环的 echo 响应，因为该服务在本地主机上运行。如果我们在主机外部，它将返回运行服务的主机 IP 地址。现在，我们运行容器调度器，如
    Docker Swarm，这些调度器有多个主机。我们现在了解了服务发现的工作原理。
- en: Now, let's have a look at what this would look like using Docker Compose.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下使用 Docker Compose 的情况。
- en: 'In order to not repeat ourselves, let''s make our `init.pp` file the same as
    the module that uses the `manifests` method. We have to make one small change
    to the `params.pp` file; `docker-compose` expects that you pass it strings. So,
    we need to remove the brackets around `$reg_volume` as shown in the following
    screenshot:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免重复，让我们将`init.pp`文件做成与使用`manifests`方法的模块一样。我们需要对`params.pp`文件进行一个小改动；`docker-compose`期望你传递字符串。所以，我们需要删除
    `$reg_volume` 周围的括号，如下图所示：
- en: '![The service discovery module](img/B05201_05_32.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_32.jpg)'
- en: 'Then, we will add our `package.pp` file as we did earlier and also create the
    two templates for our `bind` config. Then, we need to update our `docker-compose.yml.erb`
    file in our `templates` directory. We need to add our second container, `regisrator`.
    We are going to use the same parameters as we did in the manifest module earlier
    in this chapter. The code for this should look as shown in the following screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将像之前一样添加`package.pp`文件，并为我们的`bind`配置创建两个模板。接下来，我们需要更新 `templates` 目录中的
    `docker-compose.yml.erb` 文件。我们需要添加第二个容器`registrator`。我们将使用本章之前模块中的相同参数。此代码应如下图所示：
- en: '![The service discovery module](img/B05201_05_33.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_33.jpg)'
- en: 'You will also note that we changed the ports on our Consul container as we
    did earlier in the chapter (we removed port `53` and added `8600 tcp/udp`). Now,
    we can go to our terminal, change to root of our Vagrant repo, and issue the `vagrant
    up` command. Our terminal should look like the one shown in the following screenshot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到，我们更改了 Consul 容器的端口，就像本章前面所做的一样（我们删除了端口`53`并添加了`8600 tcp/udp`）。现在，我们可以进入终端，切换到
    Vagrant 仓库的根目录，并执行`vagrant up`命令。我们的终端应显示如下图所示：
- en: '![The service discovery module](img/B05201_05_34.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_34.jpg)'
- en: 'Again, we can also check our browser at `127.0.0.1:8500`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也可以在浏览器中查看 `127.0.0.1:8500`：
- en: '![The service discovery module](img/B05201_05_35.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_35.jpg)'
- en: As you can see in the preceding screenshot, it looks the same as it did earlier
    in the chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，它看起来和本章前面部分一样。
- en: 'Let''s log in to our box and test our DNS service discovery. For this, enter
    the `vagrant ssh` command and then ping a service. This time, we will choose something
    different. We will use the `ping consul-8500.service.consul` command. We should
    get the following response after this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们登录到我们的机器并测试我们的 DNS 服务发现。为此，输入`vagrant ssh`命令，然后 ping 一个服务。这一次，我们选择不同的服务。我们将使用`ping
    consul-8500.service.consul`命令。运行后，我们应该会得到以下响应：
- en: '![The service discovery module](img/B05201_05_36.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![服务发现模块](img/B05201_05_36.jpg)'
- en: So that's all for service discovery in this chapter. We will be picking it up
    again in the container scheduler chapter.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本章关于服务发现的内容就到这里。我们将在容器调度器章节中再次讨论它。
- en: Docker networking
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 网络
- en: In this topic, we are going to look at the native networking stack that comes
    with Docker Engine. There is a wealth of knowledge that you can achieve by reading
    on this subject. I strongly suggest that you do, as there is a lot you can do
    with Docker networking. If you have not used it before, I would suggest that you
    start reading the guide at [https://docs.docker.com/engine/userguide/networking/dockernetworks/](https://docs.docker.com/engine/userguide/networking/dockernetworks/).
    From here, you can read about the different types of drivers, how to use VXLAN
    to separate your networks, and the best practices when designing your Docker network.
    We are going to cover the basics now and the more advanced features in later chapters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍Docker引擎自带的本地网络堆栈。通过阅读这一主题，你将能获得大量有价值的知识。我强烈建议你这么做，因为在Docker网络方面有很多值得探索的内容。如果你以前没有使用过Docker网络，建议你从[https://docs.docker.com/engine/userguide/networking/dockernetworks/](https://docs.docker.com/engine/userguide/networking/dockernetworks/)开始阅读。在这里，你可以了解不同类型的驱动程序，如何使用VXLAN来隔离网络，以及设计Docker网络时的最佳实践。我们现在将介绍基础内容，更多高级功能将在后续章节中讨论。
- en: The prerequisites
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前提条件
- en: Before we can even start to code for our network, there are a few things we
    need. First, we need a key/value store. Docker will use this to map all the containers,
    IP addresses, and vxlans that are created. Seeing as there usually would be more
    than one host attached to a network, the key/value store is usually distributed
    to give it resiliency against failure. Luckily enough, we have already built a
    key/value store that we can take advantage of, it's Consul of course. The other
    configuration that you will need are extra args when we start our Docker Engine.
    This is to let Docker Engine know how to access the key/value store. These are
    the basic prerequisites that we need to get coding.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始为网络编写代码之前，有几件事情是必须的。首先，我们需要一个键值存储。Docker将使用这个存储来映射所有创建的容器、IP地址和vxlans。由于通常会有多个主机连接到一个网络，键值存储通常是分布式的，以增强其容错能力。幸运的是，我们已经建立了一个可以利用的键值存储，它当然就是Consul。你还需要的另一个配置是在启动Docker引擎时传递的额外参数。这是为了让Docker引擎知道如何访问键值存储。这些是我们开始编码所需要的基本前提条件。
- en: The code
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'Let''s create our first Docker network. To do this, we are going to add to
    our `consul` module. I am not going to do this twice for both manifests and `docker-compose`,
    as the configuration can be ported between the two. I am going to use the `docker-compose`
    module for my example. If this is the first time that you are creating a Docker
    network, it would be a worth while exercise to port the configuration to both.
    So, lets'' start. We are only going to make changes to our `install.pp` file.
    The first change that we are going to make is to our extra arguments for our `docker-engine`
    daemon. We do this by adding the code shown in the following screenshot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建第一个Docker网络。为此，我们将添加到我们的`consul`模块。我不会为`manifests`和`docker-compose`两者重复操作，因为配置可以在两者之间移植。我将使用`docker-compose`模块作为示例。如果这是你第一次创建Docker网络，尝试将配置移植到两者之间将是一次有价值的练习。那么，让我们开始吧。我们只会对我们的`install.pp`文件进行更改。我们要做的第一项更改是为`docker-engine`守护进程添加额外的参数。我们通过添加以下截图所示的代码来实现：
- en: '![The code](img/B05201_05_37.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_05_37.jpg)'
- en: The code sets our key/value store's address and port. Then, it also tells other
    machines what interface and port we are advertising our networks on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码设置了我们的键值存储的地址和端口。然后，它还告诉其他机器我们在什么接口和端口上发布我们的网络。
- en: 'The next code we are going to add will create the network. We will create a
    new file called `network.pp`. Then, we will add the code shown in the following
    screenshot to it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要添加的代码将创建网络。我们将创建一个名为`network.pp`的新文件。然后，我们将向其中添加以下截图所示的代码：
- en: '![The code](img/B05201_05_38.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_05_38.jpg)'
- en: 'The next thing we will have to do is make sure that our classes get installed
    in the correct order, as the Docker network is dependent on Consul being there.
    If Consul is not there, our catalogue will fail. So, we need to use the `contain`
    functionality built into Puppet. We do this by adding the code shown in the following
    screenshot:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要确保我们的类按照正确的顺序安装，因为Docker网络依赖于Consul的存在。如果没有Consul，我们的目录将无法正常工作。因此，我们需要使用Puppet内置的`contain`功能。我们通过添加以下截图所示的代码来实现：
- en: '![The code](img/B05201_05_39.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![代码](img/B05201_05_39.jpg)'
- en: 'As you can see, we are just setting up a basic network. We could set things
    such as IP address range, gateway, and so on. If we do that, it would look like
    this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你看到的，我们只是在设置一个基本的网络。我们可以设置 IP 地址范围、网关等。如果我们这么做，结果将是这样的：
- en: '![The code](img/B05201_05_40.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![The code](img/B05201_05_40.jpg)'
- en: 'Now that we have our code, let''s go to our terminal and issue the `vagrant
    up` command from root of our Vagrant repo. Our terminal output should look like
    the one shown in the following screenshot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了代码，让我们回到终端，在 Vagrant 仓库根目录下执行 `vagrant up` 命令。我们的终端输出应该像下面的截图一样：
- en: '![The code](img/B05201_05_41.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![The code](img/B05201_05_41.jpg)'
- en: 'Now, we can check to make sure that our network is there by logging in to our
    vagrant box (`vagrant ssh` from the root of our Vagrant repo). Once we log in
    to our box, we need to change to root (`sudo -i`) and then issue the `docker network
    ls` command. This will list the available networks on the box. The one we are
    looking for is `docker-internal` with the `overlay` driver:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过登录到我们的 Vagrant 虚拟机（在 Vagrant 仓库根目录下执行 `vagrant ssh`）来检查我们的网络是否存在。一旦登录到虚拟机，我们需要切换到
    root 用户（执行 `sudo -i`），然后发出 `docker network ls` 命令。这将列出虚拟机上可用的网络。我们要找的是使用 `overlay`
    驱动的 `docker-internal` 网络：
- en: '![The code](img/B05201_05_42.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![The code](img/B05201_05_42.jpg)'
- en: As you can see from the output of our terminal, we were successful and our network
    has been configured. That is all we are going to do with networking in this chapter.
    In the next chapter, we will be attaching containers and spanning our Docker network
    across multiple hosts.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从终端输出中看到的那样，我们成功了，网络已经配置好。这就是本章中关于网络配置的所有内容。在下一章中，我们将连接容器，并将我们的 Docker 网络扩展到多个主机。
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you learned a lot about how the container ecosystem handles
    service discovery. I can't emphasize on how important it will be to understand
    this topic when you start using containers at scale. I would really suggest that
    you get a solid understanding of service discovery before moving on to further
    chapters. We also covered the basics of Docker networking. Don't worry, as in
    the next chapter, we will go into Docker networking in more depth as we will be
    building multihost applications.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你学习了容器生态系统如何处理服务发现。我要强调的是，当你开始大规模使用容器时，理解这个话题将变得极为重要。在继续后续章节之前，我强烈建议你对服务发现有一个扎实的理解。我们还讨论了
    Docker 网络的基础知识。别担心，在下一章中，我们将深入探讨 Docker 网络，因为我们将构建多主机应用。
