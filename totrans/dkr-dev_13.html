<html><head></head><body>
		<div><h1 id="_idParaDest-244"><em class="italic"><a id="_idTextAnchor261"/>Chapter 11</em>: Scaling and Load Testing Docker Applications</h1>
			<p>Technology giants such as Google, Facebook, Lyft, and Amazon use container orchestration systems in part so that they can run their massive computing resources at very high levels of utilization. To do that, you must have a way to scale your applications across a fleet of servers, which might be dynamically allocated from a cloud provider. Even if you have a cluster that can scale out with high traffic and scale back in when demand subsides, you may still need additional tools to make sure it operates correctly. You also need to ensure that the service degrades gracefully if capacity limits are exceeded.</p>
			<p>You can use a service mesh such as Envoy, Istio, or Linkerd to handle those concerns. Envoy is one of the simpler options in the service mesh arena; it provides both load balancing and advanced traffic routing and filtering capabilities. All these capabilities provide the glue needed to serve traffic to demanding users. Some of the more complex service meshes use Envoy as a building block since it is so flexible.</p>
			<p>To prove that the scaling strategy works, you need to perform load testing. To do this, we will use k6.io, a cloud-native load testing and API testing tool.</p>
			<p>In this chapter, you are going to learn how to use the Horizontal Pod Autoscaler, the Vertical Pod Autoscaler, and the Cluster Autoscaler to configure your Kubernetes cluster so that it scales out. You will learn about Envoy and why you might use it to provide a proxy layer and service mesh on top of Kubernetes. This includes how to create an Envoy service mesh on top of a Kubernetes cluster, as well as how to configure it with a circuit breaker. Then, you will learn how to verify that the service mesh and autoscaler mechanisms are working as expected. Finally, you will learn how to run a load test with k6.io and observe how the service fails when subjected to a stress test.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Scaling your Kubernetes cluster</li>
				<li>What is Envoy, and why might I need it?</li>
				<li>Testing scalability and performance with k6</li>
			</ul>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor262"/>Technical requirements</h1>
			<p>You will need to have both a local Kubernetes learning environment and a working Kubernetes cluster in the cloud, as set up in <a href="B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying Docker Apps to Kubernetes</em>. You will also need to have a current version of the AWS CLI, as well as <code>kubectl</code> and <code>helm</code> 3.x installed on your local workstation, as described in the previous chapter. The Helm commands in this chapter use <code>helm</code> 3.x syntax. </p>
			<p>For your local Kubernetes learning environment, you should have a working NGINX Ingress Controller configured, which you can install by running the <code>chapter11/bin/</code><a href="http://deploy-nginx-ingress.sh">deploy-nginx-ingress.sh</a> script. You should also have a local Jaeger operator, which you can install by running the <code>chapter11/bin/deploy-jaeger.sh</code> script.</p>
			<p>For the cloud-hosted cluster, you can reuse the AWS <code>eksctl</code>. The EKS cluster must have a working ALB Ingress Controller set up. You should also have an <code>deploy-jaeger.sh</code> script against your cloud cluster as well.</p>
			<p>Check out the following video to see the Code in Action:</p>
			<p><a href="https://bit.ly/2CwdZeo">https://bit.ly/2CwdZeo</a></p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor263"/>Using the updated ShipIt Clicker v8</h2>
			<p>We will <a id="_idIndexMarker846"/>use the version of ShipIt Clicker provided in the <code>chapter11</code> directory of the following GitHub repository: <a href="https://github.com/PacktPublishing/Docker-for-Developers/">https://github.com/PacktPublishing/Docker-for-Developers/</a>.</p>
			<p>This version of the application you use, similar to what we did in the previous chapter, depends on an externally installed version of Redis from the <code>bitnami/redis</code> Helm Charts when used in Kubernetes. </p>
			<h3>Understanding the differences from the previous version of ShipIt Clicker</h3>
			<p>In each chapter, we have made enhancements to ShipIt Clicker to illustrate changes related to the <a id="_idIndexMarker847"/>chapter content, as well as to polish the application the same way we would do as part of a production release process.</p>
			<p>This version of ShipIt Clicker is similar to the one provided in the previous chapter, but it has one more API endpoint called <code>/faults/spin</code> that's used as a part of a <em class="italic">fault injection</em> testing strategy to induce CPU load on the nodes running the application, in order to test cluster autoscaling strategies. The <code>spin</code> endpoint will get slower the more frequently it is called but will recover and get faster if calls subside. This simulates the way that an application with poor performance behaves, without having to devise a complicated real set of poorly performing code and database servers. It provides an artificial CPU load that is convenient for testing CPU-based autoscaling. See the code in <code>chapter11/src/server/common/spin.js</code> and <code>chapter11/src/server/controllers/faults/controller.js</code> to see how this works.</p>
			<p>This version of ShipIt Clicker also has an enhancement related to Prometheus metrics: it exposes these metrics on a separate port by configuring Express to listen on a separate port so that it serves up the <code>/metrics</code> endpoint. This helps us avoid exposing metrics that contain information <a id="_idIndexMarker848"/>about the application that an ordinary user does not need and makes it possible for multiple containers in the same pod as ShipIt Clicker to also expose Prometheus metrics. See the code in the <code>chapter11/src/server/index.js</code> file, which adds another HTTP listener and a router for metrics. The Helm templates in <code>chapter11/shipitclicker/templates/deployment.yaml</code> also have changes to support this new endpoint.</p>
			<p>Next, we'll build and install ShipIt Clicker into our local Kubernetes learning environment.</p>
			<h3>Installing the latest version of ShipIt Clicker locally</h3>
			<p>In this section, we will build the ShipIt Clicker Docker container, tag it, and push it to Docker Hub, as we <a id="_idIndexMarker849"/>did in previous chapters. Issue the following commands, replacing <code>dockerfordevelopers</code> with your Docker Hub username:</p>
			<pre>$ docker build . -t dockerfordevelopers/shipitclicker:1.11.7
$ docker push dockerfordevelopers/shipitclicker:1.11.7
$ kubectl config use-context docker-desktop
$ helm install --set image.repository=dockerfordevelopers/shipitclicker:1.11.7 shipit-v8 shipitclicker</pre>
			<p>Inspect the running pods and services using <code>kubectl get all</code> to verify the pod is running, note its name, and <a id="_idIndexMarker850"/>then inspect the logs with <code>kubectl logs</code> to see the startup logs. There should be no errors in the log.</p>
			<p>Next, we'll install this version in EKS.</p>
			<h3>Installing the latest version of ShipIt Clicker in EKS through ECR</h3>
			<p>Now that <a id="_idIndexMarker851"/>you have built the Docker containers and installed this locally, we'll install it in AWS EKS via ECR. Edit <code>chapter11/values.yaml</code> to give this a hostname in the Route 53 DNS zone such as <a href="http://shipit-v8.eks.example.com">shipit-v8.eks.example.com</a> (replace the ECR reference with the one corresponding to your AWS account and region and replace <code>example.com</code> with your domain name):</p>
			<pre>$ docker tag dockerfordevelopers/shipitclicker:1.11.7 143970405955.dkr.ecr.us-east-2.amazonaws.com/dockerfordevelopers/shipitclicker:1.11.7
$ aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 143970405955.dkr.ecr.us-east-2.amazonaws.com
$ docker push 143970405955.dkr.ecr.us-east-2.amazonaws.com/dockerfordevelopers/shipitclicker:1.11.7
$ kubectl config use-context arn:aws:eks:us-east-2:143970405955:cluster/EKS-8PWG76O8
$ helm install shipit-v8 -f values.yaml --set image.repository=143970405955.dkr.ecr.us-east-2.amazonaws.com/dockerfordevelopers/shipitclicker:1.11.7 ./shipitclicker</pre>
			<p>Inspect the Kubernetes logs to make sure that the application has deployed cleanly to the cluster:</p>
			<pre>kubectl logs services/shipit-v8-shipitclicker shipitclicker</pre>
			<p>If all is well with the deployment, get the AWS ALB Ingress Controller's address, as described in <a href="B11641_09_Final_NM_ePub.xhtml#_idTextAnchor191"><em class="italic">Chapter 9</em></a>, <em class="italic">Cloud-Native Continuous Deployment Using Spinnaker</em>, and create DNS entries in the Route 53 console for the deployed application with the ALB address. You should then be able to reach your application at a URL similar to <a href="https://shipit-v8.eks.example.com/">https://shipit-v8.eks.example.com/</a> (replace <code>example.com</code> with your domain name).</p>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor264"/>Scaling your Kubernetes cluster</h1>
			<p>To support more traffic and more applications, your Kubernetes cluster may need to grow beyond its initial size. You can use both manual methods and dynamic programmed methods to do this, especially if you are working with a cloud-based Kubernetes cluster. To scale out <a id="_idIndexMarker852"/>an application, you need to control two dimensions: the number of pods running a particular application and the number of nodes in a cluster. You can't scale the number of pods infinitely on a cluster with the same number of nodes; practical limits related to CPU, memory, and network concerns will ultimately demand that the cluster scales out the number of nodes as well.</p>
			<p>The method that's used to scale out a cluster will vary considerably, depending on the cloud vendor and Kubernetes distribution. The Kubernetes documentation explains both the general process and some specific instructions for clusters running in the Google and Microsoft Azure clouds: </p>
			<p><a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/">https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/</a></p>
			<p>Generally speaking, you must start and configure a new server that is set up similarly to the existing cluster nodes, and then join it to the cluster by using the <code>kubeadm join</code> command:</p>
			<p><a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/</a></p>
			<p>Kubernetes distributions and cloud vendors make this easier by relying on mechanisms such as machine images and autoscaling groups. We will show you how to scale your cluster by using Amazon EKS. In <a href="B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying Docker Apps to Kubernetes</em>, we set up EKS with AWS Quick Start CloudFormation templates in the <em class="italic">Spinning up AWS EKS with CloudFormation</em> section. The following sections assume that you have used that method to set up a cluster that uses autoscaling groups.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor265"/>Scaling the cluster manually</h2>
			<p>Given that we <a id="_idIndexMarker853"/>want to increase the number of nodes in our cluster, we will want to identify and follow the procedures that are specific to our Kubernetes installation. For Amazon EKS clusters, see the following documentation: </p>
			<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html">https://docs.aws.amazon.com/eks/latest/userguide/launch-workers.html</a></p>
			<p>You could just launch an entirely new group of nodes, but you can often adjust a parameter or two in order to increase the size of your cluster. This is done when you increase the <a id="_idIndexMarker854"/>size of a cluster, which is called <em class="italic">scaling out</em>, and when you decrease the size of a cluster, which is called <em class="italic">scaling in</em>. Next, we will learn how to adjust a simple parameter so that we can scale out the number of nodes in the cluster.</p>
			<h3>Scaling nodes out manually</h3>
			<p>For the sake <a id="_idIndexMarker855"/>of simplicity, let's assume you used the AWS Quick Start for EKS CloudFormation templates to create your cluster initially. Since that uses CloudFormation to manage the cluster, you should prefer using CloudFormation to update the cluster's configuration. To manually scale your cluster out, go to the AWS console and update the CloudFormation deployment, changing the default values for <strong class="bold">Number of nodes</strong> and <strong class="bold">Maximum number of nodes</strong> from their current values to higher values, such as <strong class="bold">4</strong> and <strong class="bold">8</strong>:</p>
			<div><div><img src="img/B11641_11_001.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Updating the AWS EKS Quick Start CloudFormation template</p>
			<p>Continue through the CloudFormation update forms and apply the changes. Look at the CloudFormation events for updates and wait a few minutes. You can then check that the update to the CloudFormation template worked fine. Then, you can check the size of the autoscaling group to make sure it has grown.</p>
			<p>You could also update the autoscaling group sizes through the EC2 console, thereby setting the minimum, desired, and maximum number of nodes to <strong class="bold">4</strong>, <strong class="bold">4</strong>, and <strong class="bold">8</strong>, respectively. This will cause your deployed configuration to drift from its CloudFormation templates, however, which is undesirable as the actual state will no longer match the model that CloudFormation expects. See the following post for more on why that is problematic: <a href="https://aws.amazon.com/blogs/aws/new-cloudformation-drift-detection/">https://aws.amazon.com/blogs/aws/new-cloudformation-drift-detection/</a>.</p>
			<p>If you used <code>eksctl</code> to create your cluster instead, you can follow the instructions at <a href="https://eksctl.io/usage/managing-nodegroups/">https://eksctl.io/usage/managing-nodegroups/</a> to scale the node groups it creates.</p>
			<h3>Scaling nodes in manually</h3>
			<p>You can reverse the process to scale in the cluster (reducing its size), but beware that scaling a cluster <a id="_idIndexMarker856"/>in manually is trickier. Doing this safely <a id="_idIndexMarker857"/>involves a process called draining, which is described in the following Kubernetes documentation: <a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/</a>. Just changing the autoscaling group's size on its own will terminate an instance without letting you choose which instance to terminate or giving you a chance to drain the instance. If you <em class="italic">really</em> wanted to do this, you would have to do all the following:</p>
			<ul>
				<li>Decrement the autoscaling group minimum size by one.</li>
				<li>Drain the node with <code>kubectl drain</code>.</li>
				<li>Terminate the node using an AWS CLI command that decrements the desired capacity.</li>
			</ul>
			<p>After you've adjusted the autoscaling group's minimum size, you could issue the following commands (replace the node name and instance ID in each of these commands with the ones that match the node you want to terminate):</p>
			<pre>$ kubectl drain \
    ip-10-0-94-28.us-east-2.compute.internal \
    --ignore-daemonsets
$ aws autoscaling terminate-instance-in-auto-scaling-group \
    --instance-id i-09c88021d2324e821 \
    --should-decrement-desired-capacity</pre>
			<p>This process is involved and could easily lead to manual error. It will also lead to con<a id="_idTextAnchor266"/>figuration drift from the CloudFormation template, so you should either seek to script it or rely on automatic scaling mechanisms instead. </p>
			<h3>Scaling pods manually through deployments</h3>
			<p>Manually scaling the number of pods in a deployment or ReplicaSet is quite easy, assuming that you <a id="_idIndexMarker858"/>have enough resources in your cluster. You can use the <code>kubectl scale</code> command to set the number of replicas. You might have to issue several <code>kubectl get</code> commands before you see all the replicas become ready, as shown in this transcript:</p>
			<pre>$ kubectl get deployment/shipit-v8-shipitclicker
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
shipit-v8-shipitclicker   2/2     2            2           57m
$ kubectl scale deployment/shipit-v8-shipitclicker --replicas=4
deployment.apps/shipit-v8-shipitclicker scaled
$ kubectl get deployment/shipit-v8-shipitclicker
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
shipit-v8-shipitclicker   2/4     4            1           58m
$ kubectl get deployment/shipit-v8-shipitclicker
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
shipit-v8-shipitclicker   4/4     4            4           59m</pre>
			<p>Next, we will examine how we can apply programmatic scaling to the cluster, for both nodes and pods.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor267"/>Scaling the cluster dynamically (autoscaling)</h2>
			<p>Now that you've completed many of the exercises in the preceding three chapters, which explored <a id="_idIndexMarker859"/>the complex concepts that go along with the Kubernetes container orchestration system, you might be wondering: is all this effort worth it? In this section, we will explore the key feature that can make the pain of managing these systems worth it – autoscaling. By dynamically scaling the applications in a cluster, and the cluster itself, you can drive high utilization of cluster resources, meaning that you will need fewer computers (virtual or physical) to run your systems. When you combine dynamic scaling with the self-healing capabilities of the Kubernetes system, this becomes compelling, even though it has high complexity and a high learning curve in some areas.</p>
			<p>Kubernetes <a id="_idIndexMarker860"/>supports several dynamic scaling mechanisms, including the Cluster Autoscaler, the Horizontal Pod Autoscaler, and the Vertical Pod Autoscaler. Let's explore each of these.</p>
			<h3>Configuring the Cluster Autoscaler</h3>
			<p>The <code>kube-system</code> namespace and uses cloud APIs to launch and terminate nodes.</p>
			<p>If you used <a id="_idIndexMarker863"/>the AWS EKS Quick Start Cloudformation templates to create your cluster and told it to enable the Cluster Autoscaler, no further configuration is needed. If you used <code>eksctl</code> or another method to create the cluster, you may need to configure it further using the directions provided here: <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html">https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html</a>.</p>
			<p>You can verify that the Cluster Autoscaler is running by querying it:</p>
			<pre>$ kubectl -n kube-system get deployments | grep autoscaler
cluster-autoscaler-1592701624-aws-cluster-autoscaler   1/1     1            1</pre>
			<p>Now that we have learned a bit about the Cluster Autoscaler, let's discover how we might configure an application to take advantage of its features.</p>
			<h4>Configuring a stateless application to work with the Cluster Autoscaler</h4>
			<p>A stateless application, such as ShipIt Clicker, can tolerate starting and stopping any one of its pods <a id="_idIndexMarker864"/>and can run on any node in the cluster. It doesn't require special configuration to work with the Cluster Autoscaler. Stateful applications that mount local storage and some other classes of applications must avoid some scaling operations if possible and may require special handling. See the Autoscaling FAQ for more details: <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md</a>.</p>
			<p>You can <a id="_idIndexMarker865"/>give the Cluster Autoscaler a hint that it should not scale in pods beyond a certain point, and that it should strive to keep a certain number or percentage of healthy pods available by using a <strong class="bold">PodDisruptionBudget</strong> (<strong class="bold">PDB</strong>): <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">https://kubernetes.io/docs/tasks/run-application/configure-pdb/</a>.</p>
			<p>We have configured ShipIt Clicker with a PDB in its Helm Chart. See <code>chapter11/src/shipitclicker/templates/pdb.yaml</code> for more information. You can find the default values for it in <code>chapter11/src/shipitclicker/values.yaml</code>. The defaults now have ShipIt Clicker configured to deploy two pods and have a PDB with a minimum <a id="_idIndexMarker866"/>of one pod available. This provides hints to the Cluster Autoscaler and other Kubernetes applications that it should always keep at least one pod alive, even as node maintenance is underway.</p>
			<p>Next, we will demonstrate the Cluster Autoscaler in action.</p>
			<h4>Demonstrating the Cluster Autoscaler in action</h4>
			<p>In order to get the Cluster Autoscaler to make changes to the size of the cluster, we can start more <a id="_idIndexMarker867"/>pods than it has capacity to handle currently. To watch this process in action, it is helpful to tail the logs of the <code>cluster-autoscaler</code> service. Open a Terminal window and run the following commands to tail the logs of the service:</p>
			<pre>$ service=service/$(kubectl get services -n kube-system \
   | awk '/cluster-autoscaler/{ print $1 }')
$ kubectl logs -f -n kube-system "$service"</pre>
			<p>Every 10 seconds, you will see log entries indicating that the service is looking for <em class="italic">unschedulable</em> pods (which would cause the cluster to scale out the number of nodes) and for nodes that are eligible for scaling in.</p>
			<p>Then, in a different Terminal window, manually scale the deployment of ShipIt Clicker to <code>50</code> pods:</p>
			<pre>kubectl scale deployment/shipit-v8-shipitclicker --replicas=50</pre>
			<p>Each of the <code>t3.medium</code> nodes in the default EKS cluster can handle approximately 4 to 16 ShipIt Clicker pods, depending on how many other pods are also running on each node. This will trip the Cluster Autoscaler and make it scale out by at least one additional node. You will see entries in the Cluster Autoscaler log noting that it has found unschedulable pods, and shortly afterward, that it has completed scaling. </p>
			<p>To see the progress from the perspective of the nodes and pods in the deployment, issue the following commands every few seconds:</p>
			<pre>kubectl get nodes; kubectl get deployments</pre>
			<p>You will see nodes launching and more and more replicas becoming ready until the set of replicas stabilizes. Once that happens, scale it back down to a lower default state:</p>
			<pre>kubectl scale deployment/shipit-v8-shipitclicker --replicas=2</pre>
			<p>Once you've done that, you may notice that the nodes do not scale in immediately as they enter a cooldown condition for 10 minutes after a scale out operation completes. However, a minute after the cooldown period expires, the Cluster Autoscaler will notice that the CPU utilization <a id="_idIndexMarker868"/>of these nodes is close to zero and it will scale in the cluster, terminating the nodes that no longer have pods available. The Cluster Autoscaler will respect the PDB when it performs this scale in operation as well – allowing you to be as conservative as required when shrinking the number of pods and nodes in the cluster.</p>
			<p>Now that you have learned how to scale the cluster nodes in and out using the Cluster Autoscaler, let's learn how to use the Horizontal Pod Autoscaler to set scaling policies.</p>
			<h3>Configuring the Horizontal Pod Autoscaler</h3>
			<p>The <strong class="bold">Horizontal Pod Autoscaler</strong> allows you to set up rules for scaling out sets of Kubernetes pods <a id="_idIndexMarker869"/>using rules that can take into account CPU <a id="_idIndexMarker870"/>utilization or other custom metrics. This service can also scale pods controlled by deployments, ReplicaSets, and replication controllers. You can read more about the theory of how it works here: <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/</a>.</p>
			<p>This is the last big piece of the puzzle you need before you can achieve a cluster that automatically scales in and out in response to demand.</p>
			<p>You need Metrics Server for the Horizontal Pod Autoscaler to work. We will install this next.</p>
			<h4>Installing Metrics Server</h4>
			<p>To have more detailed statistics available in your Kubernetes cluster for use by the software components <a id="_idIndexMarker871"/>that enable dynamic scaling (including the Horizontal Pod Autoscaler), you need to run the standard <strong class="bold">Metrics Server</strong>. It aggregates <a id="_idIndexMarker872"/>statistics across the cluster regarding <a id="_idIndexMarker873"/>the memory, CPU, and other resource utilization of the nodes and among the pods in a format that the various Kubernetes autoscaler mechanisms can understand and act upon. The AWS EKS guide talks about installing that here:</p>
			<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html">https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html</a></p>
			<p>To install it, ensure your <code>kubectl config</code> context is set to your cloud cluster. Then, issue the following command from your local workstation:</p>
			<pre>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml</pre>
			<p>Once you <a id="_idIndexMarker874"/>have installed Metrics Server, verify that it is running:</p>
			<pre>$ kubectl -n kube-system get deployment metrics-server
NAME             READY   UP-TO-DATE   AVAILABLE   AGE
metrics-server   1/1     1            1           6m</pre>
			<p>Next, we will <a id="_idIndexMarker875"/>activate the Horizontal Pod Autoscaler for the ShipIt Clicker application to demonstrate how it works.</p>
			<h4>Activating the Horizontal Pod Autoscaler</h4>
			<p>The AWS EKS <a id="_idIndexMarker876"/>guide shows the steps needed to install the Horizontal Pod Autoscaler: <a href="https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html">https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html</a>.</p>
			<p>The main thing <a id="_idIndexMarker877"/>we need to install is the metrics service. It turns out that the Horizontal Pod Autoscaler is baked into Kubernetes itself. We can issue a command such as the following one to activate a Horizontal Pod Autoscaler for a deployment:</p>
			<pre>kubectl autoscale deployment shipit-v8-shipitclicker --cpu-percent=50 --min=2 --max=50</pre>
			<p>If you need to edit these parameters, you can do so with the following command:</p>
			<pre>kubectl edit hpa/shipit-v8-shipitclicker</pre>
			<p>You can get a detailed view of what the Horizontal Pod Autoscaler has done recently by issuing this command:</p>
			<pre>kubectl describe hpa/shipit-v8-shipitclicker</pre>
			<p>To test whether the Horizontal Pod Autoscaler and Cluster Autoscaler are working as expected, we need to drive CPU load. That's where the <code>/faults/spin</code> endpoint comes in handy. Later in this chapter, in the <em class="italic">Testing scalability and performance with k6</em> section, we will see how to construct a realistic load test for the ShipIt Clicker application. However, to exercise autoscaling, we are going to use a brute-force method by using the Apache Bench utility that's run via Docker (replace <code>example.com</code> with your domain name):</p>
			<pre>$ url=https://shipit-v8.eks.example.com/faults/spin
$ docker run --rm jordi/ab -c 50 -t 900 "$url"</pre>
			<p>Use the <code>kubectl get deployments</code>, <code>kubectl get pods</code>, <code>kubectl get nodes</code>, and <code>kubectl describe hpa</code> commands repeatedly to watch the deployment replicas grow. Alternatively, use a Kubernetes monitoring tool such as k9s (<a href="https://k9scli.io/">https://k9scli.io/</a>) to watch the pod and node counts grow <a id="_idIndexMarker878"/>over the first 10 minutes or so, and then subside in the 15 minutes afterward. You could also look at some Grafana dashboards <a id="_idIndexMarker879"/>and Jaeger traces, as described in the previous chapter, to see how the cluster is handling the load, or even look at the CloudWatch metrics that surfaced in the EC2 console for the active nodes.</p>
			<p>Next, we will consider when we might use the Vertical Pod Autoscaler.</p>
			<h3>Configuring the Vertical Pod Autoscaler</h3>
			<p>The Vertical Pod Autoscaler is a newer scaling mechanism that observes the amount of memory and <a id="_idIndexMarker880"/>CPU usage that pods request, versus what they actually use, in order to optimize memory and CPU requests – it performs right-sizing to drive better cluster utilization. This is the most useful scaling mechanism for stateful pods. </p>
			<p>However, the Vertical Pod Autoscaler documentation currently states that it is not compatible with <a id="_idIndexMarker881"/>the Horizontal Pod Autoscaler, so you should avoid configuring it so that it manages the same pods. You can explore using it for your application, but keep in mind the advice it specifies about not mixing it with the Horizontal Pod Autoscaler using CPU metrics. The installation procedure for the Vertical Pod Autoscaler is also more involved than configuring either of the other autoscalers, so we won't show all the steps in detail here – please refer to the Vertical Pod Autoscaler <a id="_idIndexMarker882"/>documentation for detailed configuration instructions: <a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler</a>.</p>
			<p>In this section, we learned all about how we can scale our application using both manual and dynamic methods. In the next section, we will learn all about Envoy, a service mesh that provides some advanced controls and sanity regarding communications between pods in a Kubernetes cluster.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor268"/>What is Envoy, and why might I need it?</h1>
			<p>Envoy (<a href="https://www.envoyproxy.io/">https://www.envoyproxy.io/</a>) is a <a id="_idIndexMarker883"/>C++ open source <strong class="bold">service mesh</strong> and edge <a id="_idIndexMarker884"/>proxy geared <a id="_idIndexMarker885"/>toward microservice deployments. Developed by <a id="_idIndexMarker886"/>a team at Lyft, it is especially useful for teams developing Kubernetes-hosted applications, such as the ones you have seen throughout this book.</p>
			<p>So, why exactly would we need to deploy Envoy? When developing cloud-based production systems that use multiple containers to host a distributed service, many of the problems you will encounter are related to observability and networking.</p>
			<p>Envoy aims to solve these two problems by introducing a proxy service that offers runtime-configurable networking and metrics collection that can be used as a building block for creating higher-level systems that manage these concerns. Whether you're building out a small distributed <a id="_idIndexMarker887"/>application or a large microservice architecture designed <a id="_idIndexMarker888"/>around the service mesh model, Envoy's features allow us to abstract the thorny problem of networking in a cloud and platform-agnostic fashion.</p>
			<p>The team at Lyft developed Envoy using the following concepts:</p>
			<ul>
				<li><strong class="bold">Out of process architecture</strong>: Envoy is a self-contained process that can be deployed <a id="_idIndexMarker889"/>alongside existing applications.</li>
				<li><code>localhost</code> <a id="_idIndexMarker890"/>and are ignorant of the network topology. An L3/L4 filter architecture is used for networking proxying. You can add custom filters to the proxy to support tasks such as TLS client certificate authentication.</li>
				<li><strong class="bold">Language agnosticism</strong>: Envoy works with multiple languages and allows you to mix and <a id="_idIndexMarker891"/>match application frameworks. For example, through the use of Envoy PHP and Python, containerized applications can communicate with each other.</li>
				<li><strong class="bold">HTTP L7 filters and routing</strong>: As with L3/L4 filters, filtering is also supported at the L7 layer. This allows plugins to be developed for different tasks, ranging from buffering to <a id="_idIndexMarker892"/>interacting with AWS services such as DynamoDB. Envoy's routing feature allows you to deploy a routing subsystem that can redirect requests based on a variety of criteria, such as path and content type. </li>
				<li><strong class="bold">Load balancing and front/edge proxy support</strong>: Envoy supports advanced load balancing <a id="_idIndexMarker893"/>techniques, including automatic <a id="_idIndexMarker894"/>retries, circuit breakers, health checking, and rate limiting. Additionally, you can deploy Envoy at the network edge to handle TLS termination and HTTP/2 requests.</li>
				<li><strong class="bold">Observability and transparency</strong>: Envoy collects statistics to support observability <a id="_idIndexMarker895"/>at both the application and networking layer. You can combine Envoy with Prometheus, Jaeger, Datadog, and other monitoring platforms that support metrics and tracing. </li>
			</ul>
			<p>Let's explore <a id="_idIndexMarker896"/>some of Envoy's features in more detail so that we can understand <a id="_idIndexMarker897"/>these concepts better. </p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor269"/>Network traffic management with an Envoy service mesh</h2>
			<p>You should already be familiar with the concept of a load balancer, which is one type of network <a id="_idIndexMarker898"/>traffic manager. But what <a id="_idIndexMarker899"/>exactly is a service mesh? Why would you need to use one? How does Envoy help us in this regard?</p>
			<p>A service mesh <a id="_idIndexMarker900"/>is an infrastructure layer dedicated to handling service-to-service communications, typically through a proxy service. The benefits of using a service <a id="_idIndexMarker901"/>mesh are as follows:</p>
			<ul>
				<li>Transparency and observability into network communications.</li>
				<li>You can support secure connections across the network.</li>
				<li>Metrics collection, including length of time for a retry to succeed when a service fails.</li>
				<li>You can <a id="_idIndexMarker902"/>deploy proxies as <strong class="bold">sidecars</strong>. This means they run alongside each service rather than within it. In turn, this allows us to decouple the proxying service from the application itself.</li>
			</ul>
			<p>An example of a four-application service mesh can be visualized as follows:</p>
			<div><div><img src="img/B11641_11_002.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Example of a service mesh with four microservices and sidecar proxies</p>
			<p>Here, each of our containerized applications has a corresponding sidecar proxy. The application communicates with the proxy, which, in turn, communicates across the service mesh with <a id="_idIndexMarker903"/>the other containerized services we are hosting. The application does not know that the proxy exists and does <a id="_idIndexMarker904"/>not need any modifications to work with the proxy. All the configuration can be done by wiring ports together using the container orchestration system, in a way that is invisible to the application.</p>
			<p>Now, let's gets our hands dirty and get Envoy up and running.</p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor270"/>Setting up Envoy </h2>
			<p>Because of <a id="_idIndexMarker905"/>Envoy's architecture, you have flexibility in terms of how you can deploy the software:</p>
			<ul>
				<li>Configured explicitly as a sidecar container, with a static configuration file, alongside an application container</li>
				<li>Configured dynamically as part of a service mesh control plane, where the container might be injected into a Kubernetes pod as a component, using software such as Istio (<a href="https://istio.io/">https://istio.io/</a>) or AWS App Mesh (<a href="https://aws.amazon.com/app-mesh/">https://aws.amazon.com/app-mesh/</a>)</li>
			</ul>
			<p>The second option offers additional power at the cost of adding major complexity.</p>
			<p>The Envoy sample configurations (see <a href="https://www.envoyproxy.io/docs/envoy/latest/start/start#sandboxes">https://www.envoyproxy.io/docs/envoy/latest/start/start#sandboxes</a>) are all of the first variety, with explicit Envoy proxy configurations. To learn about Envoy, it is simpler to consider the explicit configuration examples. The version of ShipIt Clicker provided in this chapter has been modified so that you can add an Envoy sidecar container using a static configuration file when it is deployed in Kubernetes, with a minimalist approach that allows us to demonstrate Envoy's features.</p>
			<h3>Configuring ShipIt Clicker for Envoy</h3>
			<p>Now, let's examine <a id="_idIndexMarker906"/>the specific changes that need to be made <a id="_idIndexMarker907"/>for Envoy to be supported in ShipIt Clicker. The application JavaScript code does not require any changes; all the changes are in the Helm Charts. See the Helm Charts in <code>chapter11/shipitclicker</code> and compare them with the ones in <code>chapter10/shipitclicker</code>; you will see a new Envoy sidecar container defined in <code>chapter11/shipitclicker/templates/deployment.yaml</code>, configured with an image defined in <code>chapter11/shipitclicker/values.yml</code>:</p>
			<pre>        - name: envoy-sidecar
          image: "{{ .Values.envoy.repository }}"
          imagePullPolicy: {{ .Values.envoy.pullPolicy }}
          command: ["/usr/local/bin/envoy"]
          args: ["-c", "/etc/envoy-config/config.yaml"]</pre>
			<p>The preceding <a id="_idIndexMarker908"/>lines in the template launch the Envoy container using a configuration file, <code>/etc/envoy-config/config.yaml</code>, defined in a ConfigMap. Envoy <a id="_idIndexMarker909"/>needs both a port definition for its administrative interface and a port definition for each service it manages or proxies:</p>
			<pre>          ports:
            - name: envoy-admin
              containerPort: 9901
              protocol: TCP
            - name: envoy-http
              containerPort: 4000
              protocol: TCP</pre>
			<p>We can query the administrative API to ensure that Envoy is both live and ready to accept traffic, in accordance with Kubernetes best practices:</p>
			<pre>          livenessProbe:
            httpGet:
              path: /server_info
              port: envoy-admin
          readinessProbe:
            httpGet:
              path: /ready
              port: envoy-admin</pre>
			<p>To expose the configuration file to the container, we use a volume mount that exposes the <code>config.yaml</code> file:</p>
			<pre>          volumeMounts:
            - name: envoy-config-vol
              mountPath: /etc/envoy-config/
      volumes:
        - name: envoy-config-vol
          configMap:
            name: {{ .Release.Name }}-envoy-sidecar-configmap
            items:
              - key: envoy-config
                path: config.yaml</pre>
			<p>The <code>config.yaml</code> file is defined in <code>chapter11/shipitclicker/templates/configmap-envoy.yaml</code> and has <a id="_idIndexMarker910"/>definitions for listeners and clusters <a id="_idIndexMarker911"/>for the following:</p>
			<ul>
				<li>An ingress proxy for the ShipIt Clicker container inside the pod</li>
				<li>An egress proxy for the Redis Kubernetes service that can be reached at <code>redis-master</code> in the cluster</li>
				<li>An ingress proxy that allows Prometheus to scrape metrics from the Envoy sidecar in the pod</li>
			</ul>
			<p>The ConfigMap for ShipIt Clicker in <code>chapter11/shipitclicker/templates/configmap.yaml</code> has been modified so that it connects to <code>localhost:6379</code> for Redis, which Envoy listens for and proxies out via a TCP L4 proxy to the Redis service. This listens elsewhere in the cluster at <code>redis-master:6379</code>. </p>
			<p>The Kubernetes service in <code>chapter11/shipitclicker/templates/service.yaml</code> now calls the <code>envoy-http</code> port instead of directly calling the application container's port. </p>
			<h4>Why not use the Envoy Redis protocol proxy?</h4>
			<p>The example files used here use a plain TCP proxy, instead of Envoy's Redis protocol proxy (see <a href="https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/redis_proxy/v3/redis_proxy.proto">https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/redis_proxy/v3/redis_proxy.proto</a> and <a href="https://github.com/envoyproxy/envoy/tree/master/examples/redis">https://github.com/envoyproxy/envoy/tree/master/examples/redis</a>). </p>
			<p>This is because <a id="_idIndexMarker912"/>the ShipIt Clicker application has a Redis password authentication set up that is not compatible with Envoy's Redis proxy. ShipIt Clicker is set up to use a password it retrieves from a Kubernetes Secret that the Bitnami Redis Helm Chart stores. However, Envoy does not pass through this password; when configured with the Redis protocol proxy, it emitted an error message stating <code>Warning: Redis server does not require a password, but a password was supplied</code> when ShipIt Clicker tried to authenticate. It turns out that if you use the Envoy Redis protocol support, you must configure the proxy itself with password authentication for the client, and optionally the server, through the configuration file stored in a ConfigMap. However, the password that the Bitnami Redis server uses is only available as a Kubernetes secret, so reworking the system to support this would add complexity. </p>
			<p>As an exercise, you could install Redis without a password and remove the password from the configuration <a id="_idIndexMarker913"/>for ShipIt Clicker if you wanted to do this. If you did this, you could also switch Redis implementations to the Bitnami <a id="_idIndexMarker914"/>Redis Cluster Helm Chart (see <a href="https://github.com/bitnami/charts/tree/master/bitnami/redis-cluster">https://github.com/bitnami/charts/tree/master/bitnami/redis-cluster</a>), and then use the Envoy support for Redis clusters in order to implement the reader/writer split pattern. </p>
			<p>So far, we've seen how to deploy Envoy to create a service mesh. Next, we are going to explore the circuit breaker pattern. </p>
			<h3>Configuring Envoy's support for the circuit breaker pattern</h3>
			<p>The circuit breaker <a id="_idIndexMarker915"/>pattern is a mechanism that's used to configure thresholds for failures. The goal here is to prevent cascading <a id="_idIndexMarker916"/>failures spreading across your microservice platform and to stop continuous requests to a non-responsive service.</p>
			<p>Configuring the pattern on Envoy is relatively simple. We can configure circuit breaking values as part of an Envoy cluster definition via the <code>circuit_breakers</code> field. </p>
			<p>To see how this works, examine the following ConfigMap file, which contains a definition of a circuit breaker (<code>chapter11/shipitclicker/templates/configmap-envoy.yaml</code>):</p>
			<pre>        circuit_breakers:
          thresholds:
            - priority: DEFAULT
              max_connections: {{ .Values.envoy.maxRequests }}
              max_pending_requests: {{ .Values.envoy.maxRequests }}
              max_requests: {{ .Values.envoy.maxRequests }}
              max_retries: {{ .Values.envoy.maxRetries }}</pre>
			<p>This threshold definition specifies the maximum number of connections Envoy will make and the maximum number of parallel requests. In our example, we have a configuration for a default priority threshold and a second one for high priority (used for HTTP 1.1) and the maximum number of requests (used for HTTP/2). If the rate of traffic that Envoy detects exceeds these thresholds, it will throw an error and deny the requests, without passing the request to the target service. Notice that since we are using Helm Charts, we specify the actual values using the Helm template variable substitution with the values coming from <code>chapter11/shipitclicker/values.yaml</code> or one of the override mechanisms for Helm Chart values. The default values are from a section of the <code>values.yaml</code> file that specifies Envoy-specific values:</p>
			<pre>envoy:
  repository: envoyproxy/envoy:v1.14.2
  pullPolicy: IfNotPresent
  accessLog: "/dev/null"
  maxRequests: 1024
  maxRetries: 2</pre>
			<p>These default <a id="_idIndexMarker917"/>values are suitable for production <a id="_idIndexMarker918"/>for this application, but how can we test that the circuit breaker works, without inducing a massive load? We will show you how do that next.</p>
			<h3>Testing the Envoy circuit beaker</h3>
			<p>In order to <a id="_idIndexMarker919"/>test that the Envoy circuit breaker is working properly, we'll deploy ShipIt Clicker to the cloud Kubernetes cluster with an artificially lowered request limit and perform a quick load test to verify that it works. Issue a Helm <code>upgrade</code> command, followed by a <code>kubectl rollout restart</code> command, similar to the following, to set the maximum simultaneous requests to <code>10</code> (replace <code>image.repository</code> with your ECR repository reference):</p>
			<pre>$ helm upgrade shipit-v8 -f values.yaml --set image.repository=143970405955.dkr.ecr.us-east-2.amazonaws.com/dockerfordevelopers/shipitclicker:1.11.7 --set envoy.maxRequests=2 ./shipitclicker
Release "shipit-v8" has been upgraded. Happy Helming!
NAME: shipit-v8
LAST DEPLOYED: Sun Jun 28 22:34:15 2020
NAMESPACE: default
STATUS: deployed
REVISION: 17
NOTES:
1. Get the application URL by running these commands:
  http://shipit-v8.eks.example.com/*
$ kubectl rollout restart deployment/shipit-v8-shipitclicker
deployment.apps/shipit-v8-shipitclicker restarted</pre>
			<p>Next, we'll use Apache Bench to test the deployed application, starting with a single concurrent request:</p>
			<pre>$ url=https://shipit-v8.eks.example.com/faults/spin
$ docker run --rm jordi/ab -c 1 -n 400 $url | grep requests:
Completed 100 requests
Completed 200 requests
Completed 300 requests
Completed 400 requests
Finished 400 requests
Complete requests:      400
Failed requests:        0</pre>
			<p>Here, you can <a id="_idIndexMarker920"/>see that when run with only one concurrent request, all the requests succeeded. Next, we'll increase the concurrency to <code>50</code> simultaneous connections:</p>
			<pre>$ docker run --rm jordi/ab -c 50 -n 400 $url | grep requests:
Completed 100 requests
Completed 200 requests
Completed 300 requests
Completed 400 requests
Finished 400 requests
Complete requests:      400
Failed requests:        72</pre>
			<p>If we set the concurrency to <code>50</code> simultaneous requests, many of them will fail as the circuit breaker kicks in. We've already seen how to set up a basic circuit breaker with two thresholds for our cluster. More advanced circuit breaker patterns exist, including breaking on latency and retries. We'll leave you to explore this further if you think your applications will need it.</p>
			<p>Now that you have tested the circuit breaker with low connection thresholds, reset the thresholds to their original values and redeploy the application to help set up the application for more load testing.</p>
			<p>If we had a good measurement of how much real user traffic each pod could handle without failing, we could <a id="_idIndexMarker921"/>use this to set a better value for the circuit breaker. However, Apache Bench is a blunt instrument that does not let us simulate a realistic user load. For that, we need to use a more sophisticated load test framework. Now, we'll take a look at how we can test scalability with k6, a Docker-based load testing framework.</p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor271"/>Testing scalability and performance with k6</h1>
			<p>The k6 framework (<a href="https://k6.io">https://k6.io</a>) is a <a id="_idIndexMarker922"/>programmable open source load testing tool. We are going to show you how to use it to generate a more realistic load pattern than <a id="_idIndexMarker923"/>you could generate using a simple <a id="_idIndexMarker924"/>load generator such as <strong class="bold">Apache Bench</strong> (<strong class="bold">ab</strong>).</p>
			<p>This framework is quite simple to set up and use thanks to its Docker image, which is available on Docker Hub. You can find the Quick Start instructions at <a href="https://k6.io/docs/getting-started/running-k6">https://k6.io/docs/getting-started/running-k6</a>.</p>
			<p>To create a load test using k6, you need to use JavaScript using k6's library routines. To perform a smoke test, your script would need to look something like this:</p>
			<pre>import http from 'k6/http';
export default function() {
  http.get('https://shipit-v8.eks.example.com/');
}</pre>
			<p>This script is roughly equivalent to using the <code>ab</code> utility to stress test a web server. Create a file called <code>hello.js</code> using the preceding source code, replacing <code>shipit-v8.eks.example.com</code> with the fully qualified domain name of one of your websites.</p>
			<p>Following Docker best practices, you should ensure that you add the <code>--rm</code> flag to the Docker command line so that you do not accumulate stale containers in your local installation:</p>
			<pre>$ docker run --rm -i loadimpact/k6 run - &lt; hello.js</pre>
			<p>This will run k6 and retrieve the URL specified in <code>hello.js</code>.</p>
			<p>There are just a few key concepts you must know about:</p>
			<ul>
				<li>You must provide a default function.</li>
				<li>K6 is <em class="italic">not</em> Node.js. It has no event loop.</li>
				<li>Your default function is known as a <strong class="bold">Virtual User (VU)</strong>.</li>
				<li>Code defined <a id="_idIndexMarker925"/>outside of the default function is evaluated once, on program startup.</li>
				<li>The default function is run repeatedly until the test is over.</li>
				<li>You can <a id="_idIndexMarker926"/>run your test with as many VUs as you want, and for as long as you want.<p class="callout-heading">Note</p><p class="callout">There are many command-line options you can use with k6 to ramp up and down VUs over time, as well as to specify how long to run the test and how many VUs to simulate. The defaults have only one VU, and only one test iteration.</p></li>
			</ul>
			<p>Let's use some of those options to run the test with more users and for a longer duration:</p>
			<pre>$ docker run --rm -i loadimpact/k6 run --vus 50 --duration 30s - &lt; hello.js</pre>
			<p>Running k6 like this will perform a load test almost identical to an Apache Bench load test, with a concurrency of <code>50</code> and a duration of <code>30</code> seconds.</p>
			<p>However, since you have the full power of JavaScript available, you can write more nuanced load tests using a variety of strategies.</p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor272"/>Recording and replaying network sessions</h2>
			<p>An alternative to writing a script such as <code>hello.js</code> by hand is to use a record-and-replay strategy. Many load <a id="_idIndexMarker927"/>testing frameworks support this paradigm, including k6. To do this, use the Chrome browser and its <strong class="bold">Inspect</strong> feature. You can <a id="_idIndexMarker928"/>use the debugger's <strong class="bold">Network</strong> tab to capture and save network traffic to and from the application's backend.</p>
			<p>You start with an empty (cleared) network history in the debugger. Then, you load and play the game. Each click will cause API requests to occur between the application running in the browser and the backend.</p>
			<p>When you are satisfied with your recording, right-click on the <strong class="bold">Network</strong> pane and choose <strong class="bold">copy all as HAR</strong>. This puts the HAR-formatted text in the system clipboard:</p>
			<div><div><img src="img/B11641_11_003.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 11.3 – Google Chrome inspector debugging console – Copy all as HAR</p>
			<p>Paste from <a id="_idIndexMarker929"/>the clipboard into a file named <code>chapter11/src/test/k6/session.har</code>. Then, run a conversion script to transform the HAR file into a JavaScript file at <code>chapter11/src/test/k6/har-session.js</code>, and run another shell script that will <a id="_idIndexMarker930"/>run k6 via Docker with the right arguments to initiate a one-user, 60-second test:</p>
			<pre>$ chapter11/bin/k6-convert-har.sh
$ chapter11/bin/k6-run-har.sh</pre>
			<p>The <code>k6-run-har.sh</code> script is set up to use environment variables that override the VUs with the <code>USERS</code> variable, and to override the test duration with the <code>DURATION</code> variable. So, you can prefix the script with those variables like this and run a 10-user test for <code>300</code> seconds:</p>
			<pre>$ USERS=10 DURATION=300 chapter11/bin/k6-run-har.sh</pre>
			<p>There are some wrinkles to note about using this playback and record strategy, though: the process is quite literal, and results in a file that has no delays between requests. Running the test will <a id="_idIndexMarker931"/>induce a large, machine-speed load on the target service. There is no randomization of the delays that should happen between requests, which is <a id="_idIndexMarker932"/>something you want to do in order to closely model the load that a real user's session would put on a service.</p>
			<p>To create a more realistic test, we are going to have to do some JavaScript programming.</p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor273"/>Hand-crafting a more realistic load test</h2>
			<p>In the <code>chapter11/src/tests/k6/</code> directory, there is a <code>test.js</code> script designed to realistically test ShipIt Clicker, whether it's deployed locally or in the cloud. </p>
			<p>This script <a id="_idIndexMarker933"/>mimics a human playing the game by using these strategies:</p>
			<ul>
				<li>Fetches the HTML, stylesheets, images, and JavaScript files that make up the application</li>
				<li>Performs HTTP post to start a new game</li>
				<li>Gets the initial score, deployments, and <code>nextPurchase</code> values</li>
				<li>Attempts to simulate the click stream a human player would make</li>
			</ul>
			<p>The HTTP requests were identified by playing the game in a web browser such as Google Chrome, using its <strong class="bold">Inspect</strong> feature, and viewing the <strong class="bold">Network</strong> tab as the game loads and is played. Then, we wrote a test that simulated the series of requests in a way that is closely modeled after real user behavior, including having realistic random delays.</p>
			<p>Let's examine the code in <code>chapter11/src/test/k6/test.js</code>. Here, we import the <code>http</code> class and the <code>sleep()</code> method from the k6 supplied libraries:</p>
			<pre>import http from "k6/http";
import { sleep } from "k6";</pre>
			<p>We pass parameters to the <code>test.js</code> script as environment variables:</p>
			<ul>
				<li>The <code>DEBUG</code> environment variable lets us trigger more verbose logging.</li>
				<li>The <code>MOVES</code> environment variable contains the number of moves per game.</li>
				<li>The <code>TARGET</code> environment variable would be something like <code>http://192.2.0.10:3011</code> for <code>localhost</code> development, where <code>192.2.0.10</code> is the IPv4 LAN address of your workstation.</li>
			</ul>
			<p>These parameters get retrieved from the <code>__ENV</code> object, as follows:</p>
			<pre>const DEBUG = __ENV.DEBUG;
const MOVES = __ENV.MOVES;
const target = __ENV.TARGET;</pre>
			<p>The <code>ENDPOINTS</code> array gets used to iterate through the three main elements that the game tracks:</p>
			<pre>const ENDPOINTS = ['score', 'deploys', 'nextPurchase'];</pre>
			<p>The <code>deploy()</code> method simulates <a id="_idIndexMarker934"/>a human clicking on the <code>http.patch()</code> twice – once to update the deployment count and once to update the score:</p>
			<pre>const deploy = id =&gt; {
  validate(
    http.patch(
      `${target}/api/v2/games/${id}/deploys`,
      JSON.stringify({
        id: id,
        element: 'deploys',
        value: 1,
      }),
      params
    )
  );</pre>
			<p>This function also updates the score:</p>
			<pre>  validate(
    http.patch(
      `${target}/api/v2/games/${id}/score`,
      JSON.stringify({
        id: id,
        element: 'score',
        value: 1,
      }),
      params
    )
  );
};</pre>
			<p>The <code>validate()</code> method <a id="_idIndexMarker935"/>that the <code>deploy()</code> method calls simply verifies that the server returns a valid response:</p>
			<pre>  validate(
    http.patch(
      `${target}/api/v2/games/${id}/score`,
      JSON.stringify({
        id: id,
        element: 'score',
        value: 1,
      }),
      params
    )
  );
};</pre>
			<p>The <code>getStaticAssets()</code> method simulates the user's browser fetching the HTML, CSS, images, and JavaScript that make up the game:</p>
			<pre>const getStaticAssets = () =&gt;
  [
    target,
    `${target}/stylesheet.css`,
    `${target}/img/shipit-640x640-lc.jpg`,
    `${target}/img/Richard-Cartoon-Headshot-Jaunty-180x180.png`,
    `${target}/app.js`,
  ]
    .map(http.get)
    .map(validate);</pre>
			<p>The <code>getGameId()</code> method simulates the start of a new game:</p>
			<pre>const getGameId = () =&gt; {
  const uri = `${target}/api/v2/games/`;
  const response = validate(http.post(uri, {}, params));
  return JSON.parse(response.body).id;
};</pre>
			<p>The <code>getScores()</code> method <a id="_idIndexMarker936"/>retrieves the existing scores using the <code>map</code> functional programming technique to both iterate over the endpoints and to run a validation function on the HTTP response: </p>
			<pre>const getScores = id =&gt; {
  return ENDPOINTS.map(element =&gt;
    http.get(`${target}/api/v2/games/${id}/${element}`)
  ).map(validate);
};</pre>
			<p>The <code>putScores()</code> method is used to reset all the game scores, such as when a new game begins:</p>
			<pre>const putScores = (id, score) =&gt; {
  return ENDPOINTS.map(element =&gt;
    http.put(
      `${target}/api/v2/games/${id}/${element}`,
      JSON.stringify({
        id: id,
        element: element,
        value: score,
      }),
      params
    )
  ).map(validate);
};</pre>
			<p>The default <a id="_idIndexMarker937"/>function is the one that k6 loops through for each virtual user: </p>
			<pre>export default function() {
  const startDelay = random_gaussian(6000, 1000) / 1000;
  log.debug(`Loading static assets, then wait ${startDelay}s to start game`);
  getStaticAssets();
  sleep(startDelay);</pre>
			<p>After this function loads the static assets, it sleeps for a random delay to simulate a user waiting at the splash screen:</p>
			<pre>  const gameDelay = random_gaussian(1500, 250) / 1000;
  const id = getGameId();
  log.debug(
    `Game ${id}: Reset game scores, then wait ${startDelay}s to start game`
  );
  getScores();
  putScores(id, 0);
  sleep(gameDelay);</pre>
			<p>After another delay, when simulating the user seeing the game screen, the test program enters a loop where it starts rapidly simulating clicks:</p>
			<pre>  log.info(`Game ${id}: Simulating ${MOVES} moves, starting in ${gameDelay}s`);
  for (let i = 0; i &lt; MOVES; i++) {
    const moveDelay = random_gaussian(125, 25) / 1000;</pre>
			<p>Notice that we use a randomly generated delay between moves with a Gaussian distribution that has a mean of <code>125</code> milliseconds and a standard deviation of <code>25</code> milliseconds. This simulates clicking at about 8 clicks/second, which is the rate we measured when playing ShipIt Clicker on an iPhone – in 1 minute, we recorded 480 clicks:</p>
			<pre>    log.debug(`Game ${id}: move #${i}, then sleep ${moveDelay}s`);
    deploy(id);
    sleep(moveDelay);
  }
  log.info(`Game ${id}: Done with ${MOVES} moves`);
}</pre>
			<p>The <code>default</code> function that's used for each virtual user fetches the same URLs that a user's browser would <a id="_idIndexMarker938"/>fetch on first page load. Note all the random delays that realistically simulate the delays that a real user would make. In a tight loop, the test simulates the user clicking as fast as a human would. The delay between clicks is subtly randomized using a random number with a normal distribution to simulate the fact that a human cannot click with robotic precision.</p>
			<p>The <code>chapter11/bin/k6-run.sh</code> script runs the test using the same environment variable pattern override that the <code>k6-har-run.sh</code> script did, but with more variables. It allows you to set these parameters:</p>
			<ul>
				<li><code>USERS</code>: Number of users</li>
				<li><code>DURATION</code>: Duration in seconds</li>
				<li><code>MOVES</code>: Number of moves in a game</li>
				<li><code>STAGES</code>: Specify a set of k6 stages, which can vary VUs over time </li>
			</ul>
			<p>The script requires a command-line argument, which is the URL target for the test. As mentioned earlier, this might be something like <code>http://192.2.0.10:80/</code> to test against the application infrastructure deployed on your workstation. Or, it could be the application as it was deployed to your cluster in the cloud, such as <a href="https://shipit-v8.eks.shipitclicker.com/">https://shipit-v8.eks.shipitclicker.com/</a>.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor274"/>Running a stress test</h2>
			<p>In order to run a stress test, you want to ramp up the amount of load on an application until it starts showing <a id="_idIndexMarker939"/>signs of failing. We can try doing that using the <code>script.js</code> k6 program and the <code>k6-run.sh</code> test harness. The key element that we must specify is the <code>STAGES</code> parameter:</p>
			<pre>$ MOVES=400 STAGES=900s:100 chapter11/bin/k6-run.sh https://shipit-v8.eks.example.com</pre>
			<p>You will likely find that with the default settings of two pods, this initial test will not show any signs of failure. You can use the <code>kubectl</code> command, plus Prometheus, Grafana, and Jaeger to monitor the test progress, plus the CPU and memory utilization in the cluster, as described in the previous chapter. For example, here is a screenshot of Grafana after the preceding load test:</p>
			<div><div><img src="img/B11641_11_004.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – The Grafana dashboard showing the rate of ShipIt Clicker deployments during the load test</p>
			<p>In order to get this deployment to fail during the stress test, we don't want it to automatically scale out. So, we will delete the Horizontal Pod Autoscaler:</p>
			<pre>kubectl delete hpa/shipit-v8-shipitclicker</pre>
			<p>We also want to stress test a single pod in order to see how much it can take, so we will shrink the number of replicas in the deployment to only <code>1</code>:</p>
			<pre>kubectl scale deployment/shipit-v8-shipitclicker --replicas=1</pre>
			<p>At this point, we can rerun the stress test using the preceding <code>k9-run.sh</code> command. Watch the output. You will probably see some failed requests, which should be logged in the k9 output with a warning that looks something like this:</p>
			<pre>time="2020-06-29T05:52:31Z" level=info msg="WARNING: PATCH https://shipit-v8.eks.example.com/api/v2/games/t2iAHlWtnhJhbsXfJI3zB/deploys: status 503"</pre>
			<p>Once we are done stress testing, we can recreate the Horizontal Pod Autoscaler and reset the number <a id="_idIndexMarker940"/>of replicas for the deployment to a higher number.</p>
			<p>At this point, we've learned how to use k6 to create a realistic load test and used it to perform a stress test of ShipIt Clicker. </p>
			<h1 id="_idParaDest-257"><a id="_idTextAnchor275"/>Summary</h1>
			<p>In this chapter, we explored the topic of scaling out clusters in Kubernetes by using the Cluster Autoscaler and the Horizontal Pod Autoscaler. We then explored the topic of service meshes and set up a minimalistic Envoy service mesh in order to provide proxying and transparent network communications for complex microservice architectures.</p>
			<p>Following this, we looked at how we could use the circuit breaker pattern to prevent a service from becoming overwhelmed by traffic. Then, we used connection thresholds to test that the circuit breaker worked, in conjunction with a simple load test technique, using Docker and Apache Bench. After this, we learned about progressively more sophisticated load testing techniques when using k6, including both record-and-playback and detailed hand-crafted load tests designed to mimic real user behavior.</p>
			<p>This brings us to the end of our <em class="italic">Running Containers in Production</em> section of this book. We're going to move on and look at security next. Here, we will learn how to apply some techniques to the projects and skills we have developed so far in this book to improve our container security posture. So, let's move on to <a href="B11641_12_Final_NM_ePub.xhtml#_idTextAnchor278"><em class="italic">Chapter 12</em></a>, <em class="italic">Introduction to Container Security</em>.</p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor276"/>Further reading</h1>
			<p>Use the following resources to expand your knowledge of autoscaling, the Envoy service mesh, and load testing:</p>
			<ul>
				<li>Envoy presentation from Lyft: <a href="https://www.slideshare.net/datawire/lyfts-envoy-from-monolith-to-service-mesh-matt-klein-lyft">https://www.slideshare.net/datawire/lyfts-envoy-from-monolith-to-service-mesh-matt-klein-lyft</a>.</li>
				<li><em class="italic">Performance Remediation Using New Relic and JMeter</em>, a three-part article series by the <em class="italic">Docker for Developers</em> co-author Richard Bullington-McGuire. This covers load testing and performance improvement basics. You can adapt these techniques to Kubernetes using Prometheus, Grafana, Jaeger, and k6.io: <a href="https://moduscreate.com/blog/performance-remediation-using-new-relic-jmeter-part-1-3/">https://moduscreate.com/blog/performance-remediation-using-new-relic-jmeter-part-1-3/</a>.</li>
				<li>Using a Network Load Balancer with the NGINX Ingress Controller on Amazon EKS – an economical and flexible alternative to using the ALB Ingress Controller for many scenarios: <a href="https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/">https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/</a>.</li>
				<li><em class="italic">Kubernetes Autoscaling 101: Cluster Autoscaler, Horizontal Pod Autoscaler, and Vertical Pod Autoscaler</em>: <a href="https://levelup.gitconnected.com/kubernetes-autoscaling-101-cluster-autoscaler-horizontal-pod-autoscaler-and-vertical-pod-2a441d9ad231">https://levelup.gitconnected.com/kubernetes-autoscaling-101-cluster-autoscaler-horizontal-pod-autoscaler-and-vertical-pod-2a441d9ad231</a>.</li>
				<li>Velero to backup and restore your Kubernetes cluster. Backup and restore your entire cluster, a namespace, or objects, filtered by tags: <a href="https://velero.io/">https://velero.io/</a>.</li>
				<li>Expose Envoy Prometheus metrics as <code>/metrics</code>. See this issue for the workaround that's integrated into ShipIt Clicker's Envoy configuration that lets you expose Envoy's metrics to the Prometheus metrics scraper by adding an additional Envoy mapping: <a href="https://github.com/prometheus/prometheus/issues/3756">https://github.com/prometheus/prometheus/issues/3756</a>.</li>
				<li>Microservicing with Envoy, Istio, and Kubernetes: <a href="https://thenewstack.io/microservicing-with-envoy-istio-and-kubernetes/">https://thenewstack.io/microservicing-with-envoy-istio-and-kubernetes/</a>.</li>
				<li>Jaeger Native Tracing with Envoy – an advanced tracing strategy: <a href="https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/jaeger_native_tracing">https://www.envoyproxy.io/docs/envoy/latest/start/sandboxes/jaeger_native_tracing</a>.</li>
				<li>Redis with Envoy Cheatsheet – setting up Redis and Envoy using TLS and Redis Auth: <a href="https://blog.salrashid.me/posts/redis_envoy/">https://blog.salrashid.me/posts/redis_envoy/</a>.</li>
				<li><em class="italic">Introduction to Modern Network Load Balancing and Proxying</em>, from Lyft's Matt Klein: <a href="https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236">https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236</a>.</li>
				<li><em class="italic">Matt Klein on the Success of Envoy and the Future of the Service Mesh</em>: <a href="https://thenewstack.io/matt-klein-on-the-success-of-envoy-and-the-future-of-the-service-mesh/">https://thenewstack.io/matt-klein-on-the-success-of-envoy-and-the-future-of-the-service-mesh/</a>.</li>
				<li><em class="italic">Cost Optimization for Kubernetes on AWS</em>. Once you get a handle on scaling, the next step is to reduce costs. The EKS cluster might cost between $10-20 per day to run with the defaults given in the AWS EKS Quick Start CloudFormation templates: <a href="https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/">https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/</a>.</li>
			</ul>
		</div>
	</body></html>