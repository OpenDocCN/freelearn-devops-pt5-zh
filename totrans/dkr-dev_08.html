<html><head></head><body>
		<div><h1 id="_idParaDest-99"><em class="italic"><a id="_idTextAnchor102"/>Chapter 6</em>: Deploying Applications with Docker Compose</h1>
			<p>The simplest possible practical deployment scenario of an application packaged with Docker involves running Docker Compose on a single host. Many of the commands that you use as a developer, such as <code>docker-compose up -d</code>, also apply to deploying Docker applications on a single host. </p>
			<p>Running Docker applications on a single host is easier to understand than running them using one of the more complex container orchestration systems because many of the same techniques you might use to run a non-Docker application apply; however, it has some significant drawbacks in terms of performance and availability. </p>
			<p>In this chapter, you will discover why this is the simplest practical option, learn how to configure Docker for production on a single host, and master some techniques for managing and monitoring a simple setup efficiently. Furthermore, you will better understand the drawbacks of running Docker on a single host, including the problems you may face.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Selecting a host and operating system for single-host deployment</li>
				<li>Preparing the host for Docker and Docker Compose</li>
				<li>Deploying using configuration files and support scripts</li>
				<li>Monitoring small deployments—logging and alerting</li>
				<li>Limitations of single-host deployment</li>
			</ul>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor103"/>Technical requirements</h1>
			<p>To complete the exercises in this chapter, you'll need Git and Docker on your local workstation, and you will need a single host capable of running Linux and Docker for your production server, connected to a network that you can SSH into and that your users can reach.</p>
			<p>The GitHub repository for this chapter can be found at <a href="https://github.com/PacktPublishing/Docker-for-Developers">https://github.com/PacktPublishing/Docker-for-Developers</a>—please refer to the <code>chapter6</code> folder.</p>
			<p>Check out the following video to see the Code in Action:</p>
			<p><a href="https://bit.ly/31OSi1H">https://bit.ly/31OSi1H</a></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor104"/>Example application – ShipIt Clicker v2</h2>
			<p>The <a id="_idIndexMarker285"/>version of ShipIt Clicker in this chapter is more polished than the one we used in <a href="B11641_05_Final_NM_ePub.xhtml#_idTextAnchor080"><em class="italic">Chapter 5</em></a>, <em class="italic">Alternatives for Deploying and Running Containers in Production</em>. It has the following features:</p>
			<ul>
				<li>An improved Dockerfile and <code>docker-compose.yml</code> file suitable for basic production use</li>
				<li>Storage of game state in Redis tied to a server session, leading to distinct game states for different client devices</li>
				<li>Improved visual and audio assets</li>
			</ul>
			<p>We will use this enhanced version of ShipIt Clicker as the application to deploy on a single host using Docker Compose.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor105"/>Selecting a host and operating system for single-host deployment</h1>
			<p>Deploying<a id="_idIndexMarker286"/> your application on a single host is the <a id="_idIndexMarker287"/>simplest possible way <a id="_idIndexMarker288"/>to run an application in production. In <a id="_idIndexMarker289"/>many ways, it resembles the user experience of performing local development using Docker and Docker Compose. If you can package the parts of your application using a <code>docker-compose.yml</code> file, you are already 70 percent of the way there. If you already have basic UNIX or Linux system administration skills, this will be very easy—this strategy requires the least effort and you can master the essentials in an hour or two.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor106"/>Requirements for single-host deployment</h2>
			<p>In order to proceed with <a id="_idIndexMarker290"/>deployment, you will need a computer running a modern Linux operating system of the same architecture as your development system, with enough memory and processor and storage capacity to run your application. If you are developing on a Windows 10 64-bit desktop using Docker Community Edition, you need a Linux system that also uses the x86_64 architecture. If you're using Docker on a Raspberry Pi 4 running Raspbian, you need an ARM architecture server. Really, you could use any bare metal or virtual machine server, either on-premises or in the cloud, as long as it supports Docker.</p>
			<p>Some cloud<a id="_idIndexMarker291"/> providers, such as <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), offer a free tier for their smallest virtual machine deployments, at least for the first year. The example in this chapter will work on a host like this, but if you have a larger application, you may need to use a larger and more expensive system.</p>
			<p>Production applications often must run <em class="italic">24*7</em>, and the users of these applications may have reliability concerns. While running Docker applications on a single host is possibly the least reliable way to proceed, it might be good enough for your application. All the single-host reliability measures that vendors such as HP, Dell, and IBM have built can be enough in many cases to ensure adequate reliability if your application requires that.</p>
			<p>You will need one of the following Linux operating system distributions that support Docker:</p>
			<ul>
				<li>Red Hat Enterprise Linux (or CentOS) 7 or 8</li>
				<li>Ubuntu 16.04 or 18.04 or newer</li>
				<li>Amazon Linux 2</li>
				<li>Debian Stretch 9</li>
				<li>Buster 10</li>
			</ul>
			<p>To minimize time to production and to maximize ease, pick one that you know already, or use CentOS 7, which is used in the following examples.</p>
			<p>Only select a Docker-focused distribution, such as Container Linux or CoreOS, if you want to take a slower, more advanced path to production, as your system administration skills may be less effective in those environments. User management in CoreOS, for example, works quite differently than it does in more mainstream distributions.</p>
			<p>Because this strategy depends <a id="_idIndexMarker292"/>only on having a host that the users of your application can reach, you have tremendous flexibility.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor107"/>Preparing the host for Docker and Docker Compose</h1>
			<p>Before you <a id="_idIndexMarker293"/>configure the software on the host, you should ensure <a id="_idIndexMarker294"/>that it has a stable IP address. Sometimes these <a id="_idIndexMarker295"/>are referred to as static IP addresses, or Elastic<a id="_idIndexMarker296"/> IP addresses, in an AWS context. You may need to specially allocate these IP address through your provider, which can often be done through the provider's console, such as with the <strong class="bold">Network</strong> tab in AWS Lightsail, or the <strong class="bold">Elastic IPs</strong> settings in the AWS EC2 console.</p>
			<p>Also, you should map<a id="_idIndexMarker297"/> an address (type <code>shipitclicker.example.com</code> instead of a raw IP address, such as <code>192.2.0.10</code>. All public cloud systems have the ability to manage <a id="_idIndexMarker298"/>DNS entries—for example, <a href="https://docs.aws.amazon.com/route53/index.html">AWS Route 53 (https://docs.aws.amazon.com/rout</a>e53/index.html), and most virtual hosting systems have this capacity as well.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor108"/>Using operating system packages to install Docker and Git</h2>
			<p>You will need to install <a id="_idIndexMarker299"/>Docker on the host. For production<a id="_idIndexMarker300"/> use, avoid the outdated <a id="_idIndexMarker301"/>Docker versions that ships with <a id="_idIndexMarker302"/>operating system distributions, and try to use the operating system  packages that Docker publishes for Docker Community Edition. You can find instructions on installing Docker Community Edition<a id="_idIndexMarker303"/> on the Docker website for various operating systems, a<a href="https://docs.docker.com/install/linux/docker-ce/centos/">s follows:</a></p>
			<ul>
				<li><a href="https://docs.docker.com/install/linux/docker-ce/centos/"><strong class="bold">CentOS</strong>: https://docs.docker.com/install/linu</a>x/docker-<a href="https://docs.docker.com/install/linux/docker-ce/debian/">ce/centos/</a></li>
				<li><a href="https://docs.docker.com/install/linux/docker-ce/debian/"><strong class="bold">Debian</strong>: https://docs.docker.com/install/linu</a>x/docker-<a href="https://docs.docker.com/install/linux/docker-ce/fedora/">ce/debian/</a></li>
				<li><a href="https://docs.docker.com/install/linux/docker-ce/fedora/"><strong class="bold">Fedora</strong>: https://docs.docker.com/install/linu</a>x/docker-<a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/">ce/fedora/</a></li>
				<li><a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/"><strong class="bold">Ubuntu</strong>: https://docs.docker.com/install/linu</a>x/docker-ce<a href="https://docs.docker.com/install/linux/docker-ce/binaries/">/ubuntu/</a></li>
				<li><a href="https://docs.docker.com/install/linux/docker-ce/binaries/"><strong class="bold">Binaries</strong>: https://docs.docker.com/install/linux/</a>docker-ce/binaries/</li>
			</ul>
			<p>Use the following <a id="_idIndexMarker304"/>commands for a fresh installation of CentOS 7:</p>
			<pre>$ sudo yum install -y yum-utils
$ sudo yum install -y device-mapper-persistent-data lvm2 
$ sudo yum-config-m<a href="https://download.docker.com/linux/centos/docker-ce.repo">anager --add-repo \</a>
<a href="https://download.docker.com/linux/centos/docker-ce.repo">https://download.docker.com/linux/c</a>entos/docker-ce.repo 
$ sudo yum install -y docker-ce docker-ce-cli containerd.io</pre>
			<p>Add <a id="_idIndexMarker305"/>your normal, non-root user to the<a id="_idIndexMarker306"/> Docker user group, and become<a id="_idIndexMarker307"/> a member of that group for this <a id="_idIndexMarker308"/>Terminal session:</p>
			<pre>$ sudo usermod -aG docker $USER
$ newgrp docker </pre>
			<p>Make sure the Docker service is enabled so that it will start on boot, and that the Docker service is started:</p>
			<pre>$ sudo systemctl enable docker
$ sudo systemctl restart docker</pre>
			<p>Install <code>docker-compose</code> by foll<a href="https://docs.docker.com/compose/install/">owing the directions at https://docs.do</a>cker.com/compose/install/. <code>1.25.3</code> is the latest version as of January 2020, but please check the version number on that page for the latest to put in the following command, which should all be one line:</p>
			<pre>$ sudo curl -L "https://github.com/docker/compose/releases/download/1.25.3/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
$ sudo chmod +x /usr/local/bin/docker-compose</pre>
			<p>Now<a id="_idIndexMarker310"/> that you have the Docker <a id="_idIndexMarker311"/>daemon running and enabled, and <a id="_idIndexMarker312"/>you also have <code>docker-compose</code> installed, you can deploy your application.</p>
			<p>Next, install <code>git</code> through<a id="_idIndexMarker313"/> your operating system's package manager. For Red Hat family distributions (such as RHEL, CentOS, Fedora, and Amazon Linux), use the following command:</p>
			<pre>$ sudo yum install -y git</pre>
			<p>For Debian family distributions (including Ubuntu), run the following command:</p>
			<pre>$ sudo apt-get update &amp;&amp; apt-get install -y git</pre>
			<p>At this point, the host is ready to deploy Docker applications. In order to complete deployment, we will use a strategy that relies on shell scripts and Docker environment configuration files.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor109"/>Deploying using configuration files and support scripts</h1>
			<p>To deploy our <a id="_idIndexMarker314"/>application to a production server, we <a id="_idIndexMarker315"/>will use a combination of simple <a id="_idIndexMarker316"/>commands and support scripts that <a id="_idIndexMarker317"/>start or update the running set of containers. Let's start by taking a close look at the two most important files required for deployment: <code>Dockerfile</code> and <code>docker-compose.yml</code>.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor110"/>Re-examining the initial Dockerfile</h2>
			<p>The <a id="_idIndexMarker318"/>Dockerfile from <a href="B11641_05_Final_NM_ePub.xhtml#_idTextAnchor080"><em class="italic">Chapter 5</em></a>, <em class="italic">Alternatives for Deploying and Running Containers in Production</em>, has good layering and has <code>package.json</code> and <code>package.json.lock</code> copied into the image before <code>RUN npm -s install</code> executes and before the main parts of the app are copied into the image. However, it has some rough edges, which we are going to smooth out in this chapter to prepare a solid production deployment. First, let's take a look at the initial Dockerfile:</p>
			<pre>FROM ubuntu:bionic
RUN apt-get -qq update &amp;&amp; \
    apt-get -qq install -y nodejs npm &gt; /dev/null
RUN mkdir -p /app/public /app/server
COPY src/package.json* /app
WORKDIR /app
RUN npm -s install
COPY src/.babelrc \
     src/.env \
     src/.nodemonrc.json \
     /app/
COPY src/public/ /app/public/
COPY src/server/ /app/server/
EXPOSE 3000
ENTRYPOINT DEBUG='shipit-clicker:*' npm run dev</pre>
			<p>The preceding <a id="_idIndexMarker319"/>Dockerfile for the ShipIt Clicker game prototype gets many things right from a local development perspective, but has some limitations, which we will address in the Dockerfile for this chapter.</p>
			<p>Very often, developers start with a base image (such as <code>FROM ubuntu:bionic</code>) that mirrors what they know best: traditional Linux distributions that you might run on your workstation. This may help with debugging the Dockerfile initially, but it comes at a steep cost because both the base and generated images are large, consisting of hundreds of megabytes. Also, the package installation for Ubuntu is quite verbose, so the <code>apt-get install</code> command has to redirect <code>stdout</code> to <code>/dev/null</code> to prevent verbose output <a href="https://askubuntu.com/a/1134785">from taking over our Terminal (</a>see <a href="https://askubuntu.com/a/1134785">https://askubuntu.com/a/1134785</a>).</p>
			<p>The rest of the initial Dockerfile has some common quirks that you should avoid for production, such as copying configuration files for all of the development tooling (see the <code>COPY</code> command, which copies dotfiles). The initial Dockerfile has an entry point (<code>ENTRYPOINT</code>) that refers to a server that is best suited for development, not production, because it was quick and easy to define that way. A real production setup requires a build step that will create a set of assets suitable for distribution, as well as a different <code>npm</code> command that <a id="_idIndexMarker320"/>launches the app using those assets.</p>
			<p>The Dockerfile for this chapter has corrections for all of these issues:</p>
			<pre>FROM alpine:20191114
RUN apk update &amp;&amp; \
    apk add nodejs nodejs-npm
RUN addgroup -S app &amp;&amp; adduser -S -G app app
RUN mkdir -p /app/public /app/server
ADD src/package.json* /app/
WORKDIR /app
RUN npm -s install
COPY src/public/ /app/public/
COPY src/server/ /app/server/
COPY src/.babelrc /app/
RUN npm run compile
USER app
EXPOSE 3000
ENTRYPOINT npm start</pre>
			<p>In this revised Dockerfile, we use Alpine Linux instead of Ubuntu for smaller images, and we pin the version of Alpine for consistent builds. The container image based on Alpine Linux is 71% smaller:</p>
			<pre>$ docker images | awk '/chapter._ship/{ print $1 " " $7}'
chapter6_shipit-clicker-web-v2 154MB
chapter5_shipit-clicker-web 524MB</pre>
			<p>In the revised Dockerfile, we also create an <code>app</code> user so that Docker runs the application as a normal UNIX user, not the <code>root</code> user, as that can exacerbate security problems. </p>
			<p>After installing the <a id="_idIndexMarker321"/>operating system packages and <code>npm</code> packages as silently as possible, we can copy the application files and the <code>.babelrc</code> configuration file into <code>/app</code>, and then run <code>RUN npm run compile</code> in order to prepare the production version of the node application, which we run as the <code>app</code> user with <code>ENTRYPOINT npm start</code>.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor111"/>Re-examining the initial docker-compose.yml file</h2>
			<p>The initial <code>docker-compose.yml</code> file from<a id="_idIndexMarker322"/> the previous chapter gets the job done of starting both a web and a Redis container, but it has some deficiencies. The initial <code>docker-compose.yml</code> file was adapted from the barebones e<a href="https://docs.docker.com/compose/">xample in the Docker documentati</a>on at <a href="https://docs.docker.com/compose/">https://docs.docker.com/compose/</a>, so it has some gaps in how ready it is for production use. Many developers adapt these examples without considering certain nuances that matter when you have to deploy an application to production. You can think of it as a starting point, rather than the final destination. The initial <code>docker-compose.yml</code> file is as follows:</p>
			<pre>---
version: '3'
services:
  shipit-clicker-web:
    build: .
    environment:
      REDIS_HOST: redis
    ports:
    - "3005:3000"
    links:
    - redis
  redis:
    image: redis
    ports:
    - "6379:6379"</pre>
			<p>The revised <code>docker-compose.yml</code> file for this chapter is much more robust. This file <a href="https://github.com/docker-library/redis/issues/111">is inspired in part by the samples at https://gith</a>ub.com/docker-library/redis/issues/111 and <a id="_idIndexMarker323"/>especially by an example by GitHub user <code>@lagden</code>, which has a nice example of a <code>docker-compose.yml</code> file that supports Redis:</p>
			<pre>---
version: '3'
services:
  shipit-clicker-web-v2:
    build: .
    environment:
        - APP_ID=shipit-clicker-v2
        - OPENAPI_SPEC=/api/v1/spec
        - OPENAPI_ENABLE_RESPONSE_VALIDATION=false
        - PORT=3000
        - LOG_LEVEL=${LOG_LEVEL:-debug}
        - REQUEST_LIMIT=100kb
        - REDIS_HOST=${REDIS_HOST:-redis}
        - REDIS_PORT=${REDIS_PORT:-6379}
        - SESSION_SECRET=${SESSION_SECRET:-mySecret-v2}</pre>
			<p>Note that we define all the environment variables explicitly for the application, and that several of them are defined with a <code>${VARIABLE_NAME:-default_value}</code> syntax that uses the value of an environment variable. These can be specified on the command line, in the usual configuration file: <code>$HOME/.profile</code>, <code>$HOME/.bashrc</code>, or the <code>.env</code> file in the same directory as the <code>docker-compose.yml</code> file:</p>
			<pre> ports:
      - "${PORT:-3006}:3000"
    networks:
      - private-redis-shipit-clicker-v2
    links:
      - redis
    depends_on:
      - redis</pre>
			<p>The preceding <code>ports</code> section defines the networking configuration for the main container; it defines a private <a id="_idIndexMarker324"/>network called <code>private-redis-shipit-clicker-v2</code>, which links the two containers. Note the use of <code>depends_on</code> in this section. This means that the ShipIt Clicker container will wait until the Redis container is started before starting. Next, let's examine the Redis container definition:</p>
			<pre>  redis:
    command: ["redis-server", "--appendonly", "yes"]
    image: redis:5-alpine3.10
    volumes:
      - redis-data-shipit-clicker:/data
    networks:
      - private-redis-shipit-clicker-v2
volumes:
  redis-data-shipit-clicker: {}
networks:
  private-redis-shipit-clicker-v2:</pre>
			<p>This has many environment variable entries—for example, <code>LOG_LEVEL</code>, <code>REDIS_HOST</code>, and <code>REDIS_PORT</code>—that allow easy overrides. It allows the override of Redis host settings, both for easier debugging and to pave the way for easy connection to cloud Redis services. It starts Redis with command-line parameters that enable persistence and allocates a Docker persistent volume to store Redis append-only log files. Otherwise, the data would vanish every time the Redis container is restarted. It makes the network where Redis and the web server communicates private. This is especially important with Redis because, with the default configuration, the Redis server operates without any authentication <a id="_idIndexMarker325"/>or authorization—it is wide open to whoever can connect! </p>
			<p>In this minimalistic, production-ready <code>docker-compose.yml</code> file, we expose the web server directly on port <code>80</code> to the world. This works, but modern browsers will show a security warning for plain HTTP content. It will work to get you to production, but many production applications require more security safeguards than running over plain HTTP. You can get around this by using either a proxy or external load balancer that terminates HTTPS on port <code>443</code>, or by configuring SSL certificates. We will cover this in more detail in later chapters.</p>
			<p>One of the features of the <code>docker-compose</code> v3 configuration is that it sets the default behavior for when a container fails to <em class="italic">always restart</em>. This should happen even if the host is rebooted, and will definitely happen if a process exits due to an unhandled exception. If you need to configure the restart behavior of your application more directly, you can do so with the settings listed in the documentation at <a href="https://docs.docker.com/compose/compose-file/#restart_policy">https://docs.docker.com/compose/compose-file/#restart_policy</a>.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor112"/>Preparing the production .env file</h2>
			<p>Clone the<a id="_idIndexMarker326"/> repository and prepare to configure <code>docker-compose</code>:</p>
			<pre>$ git clone https://github.com/PacktPublishing/Docker-for-Developers.git
$ cd Docker-for-Developers/chapter6</pre>
			<p>In order to configure your application for production, you should create a file called <code>.env</code> in the directory where your <code>docker-compose.yml</code> file lives. If you want to change any of the defaults—for example, to change the level of debugging shown in production from <code>info</code> to <code>debug</code>—you should do so through creating and editing the <code>.env</code> file associated with the <a id="_idIndexMarker327"/>production deployment. Copy the file, <code>env.sample</code>, to <code>.env</code> and edit it to suit your preferences for production.</p>
			<h3>Handling secrets</h3>
			<p>This demo application uses<a id="_idIndexMarker328"/> environment variables and an <code>.env</code> file to store secrets. This is in accordance wit<a href="https://12factor.net/config">h the 12-factor application</a> principles (see https://12factor.net/config), but it is certainly not the only way, or the most secure way, to deal with secrets. You could use a secret management system, such as HashiCorp Vault or Amazon Secrets Manager, to store and retrieve secrets. We will cover this in detail in both <a href="B11641_08_Final_AM_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying Docker Apps to Kubernetes</em>, and <a href="B11641_14_Final_NM_ePub.xhtml#_idTextAnchor316"><em class="italic">Chapter 14</em></a>, <em class="italic">Advanced Docker Security – Secrets, Secret Commands, Tagging, and Labels</em>; but for now, let's just use environment variables for the secrets.</p>
			<p>You should replace the secret in the environment variable, <code>SESSION_SECRET</code>, with a random secret and confirm whether you want to expose port <code>80</code> to the world. Use whatever editor you are comfortable with, whether that is <code>vi</code>, <code>emacs</code>, or <code>nano</code>:</p>
			<pre>cp env.sample .env
vi .env</pre>
			<p>Once you have set the environment variable overrides, you can deploy the application.</p>
			<h3>Deploying for the first time</h3>
			<p>Once you have<a id="_idIndexMarker329"/> copied your <code>.env</code> file in place, start the services in the background to deploy the application:</p>
			<pre>$ docker-compose up -d</pre>
			<p>Verify that the services are running, as follows:</p>
			<pre>$ docker-compose ps
    Name           Command      State       Ports
-----------------------------------------------------
chapter6_redi   docker-         Up      6379/tcp
s_1             entrypoint.sh
                redis ...
chapter6_ship   /bin/sh -c      Up      0.0.0.0:80-
it-clicker-     npm start               &gt;3000/tcp
web-v2_1</pre>
			<p>Check whether the system logs show any errors:</p>
			<pre>$ docker-compose logs</pre>
			<p>As long as you don't see a<a id="_idIndexMarker330"/> stream of error messages in the logs, you should then be able to reach the website at<a href="http://192.0.2.10"> the IP address o</a>f the server—for example, at <code>http://192.0.2.10</code>—substituting your IP address. If you assigned a hostname using DNS, you should be able to<a href="http://shiptclicker.example.com"> reach it using that hostname—f</a>or example, at <a href="http://shiptclicker.example.com">http://shiptclicker.example.com</a>—substituting the full canonical domain name for this one.</p>
			<h3>Troubleshooting common errors</h3>
			<p>If you get an error<a id="_idIndexMarker331"/> like this, you need to ensure that the host is not running another web server, such as Apache HTTPD or NGINX:</p>
			<pre>docker.errors.APIError: 500 Server Error: Internal Server Error ("b'Ports are not available: listen tcp 0.0.0.0:80: bind: address already in use'")</pre>
			<p>If you get this issue, you should either uninstall the web server that is running on the host or change what port it uses to listen for requests. You could also change the port that ShipIt Clicker runs on by changing the <code>PORT</code> variable in the <code>.env</code> file. For Red Hat family systems, a server listening on port <code>80</code> is likely to be Apache HTTPd, and you can remove it with the following:</p>
			<pre>$ yum remove -y httpd</pre>
			<p>For Debian family systems, it is also likely to be Apache, and you will need to use the following command to remove it:</p>
			<pre>$ apt-get remove -y apache2</pre>
			<p>It is possible that you might have some other web server running. You can find out what the process name of your web server is with <code>netstat</code>:</p>
			<pre>$ sudo netstat -nap | grep :80
tcp6       0      0 :::80                   :::*                    LISTEN      12037/httpd </pre>
			<p>You may not need to do any troubleshooting to get your application running in Docker, but in a single-host <a id="_idIndexMarker332"/>deployment scenario, you can use your system administration troubleshooting skills to figure out what might be going wrong.</p>
			<p>Once you have the application running, you may find that you run some of the same operations repeatedly, such as rebuilding the application when you have made changes. This is where support scripts come in handy.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor113"/>Supporting scripts</h2>
			<p>When running a site in <a id="_idIndexMarker333"/>production, you might have to do some operations frequently. It becomes tiresome to remember the exact sequence of the Docker commands required to restart and update the running system or to connect to the database.</p>
			<p>You should continue to develop your application on your local workstation and use the production system to deploy changes to your users, once you have tested things locally.</p>
			<p>With the improved networking setup in this chapter, it is no longer possible to connect directly to the Redis container via a direct TCP port, so we will use <code>docker exec</code> within a script to do that.</p>
			<p>If you are in the <code>Docker-for-Developers</code><code>/chapter6</code> directory, you can permanently add this directory to <code>PATH</code> with the following commands to make running these scripts more convenient:</p>
			<pre>$ echo "PATH=$PWD:$PATH" | tee -a "$HOME/.bash_profile"
$ . "$HOME/.bash_profile"</pre>
			<p>The most common operations for this application are probably restarting the application, deploying changes, and connecting to Redis to troubleshoot. For these operations, we will use the <code>restart.sh</code> script, the <code>deploy.sh</code> script, and the <code>redis-cli.sh</code> script.</p>
			<h3>Restarting</h3>
			<p>The <code>restart.sh</code> script<a id="_idIndexMarker334"/> will restart all the containers. You should run this after you make a change to the configuration file, <code>.env</code>. You could just run <code>docker-compose up -d</code>, but that alone will not tell you whether the changes took hold. This will also run <code>docker-compose ps</code> for you, which will show you whether your containers are running correctly after the change, including what the port mappings are. In the following example session, we remove the <code>.env</code> file entirely and then recreate it with just a single setting for <code>PORT=80</code>:</p>
			<pre>[centos@ip-172-26-0-237 chapter6]$ rm .env
[centos@ip-172-26-0-237 chapter6]$ deploy.sh
chapter6_redis_1 is up-to-date
Recreating chapter6_shipit-clicker-web-v2_1 ... done
              Name                            Command               State           Ports         
--------------------------------------------------------------------------------------------------
chapter6_redis_1                   docker-entrypoint.sh redis ...   Up      6379/tcp              
chapter6_shipit-clicker-web-v2_1   npm start                        Up      0.0.0.0:3006-&gt;3000/tcp
[centos@ip-172-26-0-237 chapter6]$ echo 'PORT=80' &gt; .env
[centos@ip-172-26-0-237 chapter6]$ restart.sh
chapter6_redis_1 is up-to-date
Recreating chapter6_shipit-clicker-web-v2_1 ... done
              Name                            Command               State          Ports        
------------------------------------------------------------------------------------------------
chapter6_redis_1                   docker-entrypoint.sh redis ...   Up      6379/tcp            
chapter6_shipit-clicker-web-v2_1   npm start                        Up      0.0.0.0:80-&gt;3000/tcp
[centos@ip-172-26-0-237 chapter6]$</pre>
			<p>You can see that the <code>chapter6_shipit-clicker-web-v2_1</code> application was recreated the second time that <code>restart.sh</code> was run, and that the server is now connected to the<a id="_idIndexMarker335"/> wildcard IPv4 <code>0.0.0.0</code> address on port <code>80</code>. This will allow the server to respond to an HTTP request without a special port number in the URL.</p>
			<h3>Deploying</h3>
			<p>The <code>deploy.sh</code> script<a id="_idIndexMarker336"/> pulls changes from the <code>git</code> upstream repository, builds the container, and restarts any containers requiring an update. You should use this after you have made changes to the code and tested them locally.</p>
			<h3>Redis</h3>
			<p>The <code>redis-cli.sh</code> script<a id="_idIndexMarker337"/> will allow you to connect to the running Redis server in the command line. It uses a <code>docker exec</code> command, which attaches to the running container and starts a new <code>redis-cli</code> command within it This is needed in part because now, Redis is running in an isolated network, and you should not be able to reach it via TCP sockets, even from the production host. This will let you troubleshoot any issues with the backend server.</p>
			<p>Here is a sample session showing <code>redis-cli.sh</code> in action:</p>
			<pre>[centos@ip-172-26-0-237 chapter6]$ ./redis-cli.sh
127.0.0.1:6379&gt; help
redis-cli 5.0.7
To get help about Redis commands type:
      "help @&lt;group&gt;" to get a list of commands in &lt;group&gt;
      "help &lt;command&gt;" for help on &lt;command&gt;
      "help &lt;tab&gt;" to get a list of possible help topics
      "quit" to exit
To set redis-cli preferences:
      ":set hints" enable online hints
      ":set nohints" disable online hints
Set your preferences in ~/.redisclirc
127.0.0.1:6379&gt; keys *
1) "example/deploys"
2) "example/nextPurchase"
3) "example/score"
127.0.0.1:6379&gt; get example/score
"209"
127.0.0.1:6379&gt; quit</pre>
			<p>Note that you can use this <code>redis-cli.sh</code> script to connect to the Redis server, even though it is on a private virtual network that would be inaccessible if you had installed the standard <code>redis-cli</code> program on<a id="_idIndexMarker338"/> the host. Being able to rely on tools in a container can allow you to reach deep into the configuration of an application, even though it is protected from being directly exposed to the internet.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor114"/>Exercise – keeping builds off the production server</h2>
			<p>The deployment script for<a id="_idIndexMarker339"/> this chapter does the simplest thing possible for updates: it rebuilds the container on the production server. This might, however, lead to resource exhaustion and bringing the production server down.</p>
			<p>Given what you learned about Docker Hub in <a href="B11641_04_Final_NM_ePub.xhtml#_idTextAnchor059"><em class="italic">Chapter 4</em></a>, <em class="italic">Composing Systems Using Containers</em>, how might you change the workflow of application development to revise the <code>docker-compose.yml</code> file and the <code>deploy.sh</code> script to avoid building the Docker container on the production server?</p>
			<p>Write down one or two sentences <a id="_idIndexMarker340"/>describing the workflow that you would use and what alterations to the <code>docker-compose.yml</code> configuration file would be needed.</p>
			<p class="callout-heading">Note:</p>
			<p class="callout">There are multiple ways to achieve these goals, and there is no single answer to how to achieve them. You can compare your answer with the <code>docker-compose.yml</code> file in the next chapter to see how your ideas compare to the solution for building the containers highlighted in that chapter.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor115"/>Exercise – planning to secure the production site</h2>
			<p>Imagine that you hear from<a id="_idIndexMarker341"/> your boss that the ShipIt Squirrel code and production systems are going to get some attention from your company's chief information security officer, who is going to go through everything looking for weaknesses. He is concerned that in the rush to get this live, too many shortcuts have been taken, and he wants you to provide some more information to him. Please write down the answers to these three questions:</p>
			<ol>
				<li>What could be done to secure communication between the clients and the server with SSL? Which of the following should you do? <p>a. Terminate SSL within the program itself.</p><p>b. Use an external load balancer to terminate SSL.</p><p>c. Use a web server on the host, but outside Docker, to terminate SSL.</p><p>d. Use Docker and a web server container to terminate SSL.</p></li>
				<li>How do you plan on renewing the SSL certificate periodically?</li>
				<li>Are there other weaknesses in the security of the current system that you can find, either at the Docker layer or the API layer? </li>
			</ol>
			<p>Once you have deployed the application and considered some enhancements to its security, you should learn how to monitor the deployment so that you can find out when something goes wrong before the<a id="_idIndexMarker342"/> users of your application notice.</p>
			<p class="callout-heading">Answers for how to secure the production site:</p>
			<p class="callout">Any of the four options for <em class="italic">Question 1</em> could work, but options <em class="italic">b</em> and <em class="italic">d</em> are the most robust and stable in practice. Option <em class="italic">a</em> is tricky to get right, and option <em class="italic">c</em> requires separate updates to the application environment.</p>
			<p class="callout">Regarding <em class="italic">Question 2</em>, you can either purchase an SSL certificate from a vendor, which you must renew and reinstall every year, you can rely on the vendor of your load balancer to automatically renew your certificate (if they offer that as an option), or you can use Let's Encrypt to automatically renew the certificate. See the <em class="italic">Further reading</em> section of the next chapter for more about using Let's Encrypt to renew the certificate, as well as using a set of Docker containers to terminate SSL.</p>
			<p class="callout"><em class="italic">Question 3</em> is open-ended, but the first thing that you should notice is that there is no authentication or authorization built into the web services in the <code>chapter6</code> code base.</p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor116"/>Monitoring small deployments – logging and alerting</h1>
			<p>One of the nice things about<a id="_idIndexMarker343"/> starting small is that you may be able to rely on very simple mechanisms for both logging and alerting. For any deployment using Docker and <code>Docker Compose</code> on a single host—for example, a deployment of ShipIt Clicker—you can use some basic tools and commands to deal with logging, and a variety of simple alerting services provided by third parties to deal with alerting.</p>
			<h3>Logging</h3>
			<p>For logging, in many cases, all <a id="_idIndexMarker344"/>that is required is to use the logs built into Docker. Docker captures the standard output and standard error file handles of every process it starts and makes them available as logs for each container. You can review the consolidated logs for all the services started since the last container restart with the following command, assuming you are in the directory where your <code>docker-compose.yml</code> file is present (<code>less -R</code> will interpret the ANSI color escapes that the <code>logs</code> command produces):</p>
			<pre>$ docker-compose logs 2&gt;&amp;1 | less -R</pre>
			<p>You can also do <code>docker ps</code> in order <a id="_idIndexMarker345"/>to find the name of the running containers so that you can retrieve their log streams:</p>
			<pre> [centos@ip-172-26-0-237 ~]$ docker ps
CONTAINER ID        IMAGE                            COMMAND                  CREATED      
       STATUS              PORTS                  NAMES
e947e7de33ef        chapter6_shipit-clicker-web-v2   "npm start"              4 hours ago  
       Up 4 hours          0.0.0.0:80-&gt;3000/tcp   chapter6_shipit-clicker-web-v2_1
3f91820e097b        redis:5-alpine3.10               „docker-entrypoint.s…"   4 hours ago  
       Up 4 hours          6379/tcp               chapter6_redis_1</pre>
			<p>Once you have the names of the containers, you can retrieve the individual log files for each running container separately. You can pipe them to <code>less</code>, or redirect the output of the logs to a file, for example:</p>
			<pre> [centos@ip-172-26-0-237 ~]$ docker logs chapter6_shipit-clicker-web-v2_1 &gt; shipit.log
[centos@ip-172-26-0-237 ~]$ tail shipit.log 
&gt; shipit-clicker@1.0.0 start /app
&gt; node dist/index.js
{"level":30,"time":1580087119723,"pid":16,"hostname":"e947e7de33ef","name":"shipit-clicker-
v2","msg":"Redis connection established","redis_url":"redis://redis:6379","v":1}
{"level":30,"time":1580087119934,"pid":16,"hostname":"e947e7de33ef","name":"shipit-clicker-
v2","msg":"up and running in development @: e947e7de33ef on port: 3000}","v":1}
[centos@ip-172-26-0-237 ~]$</pre>
			<p>This procedure does require you to <a id="_idIndexMarker346"/>log into the production server and run some commands there, but in practice, this is a good way to examine the logs of an application running on a single host.</p>
			<h3>Alerting</h3>
			<p>To begin, it would be enough to <a id="_idIndexMarker347"/>monitor the HTTP server on port <code>80</code> of the production server to ensure it stays alive. If you have access to a network monitoring system for your company—for example, a Nagios or Icinga server—you could use that. If the system is accessible via the internet, you can use a free monitoring service, such as <a href="https://uptimerobot.com">https://uptimerobot.com</a>, to monitor the server.</p>
			<p>In order to extend monitoring deeper, you might want to also monitor the internal services, such as Redis. This is more challenging in a simple setup like this one, though. We will go into more depth about advanced monitoring systems in <a href="B11641_10_Final_AM_ePub.xhtml#_idTextAnchor226"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring Docker Using Prometheus, Grafana, and Jaeger</em>.</p>
			<p>The basic idea here is that you want to get either an email, an SMS message, or both if the system goes down.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor117"/>Limitations of single-host deployment</h1>
			<p>What could go wrong with <a id="_idIndexMarker348"/>deploying a Docker application to a single host? Plenty! While single-host deployment offers operational simplicity, it has some major limitations. Let's look at some of the limitations in the following sections.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor118"/>No automatic failover</h2>
			<p>If either the database server container or the web service container fails and cannot be restarted automatically, the site will be down and will require manual intervention. This might be as simple as noticing that your monitoring system says that the site is down, and so you need to SSH in and reboot the server. But sometimes, a single server will be so low on memory that it must be manually rebooted from a higher-level console or even power-cycled manually. This tends to lead to significant periods of time where an application is down and not available to serve requests.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor119"/>Inability to scale horizontally to accept more load</h2>
			<p>What happens if the traffic for the system exceeds the current capacity? In single-host deployment, you may be able to switch the host to a larger computer with more memory and processors, which is called <em class="italic">vertical scaling</em>. That is much easier in a cloud environment than it is in an environment, where you have to deal with physical hardware, such as an on-premises or data center environment. It would be much harder to adapt these simple deployment techniques to a whole fleet of server instances—which is called <em class="italic">horizontal scaling</em>.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor120"/>Tracking down unstable behavior based on incorrect host tuning</h2>
			<p>Depending on your hosting provider, the base operating system you start with, and how the Docker containers are configured, you might experience instability that is hard to track down. Maybe your host gets rebooted frequently due to the provider's network detecting unstable hardware or network conditions. Maybe you have configured your operating system to install automatic updates and applying them causes periods of outages. Maybe the application grows in memory until it triggers a failure of some kind.</p>
			<p>For simplicity's sake, the<a id="_idIndexMarker349"/> examples in this chapter do not specify memory limits at an application or container level. This means that the Redis container could consume all available memory on the host since it lacks a <code>max_memory</code> setting in its application-level main configuration file. It also means that the node container running the Express <a id="_idIndexMarker350"/>web application could leak memory until the operating system <strong class="bold">Out-Of-Memory</strong> (<strong class="bold">OOM</strong>) killer terminates it or the Docker daemon.</p>
			<p>One way of mitigating this problem is by configuring virtual memory on the host using a swap file or swap partition, which makes the system look as if it has more physical memory than it actually does. If you do not configure a swap file on the host, you may find that running the <code>deploy.sh</code> script will fail. You might not see any messages in the console when this happens, but if you check <code>/var/log/messages</code>, you will find traces of the Linux kernel's OOM killer terminating the <code>npm</code> install program or another part of the Docker container build process.</p>
			<p>See the Docker documentation for more on the dangers of not configuring the memory for your containers and operating system appropriately:</p>
			<p><a href="https://docs.docker.com/config/containers/resource_constraints/">https://docs.docker.com/config/containers/resource_constraints/</a></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor121"/>Loss of single host could be disastrous – backups are essential</h2>
			<p>If you have hosted your application on a single physical or virtual server, you should ensure that the system is backed up regularly. Many providers have an image backup service that you can configure to take daily backups and preserve them for some period of time for an extra cost. You could also script backups of the critical volumes using old-school methods, such as using<a href="https://restic.readthedocs.io/en/latest/"> TAR and SSH or using a modern backup sy</a>stem, such as <code>restic</code> (see <a href="https://restic.readthedocs.io/en/latest/">https://restic.readthedocs.io/en/latest/</a>), to back up the files and volumes to a cloud storage system.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor122"/>Case study – migrating from CoreOS and Digital Ocean to CentOS 7 and AWS</h2>
			<p>One of the authors, Richard Bul<a href="https://freezingsaddles.org/">lington-McGuire, maintained </a>a winter cycling competition website, <a href="https://freezingsaddles.org/">https://freezingsaddles.org/</a>, on a Digital Ocean droplet using CoreOS for more than a year. This system would frequently be knocked offline after a reboot, and it was difficult to track down exactly what the problems were that caused the periodic outages. Lack of console access to the <a id="_idTextAnchor123"/>Digital Ocean control panel and a lack of familiarity with CoreOS <a id="_idIndexMarker351"/>made troubleshooting the system even more difficult. To ensure that the system was backed up, <code>restic</code> was installed and configured to send backups to Amazon S3. After many frustrating system administration experiences, the system was moved over to AWS using Lightsail, running CentOS 7 as a host operating system. To guard against OOM conditions, the new system ran with a swap file equal in size to RAM. After this, the system stopped randomly failing every few days and operations became much more smooth. Additionally, the new system had daily automatic snapshot backups enabled, lessening the need to back up the system with an application-level tool such as <code>restic</code>. Even so, if the system reboots, the web server does not always come up smoothly, with manual intervention required to restore the service.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor124"/>Summary</h1>
			<p>The simplest way to get your Docker-based application to production is to deploy it onto a single host with Docker Compose. If you have properly prepared the host with the right software, including Docker Compose, you can deploy your application there in a production-ready configuration. This can be completed in a matter of hours and can serve applications with low to moderate performance and availability demands efficiently. If you make the right adjustments to your configuration files, your application will be ready to deploy to production. By using shell scripts that encapsulate long, verbose commands, you can more easily handle regular maintenance and updates for your applications. In the simplest case, you can use external monitoring and alerting for this class of application and handle this concern with low effort.</p>
			<p>You can apply what you have learned in this chapter to increase the sophistication of the Dockerfile and the <code>docker-compose.yml</code> file that support your application. You can craft simple shell scripts to automate the most common applications. You will have learned<a href="https://uptimerobot.com"> that you can rely on e</a>xternal monitoring through services such as <a href="https://uptimerobot.com">https://uptimerobot.com</a> to provide simple availability monitoring, and that you can use the built-in Docker logging facilities to provide insights into the operations of your application.</p>
			<p>Once you have an application deployed, it would be a good idea to increase the level of automation surrounding it, particularly related to how you can build and deploy the application. In the next chapter, we will see how you can use Jenkins, a common continuous integration system, to<a href="https://www.packtpub.com/free-ebooks/virtualization-and-cloud/docker-cookbook-second-edition/9781788626866"> automate deployment and testing.</a></p>
			<h1 id="_idParaDest-121"><a href="https://www.packtpub.com/free-ebooks/virtualization-and-cloud/docker-cookbook-second-edition/9781788626866">Further reading</a></h1>
			<ul>
				<li><a href="https://www.packtpub.com/free-ebooks/virtualization-and-cloud/docker-cookbook-second-edition/9781788626866"><em class="italic">Docker Cookbook</em>: https://www.packtpub.com/free-ebooks/vi</a>rtualization-and-cloud/docke<a href="https://docs.docker.com/compose/production/">r-cookbook-second-edition/9781788626866</a></li>
				<li><a href="https://docs.docker.com/compose/production/"><em class="italic">Use</em></a><em class="italic"> Compose in production</em>: https:/<a href="https://geekflare.com/best-open-source-monitoring-software/">/docs.docker.com/compose/production/</a></li>
				<li><a href="https://geekflare.com/best-open-source-monitoring-software/">Open source monitoring</a> tools: https://geekflar<a href="https://www.dnsstuff.com/free-network-monitoring-software">e.com/best-open-source-monitoring-software/</a></li>
				<li><a href="https://www.dnsstuff.com/free-network-monitoring-software">Free monitori</a>ng tools: https://www.dnsstuff.com/free-ne<a href="https://vsupalov.com/docker-compose-production/">twork-monitoring-software</a></li>
				<li><a href="https://vsupalov.com/docker-compose-production/">Is <code>docker-compose</code> sui</a>ted for production? <a href="https://vsupalov.com/docker-compose-production/">https://vsupalov.com/docker-compose-production/</a></li>
				<li>Docker tip 2: the difference between <code>COPY</code> and <code>ADD</code> in a Dockerfile: <a href="https://nickjanetakis.com/blog/docker-tip-2-the-difference-between-copy-and-add-in-a-dockerile">https://nickjanetakis.com/blog/docker-tip-2-the-difference-between-copy-and-add-in-a-dockerile</a></li>
			</ul>
			<p>If you are running a real production application on a single host with docker-compose, you should strongly consider securing your site with SSL. You can use Let's Encrypt and a host of Docker sidecar containers to achieve this:</p>
			<ul>
				<li>How to use Let's Encrypt, NGINX, and Docker to secure your site with SSL: <a href="https://github.com/nginx-proxy/docker-letsencrypt-nginxproxy-companion">https://github.com/nginx-proxy/docker-letsencrypt-nginxproxy-companion</a></li>
				<li>Using <code>docker-compose.yml</code> to configure Let's Encrypt with NGINX and Docker: <a href="https://github.com/nginx-proxy/docker-letsencryptnginx-proxy-companion/blob/master/docs/Docker-Compose.md">https://github.com/nginx-proxy/docker-letsencryptnginx-proxy-companion/blob/master/docs/Docker-Compose.md</a></li>
			</ul>
		</div>
	</body></html>