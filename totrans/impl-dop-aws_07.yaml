- en: Chapter 7. Metrics, Log Collection, and Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Metrics, Log Collection, and Monitoring](img/image_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That's it. This chapter could well have ended here but I shall carry on for
    the benefit of those amongst us who would prefer things in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: A great deal of the DevOps practice(s) evolve around the idea of being able
    to review and react to the state of your infrastructure at any given time – should
    you need to.
  prefs: []
  type: TYPE_NORMAL
- en: That is not to say, setup e-mail notifications for every time the date changes
    on your host, but a stream of sensible, usable amount of event data which would
    allow an operator to make a reasonably informed decision under stress and/or uncertainy.
  prefs: []
  type: TYPE_NORMAL
- en: If you have been paying attention in life so far, you would have noticed many
    a wise man talking about *balance, the golden middle*.
  prefs: []
  type: TYPE_NORMAL
- en: You should aim to configure your monitoring system in a way that you are notified
    of events of potential interest and in a timely manner. The notifications should
    arrive in a format that is hard to overlook, and should provide enough detail
    for an operator to be able to make an informed guess at what is going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, the said monitoring system must cause the least amount of
    alert fatigue (as outlined in this concise Datadog article: [https://www.datadoghq.com/blog/monitoring-101-alerting](https://www.datadoghq.com/blog/monitoring-101-alerting)).'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately for our friendship, finding that middle ground which suits your
    case (your infrastructure and the people looking after it) is an adventure which
    you will have to go on alone. We could however spend some quality time together,
    discussing a few of the tools that could make it even more enjoyable!
  prefs: []
  type: TYPE_NORMAL
- en: 'Checklists are sophisticated, so here is one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Centralized logging**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting and storing logs with **Logstash** and **Elasticsearch**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting logs with Elasticsearch **Filebeat**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing logs with **Kibana**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting and storing metrics with **Prometheus**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gathering OS and application metrics with **Telegraf**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing metrics with **Grafana**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerting with **Prometheus**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-remediation with **Prometheus** and **Jenkins**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naturally, we would require a few hosts to form our playground for all of the
    preceding checklist. There has been sufficient practice in deploying VPC EC2 instances
    on AWS in previous chapters, thus I hereby exercise the great power of delegation
    and assume the existence of:'
  prefs: []
  type: TYPE_NORMAL
- en: A VPC with an IGW, NAT gateway, 2x private and 2x public subnets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2x standalone, vanilla Amazon Linux EC2 instances (say `t2.small`) within the
    public subnets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1x Auto Scale Group (`t2.nano`) within the private subnets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1x Internet-facing ELB passing HTTP traffic to the Auto Scale Group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the olden days, mankind has strived to use its limited attention span
    only on what really matters in life, and without having to look for it too hard
    – if possible. So we started with copying log files around, evolution brought
    us centralized (r)syslog and today (we learn from our mistakes) we have Logstash
    and Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *Elasticsearch is a distributed, open source search and analytics engine,
    designed for horizontal scalability, reliability, and easy management. It combines
    the speed of search with the power of analytics via a sophisticated, developer-friendly
    query language covering structured, unstructured, and time-series data.**Logstash
    is a flexible, open source data collection, enrichment, and transportation pipeline.
    With connectors to common infrastructure for easy integration, Logstash is designed
    to efficiently process a growing list of log, event, and unstructured data sources
    for distribution into a variety of outputs, including Elasticsearch.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --[https://www.elastic.co/products](https://www.elastic.co/products)
    |'
  prefs: []
  type: TYPE_TB
- en: Ingesting and storing logs with Logstash and Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using Logstash to receive, process and then store log events into
    Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of the demos in this chapter, we'll be installing and configuring
    services manually, directly on the hosts. When done experimenting, you should,
    of course, use configuration management instead (wink).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start by installing the two services on one of the standalone EC2 instances
    (we shall call it ELK):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Edit `/etc/elasticsearch/elasticsearch.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/etc/elasticsearch/elasticsearch.yml](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/etc/elasticsearch/elasticsearch.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It is important to select a unique name for the Elasticsearch cluster, so that
    the node does not join somebody else's inadvertently, should there be any on your
    LAN. For development, we only ask for a single shard and no replicas. Impatience
    dictates a five second refresh rate on any ES indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Logstash `patterns` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a sample NGINX pattern `/opt/logstash/patterns/nginx` (ref: [https://www.digitalocean.com/community/tutorials/adding-logstash-filters-to-improve-centralized-logging](https://www.digitalocean.com/community/tutorials/adding-logstash-filters-to-improve-centralized-logging)):'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/opt/logstash/patterns/nginx](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/opt/logstash/patterns/nginx)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `/etc/logstash/conf.d/main.conf`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/etc/logstash/conf.d/main.conf](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/etc/logstash/conf.d/main.conf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Logstash allows us to configure one or more listeners (inputs) in order to receive
    data, filters to help us process it and outputs specifying where that data should
    be forwarded once processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We expect logs to be delivered by Elasticsearch Filebeat on `TCP: 5044`. If
    the log event happens to be of type `nginx-access`, we have it modified according
    to the `NGINXACCESS` pattern then shipped to Elasticsearch on localhost `TCP:
    9200` for storage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let us start the services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Collecting logs with Elasticsearch Filebeat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have the systems in place; let us push somes from the ELK node that we are
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Filebeat to collect local logs of interest and forward those to
    Logstash (incidentally also local in this case):'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *Filebeat is a log data shipper. Installed as an agent on your servers,
    Filebeat monitors the log directories or specific log files, tails the files,
    and forwards them either to Elasticsearch or Logstash for indexing.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-overview.html*
    |'
  prefs: []
  type: TYPE_TB
- en: 'Installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'While functionality is provided to ship directly to ES, we are planning to
    use Logstash so we need to disable the Elasticsearch output and enable the logstash
    one in `/etc/filebeat/filebeat.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/etc/filebeat/filebeat.yml](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/elk/etc/filebeat/filebeat.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also list a few more log files to collect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then start the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Fun, but let us launch a few other EC2 instances for even more of it!
  prefs: []
  type: TYPE_NORMAL
- en: We shall use the Auto Scale Group we mentioned earlier. We will install Filebeat
    on each instance and configure it to forward selected logs to our Logstash node.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, ensure that the security group of the Logstash instance allows inbound
    connections from the Auto Scale Group (`TCP: 5044`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we use an EC2 User Data script to bootstrap the Filebeat binary and configuration
    onto each of the EC2 instances in our Auto Scale Group (we will call them webservers):'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/webserver/user_data.sh](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/webserver/user_data.sh)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With that in place, go ahead and scale the group up. The new web server instances,
    should start streaming logs promptly.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing logs with Kibana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have our logs collected by Filebeat and stored in Elasticsearch, how about
    browsing them?
  prefs: []
  type: TYPE_NORMAL
- en: 'Kibana, right on time:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *Kibana is an open source analytics and visualization platform designed
    to work with Elasticsearch. You use Kibana to search, view, and interact with
    data stored in Elasticsearch indices. You can easily perform advanced data analysis
    and visualize your data in a variety of charts, tables, and maps.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*https://www.elastic.co/guide/en/kibana/current/introduction.html*
    |'
  prefs: []
  type: TYPE_TB
- en: 'Install the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The default port is `TCP:5601`, if allowed in the relevant security group,
    you should be able to see the Kibana dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing logs with Kibana](img/image_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Set the **index pattern** to **filebeat-*** and click **Create**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kibana is now ready to display our Filebeat data. Switch to the **Discover**
    tab to see the list of recent events:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing logs with Kibana](img/image_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition to the standard **Syslog** messages, you will also notice some
    **NGINX access-log** entries, with various fields populated as per the filter
    we specified earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing logs with Kibana](img/image_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Logs: done. Now, how about some metrics?'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For ingesting, storing and alerting on our metrics, we shall explore another,
    quite popular open-source project called Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *Prometheus is an open-source systems monitoring and alerting toolkit
    originally built at SoundCloud.**Prometheus''s main features are:**-a multi-dimensional
    data model (time series identified by metric name and key/value pairs)**- a flexible
    query language to leverage this dimensionality**- no reliance on distributed storage;
    single server nodes are autonomous**- time series collection happens via a pull
    model over HTTP**- pushing time series is supported via an intermediary gateway**-
    targets are discovered via service discovery or static configuration**- multiple
    modes of graphing and dashboarding support* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*https://prometheus.io/docs/introduction/overview/emphasis>* |'
  prefs: []
  type: TYPE_TB
- en: 'Even though it is the kind of system that takes care of pretty much everything,
    the project still follows the popular UNIX philosophy of modular development.
    Prometheus is composed of multiple components, each providing a specific function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *- the main Prometheus server which scrapes and stores time series data**-
    client libraries for instrumenting application code**- a push gateway for supporting
    short-lived jobs**- a GUI-based dashboard builder based on Rails/SQL**- special-purpose
    exporters (for HAProxy, StatsD, Ganglia, etc.)**- an (experimental) alertmanager**-
    a command-line querying tool* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*https://prometheus.io/docs/introduction/overview/* |'
  prefs: []
  type: TYPE_TB
- en: Ingesting and storing metrics with Prometheus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our second EC2 instance is going to host the Prometheus service alongside Jenkins
    (we will come to that shortly), thus a rather appropriate name would be promjenkins.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a start, download and extract Prometheus and Alertmanager in `/opt/prometheus/server`
    and `/opt/prometheus/alertmanager` respectively (ref: [https://prometheus.io/download](https://prometheus.io/download)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a basic configuration file for the Alertmanager in `/opt/prometheus/alertmanager/alertmanager.yml`
    (replace e-mail addresses as needed):'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/alertmanager/alertmanager.yml](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/alertmanager/alertmanager.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This will simply e-mail out alert notifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure the default `TCP:9093` is allowed, then you should be able to get to
    the dashboard at `http://$public_IP_of_promjenkins_node:9093/#/status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ingesting and storing metrics with Prometheus](img/image_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Back to the Prometheus server, the default `/opt/prometheus/server/prometheus.yml`
    will suffice for now. We can start the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Open up `TCP:9090`, then try `http://$public_IP_of_promjenkins_node:9090/status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ingesting and storing metrics with Prometheus](img/image_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We are ready to start adding hosts to be monitored. That is to say targets for
    Prometheus to scrape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prometheus offers various ways in which targets can be defined. The one most
    suitable for our case is called `ec2_sd_config` (ref: [https://prometheus.io/docs/operating/configuration/#<ec2_sd_config>](https://prometheus.io/docs/operating/configuration/#<ec2_sd_config>)).
    All we need to do is provide a set of API keys with read-only EC2 access (**AmazonEC2ReadOnlyAccess**
    IAM policy) and Prometheus will do the host discovery for us (ref: [https://www.robustperception.io/automatically-monitoring-ec2-instances](https://www.robustperception.io/automatically-monitoring-ec2-instances)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We append the `ec2_sd_config` settings to: `/opt/prometheus/server/prometheus.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/server/prometheus.yml](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/server/prometheus.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We are interested only in any instances in the `us-east-1` region with a name
    matching the `^webserver` regex expression.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us bring some of those online.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering OS and application metrics with Telegraf
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the pull method of metric collection in Prometheus. This means
    that our clients (targets) will expose their metrics for Prometheus to scrape.
  prefs: []
  type: TYPE_NORMAL
- en: 'To expose OS metrics, we shall deploy InfluxData''s Telegraf (ref: [https://github.com/influxdata/telegraf](https://github.com/influxdata/telegraf)).'
  prefs: []
  type: TYPE_NORMAL
- en: It comes with a rich set of plugins, which will provide for a good deal of metrics.
    Should you need more, you have the option to write your own (in Go) or use the
    `exec` plugin which will essentially attempt to launch any type of script you
    point it at.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for application metrics, we have two options (at least):'
  prefs: []
  type: TYPE_NORMAL
- en: Build a metrics API endpoint in the application itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have the application submit metrics data to an external daemon (StatsD as an
    example)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incidentally, Telegraf comes with a built-in StatsD listener, so if your applications
    already happen to have StatsD instrumentation, you should be able to simply point
    them at it.
  prefs: []
  type: TYPE_NORMAL
- en: Following on from the ELK example, we will extend the EC2 user data script to
    get Telegraf on our the Auto Scale Group instances.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/webserver/user_data.sh](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/webserver/user_data.sh)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We append:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The important one here is `outputs.prometheus_client` with which we turn Telegraf
    into a Prometheus scrape target. By all means check the default configuration
    file if you''d like to enable more metrics during this test (ref: [https://github.com/influxdata/telegraf/blob/master/etc/telegraf.conf](https://github.com/influxdata/telegraf/blob/master/etc/telegraf.conf))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, check that TCP: `9126` is allowed into the Auto Scale Group security
    group, then launch a couple of nodes. In a few moments, you should see any matching
    instances listed in the targets dashboard (ref: `http://$ public_IP_of_promjenkins_node:9090/targets`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gathering OS and application metrics with Telegraf](img/image_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We see the new hosts under the **ec2** scrape job which we configured earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing metrics with Grafana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is true that Prometheus is perfectly capable of visualizing the data we
    are now collecting from our targets, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing metrics with Grafana](img/image_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In fact, this is the recommended approach for any ad-hoc queries you might want
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Should you have an appetite for dashboards however, you would most certainly
    appreciate *Grafana - The 8th Wonder* (ref: [http://grafana.org](http://grafana.org))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check this out to get a feel for the thing: http://play.grafana.org'
  prefs: []
  type: TYPE_NORMAL
- en: I mean, how many other projects do you know of with a *play* URL?!
  prefs: []
  type: TYPE_NORMAL
- en: 'So, yes, Grafana, let us install the service on the promjenkins node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The default Grafana port is TCP:`3000`, auth `admin:admin`. After updating
    the relevant security group, we should be able to see the screen at: `http://$
    public_IP_of_promjenkins_node:3000`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Visualizing metrics with Grafana](img/image_07_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: After logging in, first we need to create a **Data Sources** for our **Dashboards**:![Visualizing
    metrics with Grafana](img/image_07_010.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back at the home screen, choose to create a new dashboard, then use the green
    button on the left to **Add Panel** then a **Graph**:![Visualizing metrics with
    Grafana](img/image_07_011.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, adding a basic CPU usage plot looks like this:![Visualizing metrics with
    Grafana](img/image_07_012.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point I encourage you to browse [http://docs.grafana.org](http://docs.grafana.org)
    to find out more about templating, dynamic dashboards, access control, tagging,
    scripting, playlist, and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have our metrics flowing into Prometheus. We also have a way of exploring
    and visualizing them. The next step should probably be to configure some sort
    of alerts, so that we show other people we are doing real work.
  prefs: []
  type: TYPE_NORMAL
- en: Alerting with Prometheus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|   | *ALERTING OVERVIEW**Alerting with Prometheus is separated into two parts.
    Alerting rules in Prometheus servers send alerts to an Alertmanager. The Alertmanager
    then manages those alerts, including silencing, inhibition, aggregation and sending
    out notifications via methods such as e-mail, PagerDuty and HipChat.**The main
    steps to setting up alerting and notifications are:**- Setup and configure the
    Alertmanager**- Configure Prometheus to talk to the Alertmanager with the-alertmanager.url
    flag**- Create alerting rules in Prometheus* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*https://prometheus.io/docs/alerting/overview/* |'
  prefs: []
  type: TYPE_TB
- en: Let us break this down.
  prefs: []
  type: TYPE_NORMAL
- en: We already have Alertmanager running with some minimal configuration in `/opt/prometheus/alertmanager/alertmanager.yml`.
  prefs: []
  type: TYPE_NORMAL
- en: Our Prometheus instance is aware of it as we passed the `-alertmanager.url=http://localhost:9093`
    flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is left is to create alerting rules. We''ll store these in a `rules/`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to tell Prometheus about this location, so we add a `rule_files` section
    to `prometheus.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/server/prometheus.yml](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/server/prometheus.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This way we can store separate rule files, perhaps based on the type of rules
    they contain?
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let us have a keepalive and a disk usage alert:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_07_CodeFiles/promjenkins/opt/prometheus/server/rules](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_07_CodeFiles/promjenkins/opt/prometheus/server/rules)'
  prefs: []
  type: TYPE_NORMAL
- en: '`/opt/prometheus/server/rules/keepalive.rules`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`/opt/prometheus/server/rules/disk.rules`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you'll notice, we are being impatient with the `FOR 1m` and `>20`, meaning
    notifications will fire after just 60 seconds of alert detection and the alert
    threshold is only 20% of space used.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a more realistic scenario, we should wait a bit longer to filter any transient
    issues and use severities to distinguish between critical alerts and warnings
    (ref: [https://github.com/prometheus/alertmanager](https://github.com/prometheus/alertmanager)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reload Prometheus with the new rules in place. Now let us suppose that one
    of the web server nodes goes down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alerting with Prometheus](img/image_07_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Switching to the **Alerts** tab we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alerting with Prometheus](img/image_07_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Alertmanager respectively: (`http://$ public_IP_of_promjenkins_node:9093/#/alerts`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Alerting with Prometheus](img/image_07_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: At this point an e-mail notification should have gone out as well.
  prefs: []
  type: TYPE_NORMAL
- en: Self-remediation with Prometheus and Jenkins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dream of every operator is an ecosystem that looks after itself.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine for a moment an environment in which, instead of receiving alerts prompting
    for action, we received mere notifications or reports of actions taken on our
    behalf.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, no more "CRITICAL: Service X is not responding. Please check."
    but "INFO: Service X was unresponsive at nn:nn:nn and was restarted after N seconds
    at nn:nn:nn" instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Well, technically, this should not be too difficult to achieve if we were to
    provide enough context to the tools we use today. It is not uncommon to find alerts
    which tend to get resolved in the same manner under the same conditions and those
    are to be considered prime candidates for automation.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate, let us assume we inherited this old, no longer supported application.
    A cool app overall, but it does not have the habit of tidying up after itself,
    so would occasionally fill up its `tmp` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Let us also assume that while we are not particularly excited about having to
    connect to this app's server to delete `tmp` files at random times of the day,
    our friend, Mr. Jenkins - does not mind at all.
  prefs: []
  type: TYPE_NORMAL
- en: Conveniently, Jenkins allows jobs to be triggered via a relevant `JOB_URL` and
    at the same time Prometheus supports webhook calls as a method of alert notification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the plan:'
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus will make a webhook call to Jenkins whenever a `disk_space` alert
    is fired with the alert details passed as parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jenkins will use the parameters to determine which host to connect to and clean
    up the application's `tmp` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We would need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a parameterized Jenkins job which can be triggered remotely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Allow Jenkins to `ssh` into the application's host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setup a webhook receiver in Prometheus which calls the Jenkins job when a certain
    alert is fired.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First a quick Jenkins installation onto our `promjenkins` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`TCP: 8080` needs to be open, then you should be able to reach the Jenkins
    service at `http://$public_IP_of_promjenkins_node:8080`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Manage Jenkins** | **Manage Users** create an account for Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-remediation with Prometheus and Jenkins](img/image_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then, under **Manage Jenkins** | **Configure Global Security**, select Jenkins'
    own user database and **Matrix-based Security**, then add both accounts.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Untick **Prevent Cross Site Request Forgery exploits** if you find that it causes
    issues when making `curl` request to Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: 'Grant yourself **Overall Administer rights** and **Prometheus Overall Read**
    plus **Job Build/Read**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-remediation with Prometheus and Jenkins](img/image_07_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To be able to ssh into the app (web server) nodes we need a key for the Jenkins
    user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'While we are here, let us create an ssh config file for the Jenkins user (`~/.ssh/config`)
    containing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This is to allow our non-interactive jobs to ssh to instances for the first
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to take the generated public key and add it to the Auto Scale
    Group user data , so that it gets onto our web server instances. We will be using
    the standard (Amzn-Linux) ec2-user account to connect:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/webserver/user_data.sh](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/webserver/user_data.sh)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us create the Jenkins job (freestyle project) with a few parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-remediation with Prometheus and Jenkins](img/image_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will discuss those four parameters (`alertname`, `alertcount`, `instance`,
    `labels`) later. In the **Build** section, select **Execute shell** and enter
    `exit 0` as a placeholder until we are ready to configure the job further. **Save**
    and let's get back to Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we will be using the webhook receiver to trigger the
    Jenkins job. While the receiver allows us to set a URL to call, it does not seem
    to allow for any parameters to be included. To accomplish this, we will use a
    small helper application called **prometheus-am-executor** (ref: [https://github.com/imgix/prometheus-am-executor](https://github.com/imgix/prometheus-am-executor)).'
  prefs: []
  type: TYPE_NORMAL
- en: The executor sits between the Alertmanager and an arbitrary executable. It receives
    the webhook call from the Alertmanager and runs the executable, passing a list
    of alert variables to it. In our case, we will be executing a shell script which
    processes those variables and constructs a `curl` call in the format that Jenkins
    expects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us install the helper app alongside Prometheus and the Alertmanager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'On success, you should have a binary in `/opt/prometheus/executor/bin`. Now
    the script (executable) that we mentioned:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/executor/executor.sh](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/executor/executor.sh)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In essence we are constructing an HTTP call to our Jenkins job URL at `http://localhost:8080/job/prometheus_webhook/build`
    passing the `alertcount`, `alertname`, `instance` and `labels` parameters. All
    values come from the AMX environment variables which the prometheus-am-executor
    exposes (ref: [https://github.com/imgix/prometheus-am-executor](https://github.com/imgix/prometheus-am-executor)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to reconfigure the Alertmanager to use webhooks:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please refer to: [https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/alertmanager/alertmanager.yml](https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_07_CodeFiles/promjenkins/opt/prometheus/alertmanager/alertmanager.yml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have added a new sub-route which would match on `alertname`: `High_disk_space_usage`
    and use the `jenkins-webhook` receiver.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reload Alertmanager and let us start the executor. Assuming that the `executor.sh`
    has been placed in `/opt/prometheus/executor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We have the executor running (port `8888`) and ready to accept requests from
    the Alertmanager.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before triggering any test alerts, let''s go back to our Jenkins job. You are
    now familiar with the parameters it expects and the ones that we pass via the
    `webhook` | `executor` | `jenkins` setup that we have, so we can replace the contents
    of the placeholder **Build** step with this shell script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To test all of this, we need to ssh into one of the ASG (web server) instances
    which Prometheus is monitoring and setup a pretend App temporary folder like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give us a small filesystem to play with. Next, we fill it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is way over the 20% we have set in the `High_disk_space_usage` and should
    trigger it. In turn the executor should call Jenkins and run our job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Self-remediation with Prometheus and Jenkins](img/image_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see Jenkins connecting to the affected instance over SSH, then clearing
    our fake application `tmp` directory.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that while we allow ourselves root access for the purpose
    of this example, in any other circumstances you would either ensure that Jenkins
    could handle the given `tmp` directory as a non-privileged user, or if you would
    absolutely have to use `sudo` and then limit the commands and command line arguments
    that can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we looked at a way of centralizing our logs with Logstash and
    Elasticsearch then browsing them in Kibana. We configured a metrics collection
    and visualization with the help of Prometheus, Telegraf and Grafana. Finally,
    we added monitoring via Prometheus and self-remediation using Jenkins.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter takes us into the area of optimization. We shall discuss cost
    considerations and approaches to demand-based scaling.
  prefs: []
  type: TYPE_NORMAL
