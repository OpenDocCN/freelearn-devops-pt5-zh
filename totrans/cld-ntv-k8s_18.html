<html><head></head><body>
		<div><h1 id="_idParaDest-292"><em class="italic"><a id="_idTextAnchor307"/>Chapter 14</em>: Service Meshes and Serverless</h1>
			<p>This chapter discusses advanced Kubernetes patterns. First, it details the in-vogue service mesh pattern, where observability and service-to-service discovery are handled by a sidecar proxy, as well as a guide to setting up Istio, a popular service mesh. Lastly, it describes the serverless pattern and how it can be applied in Kubernetes. The major case study in this chapter will include setting up Istio for an example application and service discovery, along with Istio ingress gateways.</p>
			<p>Let's start with a discussion of the sidecar proxy, which builds the foundation of service-to-service connectivity for service meshes.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Using sidecar proxies</li>
				<li>Adding a service mesh to Kubernetes</li>
				<li>Implementing serverless on Kubernetes</li>
			</ul>
			<h1 id="_idParaDest-293"><a id="_idTextAnchor308"/>Technical requirements</h1>
			<p>In order to run the commands detailed in this chapter, you will need a computer that supports the <code>kubectl</code> command-line tool, along with a working Kubernetes cluster. See <a href="B14790_01_Final_PG_ePub.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Communicating with Kubernetes</em>, for several methods for getting up and running with Kubernetes quickly, and for instructions on how to install the <code>kubectl</code> tool.</p>
			<p>The code used in this chapter can be found in the book's GitHub repository at <a href="https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14">https://github.com/PacktPublishing/Cloud-Native-with-Kubernetes/tree/master/Chapter14</a>.</p>
			<h1 id="_idParaDest-294"><a id="_idTextAnchor309"/>Using sidecar proxies</h1>
			<p>As we mentioned earlier in this book, a sidecar is<a id="_idIndexMarker747"/> a pattern where a Pod contains another container in addition to the actual application container to be run. This additional "extra" container is the sidecar. Sidecars can be used for a number of different reasons. Some of the most popular uses for sidecars are monitoring, logging, and proxying.</p>
			<p>For logging, a sidecar container can fetch application logs from the application container (since they can share volumes and communicate on localhost), before sending the logs to a centralized logging stack, or parsing them for the purpose of alerting. It's a similar story for monitoring, where the sidecar Pod can track and send metrics about the application Pod.</p>
			<p>With a sidecar proxy, when requests come into the Pod, they first go to the proxy container, which then routes requests (after logging or performing other filtering) to the application container. Similarly, when requests leave the application container, they first go to the proxy, which can provide routing out of the Pod.</p>
			<p>Normally, proxy sidecars such as NGINX only provide proxying for requests coming into a Pod. However, in the service mesh pattern, both requests coming into and leaving the Pod go through the proxy, which provides the foundation for the service mesh pattern itself. </p>
			<p>Refer to the following diagram to see how a sidecar proxy can interact with an application container:</p>
			<div><div><img src="img/B14790_14_001.jpg" alt="Figure 14.1 – Proxy sidecar"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – Proxy sidecar</p>
			<p>As you can see<a id="_idIndexMarker748"/>, the sidecar proxy is in charge of routing requests to and from the application container in the Pod, allowing for functionality such as service routing, logging, and filtering.</p>
			<p>The sidecar proxy pattern is an alternative to a DaemonSet-based proxy, where a proxy Pod on each node handles proxying to other Pods on that node. The Kubernetes proxy itself is similar to a DaemonSet pattern. Using a sidecar proxy can provide more flexibility than using a DaemonSet proxy, at the expense of performance efficiency, since many extra containers need to be run.</p>
			<p>Some popular proxy options for Kubernetes include the following:</p>
			<ul>
				<li><em class="italic">NGINX</em></li>
				<li><em class="italic">HAProxy</em></li>
				<li><em class="italic">Envoy</em></li>
			</ul>
			<p>While NGINX and HAProxy are more traditional proxies, Envoy was built specifically for a distributed, cloud-native environment. For this reason, Envoy forms the core of popular service meshes and<a id="_idIndexMarker749"/> API gateways built for Kubernetes.</p>
			<p>Before we get to Envoy, let's discuss the installation of other proxies as sidecars.</p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor310"/>Using NGINX as a sidecar reverse proxy</h2>
			<p>Before we specify how <a id="_idIndexMarker750"/>NGINX can be used as a sidecar proxy, it is relevant to note that in an upcoming Kubernetes release, the sidecar will be a Kubernetes <a id="_idIndexMarker751"/>resource type that will allow easy injection of sidecar containers to large numbers of Pods. Currently however, sidecar containers must be specified at the Pod or controller (ReplicaSet, Deployment, and others) level.</p>
			<p>Let's take a look at how we can configure NGINX as a sidecar, with the following Deployment YAML, which we will not create just yet. This process is a bit more manual than using the NGINX Ingress Controller. </p>
			<p>We've split the YAML into two parts for space reasons and trimmed some of the fat, but you can see it in its entirety in the code repository. Let's start with the containers spec for our deployment:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Nginx-sidecar.yaml:</p>
			<pre>   spec:
     containers:
     - name: myapp
       image: ravirdv/http-responder:latest
       imagePullPolicy: IfNotPresent
     - name: nginx-sidecar
       image: nginx
       imagePullPolicy: IfNotPresent
       volumeMounts:
         - name: secrets
           mountPath: /app/cert
         - name: config
           mountPath: /etc/nginx/nginx.conf
           subPath: nginx.conf</pre>
			<p>As you can see, we <a id="_idIndexMarker752"/>specify two containers, both our main app container, <code>myapp</code>, and the <code>nginx</code> sidecar, where we inject some configuration via volume mounts, as<a id="_idIndexMarker753"/> well as some TLS certificates.</p>
			<p>Next, let's look at the <code>volumes</code> spec in the same file, where we inject some certs (from a secret) and <code>config</code> (from a <code>ConfigMap</code>):</p>
			<pre>    volumes:
     - name: secrets
       secret:
         secretName: nginx-certificates
         items:
           - key: server-cert
             path: server.pem
           - key: server-key
             path: server-key.pem
     - name: config
       configMap:
         name: nginx-configuration</pre>
			<p>As you can see, we need both a cert and a secret key. </p>
			<p>Next, we need to create the NGINX configuration using <code>ConfigMap</code>. The NGINX configuration looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">nginx.conf:</p>
			<pre>http {
    sendfile        on;
    include       mime.types;
    default_type  application/octet-stream;
    keepalive_timeout  80;
    server {
       ssl_certificate      /app/cert/server.pem;
      ssl_certificate_key  /app/cert/server-key.pem;
      ssl_protocols TLSv1.2;
      ssl_ciphers EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:!EECDH+3DES:!RSA+3DES:!MD5;
      ssl_prefer_server_ciphers on;
      listen       443 ssl;
      server_name  localhost;
      location / {
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header Host $http_host;
        proxy_pass http://127.0.0.1:5000/;
      }
    }
}
worker_processes  1;
events {
    worker_connections  1024;
}</pre>
			<p>As you can see, we have some basic NGINX configuration. Importantly, we have the <code>proxy_pass</code> field, which proxies requests to a port on <code>127.0.0.1</code>, or localhost. Since containers in a <a id="_idIndexMarker754"/>Pod can share localhost ports, this acts <a id="_idIndexMarker755"/>as our sidecar proxy. We won't review all the other lines for the purposes of this book, but check the NGINX docs for more information about what each line means (<a href="https://nginx.org/en/docs/">https://nginx.org/en/docs/</a>). </p>
			<p>Now, let's create the <code>ConfigMap</code> from this file. Use the following command to imperatively create the <code>ConfigMap</code>:</p>
			<pre>kubectl create cm nginx-configuration --from-file=nginx.conf=./nginx.conf</pre>
			<p>This will result in the following output:</p>
			<pre>Configmap "nginx-configuration" created</pre>
			<p>Next, let's make our certificates for TLS in NGINX, and embed them in a Kubernetes secret. You will need the CFSSL (CloudFlare's PKI/TLS open source toolkit) library installed to follow these instructions, but you can use any other method to create your cert.</p>
			<p>First, we<a id="_idIndexMarker756"/> need to create the <strong class="bold">Certificate Authority</strong> (<strong class="bold">CA</strong>). Start with the JSON configuration for the CA:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">nginxca.json:</p>
			<pre>{
   "CN": "mydomain.com",
   "hosts": [
       "mydomain.com",
       "www.mydomain.com"
   ],
   "key": {
       "algo": "rsa",
       "size": 2048
   },
   "names": [
       {
           "C": "US",
           "ST": "MD",
           "L": "United States"
       }
   ]
}</pre>
			<p>Now, use CFSSL to <a id="_idIndexMarker757"/>create the CA certificate:</p>
			<pre>cfssl gencert -initca nginxca.json | cfssljson -bare nginxca</pre>
			<p>Next, we will <a id="_idIndexMarker758"/>require the CA config:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Nginxca-config.json:</p>
			<pre>{
  "signing": {
      "default": {
          "expiry": "20000h"
      },
      "profiles": {
          "client": {
              "expiry": "43800h",
              "usages": [
                  "signing",
                  "key encipherment",
                  "client auth"
              ]
          },
          "server": {
              "expiry": "20000h",
              "usages": [
                  "signing",
                  "key encipherment",
                  "server auth",
                  "client auth"
              ]
          }
      }
  }
}</pre>
			<p>And we'll<a id="_idIndexMarker759"/> also need a <a id="_idIndexMarker760"/>cert request config:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Nginxcarequest.json:</p>
			<pre>{
  "CN": "server",
  "hosts": [
    ""
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  }
}</pre>
			<p>Now, we can actually make our certs! Use the following command:</p>
			<pre>cfssl gencert -ca=nginxca.pem -ca-key=nginxca-key.pem -config=nginxca-config.json -profile=server -hostname="127.0.0.1" nginxcarequest.json | cfssljson -bare server</pre>
			<p>As the final step for our <a id="_idIndexMarker761"/>cert secrets, create the Kubernetes<a id="_idIndexMarker762"/> secret from the certificate files' output by means of the last <code>cfssl</code> command:</p>
			<pre>kubectl create secret generic nginx-certs --from-file=server-cert=./server.pem --from-file=server-key=./server-key.pem</pre>
			<p>Now, we can finally create our deployment:</p>
			<pre>kubectl apply -f nginx-sidecar.yaml </pre>
			<p>This produces the following output:</p>
			<pre>deployment "myapp" created</pre>
			<p>In order to check the NGINX proxy functionality, let's create a service to direct to our deployment:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Nginx-sidecar-service.yaml:</p>
			<pre>apiVersion: v1
kind: Service
metadata:
 name:myapp
 labels:
   app: myapp
spec:
 selector:
   app: myapp
 type: NodePort
 ports:
 - port: 443
   targetPort: 443
   protocol: TCP
   name: https</pre>
			<p>Now, accessing any <a id="_idIndexMarker763"/>node of the cluster using <code>https</code> should result in a working HTTPS connection! However, since our cert is self-signed, browsers will display an <em class="italic">insecure</em> message.</p>
			<p>Now that you've seen<a id="_idIndexMarker764"/> how NGINX can be used as a sidecar proxy with Kubernetes, let's move on to a more modern, cloud-native proxy sidecar – Envoy.</p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor311"/>Using Envoy as a sidecar proxy</h2>
			<p>Envoy is a modern proxy <a id="_idIndexMarker765"/>built for cloud-native environments. In the Istio service mesh, which we'll review later in this chapter, Envoy acts as both a reverse <a id="_idIndexMarker766"/>and forward proxy. Before we get to Istio, however, let's try our hand at deploying Envoy as a proxy.</p>
			<p>We will tell Envoy where to route various requests using routes, listeners, clusters, and endpoints. This functionality is what forms the core of Istio, which we will review later in this chapter.</p>
			<p>Let's go through each of the Envoy configuration pieces to see how it all works.</p>
			<h3>Envoy listeners</h3>
			<p>Envoy allows the configuration of one or more listeners. With each listener, we specify a port for Envoy to listen on, as<a id="_idIndexMarker767"/> well as any filters we want to apply to the listener.</p>
			<p>Filters <a id="_idIndexMarker768"/>can provide complex functionality, including<a id="_idIndexMarker769"/> caching, authorization, <strong class="bold">Cross-Origin Resource Sharing</strong> (<strong class="bold">CORS</strong>) configuration, and more. Envoy supports the chaining of multiple filters together.</p>
			<h3>Envoy routes</h3>
			<p>Certain filters have route <a id="_idIndexMarker770"/>configuration, which specifies domains from which requests should be accepted, route matching, and forwarding rules.</p>
			<h3>Envoy clusters</h3>
			<p>A Cluster in Envoy represents a<a id="_idIndexMarker771"/> logical service where requests can be routed to based-on routes in listeners. A cluster likely contains more than one possible IP address in a cloud-native setting, so it supports load balancing configurations such as <em class="italic">round robin</em>.</p>
			<h3>Envoy endpoints</h3>
			<p>Finally, endpoints are specified <a id="_idIndexMarker772"/>within a cluster as one logical instance of a service. Envoy supports fetching a list of endpoints from an API (this is essentially what happens in the Istio service mesh) and load balancing between them.</p>
			<p>In a production Envoy deployment on Kubernetes, it is likely that some form of dynamic, API-driven Envoy configuration is going to be<a id="_idIndexMarker773"/> used. This feature of Envoy is called xDS, and is used by Istio. Additionally, there are other open source products and solutions that use Envoy along with xDS, including the Ambassador API gateway.</p>
			<p>For the purposes of this book, we will look at some static (non-dynamic) Envoy configuration; that way, we can pick apart each piece of the config, and you'll have a good idea of how everything works when we review Istio.</p>
			<p>Let's now dive into an Envoy configuration for a setup where a single Pod needs to be able to route requests to two services, <em class="italic">Service 1</em> and <em class="italic">Service 2</em>. The setup looks like this:</p>
			<div><div><img src="img/B14790_14_002.jpg" alt="Figure 14.2 – Outbound envoy proxy"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – Outbound envoy proxy</p>
			<p>As you can see, the<a id="_idIndexMarker774"/> Envoy sidecar in our application Pod will have configurations to route to two upstream services, <em class="italic">Service 1</em> and <em class="italic">Service 2</em>. Both services have two possible endpoints. </p>
			<p>In a dynamic setting with Envoy xDS, the Pod IPs for the endpoints would be loaded from the API, but for the purposes of our review, we will show the static Pod IPs in the endpoints. We will completely ignore Kubernetes Services and instead directly access Pod IPs in a round robin configuration. In a service mesh scenario, Envoy would also be deployed on all of the destination Pods, but we'll keep it simple for now.</p>
			<p>Now, let's look at how this network map is configured in an envoy configuration YAML (which you can find in its entirety in the code repository). This is, of course, very different from a Kubernetes<a id="_idIndexMarker775"/> resource YAML – we will get to that part later. The entire configuration has a lot of YAML involved, so let's take it piece by piece.</p>
			<h3>Understanding Envoy configuration files</h3>
			<p>First off, let's look at the first<a id="_idIndexMarker776"/> few lines of our config—some basic information about our Envoy setup:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Envoy-configuration.yaml:</p>
			<pre>admin:
  access_log_path: "/dev/null"
  address:
    socket_address:
      address: 0.0.0.0
      port_value: 8001</pre>
			<p>As you can see, we specify a port and address for Envoy's <code>admin</code>. As with the following configuration, we are running Envoy as a sidecar so the address will always be local – <code>0.0.0.0</code>. Next, we start our list of listeners with an HTTPS listener:</p>
			<pre>static_resources:
  listeners:
   - address:
      socket_address:
        address: 0.0.0.0
        port_value: 8443
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager
          stat_prefix: ingress_https
          codec_type: auto
          route_config:
            name: local_route
            virtual_hosts:
            - name: backend
              domains:
              - "*"
              routes:
              - match:
                  prefix: "/service/1"
                route:
                  cluster: service1
              - match:
                  prefix: "/service/2"
                route:
                  cluster: service2
          http_filters:
          - name: envoy.filters.http.router
            typed_config: {}</pre>
			<p>As you can see, for each Envoy listener, we have a local address and port for the listener (this listener is an HTTPS listener). Then, we have a list of filters – though in this case, we only have one. Each<a id="_idIndexMarker777"/> envoy filter type has slightly different configuration, and we won't review it line by line (check the Envoy docs for more information at <a href="https://www.envoyproxy.io/docs">https://www.envoyproxy.io/docs</a>), but this<a id="_idIndexMarker778"/> particular filter matches two routes, <code>/service/1</code> and <code>/service/2</code>, and routes them to two envoy clusters. Still under our first HTTPS listener section of the YAML, we have the TLS configuration, including certs:</p>
			<pre>      transport_socket:
        name: envoy.transport_sockets.tls
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext
          common_tls_context:
            tls_certificates:
              certificate_chain:
                inline_string: |
                   &lt;INLINE CERT FILE&gt;
              private_key:
                inline_string: |
                  &lt;INLINE PRIVATE KEY FILE&gt;</pre>
			<p>As you can see, this configuration passes in a <code>private_key</code> and a <code>certificate_chain</code>. Next, we have our second and final listener, an HTTP listener:</p>
			<pre>  - address:
      socket_address:
        address: 0.0.0.0
        port_value: 8080
    filter_chains:
    - filters:
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          "@type": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager
          codec_type: auto
          stat_prefix: ingress_http
          route_config:
            name: local_route
            virtual_hosts:
            - name: backend
              domains:
              - "*"
              routes:
              - match:
                  prefix: "/service1"
                route:
                  cluster: service1
              - match:
                  prefix: "/service2"
                route:
                  cluster: service2
          http_filters:
          - name: envoy.filters.http.router
            typed_config: {}</pre>
			<p>As you can see, this configuration is quite similar to that of our HTTPS listener, except that it listens on a different port, and does not include certificate information. Next, we move into our cluster <a id="_idIndexMarker779"/>configuration. In our case, we have two clusters, one for <code>service1</code> and one for <code>service2</code>. First off, <code>service1</code>:</p>
			<pre>  clusters:
  - name: service1
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    load_assignment:
      cluster_name: service1
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: service1
                port_value: 5000</pre>
			<p>And next, <code>Service 2</code>:</p>
			<pre>  - name: service2
    connect_timeout: 0.25s
    type: strict_dns
    lb_policy: round_robin
    http2_protocol_options: {}
    load_assignment:
      cluster_name: service2
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: service2
                port_value: 5000</pre>
			<p>For each of these clusters, we specify where requests should be routed, and to which port. For instance, for our first cluster, requests are routed to <code>http://service1:5000</code>. We also specify a load balancing policy (in this case, round robin) and a timeout for the<a id="_idIndexMarker780"/> connections. Now that we have our Envoy configuration, we can go ahead and create our Kubernetes Pod and inject our sidecar along with the envoy configuration. We'll also split this file into two since it is a bit too big to understand as is:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Envoy-sidecar-deployment.yaml:</p>
			<pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-service
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: my-service
    spec:
      containers:
      - name: envoy
        image: envoyproxy/envoy:latest
        ports:
          - containerPort: 9901
            protocol: TCP
            name: envoy-admin
          - containerPort: 8786
            protocol: TCP
            name: envoy-web</pre>
			<p>As you can see, this is a typical deployment YAML. In this case, we actually have two containers. First off is the Envoy proxy container (or sidecar). It listens on two ports. Next up, moving further <a id="_idIndexMarker781"/>down the YAML, we have a volume mount for that first container (to hold the Envoy config) as well as a start command and arguments:</p>
			<pre>        volumeMounts:
          - name: envoy-config-volume
            mountPath: /etc/envoy-config/
        command: ["/usr/local/bin/envoy"]
        args: ["-c", "/etc/envoy-config/config.yaml", "--v2-config-only", "-l", "info","--service-cluster","myservice","--service-node","myservice", "--log-format", "[METADATA][%Y-%m-%d %T.%e][%t][%l][%n] %v"]</pre>
			<p>Finally, we have our second container in the Pod, which is an application container:     </p>
			<pre>- name: my-service
        image: ravirdv/http-responder:latest
        ports:
        - containerPort: 5000
          name: svc-port
          protocol: TCP
      volumes:
        - name: envoy-config-volume
          configMap:
            name: envoy-config
            items:
              - key: envoy-config
                path: config.yaml</pre>
			<p>As you can see, this application<a id="_idIndexMarker782"/> responds on port <code>5000</code>. Lastly, we also have our Pod-level volume definition to match the Envoy config volume mounted in the Envoy container. Before we create our deployment, we need to create a <code>ConfigMap</code> with our Envoy configuration. We can do this using the following command:</p>
			<pre>kubectl create cm envoy-config 
--from-file=config.yaml=./envoy-config.yaml</pre>
			<p>This will result in the following output:</p>
			<pre>Configmap "envoy-config" created</pre>
			<p>Now we can create our deployment with the following command:</p>
			<pre>kubectl apply -f deployment.yaml</pre>
			<p>This will result in the following output:</p>
			<pre>Deployment "my-service" created</pre>
			<p>Finally, we need our downstream services, <code>service1</code> and <code>service2</code>. For this purpose, we will continue to use the <code>http-responder</code> open source container image, which will respond on port <code>5000</code>. The deployment and service specs can be found in the code repository, and we can create them using the following commands:</p>
			<pre>kubectl create -f service1-deployment.yaml
kubectl create -f service1-service.yaml
kubectl create -f service2-deployment.yaml
kubectl create -f service2-service.yaml</pre>
			<p>Now, we can test our <a id="_idIndexMarker783"/>Envoy configuration! From our <code>my-service</code> container, we can make a request to localhost on port <code>8080</code>, with the <code>/service1</code> path. This should direct to one of our <code>service1</code> Pod IPs. To make this request we use the following command:</p>
			<pre>Kubectl exec &lt;my-service-pod-name&gt; -it -- curl localhost:8080/service1</pre>
			<p>We've set up out services to echo their names on a <code>curl</code> request. Look at the following output of our <code>curl</code> command:</p>
			<pre>Service 1 Reached!</pre>
			<p>Now that we've looked at how Envoy works with a static configuration, let's move on to a dynamic service mesh based on Envoy – Istio.</p>
			<h1 id="_idParaDest-297"><a id="_idTextAnchor312"/>Adding a service mesh to Kubernetes</h1>
			<p>A <em class="italic">service mesh</em> pattern is <a id="_idIndexMarker784"/>a logical extension of the sidecar proxy. By attaching sidecar proxies to every Pod, a service mesh can control functionality for <a id="_idIndexMarker785"/>service-to-service requests, such as advanced routing rules, retries, and timeouts. In addition, by having every request pass through a proxy, service meshes can implement mutual TLS encryption between services for added security and can give administrators incredible observability into requests in their cluster.</p>
			<p>There are several service mesh projects that support Kubernetes. The most popular are as follows:</p>
			<ul>
				<li><em class="italic">Istio</em></li>
				<li><em class="italic">Linkerd</em></li>
				<li><em class="italic">Kuma</em></li>
				<li><em class="italic">Consul</em></li>
			</ul>
			<p>Each of these service meshes has different takes on the service mesh pattern. <em class="italic">Istio</em> is likely the single most popular and comprehensive solution, but is also quite complex. <em class="italic">Linkerd</em> is also a mature project, but is easier to configure (though it uses its own proxy instead of Envoy). <em class="italic">Consul</em> is an option that supports Envoy in addition to other providers, and not just on Kubernetes. Finally, <em class="italic">Kuma</em> is an Envoy-based option that is also growing in popularity.</p>
			<p>Exploring all the options is <a id="_idIndexMarker786"/>beyond the scope of this book, so we will stick with Istio, as it is often considered the default solution. That said, all of these meshes have <a id="_idIndexMarker787"/>strengths and weaknesses, and it is worth looking at each one when planning to adopt the service mesh.</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor313"/>Setting up Istio on Kubernetes</h2>
			<p>Although Istio can be installed with<a id="_idIndexMarker788"/> Helm, the Helm installation option is no longer the officially supported installation method.</p>
			<p>Instead, we use the <code>Istioctl</code> CLI tool to<a id="_idIndexMarker789"/> install Istio with configuration onto our clusters. This configuration can be completely customized, but for the purposes of this book, we will just use the "demo" configuration:</p>
			<ol>
				<li>The first step to installing Istio on a cluster is to install the Istio CLI tool. We can do this with the following command, which installs the newest version of the CLI tool:<pre><strong class="bold">curl -L https://istio.io/downloadIstio | sh -</strong></pre></li>
				<li>Next, we'll want to add the CLI tool to our path for ease of use:<pre><strong class="bold">cd istio-&lt;VERSION&gt;</strong>
<strong class="bold">export PATH=$PWD/bin:$PATH</strong></pre></li>
				<li>Now, let's install Istio! Istio configurations are called <em class="italic">profiles</em> and, as mentioned previously, they can be completely customized using a YAML file. <p>For this demonstration, we'll use the inbuilt <code>demo</code> profile with Istio, which provides some basic setup. Install profile using the following command:</p><pre><strong class="bold">istioctl install --set profile=demo</strong></pre><p>This will result in the following output:</p><div><img src="img/B14790_14_003.jpg" alt="Figure 14.3 – Istioctl profile installation output"/></div><p class="figure-caption">Figure 14.3 – Istioctl profile installation output</p></li>
				<li>Since the sidecar<a id="_idIndexMarker790"/> resource has not been released yet as of<a id="_idIndexMarker791"/> Kubernetes 1.19, Istio will itself inject Envoy proxies into any namespace that is labeled with <code>istio-injection=enabled</code>.<p>To label any namespace with this, run the following command:</p><pre><strong class="bold">kubectl label namespace my-namespace istio-injection=enabled</strong></pre></li>
				<li>To test easily, label the <code>default</code> namespace with the preceding <code>label</code> command. Once the Istio components come up, any Pods in that namespace will automatically be injected with the Envoy sidecar, just like we created manually in the previous section. <p>In order to remove Istio from the cluster, run the following command:</p><pre><strong class="bold">istioctl x uninstall --purge</strong></pre><p>This should result in a confirmation message telling you that Istio has been removed.</p></li>
				<li>Now, let's deploy a little something to test our new mesh with! We will deploy three different application services, each with a<a id="_idIndexMarker792"/> deployment and a <a id="_idIndexMarker793"/>service resource:<p>a. Service Frontend</p><p>b. Service Backend A</p><p>c. Service Backend B</p><p>Here's the Deployment for <em class="italic">Service Frontend</em>:</p><pre>apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-frontend
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: service-frontend
        version: v2
    spec:
      containers:
      - name: service-frontend
        image: ravirdv/http-responder:latest
        ports:
        - containerPort: 5000
          name: svc-port
          protocol: TCP</pre><p>And here's the Service for <em class="italic">Service Frontend</em>:</p><pre>apiVersion: v1
kind: Service
metadata:
  name: service-frontend
spec:
  selector:
    name: service-frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000</pre><p>The YAML for Service Backends A and B will be the same as <em class="italic">Service Frontend</em>, apart from <a id="_idIndexMarker794"/>swapping the names, image names, and <a id="_idIndexMarker795"/>selector labels.</p></li>
				<li>Now that we have a couple of services to route to (and between), let's start setting up some Istio resources!<p>First thing's first, we need a <code>Gateway</code> resource. In this case, we are not using the NGINX Ingress Controller, but that's fine because Istio provides a <code>Gateway</code> resource that can be used for ingress and egress. Here's what an Istio <code>Gateway</code> definition looks like:</p><pre>apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: myapplication-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"</pre><p>These <code>Gateway</code> definitions look pretty similar to ingress records. We have <code>name</code>, and <code>selector</code>, which Istio uses to decide which Istio Ingress Controller to use. Next, we<a id="_idIndexMarker796"/> have one or more servers, which are essentially ingress points on our gateway. In this case, we do not restrict the host, and we accept requests on port <code>80</code>.</p></li>
				<li>Now that we have a <a id="_idIndexMarker797"/>gateway for getting requests into our cluster, we can start setting up some routes. We do this in Istio using <code>VirtualService</code>. <code>VirtualService</code> in Istio is a set of routes that should be followed when requests to a particular hostname are made. In addition, we can use a wildcard host to make global rules for requests from anywhere in the mesh. Let's take a look at an example <code>VirtualService</code> configuration:<pre>apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: myapplication
spec:
  hosts:
  - "*"
  gateways:
  - myapplication-gateway
  http:
  - match:
    - uri:
        prefix: /app
    - uri:
        prefix: /frontend
    route:
    - destination:
        host: service-frontend
        subset: v1</pre><p>In this <code>VirtualService</code>, we route requests to any host to our entry point at <em class="italic">Service Frontend</em> if it matches one<a id="_idIndexMarker798"/> of our <code>uri</code> prefixes. In this case, we are <a id="_idIndexMarker799"/>matching on the prefix, but you can use exact matching as well by swapping out <code>prefix</code> with <code>exact</code> in the URI matcher.</p></li>
				<li>So, now we have a setup fairly similar to what we would expect with an NGINX Ingress, with entry into the cluster dictated by a route match.<p>However, what's that <code>v1</code> in our route? This actually represents a version of our <em class="italic">Frontend Service</em>. Let's go <a id="_idIndexMarker800"/>ahead and specify this version using a new resource<a id="_idIndexMarker801"/> type – the Istio <code>DestinationRule</code>. Here's what a <code>DestinationRule</code> config looks like:</p><pre>apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: service-frontend
spec:
  host: service-frontend
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2</pre><p>As you can see, we specify two different versions of our frontend service in Istio, each looking at a label selector. From our previous Deployment and Service, you see that our current frontend service version is <code>v2</code>, but we could be running both in parallel! By specifying our <code>v2</code> version in the ingress virtual service, we tell Istio to route all requests to <code>v2</code> of the service. In addition, we have our <code>v1</code> version also configured, which is referenced in the previous <code>VirtualService</code>. This hard rule is only one possible way to route requests to different subsets in Istio.</p><p>Now, we've managed to route traffic into our cluster via a gateway, and to a virtual service subset based on a destination rule. At this point, we are effectively "inside" our service mesh!</p></li>
				<li>Now, from our <em class="italic">Service Frontend</em>, we want to<a id="_idIndexMarker802"/> be able to route to <em class="italic">Service Backend A</em> and <em class="italic">Service Backend B</em>. How do we do this? More virtual services is the answer! Let's take<a id="_idIndexMarker803"/> a look at a virtual service for <em class="italic">Backend Service A</em>:<pre>apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: myapplication-a
spec:
  hosts:
  - service-a
  http:
    route:
    - destination:
        host: service-backend-a
        subset: v1</pre><p>As you can see, this <code>VirtualService</code> routes to a <code>v1</code> subset for our service, <code>service-backend-a</code>. We'll also need another <code>VirtualService</code> for <code>service-backend-b</code>, which we won't include in full (but looks nearly identical). To see the full YAML, check the code repository for <code>istio-virtual-service-3.yaml</code>.</p></li>
				<li>Once our virtual services are ready, we require some destination rules! The <code>DestinationRule</code> for <em class="italic">Backend Service A</em> is as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">         Istio-destination-rule-2.yaml:</p>
			<pre>apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: service-backend-a
spec:
  host: service-backend-a
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
  subsets:
  - name: v1
    labels:
      version: v1</pre>
			<p>And the <code>DestinationRule</code> for <em class="italic">Backend Service B</em> is similar, just with different subsets. We won't include the<a id="_idIndexMarker804"/> code, but check <code>istio-destination-rule-3.yaml</code> in the code<a id="_idIndexMarker805"/> repository for the exact specifications.</p>
			<p>These destination rules and virtual services add up to make the following routing diagram:</p>
			<div><div><img src="img/B14790_14_004.jpg" alt="Figure 14.4 – Istio routing diagram"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Istio routing diagram</p>
			<p>As you can see, requests from <em class="italic">Frontend Service</em> Pods can route to <em class="italic">Backend Service A version 1</em> or <em class="italic">Backend Service B version 3</em>, and each backend service can route to the other as well. These<a id="_idIndexMarker806"/> requests to Backend Service A or B additionally engage one of the most valuable features of Istio – mutual (two-way) TLS. In this setup, TLS <a id="_idIndexMarker807"/>security is maintained between any two points in the mesh, and this all happens automatically!</p>
			<p>Next, let's take a look at using serverless patterns with Kubernetes.</p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor314"/>Implementing serverless on Kubernetes</h1>
			<p>Serverless patterns <a id="_idIndexMarker808"/>on cloud providers have quickly been <a id="_idIndexMarker809"/>gaining in popularity. Serverless architectures consist of compute that can automatically scale up and down, even scaling all the way to zero (where zero compute capacity is being used to <a id="_idIndexMarker810"/>serve a function or other application). <strong class="bold">Function-as-a-Service</strong> (<strong class="bold">FaaS</strong>) is an extension of the serverless pattern, where function code is the only input, and the serverless system takes care of routing requests to compute and scale as necessary. AWS Lambda, Azure Functions, and Google Cloud Run are some of the more popular FaaS/serverless options officially supported by cloud providers. Kubernetes also has many<a id="_idIndexMarker811"/> different serverless frameworks and libraries that can be used to run serverless, scale-to-zero workloads as well as FaaS on Kubernetes. Some of the most popular ones are as follows:</p>
			<ul>
				<li><em class="italic">Knative</em></li>
				<li><em class="italic">Kubeless</em></li>
				<li><em class="italic">OpenFaaS</em></li>
				<li><em class="italic">Fission</em></li>
			</ul>
			<p>A full discussion of all serverless options on Kubernetes is beyond the scope of this book, so we'll focus on two different ones, which aim to serve two vastly different use cases: <em class="italic">OpenFaaS</em> and <em class="italic">Knative</em>.</p>
			<p>While Knative is highly extensible <a id="_idIndexMarker812"/>and customizable, it uses multiple coupled components that add complexity. This means that some added configuration is necessary to get started with an FaaS solution, since functions are just one of many other patterns that Knative supports. OpenFaaS, on the other hand, makes getting up and running with serverless and FaaS on Kubernetes extremely easy. Both technologies are valuable for different reasons.</p>
			<p>For this chapter's tutorial, we will look at Knative, one of the most popular serverless frameworks, and one that also supports FaaS via its eventing feature.</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor315"/>Using Knative for FaaS on Kubernetes</h2>
			<p>As mentioned<a id="_idIndexMarker813"/> previously, Knative is a modular set of building blocks for serverless patterns on Kubernetes. For this reason, it requires a bit of configuration before we can get to the actual functions. Knative can also be installed with Istio, which it <a id="_idIndexMarker814"/>uses as a substrate for routing and scaling serverless applications. Other non-Istio routing options are also available.</p>
			<p>To use Knative for FaaS, we will need to install both <em class="italic">Knative Serving</em> and <em class="italic">Knative Eventing</em>. While Knative Serving will allow us to run our serverless workloads, Knative Eventing will provide the pathway to make FaaS requests to these scale-to-zero workloads. Let's accomplish this by following these steps:</p>
			<ol>
				<li value="1">First, let's install the Knative Serving components. We will begin by installing the CRDs:<pre><strong class="bold">kubectl apply --filename https://github.com/knative/serving/releases/download/v0.18.0/serving-crds.yaml</strong></pre></li>
				<li>Next, we can<a id="_idIndexMarker815"/> install the serving components themselves:<pre><strong class="bold">kubectl apply --filename https://github.com/knative/serving/releases/download/v0.18.0/serving-core.yaml</strong></pre></li>
				<li>At this point, we'll need to<a id="_idIndexMarker816"/> install a networking/routing layer for Knative to use. Let's use Istio:<pre><strong class="bold">kubectl apply --filename https://github.com/knative/net-istio/releases/download/v0.18.0/release.yaml</strong></pre></li>
				<li>We'll need the gateway IP address from Istio. Depending on where you're running this (in other words, AWS or locally), this value may differ. Pull it using the following command:<pre><strong class="bold">Kubectl get service -n istio-system istio-ingressgateway</strong></pre></li>
				<li>Knative requires a specific DNS setup for enabling the serving component. The easiest way to do this in a cloud setting is to use <code>xip.io</code> "Magic DNS," though this will not work for Minikube-based clusters. If you're running one of these (or just want to see all the<a id="_idIndexMarker817"/> options available), check out the Knative docs at <a href="https://knative.dev/docs/install/any-kubernetes-cluster/">https://knative.dev/docs/install/any-kubernetes-cluster/</a>.<p>To set up Magic DNS, use the following command:</p><pre><strong class="bold">kubectl apply --filename https://github.com/knative/serving/releases/download/v0.18.0/serving-default-domain.yaml</strong></pre></li>
				<li>Now that we've installed Knative Serving, let's install Knative Eventing to deliver our FaaS requests. First, we'll need more CRDs. Install them using the following command:<pre><strong class="bold">kubectl apply --filename https://github.com/knative/eventing/releases/download/v0.18.0/eventing-crds.yaml</strong></pre></li>
				<li>Now, install the eventing<a id="_idIndexMarker818"/> components just like we did with serving:<pre><strong class="bold">kubectl apply --filename https://github.com/knative/eventing/releases/download/v0.18.0/eventing-core.yaml</strong></pre><p>At this point, we need to add <a id="_idIndexMarker819"/>a queue/messaging layer for our eventing system to use. Did we mention that Knative supports lots of modular components?</p><p class="callout-heading">Important note</p><p class="callout">To make things easy, let's just use the basic in-memory messaging layer, but it's good to know all the options available to you. As regards modular options for messaging channels, check the docs at <a href="https://knative.dev/docs/eventing/channels/channels-crds/">https://knative.dev/docs/eventing/channels/channels-crds/</a>. For event source options, you can look at <a href="https://knative.dev/docs/eventing/sources/">https://knative.dev/docs/eventing/sources/</a>. </p></li>
				<li>To install the <code>in-memory</code> messaging layer, use the following command:<pre><strong class="bold">kubectl apply --filename https://github.com/knative/eventing/releases/download/v0.18.0/in-memory-channel.yaml</strong></pre></li>
				<li>Thought we were done? Nope! One last thing. We need to install a broker, which will take events from the messaging layer and get them processed in the right place. Let's use the default broker layer, the MT-Channel broker layer. You can install it using the following command:<pre><strong class="bold">kubectl apply --filename https://github.com/knative/eventing/releases/download/v0.18.0/mt-channel-broker.yaml</strong></pre></li>
			</ol>
			<p>With that, we are finally done. We have installed an end-to-end FaaS implementation via Knative. As you <a id="_idIndexMarker820"/>can tell, this was not an easy task. What makes Knative amazing is the same thing that makes it a pain – it offers so many different modular options and configurations<a id="_idIndexMarker821"/> that even when selecting the most basic options for each step, we've still taken a lot of time to explain the install. There are other options available, such as OpenFaaS, which are a bit easier to get up and running with, and we'll look into that in the next section! On the Knative side, however, now that we have our setup finally ready, we can add in our FaaS.</p>
			<h3>Implementing an FaaS pattern in Knative</h3>
			<p>Now that we have<a id="_idIndexMarker822"/> Knative set up, we can use it to implement an FaaS pattern where events will trigger some code running in Knative through a trigger. To set up a simple FaaS, we will require<a id="_idIndexMarker823"/> three things:</p>
			<ul>
				<li>A broker to route our events from an entry point</li>
				<li>A consumer service to actually process our events</li>
				<li>A trigger definition that specifies when to route events to the consumer for processing</li>
			</ul>
			<p>First thing's first, our broker needs to be created. This is simple and similar to creating an ingress record or gateway. Our <code>broker</code> YAML looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Knative-broker.yaml:</p>
			<pre>apiVersion: eventing.knative.dev/v1
kind: broker
metadata:
 name: my-broker
 namespace: default</pre>
			<p>Next, we can create a consumer service. This component is really just our application that is going to process events – our function itself! Rather than showing you even more YAML than you've already seen, let's assume <a id="_idIndexMarker824"/>our consumer service is just a regular old Kubernetes Service called <code>service-consumer</code>, which routes to a four-replica deployment of Pods running our application.</p>
			<p>Finally, we're going<a id="_idIndexMarker825"/> to need a trigger. This determines how and which events will be routed from the broker. The YAML for a trigger looks like this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Knative-trigger.yaml:</p>
			<pre>apiVersion: eventing.knative.dev/v1
kind: Trigger
metadata:
  name: my-trigger
spec:
  broker: my-broker
  filter:
    attributes:
      type: myeventtype
  subscriber:
    ref:
     apiVersion: v1
     kind: Service
     name: service-consumer</pre>
			<p>In this YAML, we create a <code>Trigger</code> rule that any event that comes through our broker, <code>my-broker</code>, and has a type of <code>myeventtype</code>, will automatically be routed to our consumer, <code>service-consumer</code>. For full documentation on trigger filters in Knative, check out the docs at <a href="https://knative.dev/development/eventing/triggers/">https://knative.dev/development/eventing/triggers/</a>.</p>
			<p>So, how do we create some events? First, check the broker URL using the following command:</p>
			<pre>kubectl get broker</pre>
			<p>This should result in the following output:</p>
			<pre>NAME      READY   REASON   URL                                                                                 AGE
my-broker   True             http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker     1m</pre>
			<p>We can now finally<a id="_idIndexMarker826"/> test our FaaS solution. Let's spin up a quick Pod<a id="_idIndexMarker827"/> from which we can make requests to our trigger:</p>
			<pre>kubectl run -i --tty --rm debug --image=radial/busyboxplus:curl --restart=Never -- sh</pre>
			<p>Now, from inside this Pod, we can go ahead and test our trigger, using <code>curl</code>. The request we need to make needs to have a <code>Ce-Type</code> header that equals <code>myeventtype</code>, since this is what our trigger requires. Knative uses headers in the form <code>Ce-Id</code>, <code>Ce-Type</code>, as shown in the following code block, to do the routing.</p>
			<p>The <code>curl</code> request will look like the following:</p>
			<pre>curl -v "http://broker-ingress.knative-eventing.svc.cluster.local/default/my-broker" \
  -X POST \
  -H "Ce-Id: anyid" \
  -H "Ce-Specversion: 1.0" \
  -H "Ce-Type: myeventtype" \
  -H "Ce-Source: any" \
  -H "Content-Type: application/json" \
  -d '{"payload":"Does this work?"}'</pre>
			<p>As you can see, we are sending a <code>curl</code> <code>http</code> request to the broker URL. Additionally, we are passing some special headers along with the HTTP request. Importantly, we are passing <code>type=myeventtype</code>, which our filter on our trigger requires in order to send the request for processing.</p>
			<p>In this example, our consumer <a id="_idIndexMarker828"/>service echoes back the payload key of the body JSON, along with a <code>200</code> HTTP response, so running this <code>curl</code> request gives<a id="_idIndexMarker829"/> us the following:</p>
			<pre>&gt; HTTP/1.1 200 OK
&gt; Content-Type: application/json
{
  "Output": "Does this work?"
}</pre>
			<p>Success! We have tested our FaaS and it returns what we are expecting. From here, our solution will scale up and down to zero along with the number of events, and, as with everything Knative, there are many more customizations and configuration options to tailor our solution precisely to what we need.</p>
			<p>Next up, we'll look at the same pattern with OpenFaaS instead of Knative in order to highlight the differences between the two approaches.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor316"/>Using OpenFaaS for FaaS on Kubernetes</h2>
			<p>Now that we've discussed<a id="_idIndexMarker830"/> getting started with Knative, let's do the <a id="_idIndexMarker831"/>same with OpenFaaS. First, to install OpenFaaS itself, we are going to use the Helm<a id="_idIndexMarker832"/> charts from the <code>faas-netes</code> repository, found at <a href="https://github.com/openfaas/faas-netes">https://github.com/openfaas/faas-netes</a>.</p>
			<h3>Installing OpenFaaS components with Helm</h3>
			<p>First, we will create two<a id="_idIndexMarker833"/> namespaces to hold our OpenFaaS components:</p>
			<ul>
				<li><code>openfaas</code> to hold the <a id="_idIndexMarker834"/>actual service components of OpenFaas</li>
				<li><code>openfaas-fn</code> to hold our deployed functions</li>
			</ul>
			<p>We can add these two namespaces using a nifty YAML file from the <code>faas-netes</code> repository using the following command:</p>
			<pre>kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml</pre>
			<p>Next, we need to add the <code>faas-netes</code> <code>Helm</code> <code>repository</code> with the following Helm command:</p>
			<pre>helm repo add openfaas https://openfaas.github.io/faas-netes/
helm repo update</pre>
			<p>Finally, we actually deploy OpenFaaS! </p>
			<p>The Helm chart for <a id="_idIndexMarker835"/>OpenFaaS at the preceding <code>faas-netes</code> repository has several possible variables, but we will use the following configuration to ensure<a id="_idIndexMarker836"/> that an initial set of authentication credentials are created, and that ingress records are deployed:</p>
			<pre>helm install openfaas openfaas/openfaas \
    --namespace openfaas  \
    --set functionNamespace=openfaas-fn \
    --set ingress.enabled=true \
    --set generateBasicAuth=true </pre>
			<p>Now, that our OpenFaaS infrastructure has been deployed to our cluster, we'll want to fetch the credentials that were generated as part of the Helm install. The Helm chart will create these as part of a hook and store them in a secret, so we can get them by running the following command:</p>
			<pre>OPENFAASPWD=$(kubectl get secret basic-auth -n openfaas -o jsonpath="{.data.basic-auth-password}" | base64 --decode)</pre>
			<p>That is all the Kubernetes setup we require!</p>
			<p>Moving on, let's install the OpenFaas CLI, which will make it extremely easy to manage our OpenFaas functions.</p>
			<h3>Inst<a id="_idTextAnchor317"/>alling the OpenFaaS CLI and deploying functions</h3>
			<p>To install the OpenFaaS CLI, we can use<a id="_idIndexMarker837"/> the following command (for Windows, check the preceding OpenFaaS documents):</p>
			<pre>curl -sL https://cli.openfaas.com | sudo sh</pre>
			<p>Now, we can get started with<a id="_idIndexMarker838"/> building and deploying some functions. This is easiest to do via the CLI.</p>
			<p>When building and deploying functions for OpenFaaS, the OpenFaaS CLI provides an easy way to generate boilerplates, and build and deploy functions for specific languages. It does this via "templates," and supports various flavors of Node, Python, and more. For a full list of the template types, check the templates repository at <a href="https://github.com/openfaas/templates">https://github.com/openfaas/templates</a>.</p>
			<p>The templates created using the OpenFaaS CLI are similar to what you would expect from a hosted serverless platform such as AWS Lambda. Let's create a brand-new Node.js function using the following command:</p>
			<pre>faas-cli new my-function –lang node</pre>
			<p>This results in the following output:</p>
			<pre>Folder: my-function created.
Function created in folder: my-function
Stack file written: my-function.yml</pre>
			<p>As you can see, the <code>new</code> command generates a folder, and within it some boilerplate for the function code itself, and an OpenFaaS YAML file.</p>
			<p>The OpenFaaS YAML file will appear as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">My-function.yml:</p>
			<pre>provider:
  name: openfaas
  gateway: http://localhost:8080
functions:
  my-function:
    lang: node
    handler: ./my-function
    image: my-function</pre>
			<p>The actual function code (inside the <code>my-function</code> folder) consists of a function file – <code>handler.js</code> – and a <a id="_idIndexMarker839"/>dependencies manifest, <code>package.json</code>. For other languages, these files will be different, and we won't delve into the specifics of dependencies<a id="_idIndexMarker840"/> in Node. However, we will edit the <code>handler.js</code> file to return some text. This is what the edited file looks like:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Handler.js:</p>
			<pre>"use strict"
module.exports = (context, callback) =&gt; {
    callback(undefined, {output: "my function succeeded!"});
}</pre>
			<p>This JavaScript code will return a JSON response with our text.</p>
			<p>Now that we have our function and handler, we can move on to building and deploying our function. The OpenFaaS CLI makes it simple to build the function, which we can do with the following command:</p>
			<pre>faas-cli build -f /path/to/my-function.yml </pre>
			<p>The output of this command is long, but when it is complete, we will have a new container image built locally with our function handler and dependencies embedded!</p>
			<p>Next, we push our container image to a container repository as we would for any other container. The OpenFaaS CLI has a neat wrapper command for this, which will push the image to Docker Hub or an alternate container image repository:</p>
			<pre>faas-cli push -f my-function.yml </pre>
			<p>Now, we can deploy our function to OpenFaaS. Once again, this is made easy by the CLI. Deploy it using the following command:</p>
			<pre>faas-cli deploy -f my-function.yml</pre>
			<p>Everything is now set <a id="_idIndexMarker841"/>up for us to test our function, deployed on OpenFaaS! We used an ingress setting when deploying OpenFaaS so requests can go through that ingress. However, our generated YAML file from our new function is set to make<a id="_idIndexMarker842"/> requests on <code>localhost:8080</code> for development purposes. We could edit that file to the correct <code>URL</code> for our ingress gateway (refer to the docs at <a href="https://docs.openfaas.com/deployment/kubernetes/">https://docs.openfaas.com/deployment/kubernetes/</a> for how to do that), but instead, let's just do a shortcut to get OpenFaaS open on our localhost.</p>
			<p>Let's use a <code>kubectl port-forward</code> command to open our OpenFaaS service on localhost port <code>8080</code>. We can do this as follows:</p>
			<pre>export OPENFAAS_URL=http://127.0.0.1:8080
kubectl port-forward -n openfaas svc/gateway 8080:8080</pre>
			<p>Now, let's add our previously generated auth credentials to the OpenFaaS CLI, as follows:</p>
			<pre>echo -n $OPENFAASPWD | faas-cli login -g $OPENFAAS_URL -u admin --password-stdin</pre>
			<p>Finally, all we need to do in order to test our function is to run the following command:</p>
			<pre>faas-cli invoke -f my-function.yml my-function</pre>
			<p>This results in the following output:</p>
			<pre>Reading from STDIN - hit (Control + D) to stop.
This is my message
{ output: "my function succeeded!"});}</pre>
			<p>As you can see, we've successfully received our intended response!</p>
			<p>Finally, if we want to delete this specific function, we can do so with the following command, similar to how we would use <code>kubectl delete -f</code>:</p>
			<pre>faas-cli rm -f my-function.yml </pre>
			<p>And that's it! Our function has been removed.</p>
			<h1 id="_idParaDest-302"><a id="_idTextAnchor318"/>Summary</h1>
			<p>In this chapter, we learned about service mesh and serverless patterns on Kubernetes. In order to set the stage for these, we first discussed running sidecar proxies on Kubernetes, specifically with the Envoy proxy.</p>
			<p>Then, we moved on to service mesh, and learned how to install and configure the Istio service mesh for service-to-service routing with mutual TLS.</p>
			<p>Finally, we moved on to serverless patterns on Kubernetes, where you learned how to configure and install Knative, and an alternative, OpenFaaS, for serverless eventing, and FaaS on Kubernetes.</p>
			<p>The skills you used in this chapter will help you to build service mesh and serverless patterns on Kubernetes, setting you up for fully automated service-to-service discovery and FaaS eventing.</p>
			<p>In the next (and final) chapter, we'll discuss running stateful applications on Kubernetes.</p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor319"/>Questions</h1>
			<ol>
				<li value="1">What is the difference between static and dynamic Envoy configurations?</li>
				<li>What are the four major pieces of Envoy configuration?</li>
				<li>What are some of the downsides to Knative, and how does OpenFaaS compare?</li>
			</ol>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor320"/>Further reading</h1>
			<ul>
				<li>CNCF landscape: <a href="https://landscape.cncf.io/">https://landscape.cncf.io/</a> </li>
				<li>Official Kubernetes forums: <a href="https://discuss.kubernetes.io/">https://discuss.kubernetes.io/</a></li>
			</ul>
		</div>
	</body></html>