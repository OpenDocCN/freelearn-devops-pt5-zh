<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0; Bringing Your Infrastructure Under Configuration Management"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3.  Bringing Your Infrastructure Under Configuration Management </h1></div></div></div><p>As hinted at the end of the previous chapter, there is some more work to be done before we can claim to have fully implemented IaC.</p><p>The first step was to describe the hardware side of our infrastructure in code; now it is time to look at the software or configuration aspect of it.</p><p>Let us say we have provisioned a few EC2 nodes and would like to have certain packages installed on them, and relevant configuration files updated. Prior to <span class="strong"><strong>Configuration Management</strong></span> (<span class="strong"><strong>CM</strong></span>) tools gaining popularity, such tasks would have been performed manually by an engineer either following a checklist, running a collection of shell scripts, or both. As you can imagine, such methods do not scale well as they generally imply one engineer setting up one server at a time.</p><p>In addition, checklists or scripts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Are hard to write when it comes to configuring a host plus a full application stack running on it</li><li class="listitem" style="list-style-type: disc">Are usually targeted at a given host or application and are not very portable</li><li class="listitem" style="list-style-type: disc">Get progressively harder to comprehend the further you get from the person who originally wrote them</li><li class="listitem" style="list-style-type: disc">Build scripts tend to get executed only once, usually at the time a host is provisioned, thus configuration starts to drift from that moment on</li></ul></div><p>Fortunately, not many people use these nowadays, as Configuration Management has become a common practice. Let us examine some of the benefits:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">CM allows us to declare the desired state of a machine once and then reproduce that state as often as necessary</li><li class="listitem" style="list-style-type: disc">Powerful abstraction takes care of specifics such as environment, hardware, and OS type, allowing us to write reusable CM code</li><li class="listitem" style="list-style-type: disc">The declared machine state code is easy to read, comprehend, and collaborate on.</li><li class="listitem" style="list-style-type: disc">A CM deployment can be performed on tens, hundreds, or thousands of machines simultaneously</li></ul></div><p>In this age of DevOps, there are a variety of CM tools to choose from. You might have already heard of Puppet, Chef, Ansible, OpsWorks, or the one we are going to use-<span class="strong"><strong>SaltStack</strong></span> (<span class="strong"><strong>the Salt Open project</strong></span>).</p><p>All of these are well developed, sophisticated CM solutions with active communities behind them. I find it hard to justify any reported claims of one being better than the rest as they all do the job pretty well, each with its own set of strengths and weaknesses. So which one you use, as is often the case, is up to personal preference.</p><p>Regardless of the tool you end up using, I would like to stress the importance of two points: naming conventions and code reusability.</p><p>Following naming conventions when writing code is an obvious win as it guarantees other people will be able to understand your work with less effort. In addition to writing code however, CM involves executing it against your nodes and this is where naming also becomes important. Imagine you had four servers: <span class="strong"><strong>leonardo</strong></span>, <span class="strong"><strong>donatello</strong></span>, <span class="strong"><strong>michelangelo</strong></span>, and <span class="strong"><strong>raphael</strong></span>. Two of those are your frontend layer and two the backend, so you sit down and write your Configuration Management manifests respectively: <span class="emphasis"><em>webserver-node</em></span> and <span class="emphasis"><em>database-node</em></span>. So far, so good, given the number of hosts you can launch your CM tool and easily tell it to run the relevant manifest against each of them.</p><p>Now imagine 50, then 100 hosts, within a similar flat-naming schema, and you start to see the problem. As the size and complexity of your infrastructure grows, you will need a host-naming convention that naturally forms a hierarchy. Hostnames such as <span class="emphasis"><em>webserver-{0..10}</em></span>, <span class="emphasis"><em>db-{0..5}</em></span> and <span class="emphasis"><em>cache-{0..5}</em></span> can be further grouped into frontend and backend and then represented in a structured, hierarchical way. Such a way of grouping nodes based on role or other properties is extremely useful when applying Configuration Management.</p><p>Code reusability should already be on your mind when you start writing CM code (manifests). You will find that there are generally two ways of approaching this task. You could write a large, say, web server piece which contains instructions on how to set up the firewall, some CLI tools, NGINX, and PHP on a node, or you could break it down into smaller parts like iptables, utils, NGINX, PHP, and so on.</p><p>In my opinion, the latter design adds some overhead when writing the manifests, but the benefit of reusability is substantial. Instead of writing large sets of declarations dedicated to each server type, you maintain a collection of generic, small ones and cherry-pick from them to suit the machine in question.</p><p>To illustrate:</p><pre class="programlisting">manifests: everything_a_websrv_needs, everything_for_a_db, cache_main 
nodes: web01, db01, cache01 
CM_execution: web01=(everything_a_websrv_needs), db01=(everything_for_a_db), cache01=(cache_main) 
</pre><p>Or better:</p><pre class="programlisting">manifests: iptables, utils, nginx, postgresql, redis, php 
nodes: web01, db01, cache01 
CM_execution: web01=(iptables,utils,nginx,php), db01=(iptables,utils,postgresql), cache01=(iptables,utils,redis) 
</pre><div class="section" title="Introduction to SaltStack"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec6"/>Introduction to SaltStack</h1></div></div></div><p>
<span class="strong"><strong>SaltStack</strong></span> (see <a class="ulink" href="https://saltstack.com/">https://saltstack.com/</a>), first released in 2011, is an automation suite which offers Configuration Management plus standard and/or event-driven orchestration. It is commonly used in a master-minion setup, where a master node provides centralized control across a compute estate. It is known for its speed and scalability thanks to the fast and lightweight message bus (<span class="strong"><strong>ZeroMQ</strong></span>) used for communication between the salt-master and minions. It can also be used in an agentless fashion, where the minions are controlled over SSH, similarly to how Ansible operates.</p><p>SaltStack is written in Python and is easily extensible. You can write your own modules for it, attach long-running processes to its event bus, and inject raw Python code in unusual places.</p><p>The master-minion model is quite powerful, offers a lot of flexibility, and is the recommended approach if you are looking after anything more than a few dev nodes and want to take advantage of all the features SaltStack has to offer.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>More on how to get a salt-master up and running can be found here: <a class="ulink" href="https://docs.saltstack.com/en/latest/topics/configuration/index.html">https://docs.saltstack.com/en/latest/topics/configuration/index.html</a>
</p></div></div><p>In our case, we are going to explore the power of Configuration Management using SaltStack in a standalone or masterless mode. We will reuse parts of the Terraform template from the previous chapter to launch a set of EC2 resources, bootstrap a SaltStack minion and have it configure itself to serve a web application.</p><p>Provided all goes well, we should end up with a fully configured web server (EC2 node) behind a load-balancer (EC2 ELB).</p><p>Here is our task-list:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Prepare our SaltStack development environment.</li><li class="listitem">Write the configuration that we would like SaltStack to apply to our node(s).</li><li class="listitem">Compose the Terraform template describing our infrastructure.</li><li class="listitem">Deploy the infrastructure via Terraform and let SaltStack configure it.</li></ol></div><div class="section" title="Preparation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec19"/>Preparation</h2></div></div></div><p>SaltStack Configuration Management is performed using the following main components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>States</strong></span> are the files which describe the desired state of a machine. Here we write instructions for installing packages, modifying files, updating permissions, and so on.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pillars</strong></span> are the files in which we define variables to help make States more portable and flexible.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Grains</strong></span> are pieces of information gathered on the minion host itself. These include details about the OS, environment, the hardware platform, and others.</li><li class="listitem" style="list-style-type: disc">The <span class="strong"><strong>Salt File Server</strong></span> stores any files, scripts, or other artifacts which may be referenced in the States.</li><li class="listitem" style="list-style-type: disc">The Salt Top file(s) are used to map States and/or Pillars to minions.</li></ul></div><p>In a master-minion setup, all of these components except the Grains would be hosted on and made available to the minions by the salt-master (other backends are also supported).</p><p>We are planning to run Salt in masterless mode however, meaning that we will need a way to transfer any States, Pillars, and related files from our local environment to the minion. Git? Good idea. We will write all Salt code locally, push it to a Git repository, and then have it checked out onto each minion at boot time.</p><p>As for choosing a Git hosting solution, Github or Bitbucket are excellent services, but giving our minion EC2 nodes access to these will involve some key handling. In comparison, <span class="strong"><strong>CodeCommit</strong></span> (the AWS Git solution) offers a much smoother integration with EC2 instances via IAM Roles.</p><p>Let us start by creating a new IAM user and a CodeCommit Git repository. We will be using the user's access keys to create the repository and a SSH key to clone and work with it:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the AWS Console, create an IAM user (write down the generated access keys) and attach the <span class="strong"><strong>AWSCodeCommitFullAccess </strong></span>built-in / <span class="strong"><strong>Managed</strong></span> IAM policy to it as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_02_004.jpg" alt="Preparation"/></div><p>
</p></li><li class="listitem">On the same page, switch to the <span class="strong"><strong>Security Credentials</strong></span> tab and click on the <span class="strong"><strong>Upload SSH public key </strong></span>as shown in the following screenshot:<p>
</p><div class="mediaobject"><img src="graphics/image_03_002.jpg" alt="Preparation"/></div><p>
</p></li><li class="listitem">Configure <code class="literal">awscli</code>:<pre class="programlisting">
<span class="strong"><strong>$ export AWS_ACCESS_KEY_ID='AKIAHNPFB9EXAMPLEKEY'</strong></span>
<span class="strong"><strong>$ export AWS_SECRET_ACCESS_KEY=
     'rLdrfHJvfJUHY/B7GRFTY/VYSRwezaEXAMPLEKEY'</strong></span>
<span class="strong"><strong>$ export AWS_DEFAULT_REGION='us-east-1'</strong></span>
</pre></li><li class="listitem">Create a repository:<pre class="programlisting">
<span class="strong"><strong>$ aws codecommit create-repository --repository-name salt 
      --repository-description "SaltStack repo"</strong></span>
<span class="strong"><strong>{</strong></span>
<span class="strong"><strong>"repositoryMetadata": {</strong></span>
<span class="strong"><strong>"repositoryName": "salt",</strong></span>
<span class="strong"><strong>"cloneUrlSsh": "ssh://git-codecommit.us-
     east-1.amazonaws.com/v1/repos/salt",</strong></span>
<span class="strong"><strong>"lastModifiedDate": 1465728037.589,</strong></span>
<span class="strong"><strong>"repositoryDescription": "SaltStack repo",</strong></span>
<span class="strong"><strong>"cloneUrlHttp": 
    "https://git-codecommit.us-east-1.amazonaws.com/v1/repos/salt",</strong></span>
<span class="strong"><strong>"creationDate": 1465728037.589,</strong></span>
<span class="strong"><strong>"repositoryId": "d0628373-d9a8-44ab-942a-xxxxxx",</strong></span>
<span class="strong"><strong>"Arn": "arn:aws:codecommit:us-east-1:xxxxxx:salt",</strong></span>
<span class="strong"><strong>"accountId": "xxxxxx"</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></li><li class="listitem">Clone the new repository locally:<pre class="programlisting">
<span class="strong"><strong>$ git clone ssh://SSH_KEY_ID@git-codecommit.us-
     east-1.amazonaws.com/v1/repos/salt</strong></span>
<span class="strong"><strong>Cloning into 'salt'...</strong></span>
<span class="strong"><strong>warning: You appear to have cloned an empty repository.</strong></span>
<span class="strong"><strong>Checking connectivity... done.</strong></span>
</pre></li></ol></div><p>Here, <code class="literal">SSH_KEY_ID</code> is the one we saw after uploading a public key in step 2.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>For more options on connecting to CodeCommit see <a class="ulink" href="http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up.html">http://docs.aws.amazon.com/codecommit/latest/userguide/setting-up.html</a>
</p></div></div><p>We are ready to start populating our empty, new Salt repository.</p></div></div></div>
<div class="section" title="Writing Configuration Management code"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec7"/>Writing Configuration Management code</h1></div></div></div><p>For SaltStack to help us configure our node as a web server, we need to tell it what one of those should look like. In Configuration Management terms, we need to describe the desired state of the machine.</p><p>In our example, we will be using a combination of SaltStack States, Pillars, Grains, and Top files to describe the processes of:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating Linux user accounts</li><li class="listitem" style="list-style-type: disc">Installing services (NGINX and PHP-FPM)</li><li class="listitem" style="list-style-type: disc">Configuring and running the installed services</li></ul></div><div class="section" title="States"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec20"/>States</h2></div></div></div><p>A State contains a set of instructions which we would like to be applied to our EC2 minion(s). We will use <code class="literal">/srv/salt/states</code> on the minion as the root of the Salt State tree. States can be stored in there in the form of a single file, for example <code class="literal">/srv/salt/states/mystate.sls</code>, or organized into folders like so <code class="literal">/srv/salt/states/mystate/init.sls</code> . Later on, when we request that <code class="literal">mystate</code> is executed, Salt will look for either a <code class="literal">state_name.sls</code> or a <code class="literal">state_name/init.sls</code> in the root of the <span class="emphasis"><em>State Tree</em></span>. I find the second approach tidier as it allows for other state-related files to be kept in the relevant folder.</p><p>We begin the Configuration Management of our web server node with a state for managing Linux user accounts. Inside our Salt Git repository, we create <code class="literal">states/users/init.sls</code>:</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>Please refer to:
<a class="ulink" href="https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/users">https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/users</a>.</p></div></div><pre class="programlisting">    veselin:
      user.present:
        - fullname: Veselin Kantsev
        - uid: {{ salt['pillar.get']('users:veselin:uid') }}
        - password: {{ salt['pillar.get']('users:veselin:password') }}
        - groups:
          - wheel
    
    ssh_auth.present:
      - user: veselin
      - source: salt://users/files/veselin.pub
      - require:
        - user: veselin
    
    sudoers:
      file.managed:
        - name: /etc/sudoers.d/wheel
        - contents: '%wheel ALL=(ALL) ALL'
</pre><p>We will use YAML to write most Salt configuration. You will notice three different state modules used in the preceding section:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">user.present</code>: This module ensures that a given user account exists on the system or creates one if necessary</li><li class="listitem" style="list-style-type: disc"><code class="literal">ssh_auth.present</code>: A module for managing the SSH <code class="literal">authorized_keys</code> file of a user</li><li class="listitem" style="list-style-type: disc"><code class="literal">file.managed</code>: A module for creating/modifying files</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>SaltStack's state modules offer rich functionality. For full details of each module see <a class="ulink" href="https://docs.saltstack.com/en/latest/ref/states/all/">https://docs.saltstack.com/en/latest/ref/states/all/</a>
</p></div></div><p>To avoid hardcoding certain values under <code class="literal">user.present</code>, we make use of the SaltStack Pillars system. We will examine a pillar file shortly, but for now just note the syntax of referencing pillar values inside our state.</p><p>Two other points of interest here are the source of our key file and the <code class="literal">require</code> property. In this example, a <code class="literal">salt://</code> formatted source address refers to the Salt File Server which by default serves files from the State Tree (for supported backends, please see <a class="ulink" href="https://docs.saltstack.com/en/latest/ref/file_server/">https://docs.saltstack.com/en/latest/ref/file_server/</a>). The <code class="literal">require</code> statement enforces an order of execution, ensuring that the user account is present before trying to create an <code class="literal">authorized_keys</code> file for it.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>SaltStack follows an imperative execution model until such custom ordering is enforced, invoking a declarative mode (see <a class="ulink" href="https://docs.saltstack.com/en/latest/ref/states/ordering.html">https://docs.saltstack.com/en/latest/ref/states/ordering.html</a>).</p></div></div><p>Thanks to the readability of YAML, one can easily tell what is going on here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We create a new Linux user.</li><li class="listitem">We apply desired attributes (uid, password, group, and so on).</li><li class="listitem">We deploy an SSH <code class="literal">authorized_keys</code> file for it.</li><li class="listitem">We enable <code class="literal">sudo</code> for the wheel group of which the user is a member.</li></ol></div><p>Perhaps you could try <span class="emphasis"><em>edit this state</em></span> and <span class="emphasis"><em>add a user</em></span> for yourself? It will be useful later after we deploy.</p><p>We will now move on to an NGINX installation via <code class="literal">states/nginx/init.sls</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>Please refer to: <a class="ulink" href="https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/nginx">https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/nginx</a>.</p></div></div><p>We install NGINX using the <code class="literal">pkg.installed</code> module:</p><p>
<code class="literal">pkg.installed: []</code>
</p><p>Set the service to start on boot (<code class="literal">enable: True</code>), enable reloading instead of restarting when possible (<code class="literal">reload: True</code>), ensure the NGINX pkg has been installed (<code class="literal">require:</code>) before running the service (<code class="literal">service.running:</code>)</p><pre class="programlisting">    nginx:
      service.running:
        - enable: True
        - reload: True
        - require:
          - pkg: nginx
</pre><p>Then put a <code class="literal">config</code> file in place (<code class="literal">file.managed:</code>), ensuring the service waits for this to happen (<code class="literal">require_in:</code>) and also reloads each time the file is updated (<code class="literal">watch_in:</code>):</p><pre class="programlisting">    /etc/nginx/conf.d/default.conf:
      file.managed:
        - source: salt://nginx/files/default.conf
        - require:
          - pkg: nginx
        - require_in:
          - service: nginx
        - watch_in:
          - service: nginx
</pre><p>Note the <code class="literal">require</code>/<code class="literal">require_in</code>, <code class="literal">watch</code>/<code class="literal">watch_in</code> pairs. The difference between each of these requisites and its <code class="literal">_in</code> counterpart lies in the direction in which they act.</p><p>For example:</p><pre class="programlisting">    nginx:
      service.running:
        - watch:
          - file: nginx_config
    nginx_config:
      file.managed:
        - name: /etc/nginx/nginx.conf
        - source: salt://...
</pre><p>Has the same effect as:</p><pre class="programlisting">    nginx:
      service.running: []
      nginx_config:
      file.managed:
        - name: /etc/nginx/nginx.conf
        - source: salt://...
          - watch_in:
            - service: nginx
</pre><p>In both cases, the NGINX service restarts on <code class="literal">config</code> file changes; however, you can see how the second format can be potentially quite useful the further you get from the service block-say in a different file, as we will see in the next state.</p><p>Add in some PHP (<code class="literal">states/php-fpm/init.sls</code>):</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note23"/>Note</h3><p>Please refer to:
<a class="ulink" href="https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/php-fpm">https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/php-fpm</a>.</p></div></div><pre class="programlisting">    include:
      - nginx
    
    php-fpm:
      pkg.installed:
        - name: php-fpm
        - require:
          - pkg: nginx
    
    service.running:
      - name: php-fpm
      - enable: True
      - reload: True
      - require_in:
        - service: nginx...
</pre><p>Here you can better see the usefulness of an <code class="literal">_in</code> requisite. After we include the <code class="literal">nginx</code> state at the top, our <code class="literal">require_in</code> makes sure that <code class="literal">nginx</code> does not start before <code class="literal">php-fpm</code> does.</p><p>With NGINX and PHP-FPM now configured, let us add a quick test page (<code class="literal">states/phptest/init.sls</code>).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>Please refer to:
<a class="ulink" href="https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/phptest">https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/CodeCommit/salt/states/phptest</a>.</p></div></div><p>We set a few variables pulled from Grains (more on those shortly):</p><pre class="programlisting">{% set publqic_ipv4 = salt['cmd.shell']('ec2-metadata --public-ipv4 | awk '{ print $2 }'') %} 
{% set grains_ipv4 = salt['grains.get']('ipv4:0') %} 
{% set grains_os = salt['grains.get']('os') %} 
{% set grains_osmajorrelease = salt['grains.get']('osmajorrelease') %} 
{% set grains_num_cpus = salt['grains.get']('num_cpus') %} 
{% set grains_cpu_model = salt['grains.get']('cpu_model') %} 
{% set grains_mem_total = salt['grains.get']('mem_total') %} 
</pre><p>Then we deploy the test page and add <code class="literal">contents</code> to it directly:</p><pre class="programlisting">phptest: 
  file.managed: 
    - name: /var/www/html/index.php 
    - makedirs: True 
    - contents: | 
        &lt;?php 
          echo '&lt;p style="text-align:center;color:red"&gt;  
          Hello from {{ grains_ipv4 }}/{{ public_ipv4 }} running PHP ' . 
          phpversion() . ' on {{ grains_os }} {{ grains_osmajorrelease }}.  
          &lt;br&gt; I come with {{ grains_num_cpus }} x {{ grains_cpu_model }}  
          and {{ grains_mem_total }} MB of memory. &lt;/p&gt;'; 
          phpinfo(INFO_LICENSE); 
        ?&gt; 
</pre><p>We will use this page post-deployment to check whether both NGINX and PHP-FPM are operational.</p></div><div class="section" title="Pillars"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec21"/>Pillars</h2></div></div></div><p>Now let us look at the main mechanism for storing variables in Salt-the Pillars. These are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">YAML tree-like data structures</li><li class="listitem" style="list-style-type: disc">Defined/rendered on the salt-master, unless running masterless in which case they live on the minion</li><li class="listitem" style="list-style-type: disc">Useful for storing variables in a central place to be shared by the minions (unless they are masterless)</li><li class="listitem" style="list-style-type: disc">Helpful for keeping States portable</li><li class="listitem" style="list-style-type: disc">Appropriate for sensitive data (they can also be GPG encrypted; see <a class="ulink" href="https://docs.saltstack.com/en/latest/ref/renderers/all/salt.renderers.gpg.html">https://docs.saltstack.com/en/latest/ref/renderers/all/salt.renderers.gpg.html</a>)</li></ul></div><p>We will be using <code class="literal">/srv/salt/pillars</code> as the root of our Pillar tree on the minion. Let us go back to the <code class="literal">users</code> state and examine the following lines:</p><pre class="programlisting">- uid: {{ salt['pillar.get']('users:veselin:uid') }} 
- password: {{ salt['pillar.get']('users:veselin:password') }} 
</pre><p>The <code class="literal">uid</code> and <code class="literal">password</code> attributes are set to be sourced from a pillar named <code class="literal">users</code>. And if we check our Pillar Tree, we find a <code class="literal">/srv/salt/pillars/users.sls</code> file containing:</p><pre class="programlisting">users: 
  veselin: 
    uid: 5001 
    password: '$1$wZ0gQOOo$HEN/gDGS85dEZM7QZVlFz/' 
</pre><p>It is now easy to see how the <code class="literal">users:veselin:password</code> reference inside the state file matches against this pillar's structure.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>For more details and examples on pillar usage, see: <a class="ulink" href="https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html">https://docs.saltstack.com/en/latest/topics/tutorials/pillar.html</a>
</p></div></div></div><div class="section" title="Grains"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec22"/>Grains</h2></div></div></div><p>Unlike Pillars, Grains are considered static data:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">They get generated minion-side and are not shared between different minions</li><li class="listitem" style="list-style-type: disc">They contain facts about the minion itself</li><li class="listitem" style="list-style-type: disc">Typical examples are CPU, OS, network interfaces, memory, and kernels</li><li class="listitem" style="list-style-type: disc">It is possible to add custom Grains to a minion</li></ul></div><p>We have already made good use of Grains within our preceding test page (<code class="literal">states/phptest/init.sls</code>), getting various host details such as CPU, memory, network, and OS. Another way of using this data is when dealing with multi-OS environments. Let us look at the following example:</p><pre class="programlisting">pkg.installed: 
  {% if grains['os'] == 'CentOS' or grains['os'] == 'RedHat' %} 
    - name: httpd...  
  {% elif grains['os'] == 'Debian' or grains['os'] == 'Ubuntu' %} 
    - name: apache2 
  ... 
  {% endif %} 
</pre><p>As you see, Grains, much like Pillars, help make our States way more flexible.</p></div><div class="section" title="Top files"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec23"/>Top files</h2></div></div></div><p>We now have our States ready, even supported by some Pillars and ideally would like to apply all of those to a host so we can get it configured and ready for use.</p><p>In SaltStack, the Top File provides the mapping between States/Pillars and the minions they should be applied onto. We have a Top file (<code class="literal">top.sls</code>) in the root of both the state and pillar trees. We happen to have a single environment (base), but we could easily add more (<span class="emphasis"><em>dev</em></span>, <span class="emphasis"><em>qa</em></span>, <span class="emphasis"><em>prod</em></span>). Each could have a separate state and pillar trees with separate Top files which get compiled into one at runtime.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note26"/>Note</h3><p>Please see <a class="ulink" href="https://docs.saltstack.com/en/latest/ref/states/top.html">https://docs.saltstack.com/en/latest/ref/states/top.html</a> for more information on multi-environment setups.</p></div></div><p>Let us look at a <code class="literal">top.sls</code> example:</p><pre class="programlisting">base: 
  '*': 
    - core_utils 
    - monitoring_client 
      - log_forwarder
  'webserver-*':
    - nginx
    - php-fpm
  'dbserver-*': 
    - pgsql_server 
    - pgbouncer 
</pre><p>We are declaring that in our base (default) environment:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">All minions should have the core set of utilities, the monitoring and log forwarding agents installed</li><li class="listitem" style="list-style-type: disc">Minions with an ID matching <code class="literal">webserver-*</code>, get the <code class="literal">nginx</code> and <code class="literal">php-fpm</code> States (in addition to the previous three)</li><li class="listitem" style="list-style-type: disc">Database nodes get applied: the common three plus <code class="literal">pgsql_server</code> and <code class="literal">pgbouncer</code></li></ul></div><p>Minion targeting gets even more interesting when you include Pillars, Grains, or a mix of these (see <a class="ulink" href="https://docs.saltstack.com/en/latest/ref/states/top.html#advanced-minion-targeting">https://docs.saltstack.com/en/latest/ref/states/top.html#advanced-minion-targeting</a>).</p><p>By specifying such state/pillar to a minion association, from a security standpoint we also create a useful isolation. Say our Pillars contained sensitive data, then this is how we could limit the group of minions who are allowed access to it.</p><p>Back to our Salt repository, where we find two <code class="literal">top.sls</code> files:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">salt/states/top.sls</code>:</li></ul></div><pre class="programlisting">
<span class="strong"><strong>base:</strong></span> 
  '*': 
    - users 
    - nginx 
    - php-fpm 
    - phptest 
</pre><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">salt/pillars/top.sls</code>:</li></ul></div><pre class="programlisting">base: 
  '*': 
    - users 
</pre><p>We can allow ourselves to target <code class="literal">*</code>, as we are running in masterless mode and essentially all our States/Pillars are intended for the local minion.</p><p>We enable this mode with a few settings in a minion configuration file (<code class="literal">/etc/salt/minion.d/masterless.conf</code>).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note27"/>Note</h3><p>Please refer to:
<a class="ulink" href="https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_03_CodeFiles/CodeCommit/salt/minion.d/masterless.conf">https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/blob/master/5585_03_CodeFiles/CodeCommit/salt/minion.d/masterless.conf</a>.</p></div></div><p>These effectively tell the salt-minion process that the Salt Fileserver, the state tree and the pillar tree are all to be found on the local filesystem. You will see how this configuration file gets deployed via UserData in a moment.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note28"/>Note</h3><p>More on running masterless can be found at: <a class="ulink" href="https://docs.saltstack.com/en/latest/topics/tutorials/standalone_minion.html">https://docs.saltstack.com/en/latest/topics/tutorials/standalone_minion.html</a>
</p></div></div><p>This concludes our SaltStack internals session. As you get more comfortable, you may want to look into Salt Engines, Beacons, writing your own modules and/or Salt Formulas. And those are only some of the ninja features being constantly added to the project.</p><p>At this stage we already know how to use Terraform to deploy and now SaltStack to configure.</p></div></div>
<div class="section" title="Bootstrapping nodes under Configuration Management (end-to-end IaC)"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec8"/>Bootstrapping nodes under Configuration Management (end-to-end IaC)</h1></div></div></div><p>Without further delay, let us get our old VPC re-deployed along with a configuration-managed web service inside it.</p><p>Terraform will spawn the VPC, ELB, and EC2 nodes then bootstrap the SaltStack workflow with the use of EC2 UserData. Naturally, we strive to reuse as much code as possible; however, our next deployment requires some changes to the TF templates.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note29"/>Note</h3><p>Please refer to:
<a class="ulink" href="https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/Terraform">https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/Terraform</a>.</p></div></div><p>
<code class="literal">resources.tf</code>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We do not need the private subnets/route tables, NAT, nor RDS resources this time, so we have removed these, making the deployment a bit faster.</li><li class="listitem" style="list-style-type: disc">We will be using an IAM Role to grant permission to the EC2 node to access the CodeCommit repository.<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have declared the role:<pre class="programlisting">
<span class="strong"><strong>resource "aws_iam_role" "terraform-role" {</strong></span>
<span class="strong"><strong>name = "terraform-role"path = "/"...</strong></span>
</pre></li><li class="listitem" style="list-style-type: disc">We have added and associated a policy (granting read access to CodeCommit) with that role:<pre class="programlisting">
<span class="strong"><strong>resource "aws_iam_role_policy" "terraform-policy" {</strong></span>
<span class="strong"><strong>name = "terraform-policy"</strong></span>
<span class="strong"><strong>role = "${aws_iam_role.terraform-role.id}"...</strong></span>
</pre></li><li class="listitem" style="list-style-type: disc">We have created and associated an instance profile with the role:<pre class="programlisting">
<span class="strong"><strong>resource "aws_iam_instance_profile" "terraform-profile" {</strong></span>
<span class="strong"><strong>name = "terraform-profile"</strong></span>
<span class="strong"><strong>roles = ["${aws_iam_role.terraform-role.name}"]</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></li><li class="listitem" style="list-style-type: disc">We have updated the Auto Scaling launch-configuration with the instance profile ID:<pre class="programlisting">
<span class="strong"><strong>resource "aws_launch_configuration" "terraform-lcfg"   
             {...iam_instance_profile = 
             "${aws_iam_instance_profile.terraform-profile.id}"
              ...</strong></span>
</pre></li></ul></div></li><li class="listitem" style="list-style-type: disc">We have updated the UserData script with some SaltStack bootstrap instructions, to install Git and SaltStack, checkout and put our Salt code in place and finally run Salt:<pre class="programlisting">   user_data = &lt;&lt;EOF 
   #!/bin/bash 
   set -euf -o pipefail 
   exec 1&gt; &gt;(logger -s -t $(basename $0)) 2&gt;&amp;1 
   # Install Git and set CodeComit connection settings 
   # (required for access via IAM roles) 
   yum -y install git 
   git config --system credential.helper 
   '!aws codecommit credential-helper $@' 
   git config --system credential.UseHttpPath true 
   # Clone the Salt repository 
   git clone https://git-codecommit.us-east-1.amazonaws.com/v1/repos/
   salt/srv/salt; chmod 700 /srv/salt 
   # Install SaltStack 
   yum -y install https://repo.saltstack.com/yum/amazon/
   salt-amzn-repo-latest-1.ami.noarch.rpm 
   yum clean expire-cache; yum -y install salt-minion; 
   chkconfig salt-minion off 
   # Put custom minion config in place (for enabling masterless mode) 
   cp -r /srv/salt/minion.d /etc/salt/ 
   # Trigger a full Salt run 
   salt-call state.apply 
   EOF 
   We have moved our EC2 node (the Auto Scaling group) 
   to a public subnet and allowed incoming SSH traffic 
   so that we can connect and play with Salt on it: 
   resource "aws_security_group" "terraform-ec2" {ingress { 
   from_port = "22" 
   to_port = "22" 
   ...resource "aws_autoscaling_group" "terraform-asg" { 
   ... 
   vpc_zone_identifier = ["${aws_subnet.public-1.id}",
   ... 
</pre></li></ul></div><p>
<code class="literal">variables.tf</code>:</p><p>We have removed all RDS related variables.</p><p>
<code class="literal">outputs.tf</code>:</p><p>We have removed RDS and NAT related outputs.</p><p>
<code class="literal">iam_user_policy.json</code>:</p><p>This document will become useful shortly as we will need to create a new user for the deployment. We have removed RDS permissions and added IAM ones from it.</p><p>We are now ready for deployment. Pre-flight check:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Updated Terraform templates<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note30"/>Note</h3><p>Please refer to: <a class="ulink" href="https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/Terraform">https://github.com/PacktPublishing/Implementing-DevOps-on-AWS/tree/master/5585_03_CodeFiles/Terraform</a>) are available locally in our designated terraform folder</p></div></div></li><li class="listitem" style="list-style-type: disc">Created/updated our Terraform IAM account with the new set of permissions as per <code class="literal">iam_user_policy.json</code></li><li class="listitem" style="list-style-type: disc">Ensured we have a copy of the <code class="literal">terraform ec2 keypair</code> (for SSH-ing later)</li><li class="listitem" style="list-style-type: disc">All our SaltStack code has been pushed up to the Salt CodeCommit repository (Please refer to: <a class="ulink" href="https://git-codecommit.us-east-1.amazonaws.com/v1/repos/salt">https://git-codecommit.us-east-1.amazonaws.com/v1/repos/salt</a>)</li></ul></div><p>Let us export our credentials and launch Terraform:</p><pre class="programlisting">
<span class="strong"><strong>$ export AWS_ACCESS_KEY_ID='user_access_key'</strong></span>
<span class="strong"><strong>$ export AWS_SECRET_ACCESS_KEY='user_secret_access_key'</strong></span>
<span class="strong"><strong>$ export AWS_DEFAULT_REGION='us-east-1'$ cd Terraform/$ terraform validate</strong></span>
<span class="strong"><strong>$ terraform plan...Plan: 15 to add, 0 to change, 0 to destroy.</strong></span>
<span class="strong"><strong>$ terraform apply...Outputs:</strong></span>
<span class="strong"><strong>ELB URI = terraform-elb-xxxxxx.us-east-1.elb.amazonaws.com</strong></span>
<span class="strong"><strong>VPC ID = vpc-xxxxxx</strong></span>
</pre><p>Allow 3-5 minutes for output <code class="literal">t2.nano</code> to come into shape and then browse to the ELB URI from the following output:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_003.jpg" alt="Bootstrapping nodes under Configuration Management (end-to-end IaC)"/></div><p>
</p><p>Victory!</p><p>Try increasing the <span class="emphasis"><em>autoscaling-group-minsize</em></span> and <span class="emphasis"><em>autoscaling-group-maxsize</em></span> in <code class="literal">terraform.tfvars</code>, then re-applying the template. You should start seeing different IPs when the page is refreshed.</p><p>Given the preceding test page, we can be reasonably confident that Salt bootstrapped and applied our set of States successfully.</p><p>We did, however, enable SSH access in order to be able to experiment more with Salt, so let us do that.</p><p>We see the public IP of the node on our test page. You could SSH into it with either the <code class="literal">terraform ec2 keypair</code> or the default <code class="literal">ec2-user</code> Linux account, or if you dared create one for yourself in the <code class="literal">users/init.sls</code> state earlier, you could use it now.</p><p>Once connected, we can use the <code class="literal">salt-call</code> command (as root) to interact with Salt locally:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How about some Pillars:</li></ul></div><pre class="programlisting">
<span class="strong"><strong># salt-call pillar.items</strong></span>
</pre><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Or let us see what Grains we have:</li></ul></div><pre class="programlisting">
<span class="strong"><strong># salt-call grains.items</strong></span>
</pre><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Run individual States:</li></ul></div><pre class="programlisting">
<span class="strong"><strong># salt-call state.apply nginx</strong></span>
</pre><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Or execute a full run, that is of all assigned States as per the Top file:</li></ul></div><pre class="programlisting">
<span class="strong"><strong># salt-call state.apply</strong></span>
</pre><p>After playing with our new deployment for a bit, I suspect you are going to want to try adding or changing States/Pillars or other parts of the Salt code. As per the IaC rules we agreed upon earlier, every change we make goes through Git, but let us examine what options we have for deploying those changes afterwards:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Pull the changes down to each minion and run <code class="literal">salt-call</code></li><li class="listitem" style="list-style-type: disc">Provision new minions which will pull down the latest code</li><li class="listitem" style="list-style-type: disc">Push changes via a Salt-master</li></ul></div><p>It is easy to see that the first option will work with the couple of nodes we use for testing, but is quickly going to become hard to manage at scale.</p><p>Provisioning new minions on each deployment is a valid option if masterless Salt setup is preferred; however, you need to consider the frequency of deployments in your environment and the associated cost of replacing EC2 nodes. One benefit worth nothing here is that of blue/green deployments. By provisioning new minions to serve your code changes, you get to keep the old ones around for a while which allows you to shift traffic gradually and roll back safely if needed.</p><p>Having a Salt-master would be my recommended approach for any non-dev environments. The Salt code is kept on it, so any Git changes you make, need to be pulled down only once. You can then deploy the changed States/Pillars by targeting the minions you want from the Salt-master. You could still do blue/green for major releases or you could choose to deploy to your current minions directly if it is just a minor, safe amendment, or perhaps something critical that needs to reach all minions as soon as possible.</p><p>Another powerful feature of the Salt-master is orchestration, more specifically-remote execution. With all your minions connected to it, the salt-master becomes a command center from which you have more or less full control over them.</p><p>Executing commands on the minions is done via modules from generic ones such as <code class="literal">cmd.run</code>, which essentially allows you to run arbitrary shell commands to more specialized ones such as <code class="literal">nginx</code>, <code class="literal">postfix</code>, <code class="literal">selinux</code>, or <code class="literal">zfs</code>. The list is quite long as you can see here: <a class="ulink" href="https://docs.saltstack.com/en/latest/ref/modules/all/index.html">https://docs.saltstack.com/en/latest/ref/modules/all/index.html</a>.</p><p>And if you recall the earlier section on hostnames and naming conventions, this is where one can appreciate their value. It is quite convenient to be able to execute statements like:</p><pre class="programlisting">salt 'webserver-*' nginx.status  
salt 'db-*' postgres.db_list 
</pre><p>You can also use Pillars and/or Grains to add tags to your hosts, so you could further group them per location, role, department, or something similar.</p><p>In brief, here are a few key points of masterless versus a salt-master arrangement:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><tbody><tr><td>
<p>
<span class="strong"><strong>Salt Master</strong></span>
</p>
</td><td>
<p>
<span class="strong"><strong>Masterless</strong></span>
</p>
</td></tr><tr><td>
<p>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A powerful, centralized control platform (must be secured adequately) which allows for quick, parallel access to a vast network of minions</li><li class="listitem" style="list-style-type: disc">Advanced features such as Salt Engines, Runners, Beacons, the Reactor System</li><li class="listitem" style="list-style-type: disc">API access</li></ul></div><p>
</p>
</td><td>
<p>
</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">No salt-master node to maintain</li><li class="listitem" style="list-style-type: disc">Not having a single node which provides full access to the rest of them is more secure in some sense</li><li class="listitem" style="list-style-type: disc">Simpler Salt operation</li><li class="listitem" style="list-style-type: disc">After the initial Salt execution, the minions can be considered immutable</li></ul></div><p>
</p>
</td></tr></tbody></table></div><p>For many <span class="emphasis"><em>FOR LOOP gurus</em></span> out there, parallel execution tools like Salt are very appealing. It allows you to rapidly reach out to nodes at a massive scale, whether you simply want to query their uptime, reload a service, or react to a threat alert by stopping sshd across your cluster.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note31"/>Note</h3><p>Before you go, please remember to delete any AWS resources used in the preceding examples (VPC, ELB, EC2, IAM, CodeCommit, and so on) to avoid unexpected charges.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec9"/>Summary</h1></div></div></div><p>In this chapter, we examined the second part of <span class="emphasis"><em>Infrastructure as Code</em></span>, namely <span class="strong"><strong>Configuration Management</strong></span>.</p><p>We learned about a few different components of the CM solution SaltStack: States, Pillars, Grains, and the Top File. We learned how to use them and how to write code for them.</p><p>We then combined our previous knowledge of how to deploy infrastructure using Terraform with that of how to configure it using SaltStack, resulting in our first end-to-end IaC deployment.</p><p>Next, we are going to look into <span class="emphasis"><em>Continuous Integration</em></span>: what it is and how to setup a <span class="emphasis"><em>CI</em></span> pipeline on AWS.</p></div></body></html>