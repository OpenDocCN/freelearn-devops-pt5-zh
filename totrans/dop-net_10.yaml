- en: Chapter 10. The Impact of Containers on Networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No modern IT book would be complete without a chapter on containers. In this
    chapter, we will look at the history of containers and the options currently available
    to deploy them. This chapter will look at the changes required to support running
    containers from a networking perspective. We will then focus on some of the technologies
    used to package containers, and how they can be incorporated into a Continuous
    Delivery process. Finally, we will focus on some of the orchestration tools that
    are being used to deploy containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packaging containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container orchestration tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How containers fit into continuous integration and delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There has been a lot of hype about containers in the IT industry of late; you
    could be forgiven for thinking that containers alone will solve every application
    deployment problem possible. There have been a lot of marketing campaigns from
    vendors stating that implementing containers will make a business more agile or
    that they mean a business is implementing *DevOps* simply by deploying their applications
    in containers. This is undoubtedly the case if you listen to software vendors
    promoting their container technology or container orchestration software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers are not a new concept, though. Far from it: Solaris 10 introduced
    the concept of Solaris Zones as far back as 2005, which allowed users to segregate
    the operating system into different components and run isolated processes. Modern
    technologies such as **Docker** or **Rocket** provide a container workflow that
    allows users to package and deploy containers.'
  prefs: []
  type: TYPE_NORMAL
- en: However, like all infrastructure concepts, containers are simply facilitators
    of process, and implementing containers as a standalone initiative for the wrong
    reasons will likely bring no business value to a company. It seems it has become
    almost mandatory for large software vendors to have a container-based solution
    as part of their portfolio, given their recent popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Containers, like all tools, can be very beneficial for certain use cases. It
    is important when considering containers to consider the benefits that they bring
    to microservice architectures. It is fair to say that containers have been seen
    by some **Platform as a Service** (**PaaS**) companies as being the bridge between
    development and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Container technologies have allowed developers to package their applications
    in containers in a consistent way, while at the same time describing the way in
    which they wish to run their microservice application in production using PaaS
    technology. This construct can be understood by development and operations staff,
    as they are both familiar with the same container technology and constructs they
    use to deploy applications. This means that the container that is deployed on
    a development workstation will behave in the same way as it would on a production
    system.
  prefs: []
  type: TYPE_NORMAL
- en: This has allowed developers to define their application topology and load balancing
    requirements more consistently, so that they are deployed identically to test
    and production environments using a common suite of tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Famous success stories such as Netflix have shown that containerizing their
    whole microservice architecture is possible and can be a success. With the rise
    in popularity of microservice applications, a common requirement is to package
    and deploy a microservice application across multiple hybrid clouds. This gives
    organizations real choice over which private or public cloud provider they use.
  prefs: []
  type: TYPE_NORMAL
- en: In microservice architectures, cloud-native microservice applications can be
    scaled up or down quickly to deal with busy or quiet periods for a business. When
    using a public cloud, it is desirable to only utilize what is required, which
    can often mean that microservices can be scaled up and scaled down throughout
    the day to save running costs.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic scaling based on utilization is a common use case when deploying cloud-native
    microservices so that microservices can scale up and down based on reading data
    from their monitoring systems or from the cloud provider.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices have followed the lead of service-oriented architectures and can
    be seen as the modern implementation of this concept of **service-oriented architectures**
    (**SOA**). Microservices such as SOA allow multiple different components to communicate
    via a network of services and common set of protocols. Microservices aim to decouple
    services from one another into specific functions, so they can be tested in isolation
    and joined together to create the overall system and, as illustrated, scaled up
    or down as required.
  prefs: []
  type: TYPE_NORMAL
- en: When using microservice architectures, instead of having to deploy the whole
    system each time, different component versions can be deployed independently of
    each other without causing system downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Containers in some ways can be seen as the perfect solution for microservice
    applications as they can be used to carry out specific functions in isolation.
    Each microservice application can be deployed within the constructs of an individual
    container and networked together to provide an overall service to the end user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers already natively run on any Linux operating system and are lightweight
    by nature, meaning they can be deployed, maintained, and updated easily when utilizing
    popular container technology such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker ([https://www.docker.com/](https://www.docker.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Kubernetes ([http://kubernetes.io/](http://kubernetes.io/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mesos ([http://mesos.apache.org/](http://mesos.apache.org/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM Bluemix ([http://www.ibm.com/cloud-computing/bluemix/containers/](http://www.ibm.com/cloud-computing/bluemix/containers/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rackspace Catrina ([http://thenewstack.io/rackspace-carina-bare-metal-caas-based-openstack/](http://thenewstack.io/rackspace-carina-bare-metal-caas-based-openstack/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS Rocket ([https://coreos.com/blog/rocket/](https://coreos.com/blog/rocket/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle Solaris Zones ([https://docs.oracle.com/cd/E18440_01/doc.111/e18415/chapter_zones.htm#OPCUG426](https://docs.oracle.com/cd/E18440_01/doc.111/e18415/chapter_zones.htm#OPCUG426))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Azure Nano Server ([https://technet.microsoft.com/en-us/windows-server-docs/get-started/getting-started-with-nano-server](https://technet.microsoft.com/en-us/windows-server-docs/get-started/getting-started-with-nano-server))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware Photon ([http://blogs.vmware.com/cloudnative/introducing-photon/](http://blogs.vmware.com/cloudnative/introducing-photon/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Containerization** in essence is virtualizing processes on the operating
    system and isolating them from one another into manageable components. Container
    orchestration technologies then create network interfaces to allow multiple containers
    to be connected to each other across the operating systems, or in more complex
    scenarios, create full overlay networks to connect containers running on multiple
    physical or virtual servers using programmatic APIs and key-value stores for service
    discovery.'
  prefs: []
  type: TYPE_NORMAL
- en: Solaris Zones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2005, Solaris introduced the notion of **Solaris Zones**, and from it came
    the concept of containment. After a user logged in to a fresh Solaris operating
    system, they would find themselves in a global Solaris Zone.
  prefs: []
  type: TYPE_NORMAL
- en: Solaris then gave users the option to create new zones, configure them, install
    the packages to run them, and finally, boot them so they could be used. This allowed
    each isolated zone to be used as a contained segment within the confines of a
    single Solaris operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Solaris allowed zones to run as a completely isolated set of processes, all
    from the default global zone in terms of permissions, disk, and network configuration.
    Different persistent storage or raw devices could be exported to the zones and
    mounted to make external file systems accessible to a zone. This meant that multiple
    different applications could run within their own unique zone and communicate
    with external shared storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of networking, the global Solaris Zone would have an IP address and
    be connected to the default router. All new zones would have their own unique
    IP address on the same subnet using the same default router. Each zone could even
    have their own unique DNS entry if required. The networking setup for Solaris
    Zones is shown in the following figure, with two zones connected to the router
    by accessing the network configuration on the `/zones` file system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solaris Zones](img/B05559_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: Linux namespaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **Linux namespace** creates an abstraction layer for a system process and
    changes to that system process only affect other processes in the same namespace.
    Linux namespaces can be used to isolate processes on the Linux operating system;
    by default, when a Linux operating system is booted, all resources run under the
    default namespace so have the ability to view all the processes that are running.
  prefs: []
  type: TYPE_NORMAL
- en: 'The namespace API has the following system calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clone`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setns`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unshare`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `clone` system call creates a new process and links all specified processes
    to it; the `setns` system call, on the other hand, is used to join namespaces
    together, and the `unshare` system call moves a process out of a namespace to
    a new namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following namespaces are available on Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: Mounts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interprocess communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UTS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mounts namespace is used to isolate the Linux operating system's file system
    so specific mount points are only seen by a certain group of processes belonging
    to the same namespace. This allows different processes to have access to different
    mount points, depending on what namespace they are part of, which can be used
    to secure specific files.
  prefs: []
  type: TYPE_NORMAL
- en: The **Process ID** (**PID**) namespace allows the reuse of PID processes on
    a Linux machine as each set of PIDs is unique to a namespace. This allows containers
    to be migrated between hosts while keeping the same PIDs, so the operation does
    not interrupt the container. This also allows each container to have its own unique
    `init` process and makes containers extremely portable.
  prefs: []
  type: TYPE_NORMAL
- en: The **interprocess communication** (**IPC**) namespace is used to isolate certain
    specific resources, such as system objects and message queues between processes.
  prefs: []
  type: TYPE_NORMAL
- en: The UTS namespace allows containers to have their own domain and host name;
    this is very useful when using containers, as orchestration scripts can target
    specific host names as opposed to IPs.
  prefs: []
  type: TYPE_NORMAL
- en: The network namespace creates a layer of isolation around network resources
    such as the IP space, IP tables, and routing tables. This means that each container
    can have its own unique networking rules.
  prefs: []
  type: TYPE_NORMAL
- en: The user namespace is used to manage user permissions to namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: So, from a networking perspective, namespaces allow multiple different routing
    tables to coexist on the same Linux operating system, as they have complete process
    isolation. This means each container can have its own unique networking rules
    applied if desired.
  prefs: []
  type: TYPE_NORMAL
- en: Linux control groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The use of **control groups** (**cgroups**) allows users to control Linux operating
    system resources that are part of a namespace. The following cgroups can be used
    to control Linux resources:'
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freezer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Block I/O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CPU cgroup can use two different types of scheduler: either the **Completely
    Fair Scheduler** (**CFS**), which is based on distributing CPU based on a weighting
    system. The **Real-Time Scheduler** (**RTS**) is the other alternative, and is
    a task scheduler that caps tasks based on their real-time utilization.'
  prefs: []
  type: TYPE_NORMAL
- en: The memory cgroup is used to generate reports on memory utilization used by
    the tasks in a cgroup. It sets limits on the memory use of processes associated
    with the cgroup can use.
  prefs: []
  type: TYPE_NORMAL
- en: The freezer cgroup is used to control the process status of all processes associated
    with the freezer cgroup. The freezer cgroup can be used to control batches of
    jobs and issue the `FREEZE` command, which will stop all processes in the user
    space; the `THAW` command can be used to restart them again.
  prefs: []
  type: TYPE_NORMAL
- en: The **Block I/O** (**blkio**) cgroup monitors access to I/O on block devices
    and introduces limits on I/O bandwidth or access to resources. Blkio uses an I/O
    scheduler and can assign weights to distribute I/O or provide I/O throttling by
    setting maximum limits to throttle the amount of read or writes that a process
    can do on a device.
  prefs: []
  type: TYPE_NORMAL
- en: The devices' cgroup allows or denies access to devices by defining tasks under
    `devices.allow` and `devices.deny`, and can list device access using `devices.list`.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Containers have many benefits, with a focus on portability, agility, security,
    and as touched upon earlier in this chapter, have helped many organizations such
    as Netflix deploy their microservice architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Containers also allow users to allocate different resources on an operating
    system using namespaces and limit CPU, memory, network block I/O, and network
    bandwidth using cgroups.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are very quick to provision so can be scaled up and scaled down rapidly
    to allow elastic scaling in cloud environments. They can be scaled up rapidly
    to meet demand and containers can be migrated from one server to another using
    numerous techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Cgroups can be configured quickly based on system changes, which gives users
    complete control over the low-level scheduling features of an operating system,
    which are normally delegated to the base operating system when using virtual machines
    or bare-metal servers. Containers can be tweaked to give greater fine-grained
    control over performance.
  prefs: []
  type: TYPE_NORMAL
- en: In some scenarios, not all resources on a bare-metal server will be utilized,
    which can be wasteful, so containers can be utilized to use all of the CPU and
    RAM available on a guest operating system by running multiple instances of the
    same application isolated by namespaces at a kernel level. This means that to
    each process, they appear to be functioning on their own unique operating system.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main drawbacks with containers up until now has been that they have
    been notoriously low-level and hard to manage at scale. So, tooling such as for
    large implementation, and orchestration engines such as Docker Swarm, Google Kubernetes,
    and Apache Mesos alleviate that pain by creating abstraction layers to manage
    containers at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of containers is that they are very secure as they limit the
    attack surface area with additional layers of security added to the operating
    system through the use of different namespaces. If an operating system was compromised,
    an attacker would still need to compromise the system at the namespace level as
    opposed to having access to all processes.
  prefs: []
  type: TYPE_NORMAL
- en: Containers can be very useful when running multiple flavors of the same process;
    an example is a business that wants to run multiple versions of the same application
    for different customers. They want to prevent a spike in logins and transactions
    from one customer affecting another at the application level. Containers in this
    scenario would be a feasible solution.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the growing popularity of containers, traditional Linux distributions have
    been found to be sub-optimal and clunky when running a pure container platform.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, very minimal operating systems have been created to host containers,
    such as CoreOS and Red Hat Atomic, which have been developed specifically to run
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing information across operating systems is also a challenge for containers,
    as by design they are isolated by namespaces and cgroups to a particular host
    operating system. Key-value stores such as **etcd**, **Consul**, and **Zookeeper**
    can be used to cluster and cluster and share information across hosts.
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**CoreOS** is a Linux-based operating system specifically created to provide
    a minimal operating system to run clusters of containers. It is the widest-used
    container operating system today and designed to run at massive scale without
    the need to frequently patch and update the software on the operating system manually.'
  prefs: []
  type: TYPE_NORMAL
- en: Any application that runs on CoreOS will run in container format; CoreOS can
    run on bare-metal or virtual machines, on public and private clouds such as AWS
    and OpenStack.
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS works by automatically pulling frequent security updates without affecting
    the containers running on the operating system. This means CoreOS doesn't need
    Linux admins to intervene and patch servers, as CoreOS automatically takes care
    of this by patching using its zero downtime security updates.
  prefs: []
  type: TYPE_NORMAL
- en: CoreOS focuses on moving application dependencies out of the application and
    into the container layer, so containers are dependent on other containers for
    their dependency management.
  prefs: []
  type: TYPE_NORMAL
- en: etcd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CoreOS uses etcd, which is a distributed key-value store that allows multiple
    containers across multiple machines to connect to it for data and state.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd uses the **Raft algorithm** to elect a leader and uses followers to maintain
    consistency. When multiple etcd hosts are running, the state is pulled from the
    instance with the majority and propagated to the followers, so it is used to keep
    clusters consistent and up to date.
  prefs: []
  type: TYPE_NORMAL
- en: Applications can read and write data into etcd and it is designed to deal with
    fault and failure conditions. Etcd can be used to store connection strings to
    endpoints or other environment-specific data stores.
  prefs: []
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It would be impossible to talk about containers without mentioning Docker. In
    2013, Docker was released as an open-source initiative that could be used to package
    and distribute containers. Docker was originally based on Linux LXC containers,
    but the Docker project has since drifted away from that standard as it has become
    more opinionated and mature.
  prefs: []
  type: TYPE_NORMAL
- en: Docker works on the principle of isolating a single process per container in
    the Linux kernel. Docker uses a union-capable file system, cgroups, and kernel
    namespaces to run containers and isolate processes. It has a command-line interface
    and a well thought out workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Docker registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When container images are packaged, they need to be pushed to Docker's container
    registry server, which is an image repository for containers.
  prefs: []
  type: TYPE_NORMAL
- en: The **Docker registry** is used to store containers, which can be tagged and
    versioned much like a package repository. This allows different container versions
    to be stored for roll-forward and roll-back purposes.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the Docker registry is a file-system volume and persists data on
    a local file system. Artifact repositories such as Artifactory and Nexus now support
    Docker registry as a repository type. The Docker registry can be set up with authentication
    and SSL certificates to secure container images.
  prefs: []
  type: TYPE_NORMAL
- en: Docker daemon
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During installation, Docker deploys a daemon on the target operating system
    that has been chosen to run containers. The **Docker daemon** is used to communicate
    with the Docker image registry and issue pull commands to pull down the latest
    container images or a specific tagged version. The Docker command line can then
    be used to schedule the start-up of the containers using the container image that
    has been pulled from the registry. Docker daemons, by default, run as a constant
    process on target operating systems, but can be started or stopped using a process
    manager such as `systemd`.
  prefs: []
  type: TYPE_NORMAL
- en: Packaging containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Containers can be packaged in various different ways; two of the most popular
    ways of packaging containers is using Dockerfiles, and one of the lesser known
    ways is using a tool from **HashiCorp** called Packer. Both have slightly different
    approaches to packaging container images.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker allows users to package containers using its very own configuration-management
    tool called **Dockerfile**. Dockerfile will state the intent of the container
    by outlining the packages that should be installed on it using package managers
    at build time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Dockerfile shows NGINX being installed on CentOS by issuing `yum`
    `install` commands and exposing port `80` to the guest operating system from the
    packaged container. Port `80` is exposed so NGINX can be accessed externally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dockerfile](img/B05559_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the Dockerfile has been created, Docker''s command-line interface allows
    users to issue the following command to build a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The one downside is that applications are typically installed using configuration
    management tools such as Puppet, Chef, Ansible, and Salt. The Dockerfile is very
    brittle, which means that packaging scripts need to be completely re-written.
  prefs: []
  type: TYPE_NORMAL
- en: Packer-Docker integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Packer** from HashiCorp is a command-line tool which uses multiple drivers
    to package virtual machine images and also supports creating Docker image files.
    Packer can be used to package **Amazon Machine Image** (**AMI**) images for AWS
    or **QEMU Copy On Write** (**QCOW**) images, which can be uploaded to OpenStack
    Glance.'
  prefs: []
  type: TYPE_NORMAL
- en: When utilizing Packer, it skips the need for using Dockerfiles to create Docker
    images; instead, existing configuration-management tools such as Puppet, Chef,
    Ansible, and Salt can be used to provision and package Docker container images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Packer has the following high-level architecture and uses a JSON file to describe
    the Packer workflow, with three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Builders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Builders** are used to boot an ISO, virtual machine on a Cloud platform,
    or in this case, start a Docker container from an image file on a build server.'
  prefs: []
  type: TYPE_NORMAL
- en: Once booted, the configuration management **provisioner** will run a set of
    installation steps. This will create the desired state for the image, emulating
    what the Dockerfile would carry out. Once complete, the image will be stopped
    and packaged.
  prefs: []
  type: TYPE_NORMAL
- en: A set of **post-processors** will then be executed to push the image to an artifact
    repository or Docker registry, where it is tagged and versioned.
  prefs: []
  type: TYPE_NORMAL
- en: Using Packer means existing configuration management tools can be used to package
    virtual machines and containers in the same way rather than using a completely
    different configuration-management mechanism for containers. The Docker daemon
    will need to be installed as a prerequisite on the build server that is being
    used to package the container.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, an `nginx.json` Packer file is created; the `builders`
    section has the type `docker` defined, which lets Packer know to use the Docker
    builder.
  prefs: []
  type: TYPE_NORMAL
- en: The `export_path` is where the final Docker image will be exported to and `image`
    is the name of the Docker image file that will be pulled from the Docker registry
    and started.
  prefs: []
  type: TYPE_NORMAL
- en: One provisioner of the `ansible-local` type will then execute the `install_nginx.yml`
    playbook to install NGINX on the Docker image, using an Ansible playbook as opposed
    to the Dockerfile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the post-processors will then import the packed image, complete with
    NGINX installed, into the Docker registry with the tag `1.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Packer-Docker integration](img/B05559_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To execute the Packer build, simply execute the following command passing the
    `nginx.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Docker workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Docker workflow** fits nicely into the continuous integration process
    that we covered as part of [Chapter 7](ch07.html "Chapter 7. Using Continuous
    Integration Builds for Network Configuration"), *Using Continuous Integration
    Builds for Network Configuration* and the Continuous Delivery workflow we covered
    in [Chapter 9](ch09.html "Chapter 9. Using Continuous Delivery Pipelines to Deploy
    Network Changes"), *Using Continuous Delivery Pipelines to Deploy Network Changes*.
    After a developer pushes a new code commit, compiling and potentially packaging
    new code, the continuous integration process can be extended to execute a Dockerfile
    to package a new Docker image as a post-deployment step.
  prefs: []
  type: TYPE_NORMAL
- en: A Docker daemon is configured on each downstream test environment and production
    as part of the base operating system. At deployment time, the Docker daemon is
    scheduled to pull down the newly packaged Docker image and create a new set of
    containers doing a rolling update.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process flow can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker workflow](img/B05559_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Default Docker networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In terms of networking, when Docker is installed, it creates three default
    networks; the networks created are the `bridge`, `none`, and `host` networks,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Default Docker networking](img/B05559_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Docker daemon creates containers against the bridge (`docker0`) network
    by default; this occurs when a `docker create` and `docker start` are issued on
    the target operating system, or alternatively, just a `docker run` command can
    be issued. These commands will create and start new containers on the host operating
    system from the defined Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: The `none` network is used to create a container-specific network, which allows
    containers to be launched and left to run; it doesn't have a network interface,
    though. The `host` network adds containers to the same network as the guest operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 'When containers are launched on it, Docker''s bridge network assigns each container
    a unique IP address on the bridge network''s subnet range. The containers can
    be viewed by issuing the following `docker network inspect` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Docker allows users to inspect container configuration by using the `docker
    attach` command; in this instance, the `nginx` container can be inspected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once attached, the `/etc/hosts` file can be inspected to show the network configuration.
    Docker bridge uses a NAT network and can use port forwarding using the following
    `–p` command-line argument. For example, `-p 8080:8080` forwards port `8080` from
    the host to the container. This allows all containers that are running on an operating
    system to be accessed directly by the localhost by their IPs, using port forwarding.
  prefs: []
  type: TYPE_NORMAL
- en: In its default networking mode, Docker allows containers to be interconnected
    using a `--links` command-line argument, which is used to connect containers,
    which writes entries into the `/etc/hosts` file of containers.
  prefs: []
  type: TYPE_NORMAL
- en: The default network setup is now not recommended for use, and more sophisticated
    networking is present, but the concepts it covers are still important.
  prefs: []
  type: TYPE_NORMAL
- en: Docker allows user-defined networks to be defined to host containers, using
    network drivers to create custom networks such as custom `bridge`, `overlay`,
    or layer 2 `MACVlLAN` network.
  prefs: []
  type: TYPE_NORMAL
- en: Docker user-defined bridge network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A user-defined bridge network is much like the default Docker network, but it
    means that each container can talk to each of the other containers on the same
    bridge network; there is no need for linking as with the default Docker networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'To place containers on a user-defined network, containers can be launched on
    the `devops_for_networking_bridge` user-defined bridge network using the following
    command, with the `–net` option set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each container that is launched will reside on the same operating system guest.
    Publish is used to expose specific using of the `-p 8080-8081:8080/tcp` command.
    Therefore, ranges can be published so that portions of the network can be exposed.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overlay networks, can also be used with Docker and have already been covered
    at length in this book, are a virtualized abstraction layer for the network. Docker
    can create an overlay network for containers, which is used to create a network
    of containers that belong to multiple different operating system hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of isolating each container to a unique network existing on one host,
    Docker instead allows its overlay network to join multiple different clusters
    of containers that are deployed on separate hosts together.
  prefs: []
  type: TYPE_NORMAL
- en: This means that each container that shares an overlay network will have a unique
    IP address and name. To create an overlay network, Docker uses its own orchestration
    engine, called **Docker Swarm**.
  prefs: []
  type: TYPE_NORMAL
- en: To run Docker in swarm mode, an external key-value store such as etcd, Consul,
    or Zookeeper needs to be used with Docker. This key-value store allows Docker
    to share information between different hosts, including the shared overlay network.
  prefs: []
  type: TYPE_NORMAL
- en: Docker machine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is worth mentioning that `docker-machine` is a useful command-line utility
    that allows virtual machines to be provisioned in VirtualBox, OpenStack, AWS,
    and many more platforms that have drivers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see how a machine could be booted using `docker-machine`
    in OpenStack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: One of the more useful functions of `docker-machine` is its ability to boot
    virtual machines in cloud environments while issuing Docker Swarm commands. This
    allows machines to be set up on boot to the specific profile that is required.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another helpful tool for orchestrating containers is **Docker Compose**, as
    running a command line for every container that needs to be deployed is not a
    feasible solution at scale. Therefore, Docker Compose allows users to specify
    their microservice-architecture topology in YAML format, so container dependencies
    are chained together to form a fully-fledged application.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices will be comprised of different container types, which together
    make up a full application. Docker Compose allows each of those microservices
    to be defined as YAML in the `docker-compose` file so they can be deployed together
    in a manageable way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following `docker-compose.yml` file, `web`, `nginx`, and `db` applications
    are configured and linked together, with the load balancer being exposed on port
    `8080` for public access, and load balancing `app1`, which is connected to the
    `redis` database backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker Compose](img/B05559_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Docker Compose can be executed in the same directory as the Docker Compose
    YAML file to invoke a new deployment the following command should be issued:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Swarm architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Swarm architecture works on the principle that each host runs a Swarm agent
    and one host runs a Swarm master. The master is responsible for the orchestration
    of containers on each of the hosts where agents are running and that are a member
    of the same discovery (key-value store).
  prefs: []
  type: TYPE_NORMAL
- en: An important principle for swarm is discovery, which is catered for using a
    key-value store such as etcd, Consul, or Zookeeper.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a Docker swarm, a set up Docker machine can be used to provision
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovery server (key-value store such as etcd, Consul, or Zookeeper)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm master with swarm agent installed, pointing at a key-value store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two Swarm nodes with Swarm agent installed, pointing at a key-value store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Docker Swarm architecture shows a master node scheduling containers on
    two Docker agents while they are all advertising to the key-value store, which
    is used for service discovery:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm architecture](img/B05559_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'When setting up a Swarm agent, in this case the Swarm master, they will be
    booted with the following options: `--swarm-discovery` defines the address of
    the discovery service, while `--cluster-advertise` advertises the host machine
    on the network and `--cluster-store` points at a key-value store of choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm architecture](img/B05559_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the architecture has been set up, an overlay network needs to be created
    to run containers across the two different hosts (in this instance the overlay
    network is called `devops_for_networking_overlay`) by issuing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Containers can then be created on the network from an image using the Docker
    Swarm master to schedule the commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As each host is running in Swarm mode and attached to the key-value store, upon
    creation, the network information meta-data will be shared by the key-value store.
    This means that the network is visible to all hosts that use the same key-value
    store.
  prefs: []
  type: TYPE_NORMAL
- en: Containers can then be launched from any of the Swarm masters onto the same
    overlay network, which will join the two hosts together. This will allow each
    host to communicate with other containers, via the overlay network, across hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple overlay networks can be created; though containers can only communicate
    across the same overlay network they cannot communicate between different overlay
    networks. To mitigate this, containers can be attached to multiple different networks.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm allows many specific containers to be assigned and exposed using
    port forwarding to load balance containers. Rolling updates can also be carried
    out to allow upgrades of the containers' application version.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its completely decentralized design, Docker Swarm is very flexible in
    the number of networking use cases it can solve.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is a popular container orchestration tool from Google which was created
    in 2014 and is an open-source tool. Rather than Google coming up with their own
    container packaging tool and packaging repository, Kubernetes instead can plug
    seamlessly to use Docker registry as its container image repository.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can orchestrate containers that are created using Docker via a **Dockerfile**,
    or alternatively, using Packer aided by configuration management tools such as
    Puppet, Chef, Ansible, and Salt.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can be seen as an alternative to Docker Swarm, but takes a slightly
    different approach in terms of its architectural design and has a lot of rich
    scheduling features to help with container management.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Kubernetes cluster needs to be set up before a user can use Kubernetes to
    schedule containers. There is a wide variety of configuration management tools
    that can be used to create a production-grade Kubernetes cluster with notable
    solutions available from Ansible, Chef, and Puppet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes clustering consists of the following high-level components, which
    in turn have their own subset of services. At a high level, a Kubernetes cluster
    consists of the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubectl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker node![Kubernetes architecture](img/B05559_10_09.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes master node
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The master node is responsible for managing the whole Kubernetes cluster and
    is used to take care of orchestrating worker nodes, which is where containers
    are scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The master node, when deployed, consists of the following high-level components:'
  prefs: []
  type: TYPE_NORMAL
- en: API server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etcd key-value store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controller manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API server has a RESTful API, which allows administrators to issue commands
    to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Etcd, as covered earlier in this chapter, is a key-value store that allows Kubernetes
    to store state and push changes to the rest of the cluster after changes have
    been made. Etcd is used by Kubernetes to hold scheduling information about pods,
    services, state, or even namespace information.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes scheduler, as the name suggests, is used to schedule containers
    on Services or Pods. The Scheduler will check the availability of the Kubernetes
    cluster and make scheduling decisions based on availability of resources so it
    can schedule containers appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: The controller-manager is a daemon that allows a Kubernetes master to run different
    controller types. Controllers are used by Kubernetes to analyze the state of a
    cluster and make sure it is in the desired state, so if a pod fails it will be
    recreated or re-started. It adheres to the thresholds that are specified and is
    controlled by the Kubernetes' administrator.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes worker node
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Worker nodes are where pods run; each pod has an IP address and runs containers.
    It is the pod that determines all the networking for the containers and governs
    how they communicate across different pods.
  prefs: []
  type: TYPE_NORMAL
- en: The worker node will contain all the necessary services to manage the networking
    between the containers, communicate with the master node, and are also used to
    assign resources to the scheduled containers.
  prefs: []
  type: TYPE_NORMAL
- en: Docker also runs on each of the worker nodes and is used to pull down containers
    from the Docker registry and schedule containers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubelet** is the worker service and is installed on worker nodes. It communicates
    with the API server on the Kubernetes master and retrieves information on the
    desired state of pods. Kubelet also reads information updates from etcd and writes
    updates about cluster events.'
  prefs: []
  type: TYPE_NORMAL
- en: The `kube-proxy` takes care of load balancing and networking functions such
    as routing packets.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes kubectl
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Kubectl** is the Kubernetes command line, which issues commands to the master
    node to administer Kubernetes clusters. It can also be used to call YAML or JSON,
    as it is talking to the RESTful API server on the master node.'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes service is created as an abstraction layer above pods, which can
    be targeted using a label selector.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, kubectl can be used to create a `loadbalancing_service`
    service deployment with a selector, `app: nginx`, which is defined by the `loadbalancing_service.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes kubectl](img/B05559_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Kubectl executes the YAML file by specifying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Kubectl can then create four replica pods using the `ReplicationController`,
    these four pods will be managed by the service, as the labels `app: nginx` match
    the service''s selector and launch an NGINX container in each pod using the `nginx_pod.yml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes kubectl](img/B05559_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Kubectl creates the service using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes SDN integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes supports multiple networking techniques that could fill a whole book's
    worth of material on its own. With the Kubernetes, the pod is the major insertion
    point for networking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes supports the following networking options:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Compute Engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open vSwitch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 2 Linux Bridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project Calico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Romana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes looks to provide a pluggable framework to control a pod's networking
    configuration and aims to give users a choice; if a flat layer 2 is required,
    Kubernetes caters for it, if a more complex layer-3 overlay network is required,
    then it can cater for this, too.
  prefs: []
  type: TYPE_NORMAL
- en: With Open vSwitch being widely used with enterprise SDN controllers such as
    Nuage Networks VSP platform, which was covered in [Chapter 2](ch02.html "Chapter 2. The
    Emergence of Software-defined Networking"), *The Emergence of Software-defined
    Networking* and [Chapter 6](ch06.html "Chapter 6. Orchestrating SDN Controllers
    Using Ansible"), *Orchestrating SDN Controllers Using Ansible*. This focused upon
    how flow information could be pushed down to Open vSwitch on each hypervisor to
    create a stateful firewall and govern the ACL policies.
  prefs: []
  type: TYPE_NORMAL
- en: A similar implementation is carried out when integrating Kubernetes, with Open
    vSwitch, being deployed onto each worker node and pod traffic being deferred to
    Open vSwitch.
  prefs: []
  type: TYPE_NORMAL
- en: In Nuage's case, a version of their customized version of Open vSwitch, known
    as the VRS, is deployed on each Kubernetes worker to govern policy controlled
    by the VSD Nuage VSPs policy engine..
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for the Nuage SDN integration with Kubernetes is shown in the
    following figure, which shows that enterprise SDN controllers can integrate with
    orchestration engines such as Kubernetes and Docker to provide enterprise-grade
    networking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes SDN integration](img/B05559_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Impact of containers on networking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers have undoubtedly meant that a lot of networking has shifted into
    the application tier, so really, containers can be seen as a PaaS offering in
    its truest form.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure is, of course, still required to run containers, be it on bare-metal
    servers or virtual machines. The merits of virtual machines being used to run
    containers long term are debatable, as in a way it means a double set of virtualization,
    and anyone using nested virtualization will know it isn't always optimal for performance.
    So with more organizations using containers to deploy their microservice architectures,
    it will undoubtedly mean that users having a choice to run containers on either
    virtual or physical machines will be in demand.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud has notoriously meant virtual machines, so running containers on virtual
    machines is probably born out of necessity rather than choice. Being able to orchestrate
    containers on bare-metal servers with an overlay network on top of them is definitely
    more appealing as it pushes the container closer to the physical machine resources
    without the visualization overhead.
  prefs: []
  type: TYPE_NORMAL
- en: This allows containers to maximize the physical machine resources, and users
    then only care about anti-infinity in terms of whether the service can run across
    multiple clouds and data centers, giving true disaster recovery.
  prefs: []
  type: TYPE_NORMAL
- en: With hybrid cloud solutions, the industry is moving beyond thinking about rack
    redundancy. Instead it is moving toward a model which will focus on splitting
    applications across multiple cloud providers. So having the ability to orchestrate
    the networking and applications in an identical way using orchestration engines
    such as Docker Swarm or Kubernetes can be used to make that goal a reality.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for the network operator? It means that the role is evolving,
    it means that the network engineer's role becomes advisory, helping the developers
    architect the network in the best possible way to run their applications. Rather
    than building a network as a side project in a private cloud, network operators
    can instead focus on providing an overlay network as a service to developers while
    making the underlay network fabric fast and performant so that it can scale out
    to meet the developer's needs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Containers have been said to be a major disruptor of the virtualization market.
    Gartner have predicted the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"By 2018, more than 50-60% of new workloads will be deployed into containers
    in at least one stage of the application life cycle".*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is based on Gartner's analysis of the IT market, so this is a bold statement,
    but if it comes to fruition, it will prove to be a huge cultural shift in the
    way applications are deployed, in the same way virtualization was before it.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we showed that containers can help organizations deploy their
    microservice architectures and analyzed the internal mechanics and benefits that
    containers bring. The key benefits are portability, speed of deployment, elastic
    scalability, isolation and maximization of different resources, performance control,
    limited attack vector, and support for multiple networking types.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the benefits containers bring, this chapter looked at the Docker
    tool and illustrated how the Docker workflow can be fitted into a Continuous Delivery
    model, which is at the heart of most DevOps initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: The focus of the chapter then shifted to Docker networking and the layer-2 networking
    options available to network containers. We illustrated how to use overlay networks
    to join multiple hosts together to form a cluster and we showed how container
    technology can integrate with SDN controllers such as Nuage VSP Platform using
    Open vSwitch.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also covered container orchestration solutions such as Docker Swarm
    and Kubernetes, their unique architectures, and ways in which they can be used
    to network containers over multiple hosts and act as a Platform as a Service layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The importance of containerization and its impact on Platform as a Service
    (PaaS) solutions cannot be underestimated, with Forrester stating the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Containers as a Service (CaaS) is becoming the new Platform as a Service
    (PaaS). With the interest in containers and micro-services skyrocketing among
    developers, cloud providers are capitalizing on the opportunity through hosted
    container management services."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In summary, it is fair to conclude that containerization can have many benefits
    and help aid developers in the implementation of Continuous Delivery workflows
    and PaaS solutions. Containerization also gives the added flexibility of deploying
    workloads across multiple cloud providers, be they private or public, using a
    common orchestration layer such as Kubernetes, Apache Mesos, or Docker Swarm.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, the focus will shift from containers toward securing
    the network when using software-defined overlay networks and a Continuous Delivery
    model. It will explore techniques that can be used to help secure a modern private
    cloud in an API-driven environment, so that software-defined networking solutions
    can be implemented without compromising security requirements.
  prefs: []
  type: TYPE_NORMAL
