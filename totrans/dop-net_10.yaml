- en: Chapter 10. The Impact of Containers on Networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 容器对网络的影响
- en: No modern IT book would be complete without a chapter on containers. In this
    chapter, we will look at the history of containers and the options currently available
    to deploy them. This chapter will look at the changes required to support running
    containers from a networking perspective. We will then focus on some of the technologies
    used to package containers, and how they can be incorporated into a Continuous
    Delivery process. Finally, we will focus on some of the orchestration tools that
    are being used to deploy containers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现代IT书籍中没有一章关于容器的内容是 incomplete 的。在这一章中，我们将回顾容器的历史以及当前可以部署容器的选项。本章将探讨从网络角度支持容器运行所需的变化。接着，我们将重点讨论用于打包容器的一些技术，以及它们如何融入持续交付流程。最后，我们将重点讨论一些用于部署容器的编排工具。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overview of containers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器概述
- en: Packaging containers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器打包
- en: Container orchestration tools
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器编排工具
- en: How containers fit into continuous integration and delivery
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器如何适应持续集成和持续交付
- en: Overview of containers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器概述
- en: There has been a lot of hype about containers in the IT industry of late; you
    could be forgiven for thinking that containers alone will solve every application
    deployment problem possible. There have been a lot of marketing campaigns from
    vendors stating that implementing containers will make a business more agile or
    that they mean a business is implementing *DevOps* simply by deploying their applications
    in containers. This is undoubtedly the case if you listen to software vendors
    promoting their container technology or container orchestration software.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，IT行业对容器的宣传非常多；你可能会认为，仅凭容器就能解决所有应用程序部署的问题。许多供应商的营销活动声称，实施容器会让企业更加敏捷，或者仅仅通过将应用程序部署到容器中，企业就已经在实施*DevOps*。如果你听到软件供应商在推广其容器技术或容器编排软件时这样说，那这无疑是他们的观点。
- en: 'Containers are not a new concept, though. Far from it: Solaris 10 introduced
    the concept of Solaris Zones as far back as 2005, which allowed users to segregate
    the operating system into different components and run isolated processes. Modern
    technologies such as **Docker** or **Rocket** provide a container workflow that
    allows users to package and deploy containers.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，容器并不是一个新概念。远不是：Solaris 10早在2005年就引入了Solaris Zones的概念，这使得用户能够将操作系统分割为不同的组件并运行隔离的进程。现代技术如**Docker**或**Rocket**提供了一个容器工作流，允许用户打包和部署容器。
- en: However, like all infrastructure concepts, containers are simply facilitators
    of process, and implementing containers as a standalone initiative for the wrong
    reasons will likely bring no business value to a company. It seems it has become
    almost mandatory for large software vendors to have a container-based solution
    as part of their portfolio, given their recent popularity.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像所有基础设施概念一样，容器只是流程的促进者，错误地将容器作为独立项目实施通常不会为公司带来业务价值。鉴于容器的近期流行，似乎大型软件供应商将容器解决方案作为其产品组合的一部分几乎已成必然。
- en: Containers, like all tools, can be very beneficial for certain use cases. It
    is important when considering containers to consider the benefits that they bring
    to microservice architectures. It is fair to say that containers have been seen
    by some **Platform as a Service** (**PaaS**) companies as being the bridge between
    development and operations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 容器，像所有工具一样，对于某些使用场景是非常有益的。在考虑容器时，重要的是要考虑它们对微服务架构带来的好处。可以公平地说，容器已被一些**平台即服务**（**PaaS**）公司视为开发与运维之间的桥梁。
- en: Container technologies have allowed developers to package their applications
    in containers in a consistent way, while at the same time describing the way in
    which they wish to run their microservice application in production using PaaS
    technology. This construct can be understood by development and operations staff,
    as they are both familiar with the same container technology and constructs they
    use to deploy applications. This means that the container that is deployed on
    a development workstation will behave in the same way as it would on a production
    system.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 容器技术使得开发人员能够以一致的方式将应用程序打包在容器中，同时描述他们希望如何使用PaaS技术在生产环境中运行微服务应用程序。开发和运维人员都能理解这一构建，因为他们都熟悉相同的容器技术和他们用于部署应用程序的构建方式。这意味着在开发工作站上部署的容器与在生产系统上运行时的行为是相同的。
- en: This has allowed developers to define their application topology and load balancing
    requirements more consistently, so that they are deployed identically to test
    and production environments using a common suite of tooling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得开发人员能够更加一致地定义应用程序拓扑和负载均衡需求，以便在测试和生产环境中通过共同的工具集进行相同的部署。
- en: Famous success stories such as Netflix have shown that containerizing their
    whole microservice architecture is possible and can be a success. With the rise
    in popularity of microservice applications, a common requirement is to package
    and deploy a microservice application across multiple hybrid clouds. This gives
    organizations real choice over which private or public cloud provider they use.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 像Netflix这样的著名成功案例表明，将整个微服务架构容器化是可行的，并且能够取得成功。随着微服务应用的流行，常见的需求是将微服务应用程序打包并部署到多个混合云中。这为组织提供了选择使用哪家私有或公有云服务提供商的真实选择。
- en: In microservice architectures, cloud-native microservice applications can be
    scaled up or down quickly to deal with busy or quiet periods for a business. When
    using a public cloud, it is desirable to only utilize what is required, which
    can often mean that microservices can be scaled up and scaled down throughout
    the day to save running costs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务架构中，云原生微服务应用可以根据业务的繁忙或安静时段快速扩展或缩减。当使用公有云时，理想的做法是只利用所需的资源，这通常意味着微服务可以在一天中随时扩展或缩减，以节省运行成本。
- en: Elastic scaling based on utilization is a common use case when deploying cloud-native
    microservices so that microservices can scale up and down based on reading data
    from their monitoring systems or from the cloud provider.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于利用率的弹性扩展是部署云原生微服务时的常见用例，这样微服务可以根据其监控系统或云服务提供商提供的数据进行动态扩展或缩减。
- en: Microservices have followed the lead of service-oriented architectures and can
    be seen as the modern implementation of this concept of **service-oriented architectures**
    (**SOA**). Microservices such as SOA allow multiple different components to communicate
    via a network of services and common set of protocols. Microservices aim to decouple
    services from one another into specific functions, so they can be tested in isolation
    and joined together to create the overall system and, as illustrated, scaled up
    or down as required.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务沿袭了面向服务架构（**SOA**）的理念，并且可以被视为这一理念的现代实现。像SOA这样的微服务允许多个不同的组件通过服务网络和通用协议集进行通信。微服务旨在将服务解耦成具体的功能，使其可以单独进行测试，并将它们组合起来以创建整体系统，并且如所示，根据需求进行扩展或缩减。
- en: When using microservice architectures, instead of having to deploy the whole
    system each time, different component versions can be deployed independently of
    each other without causing system downtime.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用微服务架构时，不需要每次都部署整个系统，不同的组件版本可以独立部署，而不会导致系统停机。
- en: Containers in some ways can be seen as the perfect solution for microservice
    applications as they can be used to carry out specific functions in isolation.
    Each microservice application can be deployed within the constructs of an individual
    container and networked together to provide an overall service to the end user.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种程度上看，容器可以被视为微服务应用程序的完美解决方案，因为它们可以用来独立执行特定功能。每个微服务应用可以在单独的容器构建中部署，并通过网络连接在一起，以便为最终用户提供整体服务。
- en: 'Containers already natively run on any Linux operating system and are lightweight
    by nature, meaning they can be deployed, maintained, and updated easily when utilizing
    popular container technology such as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Docker ([https://www.docker.com/](https://www.docker.com/))
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Kubernetes ([http://kubernetes.io/](http://kubernetes.io/))
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Mesos ([http://mesos.apache.org/](http://mesos.apache.org/))
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM Bluemix ([http://www.ibm.com/cloud-computing/bluemix/containers/](http://www.ibm.com/cloud-computing/bluemix/containers/))
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rackspace Catrina ([http://thenewstack.io/rackspace-carina-bare-metal-caas-based-openstack/](http://thenewstack.io/rackspace-carina-bare-metal-caas-based-openstack/))
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoreOS Rocket ([https://coreos.com/blog/rocket/](https://coreos.com/blog/rocket/))
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oracle Solaris Zones ([https://docs.oracle.com/cd/E18440_01/doc.111/e18415/chapter_zones.htm#OPCUG426](https://docs.oracle.com/cd/E18440_01/doc.111/e18415/chapter_zones.htm#OPCUG426))
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Azure Nano Server ([https://technet.microsoft.com/en-us/windows-server-docs/get-started/getting-started-with-nano-server](https://technet.microsoft.com/en-us/windows-server-docs/get-started/getting-started-with-nano-server))
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VMware Photon ([http://blogs.vmware.com/cloudnative/introducing-photon/](http://blogs.vmware.com/cloudnative/introducing-photon/))
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Containerization** in essence is virtualizing processes on the operating
    system and isolating them from one another into manageable components. Container
    orchestration technologies then create network interfaces to allow multiple containers
    to be connected to each other across the operating systems, or in more complex
    scenarios, create full overlay networks to connect containers running on multiple
    physical or virtual servers using programmatic APIs and key-value stores for service
    discovery.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Solaris Zones
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2005, Solaris introduced the notion of **Solaris Zones**, and from it came
    the concept of containment. After a user logged in to a fresh Solaris operating
    system, they would find themselves in a global Solaris Zone.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Solaris then gave users the option to create new zones, configure them, install
    the packages to run them, and finally, boot them so they could be used. This allowed
    each isolated zone to be used as a contained segment within the confines of a
    single Solaris operating system.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Solaris allowed zones to run as a completely isolated set of processes, all
    from the default global zone in terms of permissions, disk, and network configuration.
    Different persistent storage or raw devices could be exported to the zones and
    mounted to make external file systems accessible to a zone. This meant that multiple
    different applications could run within their own unique zone and communicate
    with external shared storage.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of networking, the global Solaris Zone would have an IP address and
    be connected to the default router. All new zones would have their own unique
    IP address on the same subnet using the same default router. Each zone could even
    have their own unique DNS entry if required. The networking setup for Solaris
    Zones is shown in the following figure, with two zones connected to the router
    by accessing the network configuration on the `/zones` file system:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络方面，全球Solaris Zone将具有一个IP地址，并连接到默认路由器。所有新区域将具有相同子网内的唯一IP地址，并使用相同的默认路由器。如果需要，每个区域甚至可以有自己独特的DNS条目。Solaris
    Zones的网络配置如图所示，通过访问`/zones`文件系统上的网络配置，两个区域连接到路由器：
- en: '![Solaris Zones](img/B05559_10_01.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Solaris Zones](img/B05559_10_01.jpg)'
- en: .
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: Linux namespaces
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linux命名空间
- en: A **Linux namespace** creates an abstraction layer for a system process and
    changes to that system process only affect other processes in the same namespace.
    Linux namespaces can be used to isolate processes on the Linux operating system;
    by default, when a Linux operating system is booted, all resources run under the
    default namespace so have the ability to view all the processes that are running.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Linux命名空间**为系统进程创建了一个抽象层，对该进程的更改仅会影响同一命名空间中的其他进程。Linux命名空间可用于隔离Linux操作系统上的进程；默认情况下，当Linux操作系统启动时，所有资源都运行在默认命名空间下，因此能够查看所有正在运行的进程。'
- en: 'The namespace API has the following system calls:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间API包含以下系统调用：
- en: '`clone`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clone`'
- en: '`setns`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setns`'
- en: '`unshare`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unshare`'
- en: The `clone` system call creates a new process and links all specified processes
    to it; the `setns` system call, on the other hand, is used to join namespaces
    together, and the `unshare` system call moves a process out of a namespace to
    a new namespace.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`clone`系统调用创建一个新进程，并将所有指定的进程与之关联；而`setns`系统调用用于将命名空间连接在一起，`unshare`系统调用则将进程从一个命名空间移出并移至新的命名空间。'
- en: 'The following namespaces are available on Linux:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Linux上可用的命名空间：
- en: Mounts
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挂载
- en: Process ID
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程ID
- en: Interprocess communication
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程间通信
- en: UTS
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UTS
- en: Network
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络
- en: User
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户
- en: The mounts namespace is used to isolate the Linux operating system's file system
    so specific mount points are only seen by a certain group of processes belonging
    to the same namespace. This allows different processes to have access to different
    mount points, depending on what namespace they are part of, which can be used
    to secure specific files.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: mounts命名空间用于隔离Linux操作系统的文件系统，使得特定的挂载点仅对属于同一命名空间的进程组可见。这使得不同的进程可以根据其所属的命名空间访问不同的挂载点，从而可以用于保护特定的文件。
- en: The **Process ID** (**PID**) namespace allows the reuse of PID processes on
    a Linux machine as each set of PIDs is unique to a namespace. This allows containers
    to be migrated between hosts while keeping the same PIDs, so the operation does
    not interrupt the container. This also allows each container to have its own unique
    `init` process and makes containers extremely portable.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**进程ID**（**PID**）命名空间允许在Linux机器上重用PID进程，因为每组PID都是唯一的，并且属于特定命名空间。这允许容器在主机之间迁移，同时保持相同的PID，从而确保操作不会中断容器。这也允许每个容器拥有独特的`init`进程，并使容器具有极高的可移植性。'
- en: The **interprocess communication** (**IPC**) namespace is used to isolate certain
    specific resources, such as system objects and message queues between processes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**进程间通信**（**IPC**）命名空间用于隔离特定的资源，如进程之间的系统对象和消息队列。'
- en: The UTS namespace allows containers to have their own domain and host name;
    this is very useful when using containers, as orchestration scripts can target
    specific host names as opposed to IPs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: UTS命名空间允许容器拥有自己的域名和主机名；这在使用容器时非常有用，因为编排脚本可以针对特定的主机名，而不是IP进行操作。
- en: The network namespace creates a layer of isolation around network resources
    such as the IP space, IP tables, and routing tables. This means that each container
    can have its own unique networking rules.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 网络命名空间在网络资源（如IP空间、IP表和路由表）周围创建了一个隔离层。这意味着每个容器可以有其独特的网络规则。
- en: The user namespace is used to manage user permissions to namespaces.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 用户命名空间用于管理用户对命名空间的权限。
- en: So, from a networking perspective, namespaces allow multiple different routing
    tables to coexist on the same Linux operating system, as they have complete process
    isolation. This means each container can have its own unique networking rules
    applied if desired.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从网络角度来看，命名空间允许多个不同的路由表在同一Linux操作系统上共存，因为它们具有完全的进程隔离。这意味着每个容器可以根据需要应用其独特的网络规则。
- en: Linux control groups
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linux控制组
- en: 'The use of **control groups** (**cgroups**) allows users to control Linux operating
    system resources that are part of a namespace. The following cgroups can be used
    to control Linux resources:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**控制组**（**cgroups**）的使用使用户能够控制属于命名空间的一部分的 Linux 操作系统资源。以下控制组可用于控制 Linux 资源：'
- en: CPU
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU
- en: Memory
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存
- en: Freezer
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冰冻
- en: Block I/O
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块 I/O
- en: Devices
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备
- en: 'The CPU cgroup can use two different types of scheduler: either the **Completely
    Fair Scheduler** (**CFS**), which is based on distributing CPU based on a weighting
    system. The **Real-Time Scheduler** (**RTS**) is the other alternative, and is
    a task scheduler that caps tasks based on their real-time utilization.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 控制组可以使用两种不同类型的调度器：一种是**完全公平调度器**（**CFS**），基于加权系统分配 CPU。另一种是**实时调度器**（**RTS**），它是一种任务调度器，根据任务的实时使用情况来限制任务的执行。
- en: The memory cgroup is used to generate reports on memory utilization used by
    the tasks in a cgroup. It sets limits on the memory use of processes associated
    with the cgroup can use.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 内存控制组用于生成与控制组中的任务相关的内存利用率报告。它对与控制组相关的进程的内存使用设置限制。
- en: The freezer cgroup is used to control the process status of all processes associated
    with the freezer cgroup. The freezer cgroup can be used to control batches of
    jobs and issue the `FREEZE` command, which will stop all processes in the user
    space; the `THAW` command can be used to restart them again.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 冰冻控制组（freezer cgroup）用于控制与冰冻控制组相关的所有进程的状态。冰冻控制组可用于控制一批任务并发出`FREEZE`命令，这将停止用户空间中的所有进程；`THAW`命令则可以用来重新启动这些进程。
- en: The **Block I/O** (**blkio**) cgroup monitors access to I/O on block devices
    and introduces limits on I/O bandwidth or access to resources. Blkio uses an I/O
    scheduler and can assign weights to distribute I/O or provide I/O throttling by
    setting maximum limits to throttle the amount of read or writes that a process
    can do on a device.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**块 I/O**（**blkio**）控制组监控对块设备的 I/O 访问，并对 I/O 带宽或对资源的访问引入限制。Blkio 使用 I/O 调度程序，并可以分配权重来分配
    I/O，或通过设置最大限制来对进程在设备上读取或写入的数量进行限制，从而实现 I/O 限速。'
- en: The devices' cgroup allows or denies access to devices by defining tasks under
    `devices.allow` and `devices.deny`, and can list device access using `devices.list`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 设备控制组通过在`devices.allow`和`devices.deny`下定义任务来允许或拒绝访问设备，并可以通过`devices.list`列出设备访问。
- en: Benefits of containers
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器的好处
- en: Containers have many benefits, with a focus on portability, agility, security,
    and as touched upon earlier in this chapter, have helped many organizations such
    as Netflix deploy their microservice architectures.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 容器具有许多优势，特别是在可移植性、敏捷性、安全性方面，正如本章前面所提到的，它们帮助了许多组织（如 Netflix）部署微服务架构。
- en: Containers also allow users to allocate different resources on an operating
    system using namespaces and limit CPU, memory, network block I/O, and network
    bandwidth using cgroups.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 容器还允许用户通过使用命名空间在操作系统上分配不同的资源，并通过控制组限制 CPU、内存、网络块 I/O 和网络带宽。
- en: Containers are very quick to provision so can be scaled up and scaled down rapidly
    to allow elastic scaling in cloud environments. They can be scaled up rapidly
    to meet demand and containers can be migrated from one server to another using
    numerous techniques.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 容器的配置非常快速，因此可以迅速进行横向扩展和缩减，以支持云环境中的弹性扩展。它们可以迅速扩大以满足需求，并且容器可以通过多种技术从一台服务器迁移到另一台服务器。
- en: Cgroups can be configured quickly based on system changes, which gives users
    complete control over the low-level scheduling features of an operating system,
    which are normally delegated to the base operating system when using virtual machines
    or bare-metal servers. Containers can be tweaked to give greater fine-grained
    control over performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 控制组可以根据系统的变化迅速进行配置，赋予用户对操作系统低级调度特性的完全控制，而这些通常是在使用虚拟机或裸金属服务器时交由基础操作系统来处理的。容器可以进行微调，以提供对性能的更精细控制。
- en: In some scenarios, not all resources on a bare-metal server will be utilized,
    which can be wasteful, so containers can be utilized to use all of the CPU and
    RAM available on a guest operating system by running multiple instances of the
    same application isolated by namespaces at a kernel level. This means that to
    each process, they appear to be functioning on their own unique operating system.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景下，裸金属服务器上的所有资源并不会被充分利用，这可能会导致浪费，因此可以通过容器来利用客户操作系统上所有的 CPU 和内存，方法是在内核级别通过命名空间隔离运行多个相同应用程序的实例。这意味着对于每个进程，它们看起来像是在自己独特的操作系统上运行。
- en: One of the main drawbacks with containers up until now has been that they have
    been notoriously low-level and hard to manage at scale. So, tooling such as for
    large implementation, and orchestration engines such as Docker Swarm, Google Kubernetes,
    and Apache Mesos alleviate that pain by creating abstraction layers to manage
    containers at scale.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，容器的主要缺点之一是它们通常是低级别的，并且在大规模管理时非常困难。因此，像大规模实施和协调引擎（例如 Docker Swarm、Google
    Kubernetes 和 Apache Mesos）等工具通过创建抽象层来管理容器，从而减轻了这种痛苦。
- en: Another benefit of containers is that they are very secure as they limit the
    attack surface area with additional layers of security added to the operating
    system through the use of different namespaces. If an operating system was compromised,
    an attacker would still need to compromise the system at the namespace level as
    opposed to having access to all processes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 容器的另一个好处是它们非常安全，因为它们通过使用不同的命名空间，限制了攻击面，并为操作系统增加了额外的安全层。如果操作系统被攻击，攻击者仍然需要在命名空间级别攻破系统，而不是直接访问所有进程。
- en: Containers can be very useful when running multiple flavors of the same process;
    an example is a business that wants to run multiple versions of the same application
    for different customers. They want to prevent a spike in logins and transactions
    from one customer affecting another at the application level. Containers in this
    scenario would be a feasible solution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行多个相同进程的不同版本时，容器非常有用；例如，一个企业希望为不同的客户运行同一个应用程序的多个版本。他们希望防止一个客户的登录和交易激增影响到其他客户的应用层。容器在这种情况下是一个可行的解决方案。
- en: Deploying containers
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署容器
- en: With the growing popularity of containers, traditional Linux distributions have
    been found to be sub-optimal and clunky when running a pure container platform.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着容器的日益流行，传统的 Linux 发行版在运行纯容器平台时被发现表现不佳且笨重。
- en: As a result, very minimal operating systems have been created to host containers,
    such as CoreOS and Red Hat Atomic, which have been developed specifically to run
    containers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已经创建了非常精简的操作系统来托管容器，例如 CoreOS 和 Red Hat Atomic，这些操作系统专门设计用于运行容器。
- en: Sharing information across operating systems is also a challenge for containers,
    as by design they are isolated by namespaces and cgroups to a particular host
    operating system. Key-value stores such as **etcd**, **Consul**, and **Zookeeper**
    can be used to cluster and cluster and share information across hosts.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作系统之间共享信息对于容器来说也是一个挑战，因为按照设计，它们通过命名空间（namespaces）和控制组（cgroups）被隔离到特定的主机操作系统上。**etcd**、**Consul**
    和 **Zookeeper** 等键值存储可以用于跨主机群集和共享信息。
- en: CoreOS
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoreOS
- en: '**CoreOS** is a Linux-based operating system specifically created to provide
    a minimal operating system to run clusters of containers. It is the widest-used
    container operating system today and designed to run at massive scale without
    the need to frequently patch and update the software on the operating system manually.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoreOS** 是一个基于 Linux 的操作系统，专门设计用于提供一个最小化的操作系统来运行容器集群。它是目前最广泛使用的容器操作系统，旨在以大规模运行而无需频繁手动修补和更新操作系统上的软件。'
- en: Any application that runs on CoreOS will run in container format; CoreOS can
    run on bare-metal or virtual machines, on public and private clouds such as AWS
    and OpenStack.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 任何在 CoreOS 上运行的应用程序都将以容器格式运行；CoreOS 可以在裸机或虚拟机上运行，支持 AWS 和 OpenStack 等公共和私有云。
- en: CoreOS works by automatically pulling frequent security updates without affecting
    the containers running on the operating system. This means CoreOS doesn't need
    Linux admins to intervene and patch servers, as CoreOS automatically takes care
    of this by patching using its zero downtime security updates.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 通过自动拉取频繁的安全更新而不影响操作系统上运行的容器来工作。这意味着 CoreOS 不需要 Linux 管理员干预和修补服务器，因为 CoreOS
    会通过零停机安全更新自动处理这些修补。
- en: CoreOS focuses on moving application dependencies out of the application and
    into the container layer, so containers are dependent on other containers for
    their dependency management.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 通过将应用程序依赖性从应用程序中剥离并转移到容器层来工作，因此容器依赖于其他容器来进行依赖管理。
- en: etcd
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: etcd
- en: CoreOS uses etcd, which is a distributed key-value store that allows multiple
    containers across multiple machines to connect to it for data and state.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 使用 etcd，这是一种分布式键值存储，允许跨多台机器的多个容器连接到它，以获取数据和状态信息。
- en: Etcd uses the **Raft algorithm** to elect a leader and uses followers to maintain
    consistency. When multiple etcd hosts are running, the state is pulled from the
    instance with the majority and propagated to the followers, so it is used to keep
    clusters consistent and up to date.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd 使用**Raft 算法**来选举领导者，并通过跟随者来保持一致性。当多个 etcd 主机运行时，状态从大多数实例中拉取并传播到跟随者，因此它用于保持集群的一致性和实时更新。
- en: Applications can read and write data into etcd and it is designed to deal with
    fault and failure conditions. Etcd can be used to store connection strings to
    endpoints or other environment-specific data stores.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以读取和写入数据到 etcd，并且它被设计用来处理故障和故障条件。Etcd 可以用来存储到端点的连接字符串或其他特定环境的数据存储。
- en: Docker
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker
- en: It would be impossible to talk about containers without mentioning Docker. In
    2013, Docker was released as an open-source initiative that could be used to package
    and distribute containers. Docker was originally based on Linux LXC containers,
    but the Docker project has since drifted away from that standard as it has become
    more opinionated and mature.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不提到 Docker，谈容器将是不可能的。2013 年，Docker 作为一个开源项目发布，可以用来打包和分发容器。Docker 最初是基于 Linux
    LXC 容器的，但随着项目的发展，它已经逐渐脱离了这一标准，并变得更加有主张和成熟。
- en: Docker works on the principle of isolating a single process per container in
    the Linux kernel. Docker uses a union-capable file system, cgroups, and kernel
    namespaces to run containers and isolate processes. It has a command-line interface
    and a well thought out workflow.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 基于在 Linux 内核中将每个容器进程隔离的原则工作。Docker 使用支持联合的文件系统、cgroups 和内核命名空间来运行容器并隔离进程。它具有命令行界面和经过深思熟虑的工作流。
- en: Docker registry
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker 镜像仓库
- en: When container images are packaged, they need to be pushed to Docker's container
    registry server, which is an image repository for containers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器镜像被打包时，它们需要被推送到 Docker 的容器镜像仓库服务器，这是一个容器的镜像存储库。
- en: The **Docker registry** is used to store containers, which can be tagged and
    versioned much like a package repository. This allows different container versions
    to be stored for roll-forward and roll-back purposes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker 镜像仓库**用于存储容器，容器可以像软件包仓库一样进行标记和版本管理。这允许存储不同版本的容器，以便进行向前滚动和回滚。'
- en: By default, the Docker registry is a file-system volume and persists data on
    a local file system. Artifact repositories such as Artifactory and Nexus now support
    Docker registry as a repository type. The Docker registry can be set up with authentication
    and SSL certificates to secure container images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Docker 镜像仓库是一个文件系统卷，并在本地文件系统上持久化数据。像 Artifactory 和 Nexus 这样的工件仓库现在也支持
    Docker 镜像仓库作为一种仓库类型。Docker 镜像仓库可以通过身份验证和 SSL 证书来确保容器镜像的安全。
- en: Docker daemon
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker 守护进程
- en: During installation, Docker deploys a daemon on the target operating system
    that has been chosen to run containers. The **Docker daemon** is used to communicate
    with the Docker image registry and issue pull commands to pull down the latest
    container images or a specific tagged version. The Docker command line can then
    be used to schedule the start-up of the containers using the container image that
    has been pulled from the registry. Docker daemons, by default, run as a constant
    process on target operating systems, but can be started or stopped using a process
    manager such as `systemd`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装过程中，Docker 会在目标操作系统上部署一个守护进程，用来运行容器。**Docker 守护进程**用于与 Docker 镜像仓库通信，并发出拉取命令以拉取最新的容器镜像或特定标签版本。然后，Docker
    命令行可以用来调度启动容器，使用从仓库中拉取的容器镜像。默认情况下，Docker 守护进程会作为常驻进程运行在目标操作系统上，但可以通过 `systemd`
    等进程管理器来启动或停止。
- en: Packaging containers
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打包容器
- en: Containers can be packaged in various different ways; two of the most popular
    ways of packaging containers is using Dockerfiles, and one of the lesser known
    ways is using a tool from **HashiCorp** called Packer. Both have slightly different
    approaches to packaging container images.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 容器可以以多种方式打包；其中两种最常见的打包方式是使用 Dockerfile，另外一种较少为人所知的方式是使用 **HashiCorp** 提供的工具
    Packer。这两者在打包容器镜像时有略微不同的方法。
- en: Dockerfile
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dockerfile
- en: Docker allows users to package containers using its very own configuration-management
    tool called **Dockerfile**. Dockerfile will state the intent of the container
    by outlining the packages that should be installed on it using package managers
    at build time.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 允许用户使用其自有的配置管理工具 **Dockerfile** 来打包容器。Dockerfile 会通过列出在构建时使用包管理器安装的包来说明容器的意图。
- en: 'The following Dockerfile shows NGINX being installed on CentOS by issuing `yum`
    `install` commands and exposing port `80` to the guest operating system from the
    packaged container. Port `80` is exposed so NGINX can be accessed externally:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Dockerfile展示了通过执行`yum` `install`命令在CentOS上安装NGINX，并将端口`80`暴露给来自打包容器的客户操作系统。暴露端口`80`是为了让NGINX能够外部访问：
- en: '![Dockerfile](img/B05559_10_02.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![Dockerfile](img/B05559_10_02.jpg)'
- en: 'Once the Dockerfile has been created, Docker''s command-line interface allows
    users to issue the following command to build a container:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了Dockerfile，Docker的命令行界面允许用户发出以下命令来构建容器：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The one downside is that applications are typically installed using configuration
    management tools such as Puppet, Chef, Ansible, and Salt. The Dockerfile is very
    brittle, which means that packaging scripts need to be completely re-written.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的缺点是，应用程序通常使用配置管理工具，如Puppet、Chef、Ansible和Salt来安装。Dockerfile非常脆弱，这意味着打包脚本需要完全重写。
- en: Packer-Docker integration
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Packer-Docker集成
- en: '**Packer** from HashiCorp is a command-line tool which uses multiple drivers
    to package virtual machine images and also supports creating Docker image files.
    Packer can be used to package **Amazon Machine Image** (**AMI**) images for AWS
    or **QEMU Copy On Write** (**QCOW**) images, which can be uploaded to OpenStack
    Glance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**Packer** 是HashiCorp的一个命令行工具，使用多个驱动程序来打包虚拟机镜像，也支持创建Docker镜像文件。Packer可用于打包**Amazon
    Machine Image**（**AMI**）镜像，用于AWS，或**QEMU Copy On Write**（**QCOW**）镜像，这些镜像可以上传到OpenStack
    Glance。'
- en: When utilizing Packer, it skips the need for using Dockerfiles to create Docker
    images; instead, existing configuration-management tools such as Puppet, Chef,
    Ansible, and Salt can be used to provision and package Docker container images.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Packer时，省去了使用Dockerfile来创建Docker镜像的需求；相反，可以使用现有的配置管理工具，如Puppet、Chef、Ansible和Salt，来配置和打包Docker容器镜像。
- en: 'Packer has the following high-level architecture and uses a JSON file to describe
    the Packer workflow, with three main parts:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Packer具有以下高级架构，并使用一个JSON文件来描述Packer工作流，主要分为三部分：
- en: Builders
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建器
- en: Provisioners
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置管理工具
- en: Post-processors
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理器
- en: '**Builders** are used to boot an ISO, virtual machine on a Cloud platform,
    or in this case, start a Docker container from an image file on a build server.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建器**用于启动一个ISO、虚拟机在云平台上，或者在此情况下，从构建服务器上的镜像文件启动一个Docker容器。'
- en: Once booted, the configuration management **provisioner** will run a set of
    installation steps. This will create the desired state for the image, emulating
    what the Dockerfile would carry out. Once complete, the image will be stopped
    and packaged.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动，配置管理**配置器**将运行一组安装步骤。这将创建镜像所需的状态，模拟Dockerfile将执行的操作。完成后，镜像将停止并被打包。
- en: A set of **post-processors** will then be executed to push the image to an artifact
    repository or Docker registry, where it is tagged and versioned.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一组**后处理器**将被执行，将镜像推送到一个工件库或Docker注册表，在那里它会被标记并版本化。
- en: Using Packer means existing configuration management tools can be used to package
    virtual machines and containers in the same way rather than using a completely
    different configuration-management mechanism for containers. The Docker daemon
    will need to be installed as a prerequisite on the build server that is being
    used to package the container.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Packer意味着可以使用现有的配置管理工具来打包虚拟机和容器，而不是为容器使用完全不同的配置管理机制。Docker守护进程需要作为前提条件安装在用于打包容器的构建服务器上。
- en: In the following example, an `nginx.json` Packer file is created; the `builders`
    section has the type `docker` defined, which lets Packer know to use the Docker
    builder.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，创建了一个`nginx.json` Packer文件；`builders`部分定义了类型为`docker`，这让Packer知道使用Docker构建器。
- en: The `export_path` is where the final Docker image will be exported to and `image`
    is the name of the Docker image file that will be pulled from the Docker registry
    and started.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`export_path`是最终Docker镜像导出的位置，`image`是将从Docker注册表中拉取并启动的Docker镜像文件的名称。'
- en: One provisioner of the `ansible-local` type will then execute the `install_nginx.yml`
    playbook to install NGINX on the Docker image, using an Ansible playbook as opposed
    to the Dockerfile.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`ansible-local`类型的配置管理器将执行`install_nginx.yml`剧本，以便使用Ansible剧本而不是Dockerfile来安装NGINX到Docker镜像中。
- en: 'Finally, the post-processors will then import the packed image, complete with
    NGINX installed, into the Docker registry with the tag `1.1`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，后处理器将把打包的镜像（包含安装的NGINX）导入到Docker注册表，并使用标签`1.1`：
- en: '![Packer-Docker integration](img/B05559_10_03.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: 'To execute the Packer build, simply execute the following command passing the
    `nginx.json` file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Docker workflow
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Docker workflow** fits nicely into the continuous integration process
    that we covered as part of [Chapter 7](ch07.html "Chapter 7. Using Continuous
    Integration Builds for Network Configuration"), *Using Continuous Integration
    Builds for Network Configuration* and the Continuous Delivery workflow we covered
    in [Chapter 9](ch09.html "Chapter 9. Using Continuous Delivery Pipelines to Deploy
    Network Changes"), *Using Continuous Delivery Pipelines to Deploy Network Changes*.
    After a developer pushes a new code commit, compiling and potentially packaging
    new code, the continuous integration process can be extended to execute a Dockerfile
    to package a new Docker image as a post-deployment step.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: A Docker daemon is configured on each downstream test environment and production
    as part of the base operating system. At deployment time, the Docker daemon is
    scheduled to pull down the newly packaged Docker image and create a new set of
    containers doing a rolling update.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'This process flow can be seen as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker workflow](img/B05559_10_04.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: Default Docker networking
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In terms of networking, when Docker is installed, it creates three default
    networks; the networks created are the `bridge`, `none`, and `host` networks,
    as shown in the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Default Docker networking](img/B05559_10_05.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: The Docker daemon creates containers against the bridge (`docker0`) network
    by default; this occurs when a `docker create` and `docker start` are issued on
    the target operating system, or alternatively, just a `docker run` command can
    be issued. These commands will create and start new containers on the host operating
    system from the defined Docker image.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: The `none` network is used to create a container-specific network, which allows
    containers to be launched and left to run; it doesn't have a network interface,
    though. The `host` network adds containers to the same network as the guest operating
    system.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'When containers are launched on it, Docker''s bridge network assigns each container
    a unique IP address on the bridge network''s subnet range. The containers can
    be viewed by issuing the following `docker network inspect` command:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Docker allows users to inspect container configuration by using the `docker
    attach` command; in this instance, the `nginx` container can be inspected:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once attached, the `/etc/hosts` file can be inspected to show the network configuration.
    Docker bridge uses a NAT network and can use port forwarding using the following
    `–p` command-line argument. For example, `-p 8080:8080` forwards port `8080` from
    the host to the container. This allows all containers that are running on an operating
    system to be accessed directly by the localhost by their IPs, using port forwarding.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: In its default networking mode, Docker allows containers to be interconnected
    using a `--links` command-line argument, which is used to connect containers,
    which writes entries into the `/etc/hosts` file of containers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The default network setup is now not recommended for use, and more sophisticated
    networking is present, but the concepts it covers are still important.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Docker allows user-defined networks to be defined to host containers, using
    network drivers to create custom networks such as custom `bridge`, `overlay`,
    or layer 2 `MACVlLAN` network.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Docker user-defined bridge network
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A user-defined bridge network is much like the default Docker network, but it
    means that each container can talk to each of the other containers on the same
    bridge network; there is no need for linking as with the default Docker networking.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'To place containers on a user-defined network, containers can be launched on
    the `devops_for_networking_bridge` user-defined bridge network using the following
    command, with the `–net` option set:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each container that is launched will reside on the same operating system guest.
    Publish is used to expose specific using of the `-p 8080-8081:8080/tcp` command.
    Therefore, ranges can be published so that portions of the network can be exposed.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overlay networks, can also be used with Docker and have already been covered
    at length in this book, are a virtualized abstraction layer for the network. Docker
    can create an overlay network for containers, which is used to create a network
    of containers that belong to multiple different operating system hosts.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Instead of isolating each container to a unique network existing on one host,
    Docker instead allows its overlay network to join multiple different clusters
    of containers that are deployed on separate hosts together.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: This means that each container that shares an overlay network will have a unique
    IP address and name. To create an overlay network, Docker uses its own orchestration
    engine, called **Docker Swarm**.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: To run Docker in swarm mode, an external key-value store such as etcd, Consul,
    or Zookeeper needs to be used with Docker. This key-value store allows Docker
    to share information between different hosts, including the shared overlay network.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Docker machine
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is worth mentioning that `docker-machine` is a useful command-line utility
    that allows virtual machines to be provisioned in VirtualBox, OpenStack, AWS,
    and many more platforms that have drivers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see how a machine could be booted using `docker-machine`
    in OpenStack:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: One of the more useful functions of `docker-machine` is its ability to boot
    virtual machines in cloud environments while issuing Docker Swarm commands. This
    allows machines to be set up on boot to the specific profile that is required.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Docker Compose
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another helpful tool for orchestrating containers is **Docker Compose**, as
    running a command line for every container that needs to be deployed is not a
    feasible solution at scale. Therefore, Docker Compose allows users to specify
    their microservice-architecture topology in YAML format, so container dependencies
    are chained together to form a fully-fledged application.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Microservices will be comprised of different container types, which together
    make up a full application. Docker Compose allows each of those microservices
    to be defined as YAML in the `docker-compose` file so they can be deployed together
    in a manageable way.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following `docker-compose.yml` file, `web`, `nginx`, and `db` applications
    are configured and linked together, with the load balancer being exposed on port
    `8080` for public access, and load balancing `app1`, which is connected to the
    `redis` database backend:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker Compose](img/B05559_10_06.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'Docker Compose can be executed in the same directory as the Docker Compose
    YAML file to invoke a new deployment the following command should be issued:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Swarm architecture
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Swarm architecture works on the principle that each host runs a Swarm agent
    and one host runs a Swarm master. The master is responsible for the orchestration
    of containers on each of the hosts where agents are running and that are a member
    of the same discovery (key-value store).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: An important principle for swarm is discovery, which is catered for using a
    key-value store such as etcd, Consul, or Zookeeper.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'To set up a Docker swarm, a set up Docker machine can be used to provision
    the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Discovery server (key-value store such as etcd, Consul, or Zookeeper)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swarm master with swarm agent installed, pointing at a key-value store
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two Swarm nodes with Swarm agent installed, pointing at a key-value store
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Docker Swarm architecture shows a master node scheduling containers on
    two Docker agents while they are all advertising to the key-value store, which
    is used for service discovery:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm architecture](img/B05559_10_07.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'When setting up a Swarm agent, in this case the Swarm master, they will be
    booted with the following options: `--swarm-discovery` defines the address of
    the discovery service, while `--cluster-advertise` advertises the host machine
    on the network and `--cluster-store` points at a key-value store of choice:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Swarm architecture](img/B05559_10_08.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'Once the architecture has been set up, an overlay network needs to be created
    to run containers across the two different hosts (in this instance the overlay
    network is called `devops_for_networking_overlay`) by issuing the following command:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Containers can then be created on the network from an image using the Docker
    Swarm master to schedule the commands:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As each host is running in Swarm mode and attached to the key-value store, upon
    creation, the network information meta-data will be shared by the key-value store.
    This means that the network is visible to all hosts that use the same key-value
    store.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Containers can then be launched from any of the Swarm masters onto the same
    overlay network, which will join the two hosts together. This will allow each
    host to communicate with other containers, via the overlay network, across hosts.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Multiple overlay networks can be created; though containers can only communicate
    across the same overlay network they cannot communicate between different overlay
    networks. To mitigate this, containers can be attached to multiple different networks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm allows many specific containers to be assigned and exposed using
    port forwarding to load balance containers. Rolling updates can also be carried
    out to allow upgrades of the containers' application version.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Due to its completely decentralized design, Docker Swarm is very flexible in
    the number of networking use cases it can solve.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is a popular container orchestration tool from Google which was created
    in 2014 and is an open-source tool. Rather than Google coming up with their own
    container packaging tool and packaging repository, Kubernetes instead can plug
    seamlessly to use Docker registry as its container image repository.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can orchestrate containers that are created using Docker via a **Dockerfile**,
    or alternatively, using Packer aided by configuration management tools such as
    Puppet, Chef, Ansible, and Salt.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can be seen as an alternative to Docker Swarm, but takes a slightly
    different approach in terms of its architectural design and has a lot of rich
    scheduling features to help with container management.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes architecture
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Kubernetes cluster needs to be set up before a user can use Kubernetes to
    schedule containers. There is a wide variety of configuration management tools
    that can be used to create a production-grade Kubernetes cluster with notable
    solutions available from Ansible, Chef, and Puppet.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes clustering consists of the following high-level components, which
    in turn have their own subset of services. At a high level, a Kubernetes cluster
    consists of the following components:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Kubectl
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master node
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worker node![Kubernetes architecture](img/B05559_10_09.jpg)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes master node
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The master node is responsible for managing the whole Kubernetes cluster and
    is used to take care of orchestrating worker nodes, which is where containers
    are scheduled.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The master node, when deployed, consists of the following high-level components:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: API server
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etcd key-value store
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduler
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controller manager
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API server has a RESTful API, which allows administrators to issue commands
    to Kubernetes.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Etcd, as covered earlier in this chapter, is a key-value store that allows Kubernetes
    to store state and push changes to the rest of the cluster after changes have
    been made. Etcd is used by Kubernetes to hold scheduling information about pods,
    services, state, or even namespace information.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes scheduler, as the name suggests, is used to schedule containers
    on Services or Pods. The Scheduler will check the availability of the Kubernetes
    cluster and make scheduling decisions based on availability of resources so it
    can schedule containers appropriately.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The controller-manager is a daemon that allows a Kubernetes master to run different
    controller types. Controllers are used by Kubernetes to analyze the state of a
    cluster and make sure it is in the desired state, so if a pod fails it will be
    recreated or re-started. It adheres to the thresholds that are specified and is
    controlled by the Kubernetes' administrator.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes worker node
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Worker nodes are where pods run; each pod has an IP address and runs containers.
    It is the pod that determines all the networking for the containers and governs
    how they communicate across different pods.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The worker node will contain all the necessary services to manage the networking
    between the containers, communicate with the master node, and are also used to
    assign resources to the scheduled containers.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Docker also runs on each of the worker nodes and is used to pull down containers
    from the Docker registry and schedule containers.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubelet** is the worker service and is installed on worker nodes. It communicates
    with the API server on the Kubernetes master and retrieves information on the
    desired state of pods. Kubelet also reads information updates from etcd and writes
    updates about cluster events.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The `kube-proxy` takes care of load balancing and networking functions such
    as routing packets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes kubectl
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Kubectl** is the Kubernetes command line, which issues commands to the master
    node to administer Kubernetes clusters. It can also be used to call YAML or JSON,
    as it is talking to the RESTful API server on the master node.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes service is created as an abstraction layer above pods, which can
    be targeted using a label selector.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, kubectl can be used to create a `loadbalancing_service`
    service deployment with a selector, `app: nginx`, which is defined by the `loadbalancing_service.yml`
    file:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes kubectl](img/B05559_10_10.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: 'Kubectl executes the YAML file by specifying:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Kubectl can then create four replica pods using the `ReplicationController`,
    these four pods will be managed by the service, as the labels `app: nginx` match
    the service''s selector and launch an NGINX container in each pod using the `nginx_pod.yml`
    file:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes kubectl](img/B05559_10_11.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Kubectl creates the service using the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Kubernetes SDN integration
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes supports multiple networking techniques that could fill a whole book's
    worth of material on its own. With the Kubernetes, the pod is the major insertion
    point for networking.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes supports the following networking options:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Google Compute Engine
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open vSwitch
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 2 Linux Bridge
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project Calico
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Romana
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contiv
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes looks to provide a pluggable framework to control a pod's networking
    configuration and aims to give users a choice; if a flat layer 2 is required,
    Kubernetes caters for it, if a more complex layer-3 overlay network is required,
    then it can cater for this, too.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: With Open vSwitch being widely used with enterprise SDN controllers such as
    Nuage Networks VSP platform, which was covered in [Chapter 2](ch02.html "Chapter 2. The
    Emergence of Software-defined Networking"), *The Emergence of Software-defined
    Networking* and [Chapter 6](ch06.html "Chapter 6. Orchestrating SDN Controllers
    Using Ansible"), *Orchestrating SDN Controllers Using Ansible*. This focused upon
    how flow information could be pushed down to Open vSwitch on each hypervisor to
    create a stateful firewall and govern the ACL policies.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: A similar implementation is carried out when integrating Kubernetes, with Open
    vSwitch, being deployed onto each worker node and pod traffic being deferred to
    Open vSwitch.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: In Nuage's case, a version of their customized version of Open vSwitch, known
    as the VRS, is deployed on each Kubernetes worker to govern policy controlled
    by the VSD Nuage VSPs policy engine..
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow for the Nuage SDN integration with Kubernetes is shown in the
    following figure, which shows that enterprise SDN controllers can integrate with
    orchestration engines such as Kubernetes and Docker to provide enterprise-grade
    networking:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes SDN integration](img/B05559_10_12.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
- en: Impact of containers on networking
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers have undoubtedly meant that a lot of networking has shifted into
    the application tier, so really, containers can be seen as a PaaS offering in
    its truest form.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure is, of course, still required to run containers, be it on bare-metal
    servers or virtual machines. The merits of virtual machines being used to run
    containers long term are debatable, as in a way it means a double set of virtualization,
    and anyone using nested virtualization will know it isn't always optimal for performance.
    So with more organizations using containers to deploy their microservice architectures,
    it will undoubtedly mean that users having a choice to run containers on either
    virtual or physical machines will be in demand.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Cloud has notoriously meant virtual machines, so running containers on virtual
    machines is probably born out of necessity rather than choice. Being able to orchestrate
    containers on bare-metal servers with an overlay network on top of them is definitely
    more appealing as it pushes the container closer to the physical machine resources
    without the visualization overhead.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: This allows containers to maximize the physical machine resources, and users
    then only care about anti-infinity in terms of whether the service can run across
    multiple clouds and data centers, giving true disaster recovery.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: With hybrid cloud solutions, the industry is moving beyond thinking about rack
    redundancy. Instead it is moving toward a model which will focus on splitting
    applications across multiple cloud providers. So having the ability to orchestrate
    the networking and applications in an identical way using orchestration engines
    such as Docker Swarm or Kubernetes can be used to make that goal a reality.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for the network operator? It means that the role is evolving,
    it means that the network engineer's role becomes advisory, helping the developers
    architect the network in the best possible way to run their applications. Rather
    than building a network as a side project in a private cloud, network operators
    can instead focus on providing an overlay network as a service to developers while
    making the underlay network fabric fast and performant so that it can scale out
    to meet the developer's needs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Containers have been said to be a major disruptor of the virtualization market.
    Gartner have predicted the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '*"By 2018, more than 50-60% of new workloads will be deployed into containers
    in at least one stage of the application life cycle".*'
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is based on Gartner's analysis of the IT market, so this is a bold statement,
    but if it comes to fruition, it will prove to be a huge cultural shift in the
    way applications are deployed, in the same way virtualization was before it.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we showed that containers can help organizations deploy their
    microservice architectures and analyzed the internal mechanics and benefits that
    containers bring. The key benefits are portability, speed of deployment, elastic
    scalability, isolation and maximization of different resources, performance control,
    limited attack vector, and support for multiple networking types.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Aside from the benefits containers bring, this chapter looked at the Docker
    tool and illustrated how the Docker workflow can be fitted into a Continuous Delivery
    model, which is at the heart of most DevOps initiatives.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: The focus of the chapter then shifted to Docker networking and the layer-2 networking
    options available to network containers. We illustrated how to use overlay networks
    to join multiple hosts together to form a cluster and we showed how container
    technology can integrate with SDN controllers such as Nuage VSP Platform using
    Open vSwitch.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also covered container orchestration solutions such as Docker Swarm
    and Kubernetes, their unique architectures, and ways in which they can be used
    to network containers over multiple hosts and act as a Platform as a Service layer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'The importance of containerization and its impact on Platform as a Service
    (PaaS) solutions cannot be underestimated, with Forrester stating the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '*"Containers as a Service (CaaS) is becoming the new Platform as a Service
    (PaaS). With the interest in containers and micro-services skyrocketing among
    developers, cloud providers are capitalizing on the opportunity through hosted
    container management services."*'
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In summary, it is fair to conclude that containerization can have many benefits
    and help aid developers in the implementation of Continuous Delivery workflows
    and PaaS solutions. Containerization also gives the added flexibility of deploying
    workloads across multiple cloud providers, be they private or public, using a
    common orchestration layer such as Kubernetes, Apache Mesos, or Docker Swarm.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, the focus will shift from containers toward securing
    the network when using software-defined overlay networks and a Continuous Delivery
    model. It will explore techniques that can be used to help secure a modern private
    cloud in an API-driven environment, so that software-defined networking solutions
    can be implemented without compromising security requirements.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
