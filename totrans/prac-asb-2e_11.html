<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-158"><a id="_idTextAnchor456"/>11</h1>
<h1 id="_idParaDest-159"><a id="_idTextAnchor457"/>Container and Cloud Management</h1>
<p>Ansible is a very flexible automation tool and can be easily used to automate any aspect of your infrastructure. In the last few years, container-based workloads and cloud workloads have become more and more popular, and for this reason, we will look at how you can automate tasks related to those kinds of workloads with Ansible. This chapter will start by designing and building containers with Ansible. We will then look at how to run those containers, and finally, we will look at ways to manage various cloud platforms with Ansible.</p>
<p>Specifically, we will be covering the following topics in this chapter:</p>
<ul>
<li>Automating Docker and Podman with Ansible</li>
<li>Managing Kubernetes with Ansible</li>
<li>Exploring container-focused modules</li>
<li>Automating against Amazon Web Services</li>
<li>Complementing Google Cloud Platform with automation</li>
<li>Seamless automation integration with Azure</li>
<li>Expanding your environment with Rackspace Cloud</li>
<li>Using Ansible to orchestrate OpenStack</li>
</ul>
<p>Let’s get started!</p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor458"/>Technical requirements</h1>
<p>This chapter assumes that you have set up your control host with Ansible, as detailed in <a href="B20846_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Ansible</em>, and are using the most recent version available – the examples in this chapter were tested with Ansible 2.15. Although we will give specific examples of hostnames in this chapter, you are free to substitute them with your own hostname and/or IP address. Details of how to do this will be provided in the appropriate places. This chapter also assumes you have access to Docker, Podman, Kubernetes, and the different clouds. Since the majority of processes will be very similar across the different clouds, you will be able to gain the majority of the information even if you will read the whole chapter and just test those playbooks on the couple of environments you have access to.</p>
<p>All the examples in this chapter can be found in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Practical-Ansible-Second-Edition/tree/main/Chapter%2011">https://github.com/PacktPublishing/Practical-Ansible-Second-Edition/tree/main/Chapter%2011</a>.</p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor459"/><a id="_idTextAnchor460"/>Automating Docker and Podman with Ansible</h1>
<p>In today’s world, simply <a id="_idIndexMarker636"/>being able to run an image is not considered a production-ready setup.</p>
<p>To be able to call a deployment <strong class="bold">production-ready</strong>, you need to demonstrate that the service your application delivers will run reasonably, even in the case of a single application crash, as well as hardware failure. Often, you’ll have even more reliability constraints from your customer.</p>
<p>Luckily, your <a id="_idIndexMarker637"/>software is not the only one that has those requirements, so orchestration solutions have been developed for this purpose.</p>
<p>Today, the most successful one is Kubernetes, due to its various distributions/versions, so we are going to focus on it primarily.</p>
<p>The idea of Kubernetes is that you inform the Kubernetes control plane that you want <em class="italic">X</em> number of instances of <em class="italic">Y</em> application. Kubernetes will count how many instances of <em class="italic">Y</em> application are running on the Kubernetes Nodes to ensure that the number of instances is <em class="italic">X</em>. If there are too few instances, Kubernetes will take care of starting more instances. At the same time, if there are too many instances, the exceeding instances will be stopped.</p>
<p>Since Kubernetes constantly checks that the requested amount of instances is running, in the case of an application failure or a node failure, Kubernetes will start new instances to replace the lost instances.</p>
<p>Due to the complexity of installing and managing Kubernetes, multiple companies have started to sell distributions of Kubernetes that simplify their operations and that they are willing to support.</p>
<p>Every Kubernetes node runs an instance of a container engine. For this reason, we will start to see how the two most widely used standalone containers engine can be automated.</p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor461"/>Managing Docker</h2>
<p>Docker is now <a id="_idIndexMarker638"/>a very common, ubiquitous tool for development usage and you can leverage Ansible to easily manage your Docker instance. Since we are going to manage a Docker instance, we need to make sure we have one at hand and that the Docker command on our machine is configured correctly. We need to do this to ensure this is enough to run <code>docker images</code> on the terminal. Let’s say you get a result similar to the following:</p>
<pre class="console">
REPOSITORY TAG IMAGE ID CREATED SIZE</pre> <p>This means that everything is working properly. More lines may be provided as output if you have already-cloned images.</p>
<p>On the other hand, let’s say it returns something like this:</p>
<pre class="console">
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</pre> <p>This means that we don’t have a Docker daemon running or that our Docker console has been configured incorrectly.</p>
<p>Also, it’s important to ensure that you have the <code>docker</code> Python module installed since Ansible tries to use it to communicate with the Docker daemon. Let’s take a look:</p>
<ol>
<li>First of all, we need to create a playbook called <code>start-docker-container.yaml</code> that contains the following code:<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Start a container with a command
    community.docker.docker_container:
      name: test-container
      image: alpine
      command:
      - echo
      - "Hello, World!"</pre></li> <li>Now that we <a id="_idIndexMarker639"/>have the Ansible playbook, we just need to execute it:<pre class="source-code">
<strong class="bold">$ ansible-playbook start-docker-container.yaml</strong></pre></li> </ol>
<p>As you may expect, it will give you an output similar to the following:</p>
<pre class="source-code">
<strong class="bold">PLAY [localhost] *********************************************************************</strong>
<strong class="bold">TASK [Gathering Facts] ***************************************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [Start a container with a command] **********************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">PLAY RECAP ***************************************************************************</strong>
<strong class="bold">localhost : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</strong></pre> <ol>
<li value="3">We can now check that our command executed properly, as follows:<pre class="source-code">
<strong class="bold">$ docker container list -a</strong></pre></li> </ol>
<p>This will show the container that was run:</p>
<pre class="source-code">
<strong class="bold">CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES</strong>
<strong class="bold">c706ec55fc0d alpine "echo Hello, World!" 3 minutes ago Exited (0) About a minute ago test-container</strong></pre> <p>This proves that a container was executed.</p>
<ol>
<li value="4">To check<a id="_idIndexMarker640"/> that the <code>echo</code> command was executed, we can run the following code:<pre class="source-code">
<strong class="bold">$ docker logs test-container</strong></pre></li> </ol>
<p>This will return the following output:</p>
<pre class="source-code">
<code>community.docker.docker_container</code> module. This is not the only module Ansible has to control the Docker daemon, but it is probably one of the most widely used since it’s used to control containers running on Docker.</p>
<p>Other modules include the following:</p>
<ul>
<li><code>community.docker.docker_config</code>: Used to change the configurations of the Docker daemon</li>
<li><code>community.docker.docker_container_info</code>: Used to gather information from (inspect) a container</li>
<li><code>community.docker.docker_network</code>: Used to manage Docker networking configuration</li>
</ul>
<p>There are also many modules in the <code>community.docker</code> collection, but they are actually used to manage Docker Swarm clusters and not Docker instances. Some examples are as follows:</p>
<ul>
<li><code>community.docker.docker_node</code>: Used to manage a node in a Docker Swarm cluster</li>
<li><code>community.docker.docker_node_info</code>: Used to retrieve information about a specific node in a Docker Swarm cluster</li>
<li><code>community.docker.docker_swarm_info</code>: Used to retrieve information about a Docker Swarm cluster</li>
</ul>
<p>As we will see in<a id="_idIndexMarker641"/> the next section, there are many more modules that can be used to manage containers that are orchestrated in various ways.</p>
<p>Now that you have learned how to automate Docker with Ansible, we will see how to work with Podman, a Docker alternative.</p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor462"/>Managing Podman</h2>
<p>Podman is a <a id="_idIndexMarker642"/>container engine competitor to Docker. The reason why Red Hat started to develop an alternative container engine is due to the design of Docker. Although Docker changed some design decisions over the years, at the time, Docker was running as a daemon that required root access to the machine, and the CLI <code>docker</code> command was just a client. The risk with this approach, at least from Red Hat’s standpoint, was that a bug in the Docker daemon could have endangered the whole system’s security.</p>
<p>Podman, differently from Docker, does not have a daemon running, and all utilities Podman provides are discrete. This approach allows Podman to be much more secure than Docker, allowing containers to be run without any root access.</p>
<p>One of the critical decisions made at the very beginning of Podman development was the CLI and API compatibility between Podman and Docker. This aspect dramatically increases the simplicity of moving from one to the other without much effort.</p>
<p>As you might have imagined, from an Ansible standpoint, it is also going to be very similar.</p>
<p>Due to the simpler design Podman has, to ensure that it is present and usable on your system, it is enough to run the <code>podman</code> command. If it returns the following, it means that you do not have it installed:</p>
<pre class="console">
bash: podman: command not found</pre> <p>If it returns all the command options, this means that Podman is properly installed on your system.</p>
<p>We can therefore <a id="_idIndexMarker643"/>create our first Ansible Playbook to manage Podman, which we will call <code>start-podman-container.yaml</code>, with the following content:</p>
<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Start a container with a command
    containers.podman.podman_container:
      name: test-container
      image: alpine
      command:
      - echo
      - "Hello, World!"</pre> <p>We can now run the playbook by executing the following:</p>
<pre class="console">
$ ansible-playbook start-podman-container.yaml</pre> <p>As you may expect, it will give you an output similar to this:</p>
<pre class="console">
PLAY [localhost] *******************************************************************************************************************************************************************************
TASK [Gathering Facts] *************************************************************************************************************************************************************************
ok: [localhost]
TASK [Start a container with a command] ********************************************************************************************************************************************************
changed: [localhost]
PLAY RECAP *************************************************************************************************************************************************************************************
localhost                  : ok=2    changed=1    unreachable=0    failed=0     skipped=0    rescued=0    ignored=0</pre> <p>As we did for Docker, we <a id="_idIndexMarker644"/>can check that the command executed properly by running the following:</p>
<pre class="console">
$ podman container list -a</pre> <p>And this command will return the information of the container that ran:</p>
<pre class="console">
CONTAINER ID  IMAGE                                                 COMMAND               CREATED        STATUS                    PORTS        NAMES96531f2f960e  docker.io/library/alpine:latest                        echo Hello, World...  2 minutes ago  Exited (0) 2 minutes ago              test-container</pre> <p>The fact that it exited with <code>0</code> means that it successfully ran, as we were expecting.</p>
<p>Similarly to Docker, there are many other modules usable in the <code>containers.podman</code> collection. Differently from Docker, though, there is no such thing as Podman Swarm, and therefore all Docker modules focusing on Swarm will not have a Podman counterpart.</p>
<p>Now that we have explored the two major standalone container engines that you might encounter in development and test environments, we will discuss Kubernetes since it is the most widely used container platform in production envir<a id="_idTextAnchor463"/>onments.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor464"/>Managing Kubernetes with Ansible</h1>
<p>We will assume<a id="_idIndexMarker645"/> that you have access to either a Kubernetes or OpenShift cluster for testing. Setting these up is out of the scope of this book, so you might want to look at a distribution such as Minikube or Minishift, both of which are designed to be quick and easy to set up so that you can start learning these technologies rapidly. We also need to have the <code>kubectl</code> client or the <code>oc</code> client, depending on whether we have deployed Kubernetes or OpenShift, properly c<a id="_idTextAnchor465"/>o<a id="_idTextAnchor466"/>nfigured.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor467"/>Installing Ansible Kubernetes dependencies</h2>
<p>First of all, you <a id="_idIndexMarker646"/>need to ensure you are running a fairly updated version of Python 3 (&gt;=3.6) and install the required Python packages (you can install it either via PIP or your OS packaging system):</p>
<ul>
<li>Kubernetes &gt;= 12.0.0</li>
<li>PyYAML &gt;= 3.11</li>
<li><code>jsonpatch</code></li>
</ul>
<p>Since the <code>kubernetes.core</code> collection is not generally shipped with Ansible, you’ll need to install it by running this:</p>
<pre class="console">
$ ansible-galaxy collection install kubernetes.core</pre> <p>We are now ready for our first Kubernete<a id="_idTextAnchor468"/>s<a id="_idTextAnchor469"/> playbook!</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor470"/>Listing Kubernetes namespaces with Ansible</h2>
<p>A Kubernetes cluster <a id="_idIndexMarker647"/>has multiple namespaces internally, and you can usually find the ones a cluster has with <code>kubectl get namespaces</code>. You can do the same with Ansible by creating a file called <code>k8s-ns-show.yaml</code> with the following content:</p>
<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Get information from K8s
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Namespace
      register: ns
  - name: Print info
    ansible.builtin.debug:
      var: ns</pre> <p>We can <a id="_idIndexMarker648"/>now execute this as follows:</p>
<pre class="console">
$ ansible-playbook k8s-ns-show.yaml</pre> <p>You will now see information regarding the namespaces in the output.</p>
<p>Notice that in the seventh line of the code (<code>kind: Namespace</code>), we are specifying the type of resources we are interested in. You can specify other Kubernetes object types to see them (for example, you can try this with Deployments, Service<a id="_idTextAnchor471"/>s<a id="_idTextAnchor472"/>, and Pods).</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor473"/>Creating a Kubernetes namespace with Ansible</h2>
<p>So far, we have<a id="_idIndexMarker649"/> learned how to show existing namespaces, but usually, Ansible is used in a declarative way to achieve a desired state. So, let’s create a new playbook called <code>k8s-ns.yaml</code> with the following content:</p>
<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Ensure the myns namespace exists
    kubernetes.core.k8s:
      api_version: v1
      kind: Namespace
      name: myns
      state: present</pre> <p>Before<a id="_idIndexMarker650"/> running it, we can execute <code>kubectl get ns</code> so that we can ensure <code>myns</code> is not present. In my case, the output is as follows:</p>
<pre class="console">
$ kubectl get ns
NAME STATUS AGE
default Active 69m
kube-node-lease Active 69m
kube-public Active 69m
kube-system Active 69m</pre> <p>We can now run the playbook with the following command:</p>
<pre class="console">
$ ansible-playbook k8s-ns.yaml</pre> <p>The output should resemble the following one:</p>
<pre class="console">
PLAY [localhost] *******************************************************************
TASK [Gathering Facts] *************************************************************
ok: [localhost]
TASK [Ensure the myns namespace exists] ********************************************
changed: [localhost]
PLAY RECAP *************************************************************************
localhost : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</pre> <p>As you can see, Ansible <a id="_idIndexMarker651"/>reports that it changed the namespace state. If I execute <code>kubectl get ns</code> again, it is clear that Ansible created the namespace we were expecting:</p>
<pre class="console">
$ kubectl get ns
NAME STATUS AGE
default Active 74m
kube-node-lease Active 74m
kube-public Active 74m
kube-system Active 74m
myns Active 22s</pre> <p>Now, let’s c<a id="_idTextAnchor474"/>r<a id="_idTextAnchor475"/>eate a service.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor476"/>Creating a Kubernetes service with Ansible</h2>
<p>So far, we <a id="_idIndexMarker652"/>have seen how to create namespaces from Ansible, so now, let’s put a service in the namespace we just created. Let’s create a new playbook called <code>k8s-svc.yaml</code> with the following content:</p>
<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Ensure the Service mysvc is present
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Service
        metadata:
          name: mysvc
          namespace: myns
        spec:
          selector:
            app: myapp
            service: mysvc
          ports:
          - protocol: TCP
            targetPort: 800
            name: port-80-tcp
            port: 80</pre> <p>Before <a id="_idIndexMarker653"/>running it, we can execute <code>kubectl get svc</code> to ensure that the namespace has no services. Make sure you’re in the right namespace before running it! In my case, the output is as follows:</p>
<pre class="console">
$ kubectl get svc
No resources found in myns namespace.</pre> <p>We can now run it with the following command:</p>
<pre class="console">
$ ansible-playbook k8s-svc.yaml</pre> <p>The output should resemble the following one:</p>
<pre class="console">
PLAY [localhost] *******************************************************************
TASK [Gathering Facts] *************************************************************
ok: [localhost]
TASK [Ensure the myns namespace exists] ********************************************
changed: [localhost]
PLAY RECAP *************************************************************************
localhost : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</pre> <p>As you <a id="_idIndexMarker654"/>can see, Ansible reports that it changed the service state. If I execute <code>kubectl get svc</code> again, it is clear that Ansible created the service we were expecting:</p>
<pre class="console">
$ kubectl get svc
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
mysvc ClusterIP 10.0.0.84 &lt;none&gt; 80/TCP 10s</pre> <p>As you can see, we followed the same procedure that we used in the namespace case, but we specified a different Kubernetes object type and specified the various parameters that are needed for the Service type. You can do the same for all other Kubernetes object types.</p>
<p>Now that you have learned how to deal with Kubernetes clusters, you’ll learn how to automate Do<a id="_idTextAnchor477"/><a id="_idTextAnchor478"/>cker with Ansible.</p>
<h1 id="_idParaDest-169"><a id="_idTextAnchor479"/>Exploring container-focused modules</h1>
<p>Often, when <a id="_idIndexMarker655"/>organizations grow, they start to use multiple technologies in different parts of the organization. Another thing that usually happens is that after a department has found that a vendor worked well for them, they will be more inclined to try new technologies offered by that vendor. A mix of those two factors and time (usually, fewer technology cycles) will end up creating multiple solutions for the same problem within the same organization.</p>
<p>If your organization is in this situation with containers, Ansible can come to the rescue, thanks to its ability to interoperate with the majority of, if not all, container platforms.</p>
<p>Often, the <a id="_idIndexMarker656"/>biggest obstacle to doing something with Ansible is finding the name of the modules you need to use to achieve what you want to achieve. In this section, we will try to help in this effort, mainly in terms of the containerization space, but this might help you in the quest to find different kinds of modules.</p>
<p>The starting point for all Ansible module research should be the official Ansible documentation and eventually Ansible Galaxy.</p>
<p>Many Ansible modules relative to container services are in the cloud collections category (ECS, Docker, LXC, or LXD), containers (at the moment only Podman), or in their own one, such as Kubernetes.</p>
<p>To help you further, let’s take a look at some of the main container platforms and the main modules Ansible provides.</p>
<p><strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), back in 2014, launched <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>), which<a id="_idIndexMarker657"/> is <a id="_idIndexMarker658"/>a way to deploy and orchestrate Docker containers within their infrastructure. In the following year, AWS also launched <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>), a managed Docker Registry service. The service did not become as ubiquitous as AWS hoped, so in 2018, AWS<a id="_idIndexMarker659"/> launched <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) to allow people that wanted to run Kubernetes on AWS to have a managed service.</p>
<p>If you are using or plan to use EKS, this is just a standard managed Kubernetes cluster, so you can use the Kubernetes-specific modules that we are going to cover shortly. If you decide to use ECS, there are several modules that could help you. The most important ones are <code>community.aws.ecs_cluster</code>, which allows you to create or terminate ECS clusters; <code>community.aws.ecs_ecr</code>, which allows you to manage ECR; <code>community.aws.ecs_service</code>, which allows you to create, terminate, start, or stop a service in ECS; and <code>community.aws.ecs_task</code>, which allows you to run, start, or stop a task in ECS. In addition to those, there is <code>community.aws.ecs_service_info</code>, which allows Ansible to list or describe services in ECS.</p>
<p>Microsoft <a id="_idIndexMarker660"/>Azure, in 2018, announced <code>azure.azcollection.azure_rm_aks</code> module allows us to create, update, and delete AKS instances.</p>
<p>Google Cloud <a id="_idIndexMarker662"/>launched <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) in 2015. GKE is the <strong class="bold">Google Cloud Platform</strong><strong class="bold"> </strong>(<strong class="bold">GCP</strong>)<a id="_idIndexMarker663"/> version of managed Kubernetes, and is therefore compatible with Ansible Kubernetes modules. In addition to those, there are various GKE-specific modules, some of which are as follows:</p>
<ul>
<li><code>google.cloud.gcp_container_cluster</code>: Allows you to create a GCP cluster</li>
<li><code>google.cloud.gcp_container_cluster_info</code>: Allows you to gather facts for a GCP cluster</li>
<li><code>google.cloud.gcp_container_node_pool</code>: Allows you to create a GCP node pool</li>
<li><code>google.cloud.gcp_container_node_pool_info</code>: Allows you to gather facts for a GCP node pool</li>
</ul>
<p>Red Hat started OpenShift in 2011, and at the time, it was based on its own container runtime. Version 3, which was released in 2015, was completely based on Kubernetes, so all Ansible Kubernetes modules work. In addition to those, there is the <code>oc</code> module, which is currently still present but in a deprecated state, giving preference to the Kubernetes modules.</p>
<p>In 2015, Google released Kubernetes and, quickly, a huge community started to build around it. Ansible allows you to manage your Kubernetes clusters with some modules:</p>
<ul>
<li><code>kubernetes.core.k8s</code>: Allows you to manage any kind of Kubernetes object</li>
<li><code>kubernetes.core.k8s_auth</code>: Allows you to authenticate to Kubernetes clusters that require an explicit login step</li>
<li><code>kubernetes.core.k8s_facts</code>: Allows you to inspect Kubernetes objects</li>
<li><code>kubernetes.core.k8s_scale</code>: Allows you to set a new size for a Deployment, ReplicaSet, Replication Controller, or Job</li>
<li><code>kubernetes.core.k8s_service</code>: Allows you to manage Services on Kubernetes</li>
</ul>
<p>LXC and LXD are also systems that can be used to run containers in Linux. These systems are also supported by Ansible, thanks to the following modules:</p>
<ul>
<li><code>community.general.lxc_container</code>: Allows you to manage LXC containers</li>
<li><code>community.general.lxd_container</code>: Allows you to manage LXD containers</li>
<li><code>community.general.lxd_profile</code>: Allows you to manage LXD profiles</li>
</ul>
<p>Now that you <a id="_idIndexMarker664"/>have learned how to explore container-focused modules, you’ll learn how to automa<a id="_idTextAnchor480"/>te with AWS.</p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor481"/>Automating with Amazon Web Services</h1>
<p>In many<a id="_idIndexMarker665"/> organizations, cloud providers are used widely, while in others, they are just being introduced. However, in one way or <a id="_idIndexMarker666"/>another, you will probably have to deal with a cloud provider while doing your job. AWS is the biggest and oldest, and is perhaps somethi<a id="_idTextAnchor482"/>ng you will have to work with.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor483"/>Installation</h2>
<p>To be able to <a id="_idIndexMarker667"/>use Ansible to automate your AWS estate, you’ll need to install the <code>boto3</code> library. To do so, run the following command:</p>
<pre class="console">
$ pip install boto3</pre> <p>As for <a id="_idIndexMarker668"/>collections, at the moment, there are two collections to interact with AWS services: <code>community.aws</code> and <code>amazon.aws</code>. In many cases, you will need both of them, since many features of the former have not yet been implemented in the latter:</p>
<pre class="console">
$ ansible-galaxy collection install community.aws amazon.aws</pre> <p>Now that you have all the necessary software installed<a id="_idTextAnchor484"/>, you can set up authentication.</p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor485"/>Authentication</h2>
<p>The <code>boto</code> library <a id="_idIndexMarker669"/>looks up the necessary credentials in the <code>~/.aws/credentials</code> file. There are two different ways to ensure that the credentials file is configured properly.</p>
<p>It is possible to use the AWS CLI tool. Alternatively, this can be done with a text editor of your choice by creating a file with the following structure:</p>
<pre class="source-code">
[default]
aws_access_key_id = [YOUR_KEY_HERE]
aws_secret_access_key = [YOUR_SECRET_ACCESS_KEY_HERE]</pre> <p>Now that you’ve created the file with the necessary credentials, <code>boto</code> will be able to work against your AWS environment. Since Ansible uses <code>boto</code> for every single communication with AWS systems, this means that Ansible will be appropriately configured, even without you having to change an<a id="_idTextAnchor486"/>y Ansible-specific configuration.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor487"/>Creating your first machine</h2>
<p>Now that <a id="_idIndexMarker670"/>Ansible is able to connect to your AWS environment, you can proceed with the actual playbook by following these steps:</p>
<ol>
<li>Create the <code>aws.yaml</code> Playbook with the following content, ensuring you use your public key in the <code>key_material</code> value in the first task:<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Ensure key pair is present
    amazon.aws.ec2_key:
      name: fale
      key_material: "{{ lookup('file', '~/.ssh/fale.pub') }}"
  - name: Gather information of the EC2 VPC net in eu-west-1
    amazon.aws.ec2_vpc_net_info:
      region: eu-west-1
    register: aws_simple_net
  - name: Gather information of the EC2 VPC subnet in eu-west-1
    amazon.aws.ec2_vpc_subnet_info:
      region: eu-west-1
      filters:
        vpc-id: '{{ aws_simple_net.vpcs.0.id }}'
    register: aws_simple_subnet
  - name: Ensure wssg Security Group is present
    amazon.aws.ec2_security_group:
      name: wssg
      description: Web Security Group
      region: eu-west-1
      vpc_id: '{{ aws_simple_net.vpcs.0.id }}'
      rules:
      - proto: tcp
        from_port: 22
        to_port: 22
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 80
        to_port: 80
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 443
        to_port: 443
        cidr_ip: 0.0.0.0/0
      rules_egress:
      - proto: all
        cidr_ip: 0.0.0.0/0
    register: aws_simple_wssg
  - name: Setup instance
    amazon.aws.ec2_instance:
      assign_public_ip: true
      image: ami-3548444c
      region: eu-west-1
      exact_count: 1
      key_name: fale
      count_tag:
        Name: ws01.ansible2cookbook.com
      instance_tags:
        Name: ws01.ansible2cookbook.com
      instance_type: t2.micro
      group_id: '{{ aws_simple_wssg.group_id }}'
      vpc_subnet_id: '{{ aws_simple_subnet.subnets.0.id }}'
      volumes:
      - device_name: /dev/sda1
        volume_type: gp2
        volume_size: 10
        delete_on_termination: True</pre></li> <li>Run <a id="_idIndexMarker671"/>it using the following command:<pre class="source-code">
<strong class="bold">$ ansible-playbook aws.yaml</strong></pre></li> </ol>
<p>This command will return something like the following:</p>
<pre class="source-code">
<strong class="bold">PLAY [localhost] **********************************************************************************</strong>
<strong class="bold">TASK [Gathering Facts] ****************************************************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [Ensure key pair is present] *****************************************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [Gather information of the EC2 VPC net in eu-west-1] *****************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [Gather information of the EC2 VPC subnet in eu-west-1] **************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [Ensure wssg Security Group is present] ******************************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [Setup instance] *****************************************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">PLAY RECAP ****************************************************************************************</strong>
<strong class="bold">localhost : ok=6 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</strong></pre> <p>If you check the<a id="_idIndexMarker672"/> AWS console, you will see that you now have one machine up and running!</p>
<p>To launch a virtual machine in AWS, we need a few things to be in place:</p>
<ul>
<li>An SSH key pair</li>
<li>A network</li>
<li>A subnetwork</li>
<li>A security group</li>
</ul>
<p>By default, a <a id="_idIndexMarker673"/>network and a subnetwork are already available in your account, but you need to retrieve their IDs.</p>
<p>That’s why we started by uploading the public part of an SSH key pair to AWS, queried for information about the network and the subnetwork, then ensured that the security group we wanted to use was present, and lastly triggered the machine build.</p>
<p>Now you have learned how to automate against AWS, you’ll learn how to complement G<a id="_idTextAnchor488"/>CP with automation.</p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor489"/>Complementing Google Cloud Platform with automation</h1>
<p>Another <a id="_idIndexMarker674"/>global cloud provider is <a id="_idIndexMarker675"/>Google, with its GCP. Google’s approach to the cloud is different from other providers’ since Google does not try to simulate the data center in a virtual environment. This is because Google wants to rethink the concept of c<a id="_idTextAnchor490"/>loud provision in order to simplify it.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor491"/>Installation</h2>
<p>You <a id="_idIndexMarker676"/>need to ensure that you have the proper components installed before you can start using GCP with Ansible. More specifically, you will require the Python <code>requests</code> and <code>google-auth</code> modules. To install these modules, run the following command:</p>
<pre class="console">
$ pip install requests google-auth</pre> <p>We can now proceed to install the Google Cloud collection:</p>
<pre class="console">
$ ansible-galaxy collection install google.cloud</pre> <p>Now that you have all the dependencies present, y<a id="_idTextAnchor492"/>ou can start the authentication process.</p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor493"/>Authentication</h2>
<p>There are <a id="_idIndexMarker677"/>three different approaches to obtaining a working set of credentials in GCP:</p>
<ul>
<li>The service account using environment variables</li>
<li>The service account using a JSON file</li>
<li>The machine account</li>
</ul>
<p>The first two approaches are the suggested ones in the majority of cases since the third applies only to circumstances where Ansible is run directly within the GCP environment.</p>
<p>The first approach is, once you have created the service account, set the following environmental variables:</p>
<ul>
<li><code>GCP_AUTH_KIND</code></li>
<li><code>GCP_SERVICE_ACCOUNT_EMAIL</code></li>
<li><code>GCP_SERVICE_ACCOUNT_FILE</code></li>
<li><code>GCP_SCOPES</code></li>
</ul>
<p>Now, Ansible can use the proper service account.</p>
<p>If you prefer not to use environment variables, you can download a service account file from the GCP interface directly. For convenience’s sake, we are going to do this and we are going to save the file as <code>~/sa.json</code>.</p>
<p>The third approach is by far the easiest since Ansible will be able to auto-detect the machine account if you <a id="_idTextAnchor494"/>are running it in a GCP instance.</p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor495"/>Creating your first machine</h2>
<p>Now<a id="_idIndexMarker678"/> that Ansible is able to connect to your GCP environment, you can proceed with the actual Playbook:</p>
<ol>
<li>Create the <code>gce.yaml</code> Playbook with the following content:<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: create a instance
    google.cloud.gcp_compute_instance:
      name: TestMachine
      machine_type: n1-standard-1
      disks:
      - auto_delete: 'true'
        boot: 'true'
        initialize_params:
          source_image: family/centos-stream-9
          disk_size_gb: 10
      zone: eu-west1-c
      auth_kind: serviceaccount
      service_account_file: "~/sa.json"
      state: present</pre></li> <li>Execute it <a id="_idIndexMarker679"/>with the following command:<pre class="source-code">
<strong class="bold">$ ansible-playbook gce.yaml</strong></pre></li> </ol>
<p>This will create an output like the following one:</p>
<pre class="source-code">
<strong class="bold">PLAY [localhost] **********************************************************************************</strong>
<strong class="bold">TASK [Gathering Facts] ****************************************************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [create a instance] **************************************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">PLAY RECAP ****************************************************************************************</strong>
<strong class="bold">localhost : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</strong></pre> <p>As for the <a id="_idIndexMarker680"/>AWS example, running a machine in the cloud is very easy with Ansible.</p>
<p>In the case of GCE, you don’t need to set up the networks beforehand since the GCE defaults will kick in and provide a functional machine either way.</p>
<p>As for AWS, the list of modules you can use is huge. You can find the full list at <a href="https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml">https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml</a>.</p>
<p>Now that you have learned how to complement GCP with automation, you will learn how to seamlessl<a id="_idTextAnchor496"/>y perform automation integration with Azure.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor497"/>Seamless automation integration with Azure</h1>
<p>Another<a id="_idIndexMarker681"/> global cloud that Ansible can manage is Microsoft Azure.</p>
<p>Azure integration, like AWS integration, requires quite a few steps to be performed in Playbooks.</p>
<p>The first thing you will need to do is set up the authentication so that Ans<a id="_idTextAnchor498"/>i<a id="_idTextAnchor499"/>ble is allowed to control your Azure account.</p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor500"/>Installation</h2>
<p>To let <a id="_idIndexMarker682"/>Ansible manage the Azure cloud, you need to install the Azure SDK for Python. Do this by executing the following command:</p>
<pre class="console">
$ pip install -r ~/.ansible/collections/ansible_collections/azure/azcollection/requirements-azure.txt</pre> <p>We can now proceed to install the Azure collection:</p>
<pre class="console">
$ ansible-galaxy collection install azure.azcollection</pre> <p>Now that you have all the dependencies pres<a id="_idTextAnchor501"/>ent, you can start the authentication process.</p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor502"/>Authentication</h2>
<p>There are different <a id="_idIndexMarker683"/>ways to ensure that Ansible is able to manage Azure for you, based on the way your Azure account is set up, but they can all be configured in the <code>~/.</code><code>azure/credentials</code> file.</p>
<p>If you want Ansible to use the principal credentials for the Azure account, you will need to create a file that resembles the following:</p>
<pre class="source-code">
[default]
subscription_id = [YOUR_SUBSCIRPTION_ID_HERE]
client_id = [YOUR_CLIENT_ID_HERE]
secret = [YOUR_SECRET_HERE]
tenant = [YOUR_TENANT_HERE]</pre> <p>If you prefer to use Active Directory with a username and password, you can do something like this:</p>
<pre class="source-code">
[default]
ad_user = [YOUR_AD_USER_HERE]
password = [YOUR_AD_PASSWORD_HERE]</pre> <p>Lastly, you<a id="_idIndexMarker684"/> can opt for an Active Directory login with ADFS. In this case, you’ll need to set some additional parameters. You’ll end up with something like this:</p>
<pre class="source-code">
[default]
ad_user = [YOUR_AD_USER_HERE]
password = [YOUR_AD_PASSWORD_HERE]
client_id = [YOUR_CLIENT_ID_HERE]
tenant = [YOUR_TENANT_HERE]
adfs_authority_url = [YOUR_ADFS_AUTHORITY_URL_HERE]</pre> <p>The same parameters can be passed as parameters or as<a id="_idTextAnchor503"/> environmental variables if it makes more sense.</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor504"/>Creating your first machine</h2>
<p>Now that <a id="_idIndexMarker685"/>Ansible is able to connect to your Azure environment, you can proceed with the actual Playbook:</p>
<ol>
<li>Create the <code>azure.yaml</code> Playbook with the following content:<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Ensure the Storage Account is present
    azure.azcollection.azure_rm_storageaccount:
      resource_group: Testing
      name: mysa
      account_type: Standard_LRS
  - name: Ensure the Virtual Network is present
    azure.azcollection.azure_rm_virtualnetwork:
      resource_group: Testing
      name: myvn
      address_prefixes: "10.10.0.0/16"
  - name: Ensure the Subnet is present
    azure.azcollection.azure_rm_subnet:
      resource_group: Testing
      name: mysn
      address_prefix: "10.10.0.0/24"
      virtual_network: myvn
  - name: Ensure that the Public IP is set
    azure.azcollection.azure_rm_publicipaddress:
      resource_group: Testing
      allocation_method: Static
      name: myip
  - name: Ensure a Security Group allowing SSH is present
    azure.azcollection.azure_rm_securitygroup:
      resource_group: Testing
      name: mysg
      rules:
      - name: SSH
        protocol: Tcp
        destination_port_range: 22
        access: Allow
        priority: 101
        direction: Inbound
  - name: Ensure the NIC is present
    azure.azcollection.azure_rm_networkinterface:
      resource_group: Testing
      name: mynic
      virtual_network: myvn
      subnet: mysn
      public_ip_name: myip
      security_group: mysg
  - name: Ensure the Virtual Machine is present
    azure.azcollection.azure_rm_virtualmachine:
      resource_group: Testing
      name: myvm01
      vm_size: Standard_D1
      storage_account: mysa
      storage_container: myvm01
      storage_blob: myvm01.vhd
      admin_username: admin
      admin_password: <strong class="bold">Password!</strong>
      network_interfaces: mynic
      image:
        offer: CentOS
        publisher: OpenLogic
        sku: '8.4'
        version: latest</pre></li> <li>We <a id="_idIndexMarker686"/>can run it with the following command:<pre class="source-code">
<strong class="bold">$ ansible-playbook azure.yaml</strong></pre></li> </ol>
<p>This will return something like the following:</p>
<pre class="source-code">
<strong class="bold">PLAY [localhost] **********************************************************************************</strong>
<strong class="bold">TASK [Gathering Facts] ****************************************************************************</strong>
<strong class="bold">ok: [localhost]</strong>
<strong class="bold">TASK [Ensure the Storage Account is present] ******************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">TASK [Ensure the Virtual Network is present] ******************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">TASK [Ensure the Subnet is present] ***************************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">TASK [Ensure that the Public IP is set] ***********************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">TASK [Ensure a Security Group allowing SSH is present] ********************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">TASK [Ensure the NIC is present] ******************************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">TASK [Ensure the Virtual Machine is present] ******************************************************</strong>
<strong class="bold">changed: [localhost]</strong>
<strong class="bold">PLAY RECAP ****************************************************************************************</strong>
<strong class="bold">localhost : ok=8 changed=7 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</strong></pre> <p>You now have your machine running in the Azure cloud!</p>
<p>As you<a id="_idIndexMarker687"/> can see, in Azure, you will need all the resources to be ready before you can issue the machine creation command. This is the reason you create the storage account, the virtual network, the subnet, the public IP, the security group, and the NIC first, and only at that point, the machine itself.</p>
<p>Outside the three major players in the market, there are many additional cloud options. Many of which,<a id="_idTextAnchor505"/><a id="_idTextAnchor506"/><a id="_idTextAnchor507"/><a id="_idTextAnchor508"/><a id="_idTextAnchor509"/> including many private clouds, leverage OpenStack.</p>
<h1 id="_idParaDest-182"><a id="_idTextAnchor510"/>Using Ansible to orchestrate OpenStack</h1>
<p>As opposed to<a id="_idIndexMarker688"/> the various cloud services we just discussed, all of which are public clouds, OpenStack allows you to create your own (private) cloud.</p>
<p>Private clouds have the disadvantage that they expose more complexity to the administrator and the user, but this is the reason they <a id="_idTextAnchor511"/>can be customized to suit an organization perfectly.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor512"/>Installation</h2>
<p>The first<a id="_idIndexMarker689"/> step to being able to control an OpenStack cluster with Ansible is to ensure that <code>openstacksdk</code> is installed.</p>
<p>To install <code>openstacksdk</code>, you need to execute the following command:</p>
<pre class="console">
$ pip install openstacksdk</pre> <p>We can now proceed to install the OpenStack collection:</p>
<pre class="console">
$ ansible-galaxy collection install openstack.cloud</pre> <p>Now that you have installed <code>op<a id="_idTextAnchor513"/>enstacksdk</code>, you can start the authentication process.</p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor514"/>Authentication</h2>
<p>Since <a id="_idIndexMarker690"/>Ansible will use <code>openstacksdk</code> as its backend, you will need to ensure that <code>openstacksdk</code> is able to connect to the OpenStack cluster.</p>
<p>To do this, you can change the <code>~/.config/openstack/clouds.yaml</code> file, ensuring that there is a configuration for the cloud you want to use it for.</p>
<p>An example of what a correct OpenStack credentials set could look like is as follows:</p>
<pre class="source-code">
clouds:
 test_cloud:
 region_name: MyRegion
 auth:
 auth_url: http://[YOUR_AUTH_URL_HERE]:5000/v2.0/
 username: [YOUR_USERNAME_HERE]
 password: [YOUR_PASSWORD_HERE]
 project_name: myProject</pre> <p>It’s also<a id="_idIndexMarker691"/> possible to set a different config file location if you are willing to export the <code>OS_CLIENT_CONFIG_FILE</code> variable as an environment variable.</p>
<p>Now that you have set up the required security so that Ansible can m<a id="_idTextAnchor515"/>anage your cluster, you can create your first Playbook.</p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor516"/>Creating your first machine</h2>
<p>Since <a id="_idIndexMarker692"/>OpenStack is very flexible, many of its components can have many different implementations, which means they may differ slightly in terms of their behavior. To be able to accommodate all the various cases, the Ansible modules that manage OpenStack tend to have a lower level of abstraction compared to the ones for many public clouds.</p>
<p>So, to create a machine, you will need to ensure that the public SSH key is known to OpenStack and ensure that the OS image is present as well. After doing this, you can set up networks, subnetworks, and routers to ensure that the machine we are going to create can communicate via the network. Then, you can create the security group and its rules so that the machine can receive connections (pings and SSH traffic, in this case). Finally, you can create a machine instance.</p>
<p>To complete all the steps just described, you need to create a file called <code>openstack.yaml</code> with the following content:</p>
<pre class="source-code">
---
- hosts: localhost
  tasks:
  - name: Ensure the SSH key is present on OpenStack
    openstack.cloud.keypair:
      state: present
      name: ansible_key
      public_key_file: "{{ '~' | expanduser }}/.ssh/id_rsa.pub"
  - name: Ensure we have a CentOS image
    ansible.builtin.get_url:
      url: http://cloud.centos.org/centos/9-stream/x86_64/images/CentOS-Stream-GenericCloud-9-20230424.0.x86_64.qcow2
      dest: /tmp/CentOS-Stream-GenericCloud-9-20230424.0.x86_64.qcow2
  - name: Ensure the CentOS image is in OpenStack
    openstack.cloud.image:
      name: centos
      container_format: bare
      disk_format: qcow2
      state: present
      filename: /tmp/CentOS-Stream-GenericCloud-9-20230424.0.x86_64.qcow2
  - name: Ensure the Network is present
    openstack.cloud.network:
      state: present
      name: mynet
      external: False
      shared: False
      register: net_out
  - name: Ensure the Subnetwork is present
    openstack.cloud.subnet:
      state: present
      network_name: "{{ net_out.id }}"
      name: mysubnet
      ip_version: 4
      cidr: 192.168.0.0/24
      gateway_ip: 192.168.0.1
      enable_dhcp: yes
      dns_nameservers:
      - 8.8.8.8
  - name: Ensure the Router is present
    openstack.cloud.router:
      state: present
      name: myrouter
      network: nova
      external_fixed_ips:
      - subnet: nova
      interfaces:
      - mysubnet
  - name: Ensure the Security Group is present
    openstack.cloud.security_group:
      state: present
      name: mysg
  - name: Ensure the Security Group allows ICMP traffic
    openstack.cloud.security_group_rule:
      security_group: mysg
      protocol: icmp
      remote_ip_prefix: 0.0.0.0/0
  - name: Ensure the Security Group allows SSH traffic
    openstack.cloud.security_group_rule:
      security_group: mysg
      protocol: tcp
      port_range_min: 22
      port_range_max: 22
      remote_ip_prefix: 0.0.0.0/0
  - name: Ensure the Instance exists
    openstack.cloud.server:
      state: present
      name: myInstance
      image: centos
      flavor: m1.small
      security_groups: mysg
      key_name: ansible_key
      nics:
      - net-id: "{{ net_out.id }}"</pre> <p>Now, you <a id="_idIndexMarker693"/>can run it as follows:</p>
<pre class="console">
$ ansible-playbook openstack.yaml</pre> <p>The output should be as follows:</p>
<pre class="console">
PLAY [localhost] **********************************************************************************
TASK [Gathering Facts] ****************************************************************************
ok: [localhost]
TASK [Ensure the SSH key is present on OpenStack] *************************************************
changed: [localhost]
TASK [Ensure we have a CentOS image] **************************************************************
changed: [localhost]
TASK [Ensure the CentOS image is in OpenStack] ****************************************************
changed: [localhost]
TASK [Ensure the Network is present] **************************************************************
changed: [localhost]
TASK [Ensure the Subnetwork is present] ***********************************************************
changed: [localhost]
TASK [Ensure the Router is present] ***************************************************************
changed: [localhost]
TASK [Ensure the Security Group is present] *******************************************************
changed: [localhost]
TASK [Ensure the Security Group allows ICMP traffic] **********************************************
changed: [localhost]
TASK [Ensure the Security Group allows SSH traffic] ***********************************************
changed: [localhost]
TASK [Ensure the Instance exists] *****************************************************************
changed: [localhost]
PLAY RECAP ****************************************************************************************
localhost : ok=11 changed=10 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0</pre> <p>As you can see, this process was longer than the public cloud ones we covered. However, you <a id="_idIndexMarker694"/>did get to upload the image that you wanted to run, which is something many c<a id="_idTextAnchor517"/>louds do not allow (or allow with very complex processes).</p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor518"/>Summary</h1>
<p>In this chapter, you learned how to automate tasks, such as managing Docker and Podman containers and managing Kubernetes objects. You also explored the modules that can help you automate cloud environments, such as AWS, GCP, Azure, and OpenStack. You also learned about the different approaches various cloud providers use, including their defaults and the parameters that you will always need to add.</p>
<p>Now that you have an understanding of how Ansible interacts with the cloud, you can immediately start to automate your cloud workflows. Also, remember to check the documentation in the <em class="italic">Further reading</em> section to take a look at all the cloud modules that Ansible supports and their options.</p>
<p>In the next chapter, you wil<a id="_idTextAnchor519"/>l learn how to troubleshoot and create testing strategies.</p>
<h1 id="_idParaDest-187"><a id="_idTextAnchor520"/>Questions</h1>
<ol>
<li>Which of the following is NOT a GKE Ansible module?<ol><li><code>google.cloud.gcp_container_cluster</code></li><li><code>google.cloud.gcp_container_node_pool</code></li><li><code>google.cloud.gcp_container_node_pool_info</code></li><li><code>google.cloud.gcp_container_node_pool_count</code></li><li><code>google.cloud.gcp_container_cluster_info</code></li></ol></li>
<li>True or false: In order to manage containers in Kubernetes, you need to add <code>k8s_namespace</code> in the settings section.<ol><li>True</li><li>False</li></ol></li>
<li>True or false: When working with Azure, instances require a <strong class="bold">Network Interface Controller</strong> (<strong class="bold">NIC</strong>) to be able to connect to the network.<ol><li>True</li><li>False</li></ol></li>
<li>True or false: When working with AWS, it’s necessary to create a <a id="_idTextAnchor521"/>Security Group before creating an EC2 instance.<ol><li>True</li><li>False</li></ol></li>
</ol>
<h1 id="_idParaDest-188"><a id="_idTextAnchor522"/>Further reading</h1>
<ul>
<li>More Docker containers: <a href="https://docs.ansible.com/ansible/latest/collections/community/docker/index.xhtml%0D">https://docs.ansible.com/ansible/latest/collections/community/docker/index.xhtml</a></li>
<li>More Podman modules: <a href="https://docs.ansible.com/ansible/latest/collections/containers/podman/index.xhtml%0D">https://docs.ansible.com/ansible/latest/collections/containers/podman/index.xhtml</a></li>
<li>More Kubernetes modules: <a href="https://docs.ansible.com/ansible/latest/collections/kubernetes/core/index.xhtml%0D">https://docs.ansible.com/ansible/latest/collections/kubernetes/core/index.xhtml</a></li>
<li>More AWS modules:<ul><li><a href="https://docs.ansible.com/ansible/latest/collections/amazon/aws/index.xhtml">https://docs.ansible.com/ansible/latest/collections/amazon/aws/index.xhtml</a></li><li><a href="https://docs.ansible.com/ansible/latest/collections/community/aws/index.xhtml">https://docs.ansible.com/ansible/latest/collections/community/aws/index.xhtml</a></li></ul></li>
<li>More GCP modules: <a href="https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml%0D">https://docs.ansible.com/ansible/latest/collections/google/cloud/index.xhtml</a></li>
<li>More Azure modules: <a href="https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.xhtml%0D">https://docs.ansible.com/ansible/latest/collections/azure/azcollection/index.xhtml</a></li>
<li>More OpenStack modules: <a href="https://docs.ansible.com/ansible/latest/collections/openstack/cloud/index.xhtml">https://docs.ansible.com/ansible/latest/collections/openstack/cloud/index.xhtml</a></li>
</ul>
</div>
</div></body></html>