<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Monitoring</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>So far, we have seen a large number of tools that we can use as DevOps engineers in our company to enhance our capabilities. Now we are able to provision servers with Ansible, create Kubernetes clusters on Google Cloud Platform, and set up a delivery pipeline for our microservices. We have also dived deep into how Docker works and how we should organize our company to be a successful delivering software.</p>
<p>In this chapter, we are going to take a look at the <strong>missing piece of the puzzle</strong>: monitoring. Usually overlooked, monitoring is, in my opinion, a key component of a successful DevOps company. Monitoring is the first line of defense against problems. In <a href="127a7b5f-4bd7-4290-bea0-3e8db867e4af.xhtml">Chapter 8</a>, <em>Release Management â€“ Continuous Delivery</em>, we talked about how we should shift our focus toward being able to fix the arising problems rather than spending a huge amount of resources in trying to prevent them:</p>
<div class="packt_quote">20 percent of your time will create 80 <span>percent</span> of the functionality. The other 20 <span>percent</span> is going to cost you 80 <span>percent</span> of your time.</div>
<p>This non-written rule dominates the world. With monitoring, we can bend this rule to live comfortably with 20 <span>percent</span> of unknown outcomes as we are able to identify the problems quite quickly.</p>
<p>We will review some of the tools to monitor software, but our focus will be on Stackdriver, as it is the monitoring solution from Google Cloud Platform that, out of the box, provides us with a fairly comprehensive set of tools to deal with the flaws in our systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of monitoring</h1>
                </header>
            
            <article>
                
<p>In the SRE book from Google, there are two types of monitoring defined:</p>
<ul>
<li>BlackBox monitoring</li>
<li>WhiteBox monitoring</li>
</ul>
<p>This is generally accepted by everyone, leading to a solid amount of tools that are clearly differentiated around whitebox and blackbox monitoring.</p>
<p>One of the best comparisons I've ever heard on whitebox versus blackbox monitoring is the diagnosis of a bone fracture. When you first go to the doctor, he/she only has access to your blackbox metrics:</p>
<ul>
<li>Does the area have any bump?</li>
<li>Is it painful on movement?</li>
</ul>
<p>Then, once the initial diagnoses has been pronounced, the next step is getting X-rays from the area. Now we can confirm whether the bone is broken and, if it is, what is the impact in the system. The X-rays are the WhiteBox monitoring that the doctor is using.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Blackbox monitoring</h1>
                </header>
            
            <article>
                
<p>Whitebox monitoring is the type of monitoring that observes a system from outside without having to look into how the system is built. These metrics are the first ones that impact the users and the first external symptoms that something is going wrong in our application or server.</p>
<p>Among the metrics that can be used for blackbox monitoring, we can find the following ones:</p>
<ul>
<li>Latency</li>
<li>Throughput</li>
</ul>
<p>These two metrics are the holy grail of blackbox monitoring.</p>
<p>The latency is, by definition, how long takes our system to respond. If we are looking at an HTTP server, from the very first time that we sent the request to the time when the server on the other side of the line replies is what we understand as latency. This metric is a fairly interesting one because it is the absolute truth about how the users see our system: the bigger the latency is, the worse the experience they get.</p>
<p>Throughput is extremely related to the latency. Basically, it is the number of requests that our software can serve per time unit, usually per second. This measure is a critical one for capacity planning, and you are discouraged to measure it in real time in a running system, as it pushes a lot of load through the system, which is surely going to affect the response time for live users. In general, throughput is measured at the performance testing stage of our application, which might be tricky:</p>
<ul>
<li>The hardware for testing has to match production</li>
<li>The dataset for the database has to be similar to production</li>
</ul>
<p>The performance testing step is usually overlooked by many of the companies as it is fairly expensive. That is why preproduction is usually used for capacity testing in order to guess the amount of hardware needed in production. Nowadays, this is less problematic, as with auto scaling groups in the cloud infrastructure, it becomes less of a problem as the infrastructure is going to scale on its own when needed.</p>
<p>As you can see, these metrics are fairly simple to understand, and even though they play a key role in the error response time, they might not be the first indicators of problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Whitebox monitoring</h1>
                </header>
            
            <article>
                
<p>Whitebox monitoring, as the name indicates, is the monitoring that needs to know about how the system is built in order to raise alerts on certain events happening inside of our application or infrastructure. These metrics are quite fine-grained (unlike blackbox monitoring), and once we have been alerted, they are the answer to the main questions of a postmortem analysis:</p>
<ul>
<li>Where is the problem?</li>
<li>What is causing the problem?</li>
<li>Which flows are affected?</li>
<li>What can we do to avoid this in future?</li>
</ul>
<p>Among other metrics, these are a fairly interesting set of examples:</p>
<ul>
<li>Function execution time</li>
<li>Errors per time unit</li>
<li>Requests per time unit</li>
<li>Memory usage</li>
<li>CPU usage</li>
<li>Hard drive usage</li>
<li>I/O operations per time unit</li>
</ul>
<p>As you can see, there is an endless number of whitebox metrics to ensure the stability of our system. There are almost too many, so we usually need to pick the right ones in order to avoid the noise.</p>
<p>An important detail here is the fact that when a blackbox monitoring metric gives an abnormal reading, there is always a whitebox metric that can be used to diagnose, but it is not true the other way around. A server can have a spike in the memory usage due to an internal problem without impacting the users.</p>
<p>One of the most important artifacts in whitebox monitoring are the logging files. These files are the ordered chain of events happening in our software, and usually, they are the first line of attack to diagnose problems related to our software. The main problem with log files is the fact that they are stored on production servers and we should not access them on a regular basis just to check the log files as it is a security threat on its own. It only takes an open terminal to a server forgotten by someone to give access rights to the wrong person.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring third-party tools</h1>
                </header>
            
            <article>
                
<p>Monitoring is usually a good candidate to involve third-party companies. It requires a fair amount of redundancy in the systems in order to keep the monitoring active, which is primordial in order to ensure that we are not blind to what is happening in our system.</p>
<p>Another positive aspect of using third-party apps for monitoring is the fact that they don't live in the same data center, and if they do (usually AWS), their redundancy is enough to ensure stability.</p>
<p>In this section, we are going to take a look at three tools in particular:</p>
<ul>
<li>Pingdom</li>
<li>Logentries</li>
<li>AppDynamics</li>
</ul>
<p>That doesn't mean that they are the best or the only tools in the market. There are other interesting alternatives (for example, New Relic instead of AppDynamics) that are worth exploring, but in this case, we are going to focus on Stackdriver, the monitoring solution for Google Cloud Platform, due to a number of factors:</p>
<ul>
<li>It integrates very well with Google Cloud Platform</li>
<li>It has a very interesting free tier</li>
<li>The alerting systems are one of the most advanced systems you can find in the market</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pingdom</h1>
                </header>
            
            <article>
                
<p>Pingdom is a tool used to measure the latency of our servers from different sides of the world. As you can see, if you have worked in a 24/7 company, latency across the globe varies a lot depending on where our customers are in relation to our data centers. As a matter of curiosity, if our server is in Europe, someone from Australia will have around 2-3 seconds extra on the latency.</p>
<p>Pingdom has servers spread across the globe to monitor how our users see the system and take adequate measures to solve the problem (for example, spawning a new data center closer to them).</p>
<p>You can register in Pingdom for free with a 14-days trial, but you need to enter a credit card (don't worry; they will advise you when your trial is over so you can cancel the plan if you don't want to continue with it).</p>
<p>Take a look at the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/71c60795-f39a-4827-ae2a-ca8d4cc59ef0.jpg"/></div>
<p>As you can see, after specifying hosts, Pingdom will start issuing requests to the specified URL and measuring the response time from different parts of the world.</p>
<p>Lately, Pingdom has included fairly interesting capabilities: now it can read custom metrics through an endpoint in order to monitor an endless amount of data:</p>
<ul>
<li>Free space on the disks</li>
<li>Used amount of RAM</li>
<li>Stock levels (yes, you can send any number of items you have left in your warehouse to Pingdom)</li>
</ul>
<p>In general, I have used Pingdom quite successfully in the past to measure the latency in my servers and improve the experience of the users by distributing the data centers strategically across the globe to mitigate this problem. One of the most interesting insights that Pingdom (and similar tools) can give you is that your site might be down due to network splits on the internet or failures in some DNS servers (in the latter case, it is not really down but Pingdom and users won't be able to reach it).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logentries</h1>
                </header>
            
            <article>
                
<p>Logentries is one of these companies that makes your life much easier when dealing with a large number of logs. It basically solves one problem that was an impediment for few years: it aggregates all the logs from your system in a common place with access controls and a more than decent interface that allows you to quickly search through big datasets.</p>
<p>Creating an account is free, and it provides 30 days of usage with some limits that are more than enough for testing and evaluation.</p>
<p>Go to <a href="https://logentries.com/">https://logentries.com/</a> and create an account. Once you are logged in, the first screen should be similar to what is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9206f600-80f6-49c8-baeb-25f49c96e4bd.png"/></div>
<p>As you can see, there are explanations for how to configure the log aggregation in an endless number of platforms: you can monitor from systems to libraries going through a number of platforms (AWS, Docker, and so on).</p>
<p>Agents are usually a good choice for two reasons:</p>
<ul>
<li>They do not create coupling in your application (the agent reads the log files and sends them to the Logentries servers)</li>
<li>They push the complexity to a third-party software</li>
</ul>
<p>But there are also other interesting options, such as manual log aggregation. In this case, we are going to demonstrate how to use a custom logger to send logs from a very simple Node.js application to Logentries. Create a folder called <kbd>logentries</kbd> and execute the following command:</p>
<pre><strong>npm init</strong></pre>
<p>This assumes that Node.js is installed on your system, so if it is not, download any version of Node.js from <a href="https://nodejs.org/en/">https://nodejs.org/en/</a> and install it.</p>
<p>Now we need to install the Logentries library for Node.js. Logentries provides support for a number of platforms, but support for Node.js is particularly good. Execute the following command:</p>
<pre><strong>npm install --save le_node</strong></pre>
<p>Once it is finished, we should have the required library installed. Now it is time to create a simple <kbd>Node.js</kbd> program to demonstrate how it works, but first, we need to create a service token. On the following screen, click on <span class="packt_screen">Manual</span> and fill in the form, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a3e83f22-b897-479e-aec9-3ac93bc81282.png"/></div>
<p>Logentries is able to understand many different types of logs, but it really shines on JSON logs. We don't need to specify any type of logs for it to catch them, so leave this option empty and give a name to the log and the set. Once you click on <span class="packt_screen">Create Log Token</span>, the token should be displayed after the button. Save it for later; you are going to need it.</p>
<p>Now if we go to the main dashboard, we should be able to see our log set called <span class="packt_screen">Testing Set</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e34f1ba0-b38d-4145-89d9-0362d8b130bd.png"/></div>
<p>Now it is time to send some data:</p>
<pre>const Logger = require('le_node')<br/>const logger = new Logger({token: '5bffdd28-fb7d-46b6-857a-c3a7dfed5410'})<br/><br/>logger.info('this is a test message')<br/>logger.err('this is an error message')<br/>logger.log('debug', {message: 'This is a json debug message', json: true})</pre>
<p>This script is enough to send data to Logentries. Be aware that the token specified has to be replaced by the token obtained in the previous step. Save it as <kbd>index.js</kbd> and execute it a few times:</p>
<pre><strong>node index.js</strong></pre>
<p>Once you have executed it a few times, head back to Logentries and open <span class="packt_screen">Test log</span> inside <span class="packt_screen">Testing Set</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a75d0030-4bea-4b68-8eed-069b93254b0e.png"/></div>
<p>Now you can see our logs in Logentries being aggregated. There are some interesting things in Logentries that have been improving with time:</p>
<ul>
<li>The UI is quite slick</li>
<li>The search mechanisms are very powerful</li>
<li>Logentries is able to live stream the logs in real time (more or less)</li>
</ul>
<p>Regarding search mechanisms, Logentries has developed something called <strong>LEQL</strong>, which is basically a language designed by Logentries to search for certain events using JSON fields or just plain text searching. You can find more information about it at <a href="https://docs.logentries.com/v1.0/docs/search/">https://docs.logentries.com/v1.0/docs/search/</a>.</p>
<p>The other interesting feature is the live tailing of the logs. Let's test that feature. Create another file in the project called <kbd>livetail.js</kbd> and add the following code:</p>
<pre>const Logger = require('le_node')<br/><br/>const logger = new Logger({token: '5bffdd28-fb7d-46b6-857a-b3a7dfed5410'})<br/><br/>setInterval(() =&gt; {<br/> logger.info(`This is my timed log on ${Date.now()}`)<br/>}, 500)</pre>
<p>It does not need explanation: a function that gets executed every 500 milliseconds and sends a log line to Logentries.</p>
<p>Execute the script:</p>
<pre><strong>node livetail.js</strong></pre>
<p>It might look like nothing is happening, but things are actually happening. Go back to Logentries and click on the <span class="packt_screen">Start live tail</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/92fdd2ef-b97f-4e1f-8a25-4ada23391abc.png"/></div>
<p>After a couple of seconds (or less), the logs will start flowing. This can be done on any log file that is stored in Logentries, and it is a fairly interesting mechanism to debug problems in our servers.</p>
<p>Logentries is also able to send alerts to a certain email. You can configure it to alert your team on the following:</p>
<ul>
<li>Exceptions</li>
<li>Patterns</li>
<li>Lack of logs</li>
<li>Increased activity</li>
</ul>
<p>This alert is usually the first indicator of problems on a system, so if you want an early response to errors, the best practice is to try to reduce the noise up to a point where alerts are not missed and the false positives are reduced to a minimum.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AppDynamics</h1>
                </header>
            
            <article>
                
<p>AppDynamics was the king for a while (as it was the only real option on monitoring). It is a very curated software that allows you to explore what is going on in your software and servers: exceptions, requests per time unit, and CPU usage are among many other metrics that AppDynamics can capture for us.</p>
<p>It also captures interactions with the third-party endpoints: if our software is consuming a third-party API, AppDynamics will know about it and display the calls in a dashboard similar to the next screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/907bb61f-fb55-430f-96c0-bf5544feb3e1.png"/></div>
<p>AppDynamics is quite advanced regarding proactive measures. One of these measures is automated actions, such as restarting a Tomcat server or reloading a service running on the server on certain events. For example, if we deploy a new version of our Java application that is having a problem with the PermGen space (this is not the case anymore in Java 8+), it is usually very tricky to fix as the problem comes from many classes loaded by the JVM and only manifests a few hours after the deployment. In some cases, we can instruct AppDynamics to restart the application when the usage reaches 80 percent and more of the assigned total so that instead of crashing badly and not being able to serve to any customers, we only have a few dropouts every few hour getting an air balloon to act and fix the problem.</p>
<p>AppDynamics works with what is known an agent model. An application needs to be installed on your server (for example, Tomcat) in order to collect metrics that are sent to a centralized server to process and create the pertinent dashboards and trigger the workflows. The interesting part of AppDynamics is that if you don't feel comfortable sending data to a third-party (which is usually a security requirement for companies handling high-profile data), they provide an on-premises version of the dashboard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stackdriver</h1>
                </header>
            
            <article>
                
<p>Up until now, we have visited a set of tools from different third-parties but now we are going to take a look at Stackdriver. Stackdriver was a cloud monitoring solution acquired by Google and integrated (not fully) into Google Cloud Platform. This is an important step for GCP, as being able to provide an integrated monitoring solution is something that's pretty much mandatory nowadays.</p>
<p>With Stackdriver, we are not only able to monitor applications, but also Kubernetes clusters or even standalone VMs. As we will see, the integration is not yet as seamless as we would desire (it might be completed by the time you are reading this), but it is good enough to be considered a big player in the market.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring applications</h1>
                </header>
            
            <article>
                
<p>Stackdriver can monitor standalone applications by capturing metrics and logs. It has support for major platforms and libraries, so our technology choices should not be a concern. In this case, we are going to create a Node.js application for several reasons:</p>
<ul>
<li>It is easy to understand</li>
<li>The official examples are well documented for the Node.js version</li>
<li>Node.js is increasingly becoming a big platform for enterprise and startups</li>
</ul>
<p>The first thing we need to do is write a small Node.js application. Create a new folder and execute this command:</p>
<pre><strong>npm init</strong></pre>
<p>Follow the instructions on the screen and you should now have <kbd>package.json</kbd> in the folder that you just created. Now it is time to install the dependencies:</p>
<pre><strong>npm install --save @google-cloud/logging-bunyan @google-cloud/trace-agent express bunyan</strong></pre>
<p>We are going to use four libraries:</p>
<ul>
<li><strong>express</strong>: To handle the HTTP requests</li>
<li><strong>bunyan</strong>: To log our application activity</li>
</ul>
<p>The two libraries from Google are for interacting with Stackdriver:</p>
<ul>
<li><strong>logging-bunyan</strong>: This will send the logs from bunyan to Stackdriver</li>
<li><strong>trace-agent</strong>: This will trace the requests through our application</li>
</ul>
<p>Now let's create a simple application:</p>
<pre>require('@google-cloud/trace-agent').start()<br/>const express = require('express')<br/>const bunyan = require('bunyan')<br/>const LoggingBunyan = require('@google-cloud/logging-bunyan')<br/>const loggingBunyan = LoggingBunyan()<br/><br/>const log = bunyan.createLogger({<br/> name: "stackdriver",<br/> streams: [<br/> {stream: process.stdout},<br/> loggingBunyan.stream()<br/> ],<br/> level: 'info'<br/>})<br/><br/>const app = express()<br/><br/>app.get('/', (req, res) =&gt; {<br/> log.info(`request from ${req.connection.remoteAddress}`)<br/> res.send('Hello World!')<br/>})<br/><br/>app.listen(3000, () =&gt; {<br/> console.log('Listening in port 3000')<br/>})</pre>
<p>Now it is time to explain what the interesting parts of the code do:</p>
<ul>
<li>The first line enables the tracing for Stackdriver. It is very important that this line happen before anything else; otherwise, the tracing won't work. We'll see how amazing the tracing is.</li>
<li>In order to let Stackdriver collect logs, we need to add a stream to the bunyan logger, as shown in the code.</li>
</ul>
<p>Everything else is quite normal: an <kbd>Express.js</kbd> Node.js application that has a handler for the URL/replying with the classic Hello World.</p>
<p>There is one thing missing: there are no credentials to access the remote APIs. This is done on purpose as Google Cloud Platform has a very sophisticated system for handling credentials: basically, it will be handled for you.</p>
<p>Now, it is time to deploy our application. First, create a VM in Google Cloud Platform, as we have seen a few times in the previous chapters. A small one will suffice, but make sure that you allow HTTP traffic. Debian Stretch is a good choice as an operating system.</p>
<p>Once the machine is up, install Node.js, as shown in <a href="http://nodejs.org.">http://nodejs.org</a>.<a href="http://nodejs.org."/></p>
<p>Now we need to copy the code into our newly created machine. The best solution is to create a GitHub repository or use mine: <a href="https://github.com/dgonzalez/stackdriver">https://github.com/dgonzalez/stackdriver</a>.</p>
<p>By cloning it in our VM (don't forget to install Git first via <kbd>apt</kbd>), we just need to install the dependencies with the following command:</p>
<pre><strong>npm install</strong></pre>
<p>And we are good to go. Just run the application with the following command:</p>
<pre><strong>node index.js</strong></pre>
<p>Now go to the external IP of the machine (shown in Google Compute Engine) on port <kbd>3000</kbd>. In my case, this is <a href="http://35.195.151.10:3000/">http://35.195.151.10:3000/</a>.</p>
<p>Once we have done it, we should see Hello World in the browser and something similar to the following in the logs of our app:</p>
<pre>Listening in port 3000<br/>{"name":"stackdriver","hostname":"instance-3","pid":4722,"level":30,"msg":"request from ::ffff:46.7.23.229","time":"2017-09-18T01:50:41.483Z","v":0}</pre>
<p>If there are no errors, everything worked. In order to verify this, go to <a href="http://console.cloud.google.com">http://console.cloud.google.com</a> and open the <span class="packt_screen">Logging</span> section of Stackdriver.</p>
<div class="packt_infobox">Stackdriver is a different system from Google Cloud Platform; it might ask you to log in using a Google account.</div>
<p>Once you are there, you should see something similar to what is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/03f49c27-f2be-4088-a08b-4ee15ef61622.png"/></div>
<div class="packt_tip">Be aware that you have to select the section on the logs, in my case, GCE VM Instance, Instance-3.</div>
<p>This is exactly the log from your app uploaded to Google Cloud Platform with a bunch of very interesting information. You can play around by having different handlers for other URLs and different logging events, but the result is the same: all your logs will be aggregated here.</p>
<p>Now we can do this with Trace as well. Open the trace section of Google Cloud Platform under Stackdriver.</p>
<p>The screen should look similar to what is shown in the following screenshot (select the traces list <span>option</span>):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4f941972-0f9c-4d51-a960-6f139d553aaf.png"/></div>
<p>As you can see, there is a lot of useful information:</p>
<ul>
<li>The call stack</li>
<li>Duration</li>
<li>Start and finish time</li>
</ul>
<p>You can also play around, issue multiple calls, and get familiar yourself with how it works. Now we are going to modify our program to call a third-party API and see how Stackdriver is able to trace it:</p>
<pre>require('@google-cloud/trace-agent').start()<br/><br/>const express = require('express')<br/>const bunyan = require('bunyan')<br/>const LoggingBunyan = require('@google-cloud/logging-bunyan')<br/>const request = require('request')<br/><br/>const loggingBunyan = LoggingBunyan()<br/><br/>const URL = "https://www.googleapis.com/discovery/v1/apis"<br/><br/>const log = bunyan.createLogger({<br/> name: "stackdriver",<br/> streams: [<br/> {stream: process.stdout},<br/> loggingBunyan.stream()<br/> ],<br/> level: 'info'<br/>})<br/><br/>const app = express()<br/><br/>app.get('/', (req, res) =&gt; {<br/> log.info(`request from ${req.connection.remoteAddress}`)<br/> res.send('Hello World!')<br/>})<br/>app.get('/discovery', (req, res) =&gt; {<br/> request(URL, (error, response, body) =&gt; {<br/>   return res.send(body)<br/> })<br/>})<br/>app.listen(3000, () =&gt; {<br/> console.log('Listening in port 3000')<br/>})</pre>
<p>Now we are listing all the available APIs on Google by executing an HTTP GET into the <a href="https://www.googleapis.com/discovery/v1/apis.">https://www.googleapis.com/discovery/v1/apis</a> <span>URL.</span> Redeploy it into your VM in Google Cloud Platform and go to the endpoint/discovery of your VM. A big JSON payload will be presented on your screen, but the interesting part is happening under the hood. Go back to Stackdriver in the <span class="packt_screen">Trace list</span> section, and you'll see that there is a new trace being captured:</p>
<div class="CDPAlignCenter CDPAlign"><img height="314" width="660" src="assets/e204ee46-fdff-4a4f-9bbf-85d13fad6481.png"/></div>
<p>Here, you can see how our program contacted the remote API and that it took 68 seconds to reply.</p>
<p>Getting this type of information in real time is very powerful--if customers are getting a very large response time, we can immediately <span>see</span> what is happening inside of our application pretty much real-time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monitoring Kubernetes clusters</h1>
                </header>
            
            <article>
                
<p>Kubernetes covers 99 percent of the needs for any software company, but one part where it does not really shine is in embedded monitoring, leaving a space to be filled by third-parties. The main problem with Kubernetes comes from Docker: containers are ephemeral, so a common practice is to dump the logs into the standard output/error and use <em>syslogd</em> to gather them in a centralized location.</p>
<p>With Kubernetes, we have an added problem: the orchestrator on top of Docker needs to know how to fetch logs in order to make them available via the API or dashboard, so it is possible for the user to access them when required. But then there is another problem. Usually, logs are rotated on the basis of time and archived in order to avoid a log sprawl that can consume all the free space in our servers, preventing the application (and the OS) from functioning normally.</p>
<p>The best solution is to use an external system to aggregate logs and events inside the cluster so that we push the complexity to the side, allowing Kubernetes to focus on what it does best: orchestrate containers.</p>
<p>In this case, in order to integrate our cluster with Stackdriver in Google Cloud Platform, the only thing that we need to do is mark the two checkboxes in the cluster creation screen:</p>
<div class="CDPAlignCenter CDPAlign"><img height="609" width="447" src="assets/baf7c352-db8f-4c39-8c9b-536c8cdc520d.png"/></div>
<p>This will enable the monitoring across the different nodes in our cluster and improve the way in which we tackle problems happening in our applications. Click on <span class="packt_screen">Create</span> and allow the cluster to be provisioned (might take a few seconds or even minutes). It does not need to be a big cluster; just use the small size for the machines, and two VMs will be enough. In fairness, we will probably need to reduce the size during the load testing in order to speed up the alerting part.</p>
<div class="packt_infobox"><span>As we've seen in the previous section, with GKE monitoring active, Kubernetes also sends the logs to the Logging capabilities of Stackdriver, so you don't need to connect to the nodes to fetch the logs.</span></div>
<p>Once it is created, we are going to add monitoring from Stackdriver. The first thing that we need to do is open <span class="packt_screen">Monitoring</span> in the Stackdriver section of Google Cloud Platform. This will open a new site, the original Stackdriver site, which looks very similar to what is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/19caa285-4659-4d30-bf1f-6a762404e29f.png"/></div>
<p>Even though the UI might be a bit confusing in the beginning, after a bit of usage, it becomes clear that it is really hard to pack the huge amount of functionality that Stackdriver offers in a better UI. By default, we can see some metrics about our cluster (the bottom-right section of the image), but they are not very useful: we don't want to have someone looking at the metrics the full day in order to raise alerts. Let's automate it. The first thing that we need to do is create a group. A group is basically a set of resources that work together, in this case, our cluster. Click on <span class="packt_screen">Groups</span> and create a new one:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c5ed7da1-5fe7-4c21-904f-a7d485473a22.png"/></div>
<p>By default, Stackdriver will suggest groupings to you. In this case, in the suggested <span class="packt_screen">Groups</span> section, we can see that Stackdriver has suggested our cluster. It is possible to add more sophisticated criteria, but in this case, matching the start of the name of our machines will work as the GKE names them according to some criteria, including the name of the cluster in the very beginning.</p>
<p>Create a group (call it GKE, the default suggested name). Once the group is created, you can navigate to it and see different metrics such as CPU or even configure them, adding others such as disk I/O and similar. Get yourself familiar with the dashboard:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/83a4112e-e61b-4e53-a521-96307731700f.png"/></div>
<p>In my case, I have added a metric for capturing the used memory in the cluster. Even though this amount of available data is quite interesting, there is an even more powerful tool: the alerting policies.</p>
<p>The alerting policies are criteria in which we should get alerted: high memory usage, low disk space, or high CPU utilization, among others, are events that we want to know about in order to take actions as soon as possible. The beauty of the alerting policies is that if we configure them as appropriated, we enter a state that I call the <em>autopilot</em> mode: we don't need to worry about the performance of the system unless we get alerted, which drastically reduces the number of people required to operate a system.</p>
<p>Let's create an alerting policy by clicking on the <span class="packt_screen">Create Alerting Policy</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3e07ab7d-4293-47e0-af8f-e49942c6999b.png"/></div>
<p>As shown in the preceding screen, we need to select a metric. We are going to use the type Metric Threshold as it is included in the basic package of Stackdriver, so we don't need to upgrade to a Premium subscription. Our alert is going to be raised if any of the members of our cluster has a CPU usage of more than 30 percent for one minute.</p>
<p>The next step is to configure the notification. In the basic package, only email is included, but it is sufficient to test how the system works. Stackdriver also allows you to include text to be sent across with the notification. Just enter something like <kbd>Test alert</kbd> alongside your email and save the alert policy with the name of your choice.</p>
<p>As you can see, creating alerts is very simple in Stackdriver. This is a very simplistic example, but once you have set up your cluster, the next step is to set up the acceptable set of metrics where it should operate normally and get alerted if any of them is violated.</p>
<p>Now it is time to set off the alarm to see what happens. In order to do that, we need to overload the cluster with several replicas of the same image, and we are going to use a tool called Apache benchmark to generate a load on the system:</p>
<pre><strong>kubectl run my-nginx --image=nginx --replicas=7 --port=80</strong></pre>
<p>And now expose the deployment called <kbd>my-nginx</kbd>:</p>
<pre><strong>kubectl expose my-nginx --type=LoadBalancer</strong></pre>
<p>Be aware that you first need to configure <kbd>kubectl</kbd> to point to your cluster, as we've seen in previous chapters.</p>
<p>Once nginx is deployed, it is time to stress it out:</p>
<pre><strong>ab -k -c 350 -n 4000000 http://130.211.65.42/</strong></pre>
<p>The <kbd>ab</kbd> tool is a very powerful benchmark tool called Apache Benchmark. We are going to create 350 concurrent consumers, and they will issue 4 million requests in total. It might be possible that you need to reduce the size of your cluster in order to stress the CPU: if you reduce the size while the benchmark is running, Kubernetes will need to reschedule containers to reorganize the resources, adding more load to the system.</p>
<div class="packt_tip">I would recommend that you further explore Apache Benchmark, as it is very useful for load testing.</div>
<p>Once the CPU has gone beyond the threshold for any of our nodes for over a minute, we should receive an alert by email, and it should be displayed in the Stackdriver interface:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/54f0b210-4fd8-4c97-864e-8f60fca91024.png"/></div>
<p>In this case, I have received two alerts as two of the nodes went beyond the limits. These alerts follow a workflow. You can acknowledge them if the rule is still broken and they will go to resolved once they are acknowledged and the condition has disappeared. In this case, if you stop Apache Benchmark and acknowledge the raised alerts, they will go straight into the resolved state.</p>
<p>In the Premium version, there are more advanced policies, such as Slack messages or SMS, which allows your team to set up a rota for acknowledging incidents and managing the actions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This is the final chapter of the book. Through the entire book, we visited the most important tools used by DevOps engineers, with a strong focus on the Google Cloud Platform. In this chapter, we experimented with what, in my opinion, is a very important aspect of any system: monitoring. I am of the opinion that monitoring is the best way to tackle problems once they have hit your production systems, which, no matter how much effort you put in it, will eventually happen.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is next?</h1>
                </header>
            
            <article>
                
<p>This book did not go particularly in deep into any subject. This is intended. This book is meant to plant the seed of a big tree: the culture of DevOps, hoping that you have enough information to keep growing your knowledge of DevOps tools and processes.</p>
<p>Keeping yourself up to date with the latest tools is a full-time job on its own, but it's very necessary if you want to be on the top of the wave. My opinion is that we are very lucky to be able to participate in the rise and shine of the DevOps culture, and the future is bright. Automation and better languages (Golang, Kotlin, Node.js, and so on) will allow us to reduce human intervention, improving the overall resilience of our systems.</p>
<p>If you look five years back and compare it with what it is there in the market <span>today</span>, can you imagine how our jobs are going to be in 15 years?</p>
<p>If you want to follow up about any question or check what I am working on nowadays, you can always get in touch with me in LinkedIn:</p>
<ul>
<li><a href="https://www.linkedin.com/in/david-gonzalez-microservices/" target="_blank">https://www.linkedin.com/in/david-gonzalez-microservices/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>