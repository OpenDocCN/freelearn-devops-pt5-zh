- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Learning about Distributed Application Architecture
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习分布式应用架构
- en: This chapter introduces the concept of distributed application architecture
    and discusses the various patterns and best practices that are required to run
    a distributed application successfully. It will also discuss the additional requirements
    that need to be fulfilled to run such an application in production. You might
    be wondering, what does this have to do with Docker containers? And you are right
    to ask. At first glance, these are not related to each other. But as you will
    soon see, when introducing containers that host an application or application
    service, your application will quickly consist of several containers that will
    be running on different nodes of a cluster of computers or VMs; and voilà – you
    are dealing with a distributed application. We thought that it makes sense to
    provide you with a sense of the complexity that distributed applications introduce
    and help you avoid the most common pitfalls.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了分布式应用架构的概念，并讨论了成功运行分布式应用所需的各种模式和最佳实践。它还将讨论在生产环境中运行这种应用所需要满足的额外要求。你可能会想，这和
    Docker 容器有什么关系？你问得对，乍一看，这两者似乎没有什么关系。但正如你将很快看到的，当你引入托管应用或应用服务的容器时，你的应用很快就会由多个容器组成，这些容器将运行在计算机集群或虚拟机的不同节点上；于是，你就开始处理一个分布式应用了。我们认为，了解分布式应用带来的复杂性并帮助你避免最常见的陷阱是很有意义的。
- en: 'Here is the list of topics we are going to discuss:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将要讨论的主题列表：
- en: What is a distributed application architecture?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是分布式应用架构？
- en: Patterns and best practices
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式与最佳实践
- en: Running in production
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中运行
- en: 'After reading this chapter, you will be able to do the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你将能够完成以下任务：
- en: Draft a high-level architecture diagram of a distributed application while pointing
    out key design patterns
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 草拟一个分布式应用的高层架构图，并指出关键的设计模式
- en: Identify the possible pitfalls of a poorly designed distributed application
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别设计不当的分布式应用可能带来的陷阱
- en: Name commonly used patterns for dealing with the problems of a distributed system
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列举常见的分布式系统问题处理模式
- en: Name at least four patterns that need to be implemented for a production-ready
    distributed application
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少列举出四种需要在生产就绪的分布式应用中实现的模式
- en: Let’s get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: What is a distributed application architecture?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是分布式应用架构？
- en: In this section, we are going to explain what we mean when we talk about distributed
    application architecture. First, we need to make sure that all the words or acronyms
    we use have a meaning and that we are all talking in the same language.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释当我们谈论分布式应用架构时的含义。首先，我们需要确保我们使用的所有单词或缩写都有明确的含义，并且大家都在讲相同的语言。
- en: Defining the terminology
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义术语
- en: 'In this and subsequent chapters, we will talk a lot about concepts that might
    not be familiar to everyone. To make sure we are all talking the same language,
    let’s briefly introduce and describe the most important of these concepts or words:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章及后续章节中，我们将讨论一些可能对每个人来说都不太熟悉的概念。为了确保我们大家都在讲相同的语言，下面我们将简要介绍并描述这些概念或词汇中最重要的内容：
- en: '| **Keyword** | **Description** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **关键词** | **描述** |'
- en: '| VM | A **virtual machine** (**VM**) is a software simulation of a physical
    computer that runs on a host computer. It provides a separate operating system
    and resources, allowing multiple operating systems to run on a single physical
    machine. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 虚拟机 | **虚拟机**（**VM**）是一个在主机计算机上运行的物理计算机的软件模拟。它提供一个独立的操作系统和资源，允许多个操作系统在单一物理机器上运行。
    |'
- en: '| Cluster | A cluster is a group of connected servers that work together as
    a single system to provide high availability, scalability, and increased performance
    for applications. The nodes in a cluster are connected through a network and share
    resources to provide a unified, highly available solution. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 集群 | 集群是一个由多个相互连接的服务器组成的群体，它们作为一个整体协作工作，为应用提供高可用性、可扩展性和更高的性能。集群中的节点通过网络连接，共享资源，从而提供统一的、高可用的解决方案。
    |'
- en: '| Node | A cluster node is a single server within a cluster computing system.
    It provides computing resources and works together with other nodes to perform
    tasks as a unified system, providing high availability and scalability for applications.
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 节点 | 集群节点是集群计算系统中的一台单独服务器。它提供计算资源，并与其他节点一起协作执行任务，作为一个统一的系统，为应用提供高可用性和可扩展性。
    |'
- en: '| Network | A network is a group of interconnected devices that can exchange
    data and information. Networks can be used to connect computers, servers, mobile
    devices, and other types of devices and allow them to communicate with each other
    and share resources, such as printers and storage.More specifically in our case,
    these are physical and software-defined communication paths between individual
    nodes of a cluster and programs running on those nodes. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | 网络是一组互联的设备，可以交换数据和信息。网络可以用于连接计算机、服务器、移动设备和其他类型的设备，并允许它们相互通信并共享资源，如打印机和存储设备。在我们这个例子中，更具体地说，这些是集群中各个节点之间以及这些节点上运行的程序之间的物理和软件定义的通信路径。
    |'
- en: '| Port | A port is a communication endpoint in a network-attached device, such
    as a computer or server. It allows the device to receive and send data to other
    devices on the network through a specific network protocol, such as TCP or UDP.
    Each port has a unique number that is used to identify it, and different services
    and applications use specific ports to communicate. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 端口 | 端口是网络连接设备中的通信端点，如计算机或服务器。它允许设备通过特定的网络协议（如 TCP 或 UDP）接收和发送数据到网络中的其他设备。每个端口都有一个唯一的编号，用于标识它，不同的服务和应用程序使用特定的端口进行通信。
    |'
- en: '| Service | Unfortunately, this is a very overloaded term and its real meaning
    depends on the context that it is used in. If we use the term service in the context
    of an application, such as an application service, then it usually means that
    this is a piece of software that implements a limited set of functionalities that
    are then used by other parts of the application. As we progress through this book,
    other types of services that have slightly different definitions will be discussed.
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 服务 | 不幸的是，这是一个非常含糊的术语，其真正含义取决于使用的上下文。如果我们在应用程序的上下文中使用“服务”一词，比如应用服务，它通常意味着这是一个实现有限功能集的软件，其他应用程序部分将使用这些功能。随着本书的进展，其他类型的服务也会被讨论，这些服务具有稍微不同的定义。
    |'
- en: 'Naively said, a distributed application architecture is the opposite of a monolithic
    application architecture, but it is not unreasonable to look at this monolithic
    architecture first. Traditionally, most business applications are written in such
    a way that the result can be seen as a single, tightly coupled program that runs
    on a named server somewhere in a data center. All its code is compiled into a
    single binary, or a few very tightly coupled binaries that need to be co-located
    when running the application. The fact that the server – or more generally – the
    host, that the application is running on has a well-defined name or static IP
    address is also important in this context. Let’s look at the following diagram,
    which illustrates this type of application architecture a bit more precisely:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，分布式应用架构是单体应用架构的对立面，但首先看一下单体架构也并不无道理。传统上，大多数业务应用程序都是以一种方式编写的，结果可以被看作是一个运行在数据中心某个命名服务器上的单一、紧密耦合的程序。它的所有代码被编译成一个单一的二进制文件，或者是几个需要共同部署的紧密耦合的二进制文件。应用程序运行所在的服务器——或者更广泛地说——主机的名称或静态
    IP 地址在这个上下文中也很重要。让我们来看一下下面的图，它更精确地说明了这种类型的应用架构：
- en: '![Figure 9.1 – Monolithic application architecture](img/B19199_09_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 单体应用架构](img/B19199_09_01.jpg)'
- en: Figure 9.1 – Monolithic application architecture
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 单体应用架构
- en: In the preceding diagram, we can see a server named `blue-box-12a` with an IP
    address of `172.52.13.44` running an application called `pet-shop`, which is a
    monolith consisting of a main module and a few tightly coupled libraries.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到一个名为 `blue-box-12a` 的服务器，IP 地址为 `172.52.13.44`，它运行着一个名为 `pet-shop`
    的应用程序，这是一个由主模块和几个紧密耦合的库组成的单体架构。
- en: 'Now, let’s look at the following diagram:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看以下图示：
- en: '![Figure 9.2 – Distributed application architecture](img/B19199_09_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 分布式应用架构](img/B19199_09_02.jpg)'
- en: Figure 9.2 – Distributed application architecture
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 分布式应用架构
- en: Here, all of a sudden, we do not have just a single named server anymore; instead,
    we have a lot of them, and they do not have human-friendly names, but rather some
    unique IDs that can be something such as a `pet-api`, `pet-web`, and `pet-inventory`.
    Furthermore, each service runs in multiple instances in this cluster of servers
    or hosts.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，突然间，我们不再只有一个命名服务器；相反，我们有许多服务器，它们没有人类友好的名称，而是一些独特的 ID，如 `pet-api`、`pet-web`
    和 `pet-inventory`。此外，每个服务在这个服务器或主机的集群中运行多个实例。
- en: You might be wondering why we are discussing this in a book about Docker containers,
    and you are right to ask. While all the topics we’re going to investigate apply
    equally to a world where containers do not (yet) exist, it is important to realize
    that containers and container orchestration engines help address all these problems
    in a much more efficient and straightforward way. Most of the problems that used
    to be very hard to solve in a distributed application architecture become quite
    simple in a containerized world.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们在一本关于 Docker 容器的书中讨论这些内容，你问得很对。虽然我们将要探讨的所有话题同样适用于容器尚不存在的世界，但重要的是要意识到，容器和容器编排引擎能够以更高效和更直接的方式解决所有这些问题。在容器化的世界中，许多曾经在分布式应用架构中非常难以解决的问题变得相当简单。
- en: Patterns and best practices
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模式和最佳实践
- en: A distributed application architecture has many compelling benefits, but it
    also has one very significant drawback compared to a monolithic application architecture
    – the former is way more complex. To tame this complexity, the industry has come
    up with some important best practices and patterns. In the following sections,
    we are going to investigate some of the most important ones in more detail.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式应用架构有许多显著的优点，但与单体应用架构相比，它也有一个非常重要的缺点——前者远比后者复杂。为了驯服这种复杂性，业界提出了一些重要的最佳实践和模式。在接下来的章节中，我们将更详细地探讨其中一些最重要的内容。
- en: Loosely coupled components
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 松散耦合的组件
- en: The best way to address a complex subject has always been to divide it into
    smaller subproblems that are more manageable. As an example, it would be insanely
    complex to build a house in a single step. It is much easier to build a house
    from simple parts that are then combined into the final result.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 处理复杂主题的最佳方法一直是将其拆分成更容易管理的小问题。例如，一次性建造一座房子会非常复杂。将房子从简单的部件构建出来，然后将这些部件组合成最终结果会容易得多。
- en: The same also applies to software development. It is much easier to develop
    a very complex application if we divide this application into smaller components
    that interoperate and make up the overall application. Now, it is much easier
    to develop these components individually if they are loosely coupled with each
    other. What this means is that component A makes no assumptions about the inner
    workings of, say, components B and C, and is only interested in how it can communicate
    with those two components across a well-defined interface.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的原则也适用于软件开发。如果我们将一个非常复杂的应用程序拆分成多个较小的组件，这些组件可以互相协作，共同构成整个应用程序，那么开发这些组件会变得容易得多。如果这些组件之间是松散耦合的，那么开发它们将更加容易。意味着组件
    A 对组件 B 和 C 的内部工作没有任何假设，它只关心如何通过一个定义良好的接口与这两个组件进行通信。
- en: If each component has a well-defined and simple public interface through which
    communication with the other components in the system and the outside world happens,
    then this enables us to develop each component individually, without implicit
    dependencies on other components. During the development process, other components
    in the system can easily be replaced by stubs or mocks to allow us to test our
    components.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个组件都有一个定义良好且简单的公共接口，通过该接口与系统中的其他组件以及外部世界进行通信，那么这将使我们能够单独开发每个组件，而不依赖于其他组件。在开发过程中，系统中的其他组件可以通过存根或模拟对象轻松替换，从而使我们能够测试我们的组件。
- en: Stateful versus stateless
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态与无状态
- en: Every meaningful business application creates, modifies, or uses data. In IT,
    a synonym for data is **state**. An application service that creates or modifies
    persistent data is called a **stateful component**. Typical stateful components
    are database services or services that create files. On the other hand, application
    components that do not create or modify persistent data are called **stateless
    components**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个有意义的业务应用程序都会创建、修改或使用数据。在 IT 中，数据的同义词是**状态**。一个创建或修改持久数据的应用服务称为**有状态组件**。典型的有状态组件包括数据库服务或创建文件的服务。另一方面，不创建或修改持久数据的应用组件称为**无状态组件**。
- en: In a distributed application architecture, stateless components are much simpler
    to handle than stateful components. Stateless components can easily be scaled
    up and down. Furthermore, they can be quickly and painlessly torn down and restarted
    on a completely different node of the cluster – all because they have no persistent
    data associated with them.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式应用架构中，无状态组件比有状态组件要容易处理得多。无状态组件可以轻松地进行横向扩展和收缩。此外，它们可以在集群中的完全不同节点上迅速且无痛地被销毁并重新启动——这一切都因为它们没有与之相关的持久数据。
- en: Given this, it is helpful to design a system in a way that most of the application
    services are stateless. It is best to push all the stateful components to the
    boundaries of the application and limit how many are used. Managing stateful components
    is hard.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，设计一个大多数应用服务是无状态的系统是非常有帮助的。最好将所有有状态的组件推到应用程序的边界，并限制其使用数量。管理有状态的组件很困难。
- en: Service discovery
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务发现
- en: As we build applications that consist of many individual components or services
    that communicate with each other, we need a mechanism that allows the individual
    components to find each other in the cluster. Finding each other usually means
    that you need to know on which node the target component is running, and on which
    port it is listening for communication. Most often, nodes are identified by an
    **IP address** and a **port**, which is just a number in a well-defined range.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建由多个独立组件或服务组成的应用程序，并且这些组件相互通信时，我们需要一种机制来允许各个组件在集群中找到彼此。相互找到通常意味着你需要知道目标组件运行在哪个节点上，以及它在哪个端口上监听通信。通常，节点通过**IP地址**和**端口**来标识，端口只是一个在定义范围内的数字。
- en: 'Technically, we could tell **Service A**, which wants to communicate with a
    target, **Service B**, what the IP address and port of the target are. This could
    happen, for example, through an entry in a configuration file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们可以告诉**服务A**，它希望与目标**服务B**进行通信，目标的IP地址和端口是什么。例如，这可以通过配置文件中的一项条目来实现：
- en: '![Figure 9.3 – Components are hardwired](img/B19199_09_03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 组件是硬接线的](img/B19199_09_03.jpg)'
- en: Figure 9.3 – Components are hardwired
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 组件是硬接线的
- en: While this might work very well in the context of a monolithic application that
    runs on one or only a few well-known and curated servers, it falls apart in a
    distributed application architecture. First of all, in this scenario, we have
    many components, and keeping track of them manually becomes a nightmare. This
    is not scalable. Furthermore, typically, Service A should or will never know on
    which node of the cluster the other components run. Their location may not even
    be stable as component B could be moved from node *X* to another node, *Y*, due
    to various reasons external to the application. Thus, we need another way in which
    Service A can locate Service B, or any other service, for that matter. Commonly,
    an external authority that is aware of the topology of the system at any given
    time is used.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在一个运行在一个或仅几个知名且精心管理的服务器上的单体应用程序中，这种方式可能非常有效，但在分布式应用架构中，它就会崩溃。首先，在这种情况下，我们有很多组件，手动跟踪它们变成了一场噩梦。这是不可扩展的。此外，通常，服务A应该或者永远不应该知道其他组件运行在哪个集群节点上。它们的位置甚至可能不稳定，因为组件B可能会由于与应用无关的各种原因，从节点*X*迁移到另一个节点*Y*。因此，我们需要另一种方式，允许服务A定位服务B，或者任何其他服务。通常，会使用一个外部权威来感知系统在任何给定时刻的拓扑结构。
- en: 'This external authority or service knows all the nodes and their IP addresses
    that currently pertain to the cluster; it knows about all the services that are
    running and where they are running. Often, this kind of service is called a **DNS
    service**, where DNS stands for **Domain Name System**. As we will see, Docker
    has a DNS service implemented as part of its underlying engine. Kubernetes – the
    number one container orchestration system, which we’ll discuss in [*Chapter 13*](B19199_13.xhtml#_idTextAnchor276),
    *Introducing Container Orchestration* – also uses a DNS service to facilitate
    communication between components running in a cluster:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个外部权威或服务知道当前属于集群的所有节点及其IP地址；它知道所有正在运行的服务以及它们的运行位置。通常，这种服务被称为**DNS服务**，其中DNS代表**域名系统**。正如我们将看到的，Docker在其底层引擎中实现了一个DNS服务。Kubernetes——我们将在[*第13章*](B19199_13.xhtml#_idTextAnchor276)中讨论的第一大容器编排系统——也使用DNS服务来促进集群中组件之间的通信：
- en: '![Figure 9.4 – Components consulting an external locator service](img/B19199_09_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 组件咨询外部定位服务](img/B19199_09_04.jpg)'
- en: Figure 9.4 – Components consulting an external locator service
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 组件咨询外部定位服务
- en: In the preceding diagram, we can see how Service A wants to communicate with
    Service B, but it can’t do this directly. First, it has to query the external
    authority, a registry service (here, this is called **DNS Service**), about the
    whereabouts of Service B. The registry service will answer with the requested
    information and hand out the IP address and port number that Service A can use
    to reach Service B. Service A then uses this information and establishes communication
    with Service B. Of course, this is a naive picture of what’s happening at a low
    level, but it is a good picture to help us understand the architectural pattern
    of service discovery.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，我们可以看到服务A如何想要与服务B进行通信，但不能直接进行。首先，它必须查询外部权限，即注册服务（这里称为**DNS服务**），以获取服务B的位置信息。注册服务会返回请求的信息，并提供服务A可以用来访问服务B的IP地址和端口号。服务A随后使用这些信息并与服务B建立通信。当然，这只是一个低层次的简单示意图，但它有助于我们理解服务发现的架构模式。
- en: Routing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路由
- en: Routing is the mechanism of sending packets of data from a source component
    to a target component. Routing is categorized into different types. The so-called
    OSI model (see the reference to this in the *Further reading* section at the end
    of this chapter for more information) is used to distinguish between different
    types of routing. In the context of containers and container orchestration, routing
    at layers 2, 3, 4, and 7 are relevant. We will look at routing in more detail
    in subsequent chapters. For now, let’s just say that layer 2 routing is the most
    low-level type of routing, which connects a MAC address to another MAC address,
    while layer 7 routing, which is also called application-level routing, is the
    most high-level one. The latter is, for example, used to route requests that have
    a target identifier – that is, a URL such as [https://acme.com/pets](https://acme.com/pets)
    – to the appropriate target component in our system.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 路由是将数据包从源组件发送到目标组件的机制。路由分为不同的类型。所谓的OSI模型（更多信息请参考本章最后的*进一步阅读*部分）用于区分不同类型的路由。在容器和容器编排的上下文中，第2层、第3层、第4层和第7层的路由是相关的。我们将在后续章节中更详细地探讨路由。现在，我们只需知道，第2层路由是最低级别的路由，它将一个MAC地址连接到另一个MAC地址，而第7层路由，也称为应用层路由，是最高级别的路由。例如，应用层路由用于将带有目标标识符的请求——也就是像[https://acme.com/pets](https://acme.com/pets)这样的URL——路由到系统中的适当目标组件。
- en: Load balancing
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡
- en: 'Load balancing is used whenever **Service A** needs to communicate with **Service
    B**, such as in a request-response pattern, but the latter is running in more
    than one instance, as shown in the following diagram:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡在**服务A**需要与**服务B**进行通信时使用，例如在请求-响应模式中，但后者运行在多个实例中，如下图所示：
- en: '![Figure 9.5 – The request of Service A is being load balanced to Service B](img/B19199_09_05.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 服务A的请求正在被负载均衡到服务B](img/B19199_09_05.jpg)'
- en: Figure 9.5 – The request of Service A is being load balanced to Service B
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 服务A的请求正在被负载均衡到服务B
- en: If we have multiple instances of a service such as Service B running in our
    system, we want to make sure that every one of those instances gets an equal amount
    of workload assigned to it. This task is a generic one, which means that we don’t
    want the caller to have to do the load balancing but, rather, an external service
    that intercepts the call and takes over the role of deciding which of the target
    service instances to forward the call to. This external service is called a load
    balancer. Load balancers can use different algorithms to decide how to distribute
    incoming calls to target service instances. The most common algorithm that’s used
    is called round-robin. This algorithm assigns requests repetitively, starting
    with instance 1, then 2, until instance *n*. After the last instance has been
    served, the load balancer starts over with instance number 1.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的系统中有多个服务实例（例如，服务B）在运行，我们希望确保这些实例能够平等地分配工作负载。这个任务是一个通用任务，这意味着我们不希望调用方去做负载均衡，而是希望有一个外部服务来拦截调用并决定将请求转发到哪个目标服务实例。这个外部服务称为负载均衡器。负载均衡器可以使用不同的算法来决定如何将传入的请求分配给目标服务实例。最常用的算法叫做轮询算法。这个算法会重复地分配请求，从实例1开始，然后是实例2，一直到实例*n*。当最后一个实例被处理完后，负载均衡器会重新从实例1开始。
- en: In the preceding example, a load balancer also facilitates high availability
    since a request from Service A will be forwarded to a healthy instance of Service
    B. The load balancer also takes the role of periodically checking the health of
    each instance of B.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，负载均衡器也有助于高可用性，因为来自服务A的请求会被转发到健康的服务B实例。负载均衡器还负责定期检查每个B实例的健康状况。
- en: Defensive programming
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御性编程
- en: When developing a service for a distributed application, it is important to
    remember that this service is not going to be standalone and that it’s dependent
    on other application services or even on external services provided by third parties,
    such as credit card validation services or stock information services, to just
    name two. All these other services are external to the service we are developing.
    We have no control over their correctness or their availability at any given time.
    Thus, when coding, we always need to assume the worst and hope for the best. Assuming
    the worst means that we have to deal with potential failures explicitly.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在为分布式应用开发服务时，重要的是要记住，服务不是独立的，它依赖于其他应用服务，甚至依赖于第三方提供的外部服务，如信用卡验证服务或股票信息服务，仅举两例。所有这些其他服务都是我们正在开发的服务之外的。我们无法控制它们的正确性或在任何给定时刻的可用性。因此，在编写代码时，我们需要始终假设最坏的情况，并期望最好的结果。假设最坏的情况意味着我们必须明确处理潜在的故障。
- en: Retries
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重试
- en: When there is a possibility that an external service might be temporarily unavailable
    or not responsive enough, then the following procedure can be used. When the call
    to the other service fails or times out, the calling code should be structured
    in such a way that the same call is repeated after a short wait time. If the call
    fails again, the wait should be a bit longer before the next trial. The calls
    should be repeated up to a maximum number of times, each time increasing the wait
    time. After that, the service should give up and provide a degraded service, which
    could mean returning some stale cached data or no data at all, depending on the
    situation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在外部服务可能暂时不可用或响应不足的情况时，可以使用以下程序。当调用其他服务失败或超时时，调用代码应以一种结构化的方式进行，使得在短暂等待时间后重复同一调用。如果调用再次失败，等待时间应比上次稍长，再次尝试。调用应重复直到达到最大次数，每次增加等待时间。之后，服务应放弃并提供降级服务，具体可能意味着返回一些过时的缓存数据或根本不返回数据，具体取决于情况。
- en: Logging
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 日志记录
- en: Important operations that are performed on a service should always be logged.
    Logging information needs to be categorized to be of any real value. A common
    list of categories includes *debug*, *info*, *warning*, *error*, and *fatal*.
    Logging information should be collected by a central log aggregation service and
    not stored on an individual node of the cluster. Aggregated logs are easy to parse
    and filter for relevant information. This information is essential to quickly
    pinpoint the root cause of a failure or unexpected behavior in a distributed system
    consisting of many moving parts, running in production.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对服务执行的重要操作应始终进行日志记录。日志信息需要进行分类，以便具有实际价值。常见的分类包括*调试*、*信息*、*警告*、*错误*和*致命*。日志信息应由一个中央日志聚合服务收集，而不是存储在集群的单个节点上。聚合日志易于解析和过滤相关信息。这些信息对于快速定位分布式系统中多部件运行时的故障或异常行为的根本原因至关重要。
- en: Error handling
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误处理
- en: As we mentioned earlier, each application service in a distributed application
    is dependent on other services. As developers, we should always expect the worst
    and have appropriate error handling in place. One of the most important best practices
    is to fail fast. Code the service in such a way that unrecoverable errors are
    discovered as early as possible and, if such an error is detected, have the service
    fail immediately. But don’t forget to log meaningful information to `STDERR` or
    `STDOUT`, which can be used by developers or system operators later to track malfunctions
    in the system. Also, return a helpful error to the caller, indicating as precisely
    as possible why the call failed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，分布式应用中的每个应用服务都依赖于其他服务。作为开发人员，我们应该始终预期最坏的情况，并做好适当的错误处理。最重要的最佳实践之一是尽早失败。编写服务时，要确保无法恢复的错误尽早被发现，并且一旦检测到此类错误，服务应立即失败。但不要忘记将有意义的信息记录到`STDERR`或`STDOUT`，以供开发人员或系统操作员在以后跟踪系统故障时使用。此外，还应向调用者返回一个有帮助的错误信息，尽可能准确地说明调用失败的原因。
- en: One sample of fail fast is always checking the input values provided by the
    caller. Are the values in the expected ranges and complete? If not, then do not
    try to continue processing; instead, immediately abort the operation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速失败的示例是始终检查调用者提供的输入值。值是否在预期范围内并且完整？如果不是，就不要尝试继续处理，而是立即终止操作。
- en: Redundancy
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冗余
- en: A mission-critical system has to be available at all times, around the clock,
    365 days a year. Downtime is not acceptable since it might result in a huge loss
    of opportunities or reputation for the company. In a highly distributed application,
    the likelihood of a failure of at least one of the many involved components is
    non-neglectable. We can say that the question is not whether a component will
    fail, but rather when a failure will occur.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键任务系统必须始终可用，全天候、全年无休。停机是不可接受的，因为它可能会导致公司巨大的机会损失或声誉损害。在一个高度分布式的应用中，至少有一个组件发生故障的可能性是不可忽视的。我们可以说，问题不在于某个组件是否会发生故障，而是故障何时发生。
- en: To avoid downtime when one of the many components in the system fails, each
    part of the system needs to be redundant. This includes the application components,
    as well as all infrastructure parts. What that means is that if we have a payment
    service as part of our application, then we need to run this service redundantly.
    The easiest way to do that is to run multiple instances of this very service on
    different nodes of our cluster. The same applies, say, to an edge router or a
    load balancer. We cannot afford for these to ever go down. Thus, the router or
    load balancer must be redundant.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免当系统中的某个组件发生故障时导致停机，每个系统部分都需要冗余。这不仅包括应用组件，还包括所有基础设施部分。这意味着，如果我们有一个支付服务作为应用的一部分，那么我们需要对该服务进行冗余部署。最简单的方式是在集群的不同节点上运行该服务的多个实例。同样的原则也适用于边缘路由器或负载均衡器。我们不能承受这些服务出现故障。因此，路由器或负载均衡器必须是冗余的。
- en: Health checks
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 健康检查
- en: We have mentioned various times that in a distributed application architecture,
    with its many parts, the failure of an individual component is highly likely and
    that it is only a matter of time until it happens. For that reason, we must run
    every single component of the system redundantly. Load balancers then distribute
    the traffic across the individual instances of a service.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次提到，在分布式应用架构中，由于其包含多个组件，单个组件发生故障的可能性非常高，而且这只是时间问题。因此，我们必须对系统中的每个组件进行冗余部署。负载均衡器会将流量分配到服务的各个实例上。
- en: But now, there is another problem. How does the load balancer or router know
    whether a certain service instance is available? It could have crashed, or it
    could be unresponsive. To solve this problem, we can use so-called **health checks**.
    The load balancer, or some other system service on behalf of it, periodically
    polls all the service instances and checks their health. The questions are basically,
    *Are you still there? Are you healthy?* The answer to each question is either
    *Yes* or *No*, or the health check times out if the instance is not responsive
    anymore.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，另一个问题出现了。负载均衡器或路由器如何知道某个服务实例是否可用？它可能已经崩溃，或者可能没有响应。为了解决这个问题，我们可以使用所谓的**健康检查**。负载均衡器，或者代表它的其他系统服务，会定期轮询所有服务实例并检查它们的健康状况。基本问题是，*你还在吗？你健康吗？*
    每个问题的答案要么是*是*，要么是*否*，如果实例不再响应，则健康检查会超时。
- en: If the component answers with *No* or a timeout occurs, then the system kills
    the corresponding instance and spins up a new instance in its place. If all this
    happens in a fully automated way, then we say that we have an auto-healing system
    in place. Instead of the load balancer periodically polling the status of the
    components, responsibility can also be turned around. The components could be
    required to periodically send live signals to the load balancer. If a component
    fails to send live signals over a predefined, extended period, it is assumed to
    be unhealthy or dead.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果组件的回答是*否*或发生超时，系统将终止对应实例并启动一个新的实例替代。如果所有这些都是自动化完成的，我们就可以说系统实现了自愈功能。负载均衡器不再周期性地轮询组件的状态，责任可以反过来，组件可以要求定期向负载均衡器发送存活信号。如果某个组件在预定的较长时间内未发送存活信号，则假定其处于不健康状态或已经死亡。
- en: There are situations where either of the described ways is more appropriate.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况下，上述描述的方式中的任何一种都更为合适。
- en: Circuit breaker pattern
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熔断器模式
- en: A circuit breaker is a mechanism that is used to avoid a distributed application
    going down due to the cascading failure of many essential components. Circuit
    breakers help us avoid one failing component tearing down other dependent services
    in a domino effect. Like circuit breakers in an electrical system, which protect
    a house from burning down due to the failure of a malfunctioning plugged-in appliance
    by interrupting the power line, circuit breakers in a distributed application
    interrupt the connection from **Service A** to **Service B** if the latter is
    not responding or is malfunctioning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器是一种机制，用于避免由于多个关键组件的级联故障而导致分布式应用程序崩溃。断路器帮助我们避免一个失败的组件通过多米诺效应拖垮其他依赖服务，就像电气系统中的断路器保护房屋免于由于故障的插入式电器而导致燃烧，中断电源线一样，分布式应用程序中的断路器在服务
    A 到服务 B 的连接不响应或者故障时会中断连接。
- en: 'This can be achieved by wrapping a protected service call in a circuit breaker
    object. This object monitors for failures. Once the number of failures reaches
    a certain threshold, the circuit breaker trips. All subsequent calls to the circuit
    breaker will return with an error, without the protected call being made at all:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过将受保护的服务调用封装在一个断路器对象中来实现。该对象会监控失败情况。一旦失败次数达到某个阈值，断路器将被触发。所有后续对断路器的调用都将返回错误，而根本不会进行受保护的调用：
- en: '![Figure 9.6 – Circuit breaker pattern](img/B19199_09_06.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 断路器模式](img/B19199_09_06.jpg)'
- en: Figure 9.6 – Circuit breaker pattern
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 断路器模式
- en: In the preceding diagram, we have a circuit breaker that tips over after the
    second timeout is received when calling **Service B**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们有一个断路器，在调用服务 B 时接收到第二个超时后被触发。
- en: Rate limiter
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 速率限制器
- en: In the context of a circuit breaker, a rate limiter is a technique that’s used
    to control the rate at which requests are processed by a system or service. By
    limiting the number of requests allowed within a specific time window, rate limiters
    help prevent overloading and ensure the stability and availability of the service.
    This mechanism proves useful in mitigating the impact of sudden traffic spikes,
    protecting backend systems from being overwhelmed, and avoiding cascading failures
    throughout a distributed system. By integrating rate limiting with circuit breakers,
    systems can effectively maintain optimal performance and gracefully handle unexpected
    surges in demand.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在断路器的背景下，速率限制器是一种控制系统或服务处理请求速率的技术。通过限制在特定时间窗口内允许的请求数量，速率限制器有助于防止过载，并确保服务的稳定性和可用性。这种机制在减少突发流量冲击的影响、保护后端系统免受压倒性影响以及避免分布式系统中级联故障方面非常有用。通过将速率限制与断路器结合使用，系统能够有效地保持最佳性能，并优雅地处理需求的突然增加。
- en: Bulkhead
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 防火墙
- en: In addition to that, and still in the context of a circuit breaker, a bulkhead
    is a resilience pattern that’s used to isolate components or resources within
    a system, ensuring that a failure in one area does not cause a cascading effect
    on the entire system. By partitioning resources and segregating operations into
    separate, independent units, bulkheads help prevent a single point of failure
    from bringing down the entire service. This mechanism is useful in maintaining
    system stability, improving fault tolerance, and ensuring that critical operations
    can continue functioning, even in the event of localized failures. When combined
    with circuit breakers, bulkheads contribute to a more robust and resilient system,
    capable of handling failures and maintaining overall system performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，在断路器的背景下，防火墙是一种用于隔离系统中的组件或资源的弹性模式，确保一个区域的故障不会对整个系统造成级联效应。通过将资源分区并将操作隔离到独立的单元中，防火墙有助于防止单点故障导致整个服务崩溃。这种机制有助于维持系统稳定性，提高容错能力，并确保关键操作即使在本地化故障时也能继续正常运行。与断路器结合使用时，防火墙有助于构建更加健壮和弹性的系统，能够处理故障并保持整体系统性能。
- en: Running in production
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产中运行
- en: To successfully run a distributed application in production, we need to consider
    a few more aspects beyond the best practices and patterns that were presented
    in the preceding sections. One specific area that comes to mind is introspection
    and monitoring. Let’s go through the most important aspects in detail.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功地在生产环境中运行分布式应用程序，我们需要考虑一些超出前述部分介绍的最佳实践和模式的特定领域。一个具体的领域就是自省和监控。让我们详细讨论最重要的方面。
- en: Logging
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志记录
- en: Once a distributed application is in production, it is not possible to live
    debug it. But how can we then find out what the root cause of the application
    malfunctioning is? The solution to this problem is that the application produces
    abundant and meaningful logging information while running. We briefly discussed
    this topic in an earlier section. But due to its importance, it is worth reiterating.
    Developers need to instrument their application services in such a way that they
    output helpful information, such as when an error occurs or a potentially unexpected
    or unwanted situation is encountered. Often, this information is output to `STDOUT`
    and `STDERR`, where it is then collected by system daemons that write the information
    to local files or forward it to a central log aggregation service.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: If there is sufficient information in the logs, developers can use those logs
    to track down the root cause of the errors in the system.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed application architecture, with its many components, logging
    is even more important than in a monolithic application. The paths of execution
    of a single request through all the components of the application can be very
    complex. Also, remember that the components are distributed across a cluster of
    nodes. Thus, it makes sense to log everything of importance and add things to
    each log entry, such as the exact time when it happened, the component in which
    it happened, and the node on which the component ran, to name just a few. Furthermore,
    the logging information should be aggregated in a central location so that it
    is readily available for developers and system operators to analyze.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tracing is used to find out how an individual request is funneled through a
    distributed application and how much time is spent overall on the request and
    in every individual component. This information, if collected, can be used as
    one of the sources for dashboards that show the behavior and health of the system.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Operation engineers like to have dashboards showing live key metrics of the
    system, which show them the overall health of the application at a glance. These
    metrics can be nonfunctional metrics, such as memory and CPU usage, the number
    of crashes of a system or application component, and the health of a node, as
    well as functional and, hence, application-specific metrics, such as the number
    of checkouts in an ordering system or the number of items out of stock in an inventory
    service.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Most often, the base data that’s used to aggregate the numbers that are used
    for a dashboard is extracted from logging information. This can either be system
    logs, which are mostly used for non-functional metrics, or application-level logs,
    for functional metrics.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Application updates
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the competitive advantages for a company is to be able to react promptly
    to changing market situations. Part of this is being able to quickly adjust an
    application to fulfill new and changed needs or to add new functionality. The
    faster we can update our applications, the better. Many companies these days roll
    out new or changed features multiple times per day.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Since application updates are so frequent, these updates have to be non-disruptive.
    We cannot allow the system to go down for maintenance when upgrading. It all has
    to happen seamlessly and transparently.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One way of updating an application or an application service is to use rolling
    updates. The assumption here is that the particular piece of software that has
    to be updated runs in multiple instances. Only then can we use this type of update.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: What happens is that the system stops one instance of the current service and
    replaces it with an instance of the new service. As soon as the new instance is
    ready, it will be served traffic. Usually, the new instance is monitored for some
    time to see whether it works as expected; if it does, the next instance of the
    current service is taken down and replaced with a new instance. This pattern is
    repeated until all the service instances have been replaced.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Since there are always a few instances running at any given time, current or
    new, the application is operational all the time. No downtime is needed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In blue-green deployments, the current version of the application service, called
    **blue**, handles all the application traffic. We then install the new version
    of the application service, called **green**, on the production system. This new
    service is not wired with the rest of the application yet.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the green service has been installed, we can execute **smoke tests** against
    this new service. If those succeed, the router can be configured to funnel all
    traffic that previously went to blue to the new service, green. The behavior of
    the green service is then observed closely and, if all success criteria are met,
    the blue service can be decommissioned. But if, for some reason, the green service
    shows some unexpected or unwanted behavior, the router can be reconfigured to
    return all traffic to the blue service. The green service can then be removed
    and fixed, and a new blue-green deployment can be executed with the corrected
    version:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Blue-green deployment](img/B19199_09_07.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Blue-green deployment
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at canary releases.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Canary releases
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Canary releases are releases where we have the current version of the application
    service and the new version installed on the system in parallel. As such, they
    resemble blue-green deployments. At first, all traffic is still routed through
    the current version. We then configure a router so that it funnels a small percentage,
    say 1%, of the overall traffic to the new version of the application service.
    Subsequently, the behavior of the new service is monitored closely to find out
    whether it works as expected. If all the criteria for success are met, then the
    router is configured to funnel more traffic, say 5% this time, through the new
    service. Again, the behavior of the new service is closely monitored and, if it
    is successful, more and more traffic is routed to it until we reach 100%. Once
    all the traffic has been routed to the new service and it has been stable for
    some time, the old version of the service can be decommissioned.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Why do we call this a canary release? It is named after coal miners who would
    use canary birds as an early warning system in mines. Canaries are particularly
    sensitive to toxic gas and if such a bird died, the miners knew they had to abandon
    the mine immediately.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Irreversible data changes
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If part of our update process is to execute an irreversible change in our state,
    such as an irreversible schema change in a backing relational database, then we
    need to address this with special care. It is possible to execute such changes
    without downtime if we use the right approach. It is important to recognize that,
    in such a situation, we cannot deploy the code changes that require the new data
    structure in the data store at the same time as the changes to the data. Rather,
    the whole update has to be separated into three distinct steps. In the first step,
    we roll out a backward-compatible schema and data change. If this is successful,
    then we roll out the new code in the second step. Again, if that is successful,
    we clean up the schema in the third step and remove the backward compatibility:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Rolling out an irreversible data or schema change](img/B19199_09_08.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Rolling out an irreversible data or schema change
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows how the data and its structure are updated, how
    the application code is updated, and how the data and data structure are cleaned
    up.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Changing the data structure at scale
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Over time, an application may produce an enormous amount of data. Changing the
    data structure at scale refers to the process of altering the format, organization,
    or layout of large amounts of data stored in a database or other type of data
    storage system. This can involve adding, removing, or modifying fields, tables,
    or other elements within the data structure. The goal is to optimize the data
    for a specific use case or business requirement while preserving the data’s accuracy
    and integrity. This process typically involves analyzing the existing data structure,
    planning and testing the changes, and then executing the update in a controlled
    manner. In large-scale data structure changes, it is important to have a well-defined
    strategy, a robust testing and validation process, and adequate resources, including
    technical expertise and backup systems, to minimize the risk of data loss or corruption
    during the migration process.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: In a dynamic data migration scenario, data is constantly updated in real time
    as it is being used, making the migration process more complex and challenging.
    This type of migration requires a more sophisticated approach to ensure data consistency
    and integrity throughout the migration process. The solution should be able to
    keep track of changes made to the data in the source system and replicate them
    in the target system while minimizing downtime and data loss. This may involve
    using specialized tools, such as data replication or mirroring software, or employing
    a multi-step process that includes data synchronization and reconciliation. Additionally,
    it is essential to have robust testing and validation procedures in place, as
    well as a clear rollback plan, to minimize the risk of data loss or corruption
    during the migration process.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Rollback and roll forward
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we have frequent updates for our application services that run in production,
    sooner or later, there will be a problem with one of those updates. Maybe a developer,
    while fixing a bug, introduced a new one, which was not caught by all the automated,
    and maybe manual, tests, so the application is misbehaving. In this case, we must
    roll back the service to the previous good version. In this regard, a rollback
    is a recovery from a disaster.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Again, in a distributed application architecture, it is not a question of whether
    a rollback will ever be needed, but rather when a rollback will have to occur.
    Thus, we need to be sure that we can always roll back to a previous version of
    any service that makes up our application. Rollbacks cannot be an afterthought;
    they have to be a tested and proven part of our deployment process.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: If we are using blue-green deployments to update our services, then rollbacks
    should be fairly simple. All we need to do is switch the router from the new green
    version of the service back to the previous blue version.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: If we adhere to continuous delivery and the main branch of our code is always
    in a deployable state, then we can also consider rolling forward instead of rolling
    back. Often, it is faster to fix a production issue and roll out the fix immediately
    instead of trying to roll back our system to a previous state. The technique of
    rolling forward is of particular interest if the previous change introduced some
    backward incompatibility.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what a distributed application architecture is and
    what patterns and best practices are helpful or needed to successfully run a distributed
    application. We also discussed what is needed to run such an application in production.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into networking limited to a single host.
    We are going to discuss how containers living on the same host can communicate
    with each other and how external clients can access containerized applications
    if necessary.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following articles provide more in-depth information regarding what was
    covered in this chapter:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '*Circuit* *breakers*: [http://bit.ly/1NU1sgW](http://bit.ly/1NU1sgW)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The OSI model* *explained*: [http://bit.ly/1UCcvMt](http://bit.ly/1UCcvMt)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Blue-green* *deployments*: [http://bit.ly/2r2IxNJ](http://bit.ly/2r2IxNJ)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please answer the following questions to assess your understanding of this
    chapter’s content:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: When and why does every part in a distributed application architecture have
    to be redundant? Explain this in a few short sentences.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need DNS services? Explain this in three to five sentences.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a circuit breaker and why is it needed?
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the important differences between a monolithic application
    and a distributed or multi-service application?
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a blue-green deployment?
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are the possible answers to this chapter’s questions:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: In a distributed application architecture, every piece of the software and infrastructure
    needs to be redundant in a production environment, where the continuous uptime
    of the application is mission-critical. A highly distributed application consists
    of many parts and the likelihood of one of the pieces failing or misbehaving increases
    with the number of parts. It is guaranteed that, given enough time, every part
    will eventually fail. To avoid outages of the application, we need redundancy
    in every part, be it a server, a network switch, or a service running on a cluster
    node in a container.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In highly distributed, scalable, and fault-tolerant systems, individual services
    of the application can move around due to scaling needs or due to component failures.
    Thus, we cannot hardwire different services with each other. Service A, which
    needs access to Service B, should not have to know details about Service B, such
    as its IP address. It should rely on an external provider for this information.
    TheDNS is such a provider of location information. Service A just tells it that
    it wants to talk to Service B and the DNS service will figure out the details.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A circuit breaker is a means to avoid cascading failures if a component in a
    distributed application is failing or misbehaving. Similar to a circuit breaker
    in electric wiring, a software-driven circuit breaker cuts the communication between
    a client and a failed service. The circuit breaker will directly report an error
    back to the client component if the failed service is called. This allows the
    system to recover or heal from failure.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A monolithic application is easier to manage than a multi-service application
    since it consists of a single deployment package. On the other hand, a monolith
    is often harder to scale to account for increased demand in one particular area
    of the application. In a distributed application, each service can be scaled individually
    and each service can run on optimized infrastructure, while a monolith needs to
    run on infrastructure that is OK for all or most of the features implemented in
    it. However, over time, this has become less of a problem since very powerful
    servers and/or VMs are made available by all major cloud providers. These are
    relatively cheap and can handle the load of most average line-of-business or web
    applications with ease.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maintaining and updating a monolith, if not well modularized, is much harder
    than a multi-service application, where each service can be updated and deployed
    independently. The monolith is often a big, complex, and tightly coupled pile
    of code. Minor modifications can have unexpected side effects. (Micro) Services,
    in theory, are self-contained, simple components that behave like black boxes.
    Dependent services know nothing about the inner workings of the service and thus
    do not depend on it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: The reality is often not so nice – in many cases, microservices are hard coupled
    and behave like distributed monoliths. Sadly, the latter is the worst place a
    team or a company can be in as it combines the disadvantages of both worlds, with
    the monolith on one side and the distributed application on the other.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: A blue-green deployment is a form of software deployment that allows for zero
    downtime deployments of new versions of an application or an application service.
    If, say, Service A needs to be updated with a new version, then we call the currently
    running blue version. The new version of the service is deployed into production,
    but not yet wired up with the rest of the application. This new version is called
    green. Once the deployment succeeds and smoke tests have shown it’s ready to go,
    the router that funnels traffic to the blue version is reconfigured to switch
    to the green version. The behavior of the green version is observed for a while
    and if everything is OK, the blue version is decommissioned. On the other hand,
    if the green version causes difficulties, the router can simply be switched back
    to the blue version, and the green version can be fixed and later redeployed.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
