<html><head></head><body>
        

                            
                    Assessments
                
            
            
                
<h1 id="uuid-ef61fa95-1bba-47e7-8d5f-f298abe66f73">Chapter 1</h1>
<ol>
<li>b and c: We can run more than one process per container, but it is not recommended because Docker Engine will only manage the main container process. We will need to manage additional logic between processes to start and stop everything at once. It is not easy and you can leave "zombie" processes in your hosts. Microservices are based on minimal functionality for each application component, which fits with containers very well.</li>
<li>b: Control groups, or cgroups, will manage the host resources provided to each container, but it is very important to understand that, by default, containers will run with unlimited resources.</li>
<li>a and b: Containers will run as root unless the source image has a non-root user definition or we specify a non-root user upon container creation. User namespaces allow us to use the root inside containers, although a real user outside the container can have a non-root ID. This is useful when processes require UID 0 to work.</li>
<li>d. All of the above sentences are true: Windows hosts will run two different types of isolation. We can run Linux containers on Windows, but this is not true in reverse.</li>
<li>a, b, and c: We can use <kbd>systemd</kbd> unit files or <kbd>/etc/docker/daemon.json</kbd> to configure the Docker daemon on Linux. On Windows hosts, <kbd>daemon.json</kbd> is located in the <kbd>%programdata%\docker\config\</kbd> directory. In both cases, Docker daemon remote access is not secure by default. </li>
</ol>
<h1 id="uuid-f6486050-990a-4aac-a0ba-6eda8c944352">Chapter 2</h1>
<ol>
<li>b: The image ID is the only identification of uniqueness when listing or managing images. We can have one ID with many names, including the registry part, and tags.</li>
<li>d: All the methods described are valid.</li>
</ol>
<ol start="3">
<li>b: Using a Dockerfile is a reproducible method as we describe all actions to add software, execute commands, add files, and more, in order to build a new image. We can automate and use templates to build images with Dockerfiles and this is the preferred method.</li>
<li>a and c: Only <kbd>RUN</kbd>, <kbd>CMD</kbd>, and <kbd>ENTRYPOINT</kbd> instructions admit shell and exec formats.</li>
<li>a: Using the shell format, the container main process, as defined by the <kbd>ENTRYPOINT</kbd> key, cannot be modified with arguments.</li>
</ol>
<h1 id="uuid-fc4533e0-fe09-451c-83ab-f0b61171da02">Chapter 3</h1>
<ol>
<li>a and c: <kbd>build</kbd> is only available for image objects, and <kbd>destroy</kbd> does not exist for any object.</li>
<li>b: This is not true. The Docker daemon will wait a defined amount of time (10 seconds, by default) before issuing a <kbd>SIGKILL</kbd> signal to the container's main process.</li>
<li>b: <kbd>docker kill</kbd> will immediately send a <kbd>SIGKILL</kbd> signal to the container's main process. Not all processes will be killed if they were executed in the background; for example, inside the container. It can leave zombie processes if they do not have parent-child dependencies. As we learned, containers must be removed by hand and <kbd>docker kill</kbd> will not remove them.</li>
</ol>
<ol start="4">
<li>b: <kbd>docker container update</kbd> will only change the container restart policy and its access to host resources.</li>
</ol>
<ol start="5">
<li>c: We have launched a privileged container; therefore no resource limits will be applied, although we have used <kbd>-memory</kbd> to confine memory usage. The privileged mode does not affect the filesystem. It will only modify the main process behavior, but in this case, we used a non-root user to create a new file on a directory owned by root and, as a result, it could not be created.</li>
</ol>
<h1 id="uuid-9fa4debc-7831-4462-aa92-57c9bfd60cb7" class="mce-root">Chapter 4</h1>
<ol>
<li>c: Each container will use its own filesystem unless we declare a shared volume for them.</li>
<li>a, b, and c: There are different types of volumes and it is not only allowed on container creation or execution.</li>
</ol>
<ol start="3">
<li>b: Docker volumes can be removed along with their associated container using the <kbd>--volumes</kbd> (or <kbd>-v</kbd>) option. A Docker volume purge will remove all unused volumes; those not associated to any container. But Docker will never remove a bind mount volume content (a local directory mounted on a container).</li>
<li>c: Only custom bridge networks are attachable after container creation. If we create or start a container and we want it to be connected to the default bridge network, we need to recreate it and attach it to that network on container creation.</li>
<li>b: Using <kbd>--publish-all</kbd> or <kbd>-P</kbd> will associate a random port between <kbd>32768</kbd> and <kbd>65535</kbd> to internal container port <kbd>80</kbd>. A NAT rule will automatically be created by the Docker daemon to allow this communication. You can disable the Docker daemon iptables management, but it is enabled by default.</li>
</ol>
<h1 id="uuid-6e11565f-4c00-4843-a1b4-0351f83c122c">Chapter 5</h1>
<ol>
<li>a: Docker Compose will run all application components just in one host. We will also use <kbd>docker-compose</kbd> files for deploying Swarm orchestrated applications with their components distributed on different hosts, but that requires a cluster running. In that case, we will not use the <kbd>docker-compose</kbd> binary to deploy the application; only the definition file will be valid and we will use it with the <kbd>docker stack</kbd> action. In Docker Swarm, we deploy swarm services, not containers.</li>
<li>d: Docker Compose provides all required actions to build, share, and deploy multi-container applications.</li>
<li>a and c: Docker Compose will review whether project images are present in the host. If they are not, the Docker daemon will try to download all not-present ones. Once the Docker daemon has all the required images, it will start all project containers and our terminal will be attached to containers' standard and error output unless the <kbd>--detach</kbd> or <kbd>-d</kbd> argument is used.</li>
<li>a: Docker Compose will allow us to scale the number of containers associated with a service. By default, Docker Compose will create a bridge network for our deployment, therefore an internal DNS will be associated and will manage all application IP addresses and names. In scaled services, we will receive one of the IP addresses of the replicas each time we ask for the defined service name. It uses round-robin DNS resolution.</li>
</ol>
<ol start="5">
<li>d: In this case, we could say that answer c is almost right, but it is incomplete. Docker Compose will remove all containers. If containers were running, they will be stopped before they are deleted. All associated resources created during the application execution will also be removed.</li>
</ol>
<h1 id="uuid-45fb96ae-838c-4b3e-b82b-dab10eee7b11">Chapter 6</h1>
<ol>
<li>c: Docker Content Trust is based on <strong>The Update Framework</strong> (<strong>TUF</strong>) and this framework was created to ensure the release of content between updates using different keys. It is possible to validate the trustfulness of a package or any other content using TUF.</li>
<li>a and d: Docker Content Trust will use Root, Targets, Snapshot, and Timestamp keys to ensure content.</li>
<li>c: We ensure image freshness using Content Trust, but it is true that we cannot ensure that the image tagged as "latest" in a given repository is actually the latest one created. We can only ensure that the image tagged as "latest" will be used. It is always recommended to use tags avoiding the use of "latest".</li>
<li>b: We tried to sign a version of a non-public write repository. We are not allowed to modify root repositories at docker.io.</li>
<li>d: We can recover the key if we have a backup. If it is not possible, we can generate a new one or let Docker generate one for us on first signing. Although we will be able to sign images after the new key has been generated, all our previously signed images will be untrusted because we changed our signature.</li>
</ol>
<h1 id="uuid-a64ccdb8-e7f6-441b-8a9c-cc24865f5537">Chapter 7</h1>
<ol>
<li>a: Orchestrators will not know anything about your application logic. On the other hand, we have quickly reviewed the interfaces that use orchestration to ensure that containers get the appropriate volumes of data on distributed environments.</li>
<li>c: Orchestrators will not manage application data, nor do they know anything about your application logic. The orchestrator will take care of the application components' health and will run a new instance if one of the required instances dies.</li>
<li>a and b: Distributed environments will help us to deploy applications with high availability and improved performance. But on the other hand, we will have new challenges because we need to be able to distribute application logic and components' interactions on different nodes.</li>
</ol>
<ol start="4">
<li>a and b: Answers a and b are correct, while c is not, because application components can be managed one at a time. Therefore, upgrades will only impact one application component if the application logic knows how to manage the situation.</li>
<li>
<p>c: All sentences are correct. We learned that we can define container limits and the required resources. Orchestrators will review these specifications and will deploy them on nodes with enough resources to ensure their correct execution. We can guide orchestration to choose labeled nodes, for example, to ensure application disk I/O, along with many other features. Each orchestrator will manage different rules and workflows to choose the best node for each workload.</p>
</li>
</ol>
<h1 id="uuid-c8299e0f-b3f5-4f8f-b695-f3296a6efcd9">Chapter 8</h1>
<ol>
<li class="mce-root">a: Docker Swarm is built into Docker Engine, but we have to enable Swarm mode for it to work. We can deploy other orchestrators such as Kubernetes, but it will involve extra work to deploy them. Orchestrators allow us to deploy applications on clusters, hence Swarm will deploy distributed applications.</li>
<li class="mce-root">d: Docker Swarm provides service discovery via DNS, internal load balancing for services and their tasks, and overlay networking for services and containers distributed on different nodes.</li>
<li class="mce-root">b: Each cluster has only one leader node. The leader is elected from the available managers. When we initialize a cluster, the first node will be the leader until a new election is required. All managers will run workloads unless we specifically avoid them using service constraints.</li>
<li class="mce-root">d: Roles can be changed as we require, such as for maintenance, for example. We need to always maintain the defined number of odd managers to avoid cluster instability.</li>
<li class="mce-root">a and b: By default, Docker Swarm will deploy stacks on its own network, unless others are specified. Everything related to the application to be deployed must be configured in the Docker Compose stack file. We can add externally created components, but they must exist before the stack is deployed and we will set them as external in the infrastructure-as-code stack file.</li>
</ol>
<h1 id="uuid-8a646fad-c1e9-4e40-9529-662be5bf4508">Chapter 9</h1>
<ol>
<li>a and b: Kubernetes requires etcd to work. Most of the Kubernetes deployment solutions will deploy etcd for you, but it is an external application and therefore it is up to you to manage and ensure that the key-value solution provides high availability. Kubernetes internal networking will work out of the box, but communications between components deployed on different hosts rely on external plugins (the CNI standard). Therefore, we will need to choose and deploy ourselves a solution to provide this kind of communication.</li>
<li>b and c: We will deploy pods in Kubernetes, hence these are the minimum unit of deployment. We can deploy more than one container in a pod. The container density is higher in Kubernetes. Scaling pods will replicate all their components at once.</li>
<li>d: All sentences are true. All containers in a pod share the same IP address and localhost. They also share pod volumes. <strong>Container Network Interface</strong> (<strong>CNI</strong>) is not required for connections between pods running on the same host. They all are accessible using their virtual IP addresses.</li>
<li>a, b, and c: ReplicaSets allow us to manage replicated environments. Deployments create ReplicaSets and allow us to scale application pods both up and down. They will also maintain the application health based on the required running pods. DaemonSets will ensure one replica on each cluster node.</li>
<li>a: The ClusterIP service type will only provide internal access to a service. The assigned IP is not available from the cluster nodes.</li>
</ol>
<p class="mce-root"/>
<h1 id="uuid-f37951bb-1df9-4d57-aec4-ccceffe38bd0">Chapter 10</h1>
<ol>
<li>c: Docker Machine is maintained by the Docker community.</li>
<li>a: Docker Enterprise provides a supported and enterprise-ready CaaS platform, with supported Kubernetes, <strong>Universal Control Plane</strong> (<strong>UCP</strong>), and <strong>Docker Trusted Registry</strong> (<strong>DTR</strong>, based on the Docker Registry community). We can deploy Docker Swarm in production even with Docker support using Docker Enterprise Engine.</li>
<li>a, b, and d: Docker Enterprise provides Kubernetes out of the box when we deploy UCP – we do not need to install Kubernetes manually.</li>
</ol>
<ol start="4">
<li>d: We will use fixed IP addresses for all components. We will use an external load balancer to forward traffic to all manager nodes for UCP and all worker nodes with DTR running for the registry. Forwarding traffic to just one node will not provide high availability if it fails. The Docker UCP installation will deploy Calico by default for Kubernetes, but we will need to review <kbd>pod-cidr</kbd> and <kbd>service-cidr</kbd> to ensure that the subnets defined by default will be valid in our environment.</li>
<li>c: To provide high availability for workloads, we will deploy at least two Linux nodes. Although it is possible to run DTR on UCP managers, it is not recommended because managers need to have enough resources for control plane tasks and image scanning can affect cluster stability. We will also need to choose different ports for the applications' frontends because both use port <kbd>443</kbd>.</li>
</ol>
<h1 id="uuid-213db2cd-e5b7-4a3c-9a54-ea63a40e49e4">Chapter 11</h1>
<ol>
<li class="mce-root">b: As we have learned, Docker Enterprise Engine is required to install UCP. It will not be installed automatically for us. We can use Web UI, the UCP bundle, and the UCP API to manage our workloads and cluster configurations. UCP's RBAC system will manage authorizations, but it is true that it will also authenticate users if no external authorization source is configured or it is not available.</li>
<li class="mce-root">b and d: Docker provides a complete UCP backup and restore solution with the <kbd>docker/ucp</kbd> image, but remember that we should take care of Docker Swarm's filesystem because it is not part of UCP's backup. We should use the appropriate <kbd>docker/ucp</kbd> image release for our environment. In fact, we will use the same installed release for any action other than upgrading. UCP removal can be executed from the <kbd>docker/ucp</kbd> image and this will remove UCP components from all nodes in the cluster. We should then remove the <kbd>docker/ucp</kbd> image. The upgrade process can be achieved automatically using the <kbd>docker/ucp</kbd> image, but this may impact your users. We will usually upgrade UCP managers automatically and then execute upgrade steps manually on worker nodes.</li>
<li class="mce-root">a and b: We can use <kbd>--controller-port</kbd> and:<kbd>-kube-apiserver-port</kbd> to modify the UCP controller and Kubernetes' API server ports. We can also isolate the control plane from the data plane by choosing different interfaces in multihomed hosts using <kbd>--data-path-addr</kbd>. <strong>Subject Alias Names</strong> (<strong>SANs</strong>) will add alias names to UCP's certificate. We can add all required aliases for our environment using <kbd>--san</kbd> multiple times.</li>
</ol>
<ol start="4">
<li class="mce-root">a, c, and d: UCP deploys a Kubernetes cluster with high availability on top of Docker Swarm. As UCP is deployed on Docker Swarm, we will need at least three nodes to provide high availability. All managers will run the same control plane processes and an external load balancer is required to distribute access between them. This requires a transparent proxy configuration to allow managers to manage encrypted communications. We will use the <kbd>/_ping</kbd> endpoint to verify manager nodes' health and it can be used on load balancers as a backend health check.</li>
<li class="mce-root">a and c: UCP provides <strong>None</strong>, <strong>View Only</strong>, <strong>Restricted Control</strong>, <strong>Scheduler</strong>, and <strong>Full Control</strong>. We can create new roles, but there are no privileged or administrator roles by default. Docker Enterprise administrators are not defined as roles. There is a checkbox in the user's properties to enable this feature. Only administrators can create grants, users, teams, organizations, and collections.</li>
</ol>
<h1 id="uuid-049b4465-261b-4360-8b68-dd502b78b5da">Chapter 12</h1>
<ol>
<li class="mce-root">b and c: There are two labels that are always required. We will need to ensure Interlock forwards the service's requests using <kbd>com.docker.lb.hosts</kbd> and <kbd>com.docker.lb.port</kbd>. These will have all the required information, but <kbd>com.docker.lb.network</kbd> is recommended and required if the service's instances are attached to more than one network. We need to specify which network should be used as an ingress.</li>
<li class="mce-root">b: The Interlock solution is based on a main process named <kbd>interlock</kbd>, a process for managing external proxy services and configurations, and an <kbd>interlock-proxy</kbd> service that will run inside the Docker Enterprise environment if no external load balancer is specified. These three processes run as services within Docker Swarm and they are prefixed with <kbd>ucp-</kbd>. <kbd>ucp-interlock-controller</kbd> does not exist.</li>
<li class="mce-root">d: By default, only the <kbd>ucp-interlock</kbd> service will be located by the node's roles. All other components can run anywhere. We will use location constraints to run the <kbd>ucp-interlock-proxy</kbd> and <kbd>ucp-interlock-extension</kbd> components on worker nodes.</li>
<li class="mce-root">a, b, and c: Interlock allows us to either manage SSL/TLS tunnels on <kbd>ucp-interlock-proxy</kbd> or configure it as a transparent proxy. In this case, our services' backends should manage SSL/TLS certificates. Interlock interacts with the Docker API and all changes will be updated automatically on Interlock's proxy component. Interlock is a Layer 7 load balancer; reverse proxy, TCP, and UDP protocols should be published using a routing mesh or host mode.</li>
</ol>
<ol start="5">
<li class="mce-root">a and b: Ingress controllers and Interlock have a common logic, using a few published ports. They will manage all ingress traffic using load balancing and reverse proxy features. We will not publish applications directly. No application's service has to be exposed directly. The ingress controllers (and Interlock) will be exposed and they will route requests to the application's defined services. Interlock has to interact with the application's services, hence it has to connect to their networks. This will happen automatically. Docker Enterprise will connect the <kbd>interlock-proxy</kbd> service to our application's networks.</li>
</ol>
<h1 id="uuid-ea74af19-e4c1-42c1-8cd0-f092dd38c6ea">Chapter 13</h1>
<ol>
<li>b: This list only shows one valid feature. DTR provides repository mirroring. Neither repository load balancing nor repository signing are valid features. We do not sign repositories. We sign repositories' images/tags.</li>
<li>b: DTR does not manage images' data with high availability. Deploying more than one replica will provide high availability for DTR's processes. DTR replication requires data sharing between replicas, but we must include third-party solutions to provide high availability for our storage.</li>
<li>a and b: The DTR installation runs the <kbd>dtr-garant</kbd> and <kbd>dtr-jobrunner</kbd> containers. The first will manage user authentication, while jobrunner will execute DTR's maintenance tasks to remove unreferenced layers. <kbd>dtr-notary-server</kbd> and <kbd>dtr-notary-signer</kbd> will be deployed within DTR to manage Docker Content Trust metadata.</li>
<li>d: All the question's sentences describe required steps for deploying DTR with high availability.</li>
<li>a: DTR backups do not include images' layers. This can constitute a great amount of data and is the key to recovering your images. You should prepare third-party solutions for this data. On the other hand, repository metadata, RBAC configurations, and images' signatures will be stored within your backup TAR file.<br/></li>
</ol>
<h1 id="uuid-9dfad9b1-8759-4e02-9646-493f9441aa41">Exam answers</h1>
<p>1 - b and c</p>
<p>2 - c</p>
<p>3 - b</p>
<p>4 - a, b, and c</p>
<p>5 - b</p>
<p>6 - c</p>
<p>7 - a</p>
<p>8 - a, c, and d</p>
<p>9 - c</p>
<p>10 - a and c</p>
<p>11 - b</p>
<p>12 - a</p>
<p>13 - d</p>
<p>14 - b</p>
<p>15 - c</p>
<p>16 - a, b, and c</p>
<p>17 - a, b, and c</p>
<p>18 - a</p>
<p>19 - b and c</p>
<p>20 - a and b</p>
<p>21 - d</p>
<p>22 - c</p>
<p>23 - b, c, and d</p>
<p class="mce-root"/>
<p>24 - c and d</p>
<p>25 - a, c, and d</p>
<p>26 - a and c</p>
<p>27 - c</p>
<p>28 - c</p>
<p>29 -c</p>
<p>30 - b</p>
<p>31 - d</p>
<p>32 - a and b</p>
<p>33 - d</p>
<p>34 - b and c</p>
<p>35 - d</p>
<p>36 - b</p>
<p>37 - a</p>
<p>38 - c</p>
<p>39 - c</p>
<p>40 - a and b</p>
<p>41 - a and b</p>
<p>42 - b and c</p>
<p>43 - a and c</p>
<p>44 - c</p>
<p>45 - a and c</p>
<p>46 - b</p>
<p>47 - d</p>
<p>48 - b, c, and d</p>
<p>49 - a and b</p>
<p>50 - a</p>
<p>51 - a</p>
<p>52 - a, b, and c</p>
<p>53 - b</p>
<p>54 - a</p>
<p>55 - c</p>
<p>56 - b</p>
<p>57 - c and d</p>
<p>58 - d</p>
<p>59 - d</p>
<p>60 - b</p>
<p>61 - a</p>
<p>62 - b and c</p>
<p>63 - b</p>
<p>64 - b</p>
<p>65 - a</p>
<p>66 - a, b, and c</p>
<p>67 - b</p>
<p>68 - a and b</p>
<p>69 - a</p>
<p>70 - b and d</p>


            

            
        
    </body></html>