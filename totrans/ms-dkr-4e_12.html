<html><head></head><body><div><div><p id="_idParaDest-214" class="chapter-number"><a id="_idTextAnchor394"/><em class="italic">Chapter 12</em></p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor395"/>Discovering other Kubernetes options</h1>
			<p>In this chapter, we will look at alternatives to using Docker's in-built support for running your own local Kubernetes single-node and multi-node clusters.</p>
			<p>We will be discussing and looking at the following tools:</p>
			<ul>
				<li>Minikube</li>
				<li><strong class="bold"><a id="_idTextAnchor396"/>K<a id="_idTextAnchor397"/>ind</strong> (<strong class="bold">Kubernetes in Docker</strong>)</li>
				<li><a id="_idTextAnchor398"/>M<a id="_idTextAnchor399"/>icroK8s</li>
				<li><a id="_idTextAnchor400"/>K<a id="_idTextAnchor401"/>3s</li>
			</ul>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor402"/>Technical requirements</h1>
			<p>We will be using one of the tools we discussed in <a href="B15659_06_Final_JM_ePub.xhtml#_idTextAnchor187"><em class="italic">Chapter 6</em></a>,<em class="italic"> Docker Machine, Vagrant, and Multipass</em>, along with some standalone tools that can be used to bootstrap your own local Kubernetes installation.  </p>
			<p>Again, the screenshots in this chapter will be from my preferred operating system, macOS</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/3byY104">https://bit.ly/3byY104</a>.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor403"/>D<a id="_idTextAnchor404"/>eploying Kubernetes using Minikube</h1>
			<p>First in our<a id="_idIndexMarker857"/> list of alternative<a id="_idTextAnchor405"/> <a id="_idTextAnchor406"/>Kubernetes<a id="_idIndexMarker858"/> cluster installers is Minikube. It was initially released in May 2016, making it the oldest of the tools we will be discussing in this chapter.</p>
			<p>Before we look at installing Minikube, we should probably discuss why it was needed in the first place.</p>
			<p>At the time of its original release, Kubernetes <code>1.2</code> had been out for a few months, and it was almost a year after the <code>1.0</code> release of Kubernetes.</p>
			<p>While installing Kubernetes had become a lot simpler since the original release, it typically still boiled down to a bunch of installation scripts and step-by-step instructions that were designed to mostly bootstrap cloud-hosted clusters utilizing the cloud provider's APIs or command-line tools.</p>
			<p>If you wanted to run an installation locally for development purposes, then you would have to<a id="_idIndexMarker859"/> either hack together your installer from existing scripts or download a Vagrant box where you would be trusting the author of the box to have built it using the best practices promoted by the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>), which was trying to ensure consistency between clusters.</p>
			<p>Although Minikube started life as a way of creating a local Kubernetes node on just Linux and macOS hosts, Windows was soon introduced, and the project has grown to be an important <a id="_idIndexMarker860"/>part of the Kubernetes project as it is <a id="_idIndexMarker861"/>many people's first taste of interacting with Kubernetes and it forms part of the official Kubernetes documentation.</p>
			<p>Now that we know a little about Minikube's background, let's look at getting it installed and then launching a single-node cluster.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor407"/>Installing Minikube</h2>
			<p>Minikube is<a id="_idIndexMarker862"/> designed to run a single binary, and like Kubernetes itself, it is written in Go, which means that it can easily be compiled to run on the three platforms we have been looking at in this title. To start with, let's have a look at how to install Minikube on macOS.</p>
			<h3>Installing Minikube on macOS</h3>
			<p>When we have <a id="_idIndexMarker863"/>discussed installing software and tools on <a id="_idIndexMarker864"/>macOS in previous chapters, we have mentioned and used Homebrew.</p>
			<p>Installing Minikube follows this path, meaning that it can be installed using a single command, which as you may have already guessed, looks like the following:</p>
			<pre>$ brew install minikube</pre>
			<p>Once installed, you can verify everything is working as expected by running the fol<a id="_idTextAnchor408"/>l<a id="_idTextAnchor409"/>owing:</p>
			<pre>$ minikube version</pre>
			<p>This will output the version of Minikube that was installed along with the commit ID that version was compile<a id="_idTextAnchor410"/>d<a id="_idTextAnchor411"/> from.</p>
			<h3>Installing Minikube on Windows 10</h3>
			<p>Like Homebrew <a id="_idIndexMarker865"/>on macOS, when it comes to <a id="_idIndexMarker866"/>package managers on Windows, the<a id="_idTextAnchor412"/> <a id="_idTextAnchor413"/>â€¨go-to is Chocolatey. If you have it installed, you can install Minikube using the following command:</p>
			<pre>$ choco install minikube</pre>
			<p>If you are not using Chocolatey, then you can download a Windows installer from <a href="https://storage.googleapis.com/minikube/releases/latest/minikube-installer.exe">https://storage.googleapis.com/minikube/releases/latest/minikube-installer.exe</a>.</p>
			<p>Once you have Minikube installed, you can run the following<a id="_idTextAnchor414"/> <a id="_idTextAnchor415"/>command:</p>
			<pre>$ minikube version</pre>
			<p>This will confirm the installed version and commit.</p>
			<h3>Installing Minikube on Linux</h3>
			<p>Depending on<a id="_idIndexMarker867"/> the version of Linux you are running, there<a id="_idIndexMarker868"/> are a few different ways to install Minikube.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Before running the commands that follow, check the project's release page on GitHub to confirm what version of Minikube to install. The page can be found at <a href="https://github.com/kubernetes/minikube/releases/">https://github.com/kubernetes/minikube/releases/</a>. At the time of writing, the latest release is 1.9.2-0, so the instructions will use that version.</p>
			<p>If you are running a Debian-based system that uses an APT such as Ubuntu or Debian itself, then you can download and install a <code>.deb</code> using the followin<a id="_idTextAnchor416"/>g <a id="_idTextAnchor417"/>c<a id="_idTextAnchor418"/>o<a id="_idTextAnchor419"/>m<a id="_idTextAnchor420"/>mands:</p>
			<pre>$ MK_VE<a id="_idTextAnchor421"/>R<a id="_idTextAnchor422"/>=1.9.2-0
$ curl -LO https://storage.googleapis.com/minikube/releases/
lat<a id="_idTextAnchor423"/>e<a id="_idTextAnchor424"/>s<a id="_idTextAnchor425"/>t<a id="_idTextAnchor426"/>/<a id="_idTextAnchor427"/>minikube_$MK_VER_amd64.deb
$ sudo dpkg -i minikube_$MK_VER _amd64.deb</pre>
			<p>If you are<a id="_idIndexMarker869"/> running an RPM-based system, such as CentOS or <a id="_idIndexMarker870"/>Fedora, then you can use the following commands:</p>
			<pre>$ MK_<a id="_idTextAnchor428"/>V<a id="_idTextAnchor429"/>ER=1.9.2-0
$ curl -LO https://storage.googleapis.com/minikube/releases/
l<a id="_idTextAnchor430"/>a<a id="_idTextAnchor431"/>test/minikube-$MK_VER.x86_64.rpm
sudo rpm -ivh minikube-$MK_VER.x86_64.rpm</pre>
			<p>Finally, if you would prefer not to use a package manager, then you can download the latest static binary using the following commands:</p>
			<pre>$ curl -LO https://storage.googleapis.com/minikube/releases/
latest/minikube-<a id="_idTextAnchor432"/>l<a id="_idTextAnchor433"/>inux-amd64
$ sudo install minikube-Linux-amd64 /usr/local/bin/minikube</pre>
			<p>Whichever way you have chosen to install Minikube, you can run the following to confirm that everything is installed and working as expected:</p>
			<pre>$ minikube version</pre>
			<p>Again, this will <a id="_idIndexMarker871"/>confirm the installed version and the commit it was <a id="_idIndexMarker872"/>compiled from.</p>
			<h3>Minikube drivers</h3>
			<p>As we have already<a id="_idIndexMarker873"/> mentioned, Minikube is a standalone static binary that helps you launch a Kubernetes node on your local machine; it does this by interacting with several hypervisors. Before we start to use Minikube, let's quickly look at the<a id="_idTextAnchor434"/> <a id="_idTextAnchor435"/>options:</p>
			<ul>
				<li><strong class="bold">Docker (macOS, Windows, and Linux)</strong>: This driver uses Docker Machine to launch containers that host your Kubernetes node. On Linux, it will use just containers, and on macOS and Windows, it will launch a small virtual machine too and deploy the containers there.</li>
				<li><strong class="bold">VirtualBox (macOS, Windows, and Linux)</strong>: This driver works across three operating systems and will launch a virtual machine and then configure your node.</li>
				<li><strong class="bold">HyperKit (macOS)</strong>: This uses the hypervisor built into macOS to host a virtual machine where your node will be configured.</li>
				<li><strong class="bold">Hyper-V (Windows 10 Pro)</strong>: This uses the native hypervisor that is built into Windows<a id="_idTextAnchor436"/> <a id="_idTextAnchor437"/>10 Professional to host a virtual machine where your node will be launched and configured.</li>
				<li><strong class="bold">KVM2 (Linux)</strong>: This driver uses <strong class="bold">(Kernel-based Virtual Machine KVM)</strong> to launch a virtual machine.</li>
				<li><strong class="bold">Podman (Linux)</strong>: This experimental driver uses a Docker replacement called Podman to launch containers that make up your cluster.</li>
				<li><strong class="bold">Pa<a id="_idTextAnchor438"/>r<a id="_idTextAnchor439"/>allels (macOS)</strong>: Uses the macOS-only Parallels to host a virtual machine where your node will be launched and configured.</li>
				<li><strong class="bold">VMware (macOS)</strong>: Uses the macOS-only VMware Fusion to host a virtual machine where your node will be launched and configured.</li>
				<li><strong class="bold">None (Linux)</strong>: This final option does away with containers and virtual machines launched in your local hypervisor; it simply installs your cluster node directly on the host you are running.</li>
			</ul>
			<p>If you have made it this far through the book, then you will have already installed the perquisites for at least one of the drivers mentioned above on your host machine. Minikube is intelligent enough to figure out the best driver to use on your host so we should be able to proceed without worrying that things won't work.</p>
			<p>However, should you get an error because a support driver for your chosen host operating system is not found, then see the <em class="italic">Further reading</em> section for links to each of the preceding projects.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Should you find yourself without a supported driver install, I would recommend installing VirtualBox as it has the least requirements and complexity out of all of the drivers.</p>
			<p>Now that we have Minikube installed, and we have looked at the drivers that it needs to launch and configure your cluster node, we can get to work and launch our cluster node.</p>
			<p>Launching a cluster node using Minikube</p>
			<p>It doesn't matter<a id="_idIndexMarker874"/> what your host operating <a id="_idIndexMarker875"/>system, launching a Kubernetes cluster node using Minikube is a single, simple command:</p>
			<pre>$ minikube start</pre>
			<p>When you first run the command, it may take several minutes after detecting your local configuration as it has to download and configure various supporting tools, virtual machine images, and containers (depending on the driver it is using).</p>
			<p>You may also find that it asks for passwords for parts of the installation that require elevated privileges. These steps are a one-off and once you have launched your initial Kubernetes cluster node, subsequent cluster node launches should be a lot quicker.</p>
			<p>The following Terminal output shows my initial Minikube cluster node launching:</p>
			<div><div><img src="img/image_00_0013.jpg" alt="Figure 12.1 â€“ Launching a Kubernetes cluster node on macOS&#13;&#10;" width="1217" height="745"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 â€“ Launching a Kubernetes cluster node on macOS</p>
			<p>Once your <a id="_idIndexMarker876"/>cluster node is up and running, you may notice a <a id="_idIndexMarker877"/>message at the bottom of the output highlighting<a id="_idTextAnchor440"/> <a id="_idTextAnchor441"/>that the version of <code>kubectl</code> is out of date and may not be compatible with the version of Kubernetes that is installed on the Minikube launched cluster node.</p>
			<p>Luckily, you can work around this by prefixing our <code>kubectl</code> commands with <code>minikube</code>, and adding <code>â€“</code> after. This will download and install a compatible version of <code>kubectl</code>, but it will be isolated in our Minikube workspace:</p>
			<pre>$ minikube kubectl -- get pods</pre>
			<p>You should see something like the following output:</p>
			<div><div><img src="img/image_00_0023.jpg" alt="Figure 12.2 â€“ Initializing a compatible version of kubectl using Minikube&#13;&#10;" width="995" height="144"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 â€“ Initializing a compatible version of kubectl using Minikube</p>
			<p>So, why did we get this message in the first place? Well, the Docker for Mac installation comes bundled with a version of <code>kubectl</code> that supports the version of Kubernetes natively supported within Docker, which at the time of writing is version <code>1.15.5</code>, whereas Minikube has pulled down the latest stable release of K<a id="_idTextAnchor442"/>u<a id="_idTextAnchor443"/>bernetes, which is <code>1.18.0</code>.</p>
			<p>One of the advantages of using Minikube over the native Docker-supported Kubernetes is that you can run different versions of Kubernetes and easily isolate the versions of the <a id="_idIndexMarker878"/>supporting tools required to interact<a id="_idIndexMarker879"/> with various versions by prefixing your <code>kubectl</code> commands with <code>minikube</code> and adding <code>--</code> immediately after.</p>
			<p>This is great when you have to test how your applications will react when it is time to update your production<a id="_idTextAnchor444"/> <a id="_idTextAnchor445"/>K<a id="_idTextAnchor446"/>u<a id="_idTextAnchor447"/>bernetes clusters.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor448"/>Interacting with your Kubernetes cluster node</h2>
			<p>Now that we<a id="_idIndexMarker880"/> have our cluster node<a id="_idIndexMarker881"/> up and running, we can run through some common commands and launch a test application.</p>
			<p>To start with, let's get a little information abo<a id="_idTextAnchor449"/>u<a id="_idTextAnchor450"/>t our cluster node. In <a href="B15659_11_Final_JM_ePub.xhtml#_idTextAnchor294"><em class="italic">Chapter 11</em></a><em class="italic">, Docker and Kubernetes</em>, we ran the following commands to get information on the cluster node and namespaces:</p>
			<pre>$ kubectl get nodes
$ kubectl get namespaces</pre>
			<p>Let's run them again, remembering to prefix <code>minikube</code> before and <code>--</code> after each command so that t<a id="_idTextAnchor451"/><a id="_idTextAnchor452"/><a id="_idTextAnchor453"/><a id="_idTextAnchor454"/>hey now look like this:</p>
			<pre>$ minikube kubectl -- get nodes
$ minikube kubectl -- get namespaces
$ minikube kubectl -- get --namespace kube-system pods</pre>
			<p>Take a look at the following Terminal output:</p>
			<div><div><img src="img/image_00_0033.jpg" alt="Figure 12.3 â€“ Getting information on the cluster node&#13;&#10;" width="856" height="584"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 â€“ Getting information on the cluster node</p>
			<p>The output <a id="_idIndexMarker882"/>we get when running the <a id="_idIndexMarker883"/>commands is similar to that we got when we executed<a id="_idTextAnchor455"/> <a id="_idTextAnchor456"/>the equivalent commands in <a href="B15659_11_Final_JM_ePub.xhtml#_idTextAnchor294"><em class="italic">Chapter 11</em></a>, <em class="italic">Docker and Kubernetes</em>, although the pods listed for the <code>kube-system</code> namespace are different.</p>
			<p>Next up, we can launch a test application by runnin<a id="_idTextAnchor457"/>g <a id="_idTextAnchor458"/>t<a id="_idTextAnchor459"/>he following command:</p>
			<pre>$ minikube kubectl -- create deployment hello-node --image=k8s.
gcr.io/echoserver:1.4</pre>
			<p>Once the deployment has been created, you can view its status and the pod involved b<a id="_idTextAnchor460"/>y<a id="_idTextAnchor461"/> running the following:</p>
			<pre>$ minikube kubectl -- get deployments
$ minikube kubectl -- get pods</pre>
			<p>You should see something similar to the following Terminal output:</p>
			<div><div><img src="img/image_00_0043.jpg" alt="Figure 12.4 â€“ Launching an application deployment and checking the status&#13;&#10;" width="1094" height="281"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 â€“ Launching an application deployment and checking the status</p>
			<p>Now that <a id="_idIndexMarker884"/>our application deployment has <a id="_idIndexMarker885"/>been launched, we need a way of interacting with it. To do this, we can run the following command, which will launch a load balan<a id="_idTextAnchor462"/>c<a id="_idTextAnchor463"/>e<a id="_idTextAnchor464"/>r<a id="_idTextAnchor465"/> <a id="_idTextAnchor466"/>service on port <code>8080</code>:</p>
			<pre>$ minikube kubectl -- expose deployment hello-node 
--type=LoadBalancer --port=8080</pre>
			<p>Once the service has been exposed, we can run the following command to get more informa<a id="_idTextAnchor467"/>t<a id="_idTextAnchor468"/>i<a id="_idTextAnchor469"/>o<a id="_idTextAnchor470"/>n<a id="_idTextAnchor471"/> on running services:</p>
			<pre>$ minikube kubectl -- get services</pre>
			<p>If our cluster node was hosted on a public cloud, the command will let us know what the external IP address of the service is, however, when we run the command, we ge<a id="_idTextAnchor472"/>t <a id="_idTextAnchor473"/>the following output:</p>
			<div><div><img src="img/image_00_0053.jpg" alt="Figure 12.5 â€“ Exposing the service on port 8080&#13;&#10;" width="1065" height="226"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 â€“ Exposing the service on port 8080</p>
			<p>As you can see, <code>EXTERNAL-IP</code> is listed as <code>&lt;pending&gt;</code>, so how do we access the application we deployed?</p>
			<p>To this, we need to use <a id="_idTextAnchor474"/>t<a id="_idTextAnchor475"/>he following command:</p>
			<pre>$ minikube service hello-node</pre>
			<p>This command will open the <code>hello-node</code> service in the default browser on your machine as well as printing the URL you can access the service on in the Terminal:</p>
			<div><div><img src="img/image_00_0063.jpg" alt="Figure 12.6 â€“ Opening the hello-node service&#13;&#10;" width="877" height="254"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6 â€“ Opening the hello-node service</p>
			<p>The <code>hello-node</code> application <a id="_idIndexMarker886"/>simply echoes<a id="_idIndexMarker887"/> back the headers sent by your browser:</p>
			<div><div><img src="img/image_00_0073.jpg" alt="Figure 12.7 â€“ Viewing the hello-node application in a browser&#13;&#10;" width="1103" height="476"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.7 â€“ Viewing the hello-node application in a browser</p>
			<p>Before we finish looking at Minikube, we should look at launching the cluster application we have been running in the previous chapters. To do this, run the following commands to create the deployment, expose the service, and get some information on the ru<a id="_idTextAnchor476"/>n<a id="_idTextAnchor477"/>ning pods and services:</p>
			<pre>$ minikube kubectl -- create deployment cluster 
--image=russmckendrick/cluster:latest
$ minikube kubectl -- expose deployment cluster 
--type=LoadBalancer --port=80
$ minikube kubectl -- get svc,pods</pre>
			<p>Once launched, we can list the URLs of all of the exposed services on our cluster node by running the following:</p>
			<pre>$ minikube service list</pre>
			<p>Opening <a id="_idIndexMarker888"/>the URL for the cluster service in a<a id="_idIndexMarker889"/> browser should show the application as expected:</p>
			<div><div><img src="img/image_00_0083.jpg" alt="Figure 12.8 â€“ Viewing the cluster application in a browser&#13;&#10;" width="924" height="617"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8 â€“ Viewing the cluster application in a browser</p>
			<p>Before we move on to the next tool, let's have a quick look at some of the other Minikube commands.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor478"/>Managing Minikube</h2>
			<p>There are a few other<a id="_idIndexMarker890"/> commands we can use to manage our cluster node. The first of which is a command that allows you to quickly access the Kubernetes dashboard.</p>
			<h3>Minikube dashboard</h3>
			<p>Accessing the <a id="_idIndexMarker891"/>Kubernetes dashboard <a id="_idIndexMarker892"/>is a little more straight forward using Minikube; in fact, <a id="_idTextAnchor479"/>i<a id="_idTextAnchor480"/>t is just a single command:</p>
			<pre>$ minikube dashboard</pre>
			<p>This will enable the dashboard, launch the proxy, and then open the dashboard in your default browser:</p>
			<div><div><img src="img/image_00_0093.jpg" alt="Figure 12.9 â€“ Viewing the Kubernetes dashboard&#13;&#10;" width="1026" height="470"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.9 â€“ Viewing the Kubernetes dashboard</p>
			<p>There is no additional work to configure authentication needed this time. Also, you can use the following command to just get the URL you need to access the dashboard:</p>
			<pre>$ minikube dashboard</pre>
			<p>This will return something like the following output:</p>
			<div><div><img src="img/image_00_0103.jpg" alt="Figure 12.10 â€“ Getting the dashboard URL&#13;&#10;" width="1259" height="254"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.10 â€“ Getting the dashboard URL</p>
			<p>Now let's <a id="_idIndexMarker893"/>look at launching a different<a id="_idIndexMarker894"/> version of Kubernetes.</p>
			<h3>Minikube start with Kubernetes version</h3>
			<p>When we first<a id="_idIndexMarker895"/> launched our cluster node, I mentioned<a id="_idIndexMarker896"/> it is possible to launch a cluster node with a different version of Kubernetes. Using the following command, we can launch a second cluster node running Kubernetes <code>v1.15.5</code>, which is the same version currently supported by Docker for Mac:</p>
			<pre>$ minikube start --kubernetes-version=1.15.5 -p node2</pre>
			<p>This command should show you something similar to the output we originally saw when we first launched our cluster node:</p>
			<div><div><img src="img/image_00_0113.jpg" alt="Figure 12.11 â€“ Installing Kubernetes v1.15.5 on a second cluster node&#13;&#10;" width="1021" height="446"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.11 â€“ Installing Kubernetes v1.15.5 on a second cluster node</p>
			<p>As you can see, the process was pretty painless, and this time we did not get a complaint about potential compatibility issues with the locally installed <code>kubectl</code>.</p>
			<p>We can view the different cluster no<a id="_idTextAnchor481"/>d<a id="_idTextAnchor482"/>es by running the following:</p>
			<pre>$ minikube profile list</pre>
			<p>This will list the available cluster nodes. To check which one you are currently using, run the following:</p>
			<pre>$ minikube profile</pre>
			<p>To switch to a different cluster node run this:</p>
			<pre>$ minikube profile node2</pre>
			<p>Make sure you<a id="_idIndexMarker897"/> use the cluster node name you want to <a id="_idIndexMarker898"/>switch to:</p>
			<div><div><img src="img/image_00_0123.jpg" alt="Figure 12.12 â€“ Switching cluster nodes&#13;&#10;" width="967" height="419"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.12 â€“ Switching cluster nodes</p>
			<p>Now we can switch cluster nodes, what about accessing them?</p>
			<h3>Minikube SSH</h3>
			<p>Although you<a id="_idIndexMarker899"/> shouldn't need to access the cluster node itself, you can run this to get shell access to the currently selected cluster node:</p>
			<pre>$ minikube ssh</pre>
			<p>This is useful if you are curious to see what is going on under the hood.</p>
			<h3>Minikube stop and delete</h3>
			<p>The final commands <a id="_idIndexMarker900"/>we are going to look at are the ones to stop our cluster nodes or remove them altogether; you may have already guess<a id="_idTextAnchor483"/>e<a id="_idTextAnchor484"/>d the command to stop a node:</p>
			<pre>$ minikube stop</pre>
			<p>This will stop the currently selected cluster node, and it can easily b<a id="_idTextAnchor485"/>e<a id="_idTextAnchor486"/> started up with the followi<a id="_idTextAnchor487"/>n<a id="_idTextAnchor488"/>g:</p>
			<pre>$ minikube start</pre>
			<p>You can add <code>-p &lt;profile name&gt;</code> to stop or start another cluster node. To remove the<a id="_idTextAnchor489"/> c<a id="_idTextAnchor490"/>l<a id="_idTextAnchor491"/>uster node, you can run this:</p>
			<pre>$ minikube delete</pre>
			<p>This will remove the currently selected cluster node. Again, you can add <code>-p &lt;profile name&gt;</code> to interact with any other cluster node. Running the following will de<a id="_idTextAnchor492"/>l<a id="_idTextAnchor493"/>ete all of the cluster nodes:</p>
			<pre>$ minikube delete --all </pre>
			<p>There is no warning or "are you sure?" prompt when running the <code>minikube delete</code> command, so please be careful.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor494"/>Minikube summary</h2>
			<p>As I am sure you will <a id="_idIndexMarker901"/>agree, Minikube has a wealth of options and is extremely straightforward to use. With it being part of the Kubernetes project itself, you will find that it is always a more up-to-date Kubernetes experience than you would get enabling Kubernetes in Docker for Mac or Docker for Windows, and it also has Linux support.</p>
			<p>Finally, you also get an environment that is a lot closer to a CNCF-compliant Kubernetes cluster running on a public cloud, which we'll be looking at in more detail in <a href="B15659_13_Final_JM_ePub.xhtml#_idTextAnchor626"><em class="italic">Chapter 13</em></a>,<em class="italic"> Running Kubernetes in Public Clouds</em>.</p>
			<p>Minikube takes a similar approach to Docker in that it deploys a small managed virtual machine to run your environment. The next tool we are going to look at takes a more modern, and<a id="_idIndexMarker902"/> at the time of writing, experimental approach to running Kubernet<a id="_idTextAnchor495"/>es.</p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor496"/>Deploying Kubernetes using kind</h1>
			<p>The next tool we<a id="_idIndexMarker903"/> are going to look at is Kind, which<a id="_idIndexMarker904"/> is short for <strong class="bold">Kubernetes in Docker</strong>. This is exactly what you think it would be, based on the name â€“ a Kubernetes cluster node condensed down into a container. Kind is a very recent project â€“ so recent, in fact, that at the time of writing it is still undergoing a lot of active development. Because of this, we aren't going to spend too much time on it.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor497"/>Installing Kind</h2>
			<p>Like Minikube, Kind is <a id="_idIndexMarker905"/>distributed as a single static binary â€“ meaning its installation is very similar.</p>
			<p>To install it on macOS, we need to run the following:</p>
			<pre>$ brew install kind</pre>
			<p>On Windows, run this:</p>
			<pre>$ choco install kind</pre>
			<p>Finally, on Linux, you can run the following:</p>
			<pre>$ KIND_VER=v0.8.1
$ curl -LO https://kind.sigs.k8s.io/dl/$KIND_VER/kind-$(uname)-
amd64
$sudo install kind-$(uname)-amd64 /usr/local/bin/kind</pre>
			<p>The release page to<a id="_idIndexMarker906"/> confirm the version number can be found at <a href="https://github.com/kubernetes-sigs/kind/releases/">https://github.com/kubernetes-sigs/kind/releases/</a>.</p>
			<p>As we already have Docker installed, we don't need to worry about drivers, hypervisors, or anything to run a supporting virtual machine as Kind will simply use our local Docker<a id="_idIndexMarker907"/> installation.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor498"/>Launching a Kind cluster</h2>
			<p>Once the kind<a id="_idIndexMarker908"/> binary has been installed, launching a cluster node is a very simple pr<a id="_idTextAnchor499"/><a id="_idTextAnchor500"/>ocess; just run the following command:</p>
			<pre>$ kind create cluster</pre>
			<p>As you can see from the following Terminal output, this will download the necessary images and take care of configuring both the cluster and creating a context on your local Docker host so that you can use <code>kubectl</code> to interact with the cluster node:</p>
			<div><div><img src="img/image_00_0132.jpg" alt="Figure 12.13 â€“ Launching a cluster node with Kind&#13;&#10;" width="739" height="446"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.13 â€“ Launching a cluster node with Kind</p>
			<p>Now that we have our cluster node up a<a id="_idTextAnchor501"/>n<a id="_idTextAnchor502"/>d running, let's do something with it.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor503"/>Interacting with your Kubernetes cluster node</h2>
			<p>Now that we have our <a id="_idIndexMarker909"/>cluster node up <a id="_idIndexMarker910"/>and running, we can rerun the commands and launch a test application as we did in the previous section of the chapter.</p>
			<p>This time, we will be using the <code>kubectl</code> command on our Docker host, rather than using a wrapper; however, we will be adding a context to make sure that our Kind Kubernetes node cluster is used. This means that the commands <a id="_idTextAnchor504"/>w<a id="_idTextAnchor505"/>e<a id="_idTextAnchor506"/> <a id="_idTextAnchor507"/>need to run look like the following:</p>
			<pre><a id="_idTextAnchor508"/>$<a id="_idTextAnchor509"/> <a id="_idTextAnchor510"/>kubectl --context kind-kind get nodes
$ kub<a id="_idTextAnchor511"/>e<a id="_idTextAnchor512"/>c<a id="_idTextAnchor513"/>t<a id="_idTextAnchor514"/>l --context kind-kind get namespaces
$ kubectl --context kind-kind get --namespace kube-system pods</pre>
			<p>As you can see from the following Terminal output, we again see similar output to the last time we queried<a id="_idTextAnchor515"/> <a id="_idIndexMarker911"/>o<a id="_idTextAnchor516"/>u<a id="_idTextAnchor517"/>r nodes, namespaces, and system <a id="_idIndexMarker912"/>pods:</p>
			<div><div><img src="img/image_00_0142.jpg" alt="Figure 12.14 â€“ Viewing the nodes, namespaces, and system pods&#13;&#10;" width="996" height="611"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.14 â€“ Viewing the nodes, namespac<a id="_idTextAnchor518"/>e<a id="_idTextAnchor519"/>s, and system pods</p>
			<p>Next, let's deploy the <strong class="bold">hello-node</strong> application again, using the following commands:</p>
			<pre>$ kub<a id="_idTextAnchor520"/>e<a id="_idTextAnchor521"/>c<a id="_idTextAnchor522"/>tl --context kind<a id="_idTextAnchor523"/>-<a id="_idTextAnchor524"/>kind create deployment hell<a id="_idTextAnchor525"/>o<a id="_idTextAnchor526"/>-node 
--image=k8s.gcr.io/echoserver:1.4
$ kubectl --context kind-kind get deployments
$ kubectl --context kind-kind get pods
$ kubectl --context kind-kind expose deployment hello-node 
--type=LoadBalancer --port=8080
$ kubectl --context kind-kind get services</pre>
			<p>So far so good, you may be thinking, but unfortunately that is about as far as we can take the installation with the current configuration â€“ while we can deploy pods and services, Kind does not come with an Ingress controller by default at the moment.</p>
			<p>To enable an Ingress controller, we first have to delete our cluster. To do that, run the following command:</p>
			<pre>$ kind delete cluster </pre>
			<p>Once the cluster has been deleted, we can re-launch it with the Ingress configuration enabled. The configuration is too long to list here, but you can find a copy of it in the re<a id="_idTextAnchor527"/>p<a id="_idTextAnchor528"/>ository that accompanies this book in the <code>chapter12/kind</code> folder. To launch <a id="_idTextAnchor529"/>t<a id="_idTextAnchor530"/>he cluster <a id="_idIndexMarker913"/>with the config, change to<a id="_idIndexMarker914"/> the <code>chapter12/kind</code> folder in <a id="_idTextAnchor531"/>y<a id="_idTextAnchor532"/>our Terminal and run the following command:</p>
			<pre>$ kind create cluster --config cluster-config.yml</pre>
			<p>O<a id="_idTextAnchor533"/>n<a id="_idTextAnchor534"/>ce launched, the next step is to enable the NGINX Ingress controller. You will need to execute the following command to do this:</p>
			<pre>$ kubectl --context kind-kind apply -f https://raw.
githubusercontent.com/kubernetes/ingress-nginx/master/deploy/
static/provider/kind/deploy.yaml</pre>
			<p>That will configure the cluster to use the NGINX Ingress controller. The controller itself take a minute or two to launch â€“ you can check the status by running the following:</p>
			<pre>$ kubectl --context kind-kind get --namespace ingress-nginx 
pods</pre>
			<p>Once you have the <code>Ingress-nginx-controller</code> <a id="_idTextAnchor535"/>p<a id="_idTextAnchor536"/>od ready and running, you can then relaunch the <code>hello-node</code> application using the <code>hello-node.yml</code> fi<a id="_idTextAnchor537"/>l<a id="_idTextAnchor538"/>e in the <code>chapter12/kind</code> folder with this:</p>
			<pre>$ kubectl --context kind-kind apply -f hello-node.yml</pre>
			<p>Once launched, you should be able to access the <code>hello-node</code> application at <a href="http://localhost/hello-node/">http://localhost/hello-node/</a> as seen in the following screenshot:</p>
			<div><div><img src="img/image_00_0152.jpg" alt="Figure 12.15 â€“ The output of the hello-node application&#13;&#10;" width="1246" height="566"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.15<a id="_idTextAnchor539"/> <a id="_idTextAnchor540"/>â€“ The output of the hello-node application</p>
			<p>Once <a id="_idIndexMarker915"/>f<a id="_idTextAnchor541"/><a id="_idTextAnchor542"/>inished, you can delete the cluster <a id="_idIndexMarker916"/>with the <code>kind delete cluster</code> command.</p>
			<h2 id="_idParaDest-226"><a id="_idTextAnchor543"/>Kind summary</h2>
			<p>So, you may be thinking<a id="_idIndexMarker917"/> to yourself, what is the point of Kind â€“ why on earth would you want to run a Kubernetes cluster in a single container? Well, its main use is to test Kubernetes itself; however, it could be used to test deployments as part of a continuous delivery or continuous deployment pipeline where you need to test your Kubernetes definition files are working as expected and your application launches without any problems.</p>
			<p>As it stands at the moment, Kind is probably too slow and too <a id="_idTextAnchor544"/>heavy in development to be used to develop on.</p>
			<p>Let's move onto the next tool, which takes us back to running a virtual machine to deploy our local Kubernetes cluster on.</p>
			<h1 id="_idParaDest-227">Deploy<a id="_idTextAnchor545"/>i<a id="_idTextAnchor546"/>ng Kubernetes using MicroK8s</h1>
			<p>Next up, we <a id="_idIndexMarker918"/>have MicroK8s by Canonical <a id="_idIndexMarker919"/>who, as you may remember from <a href="B15659_06_Final_JM_ePub.xhtml#_idTextAnchor187"><em class="italic">Chapter 6</em></a>,<em class="italic"> Docker Machine, Vagrant, and Multipass</em>, are the creators of Multipass and also the Linux distribution Ubuntu.</p>
			<p>The mantra of the MicroK8s project is to provide a lightweight Kubernetes node with only a minimal number of basic services enabled by default while prov<a id="_idTextAnchor547"/>i<a id="_idTextAnchor548"/>ding additional services as needed via plugins.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor549"/>Installing MicroK8s</h2>
			<p>Unlike Minikube <a id="_idIndexMarker920"/>and Kind, the standalone binary for MicroK8s only works on Linux-based machines. Because of this, we are going to use Multipass to launch a virtual machine and use that as our installation target.</p>
			<p>To launch the virtual machine, we nee<a id="_idTextAnchor550"/>d<a id="_idTextAnchor551"/> to run the following:</p>
			<pre>$ multipass launch --name microk8s</pre>
			<p>Once the virtual machine is up and running, we can then enable<a id="_idTextAnchor552"/> <a id="_idTextAnchor553"/>and install MicroK8s with the following command:</p>
			<pre>$ multipass exec microk8s -- \
	/bin/bash -c 'sudo snap install microk8s --classic'</pre>
			<p>Once installed, it will take a little while for MicroK8s to start up and the cluster node to be ready to use. Run the following command to poll the status and check MicroK8s is up and running:</p>
			<pre>$ multipass exec microk8s -- \
	/bin/bash -c 'sudo microk8s status --wait-ready'</pre>
			<p>Next up, as MicroK8s by default is a minimal Kubernetes cluster node, we need to enable the <code>dns</code> and <code>Ingress</code> plugins, which will allow us to access our applications when we launch them:</p>
			<pre>$ multipass exec microk8s -- \
	/bin/bash -c 'sudo microk8s enable dns ingress'</pre>
			<p>Once enabled, the final thing we need to do is get a copy of the configuration on our host <a id="_idTextAnchor554"/>m<a id="_idTextAnchor555"/>achine. To do this, run the following command:</p>
			<pre>$ multipass exec mic<a id="_idTextAnchor556"/>r<a id="_idTextAnchor557"/>ok8s -- \
	/bin/bash -c 'sudo microk8s.config' &gt; m<a id="_idTextAnchor558"/>i<a id="_idTextAnchor559"/>c<a id="_idTextAnchor560"/>r<a id="_idTextAnchor561"/>ok8s.yml</pre>
			<p>This will leave us with a file called <code>microk8s.yml</code> in the current directory on our host machine. We can now use this configuration when we run <code>kubectl</code> to access our newly launched<a id="_idTextAnchor562"/><a id="_idIndexMarker921"/><a id="_idTextAnchor563"/> and configured MicroK8s Kubernetes cluster node.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor564"/>Interacting with your Kubernetes cluster node</h2>
			<p>Now that we <a id="_idIndexMarker922"/>have our cluster node up <a id="_idIndexMarker923"/>and running, we can rerun the commands and launch a test application as we did in the previous section of the chapter.</p>
			<p>As already mentioned, this time we will be using the <code>kubectl</code> command on our host machine and pass the flag to make sure it uses the <code>microk8s.yml</code> config file. This means that <a id="_idTextAnchor565"/>t<a id="_idTextAnchor566"/>h<a id="_idTextAnchor567"/>e<a id="_idTextAnchor568"/> <a id="_idTextAnchor569"/>commands we need to run look like the following:</p>
			<pre>$ kubectl --kubeconfig=microk8s.yml get nodes
$ kubectl --kubeconfig=microk8s.yml get namespaces
$ kubectl --kubeconfig=microk8s.yml get --namespace kube-system 
pods </pre>
			<p>As you can see from the following Terminal output, we again see similar output to the last time we queried our nodes and namespaces until we get to the system pods:</p>
			<div><div><img src="img/image_00_0162.jpg" alt="Figure 12.16 â€“ Viewing the nodes, namespaces, and system pods&#13;&#10;" width="892" height="419"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.16 â€“ Viewing the nodes, namespaces, and system pods</p>
			<p>The reason why we can't see any of the Pods outside of the <code>coredns</code> one is that the user attached to the configuration we downloaded doesn't have the necessary permissions to do so. Although, that's not a problem for us as we don't need to touch those pods.</p>
			<p>Next up, we can<a id="_idIndexMarker924"/> launch the <code>hello-node</code> application. This time, we will use the YAML file straight out of the GitHub <a id="_idIndexMarker925"/>repository by running the following command:</p>
			<pre>$ kubectl --kubeconfig=microk8s.yml apply -f https://raw.
githubusercontent.com/PacktPublishing/Mastering-Docker-Fourth-
Edition/master/chapter12/kind/hello-node.yml</pre>
			<p>The reason we are using this file is that it has the definition for the Ingress controller, which means we just need to get the IP address of our MicroK8s cluster node and then enter that URL into our browser. To get the IP address, run the following:</p>
			<pre>$ multipass info microk8s</pre>
			<p>Once you know the IP address, which should be listed as IPv4, open a browser and go to <code>https://&lt;IP Address&gt;/hello-node/</code>. In my case, the URL was <code>https://192.168.64.16/hello-node/</code>. You will note that, this time, we are using HTTPS rather than HTTP. That is because the Ingress controller we enabled installs a self-signed certificate and redirects all traffic to HTTPS. Depending on the browser settings, you may be asked to accept or install the certificate when going to the page.</p>
			<p>Once finished, you can delete the cluster node using the <code>multipass delete --purge microk8s</code> command. Also, don't forget to remove the <code>microk8s.yml</code> file. Once you have deleted <a id="_idIndexMarker926"/>the cluster, the <a id="_idTextAnchor570"/>c<a id="_idTextAnchor571"/>onfig <a id="_idIndexMarker927"/>file would not work if you were to relaunch it.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor572"/>MicroK8s summary</h2>
			<p>MicroK8s delivers on<a id="_idIndexMarker928"/> its promise of a small, lightweight, but still functional and extendable Kubernetes cluster controller. Coupled with Multipass, you are easily able to spin up a virtual machine and quickly bootstrap your Kubernetes cluster node on your local workstation.</p>
			<p>Also, Canonical has made sure that MicroK8s isn't just for local use; the cluster node itself can be considered to be production-ready, meaning that it is perfect for running Kubernetes on both edge and IoT devices â€“ both of which traditionally have lower specifications and would normally run a single node.</p>
			<p>The final tool we are going to look at will allow us to run multiple Kubernetes nodes locally, taking us closer to what a production environment might look like.</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor573"/>Deploying Kubernetes using K3s</h1>
			<p>The final tool<a id="_idIndexMarker929"/> we are going to take a look at is K3s from <a id="_idIndexMarker930"/>Rancher. Like MicroK8s, K3s is a lightweight Kubernetes distribution designed for edge and IoT devices. This again makes it perfect for local development too as K3s is also a certified Kubernetes distribution â€“ as is Docker, Kind, and MicroK8s.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You may be wondering why on earth it is called K3s. There is some logic behind it. As Rancher's main design aim for K3s was to produce something with half of the memory footprint of a typical Kubernetes distribution, <a id="_idTextAnchor574"/>t<a id="_idTextAnchor575"/>hey decided that as Kubernetes is a 10-letter word but is stylized as K8s, then their distribution would be half the size â€“ 5 letters â€“ and would, therefore, be stylized as K3s. However, there is no long-form for K3s and nor is there an official pronunciation.</p>
			<p>Finally, K3s supports multi-node clusters, so we are going to look at building a three-node cluster. The<a id="_idIndexMarker931"/> commands we'll be using in this section of the<a id="_idIndexMarker932"/> chapter will cover macOS and Linux systems as we will be creating environment variables and using non-Windows tools to streamline the installation process as much as possible.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor576"/>Installing K3s</h2>
			<p>Like MicroK8s, we<a id="_idIndexMarker933"/> are going to be using Multipass to launch our host machines. To d<a id="_idTextAnchor577"/>o<a id="_idTextAnchor578"/> this, run the following commands:</p>
			<pre>$ multipass launch --name k3smaster
$ multipass launch --name k3snode1
$ multipass launch --name k3snode2</pre>
			<p>Once we have our three VMs up and running, we can configure the master node by running the following:</p>
			<pre>$ multipass exec k3smaster -- \
	/bin/bash -c 'curl -sfL https://get.k3s.io | K3S_KUBECONFIG_
MODE='644' sh -'</pre>
			<p>Once we have the master node up and running, we need a little information to be able to bootstrap the two remaining nodes.</p>
			<p>The first piece of information we need is the URL of the master node. To create an environment variable, we can run the following command:</p>
			<pre>$ K3SMASTER='https://$(multipass info k3smaster | grep 'IPv4' |
 awk -F' ' '{print $2}'):6443'</pre>
			<p>Now we have the URL of the master node, we need to grab the access token. To do that, run the following:</p>
			<pre>$ K3STOKEN='$(multipass exec k3smaster -- /bin/bash -c 'sudo 
cat /var/lib/rancher/k3s/server/node-token')'</pre>
			<p>Now we have the two bits of information needed to bootstrap the nodes, we can run the following two commands:</p>
			<pre>$ multipass exec k3snode1 -- \
	/bin/bash -c 'curl -sfL https://get.k3s.io | K3S_
URL=${K3SMASTER} K3S_TOKEN=${K3STOKEN} sh -'
$ multipass exec k3snode2 -- \
	/bin/bash -c 'curl -sfL https://get.k3s.io | K3S_
URL=${K3SMASTER} K3S_TOKEN=${K3STOKEN} sh -'</pre>
			<p>We should now have <a id="_idIndexMarker934"/>our three no<a id="_idTextAnchor579"/>d<a id="_idTextAnchor580"/>es configured, and all that is left is to configure our local <code>kubectl</code> so that it can interact with the cluster nodes. The first thing we need to do is copy the configuration to our local machine. To do this, run the following:</p>
			<pre>$ multipass exec k3smaster -- \
	/bin/bash -c 'sudo cat /etc/rancher/k3s/k3s.yaml' &gt; 
${HOME}/.kube/k3s.yml</pre>
			<p>If we were to use the configuration file as it is, then it would fail as, by default, K3s is configured to communicate on t<a id="_idTextAnchor581"/>h<a id="_idTextAnchor582"/>e localhost, so to up<a id="_idTextAnchor583"/>d<a id="_idTextAnchor584"/>ate that run the following:</p>
			<pre>$ sed -ie s,https://127.0.0.1:6443,${K3SMASTER},g ${HOME}/.
kube/k3s.yml</pre>
			<p>As you can see, this replaces <a href="https://127.0.0.1:6443">https://127.0.0.1:6443</a> with the value of <code>${K3SMASTER}</code> in our loc<a id="_idTextAnchor585"/>a<a id="_idTextAnchor586"/>l <code>k3s.yml</code> configur<a id="_idTextAnchor587"/>a<a id="_idTextAnchor588"/>tion file. Once replaced, we can configure <code>kubectl</code> to use our <code>k3s.yml</code> configuration file for the remainder of the Terminal session by running the following:</p>
			<pre>$ export KUBECONFIG=${HOME}/.kube/k3s.yml</pre>
			<p>Now that our cluster <a id="_idIndexMarker935"/>is accessible using our local kubectl binary, we can launch our application.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor589"/>Interacting with your Kubernetes cluster nodes</h2>
			<p>Now that we<a id="_idIndexMarker936"/> have our cluster node up <a id="_idIndexMarker937"/>and running, we can rerun the commands and launch a test application as we did in the previous section of the chapter. This time, as we have configured <code>kubectl</code> to use our <code>k3s.yml</code> configuration file, we can just run the following:</p>
			<pre>$ kubectl get nodes
$ kubectl get namespaces
$ kubectl get --namespace kube-system pods </pre>
			<p><a id="_idTextAnchor590"/>Thi<a id="_idTextAnchor591"/>s will give you something like the following Terminal output:</p>
			<div><div><img src="img/image_00_0172.jpg" alt="Figure 12.17 â€“ Viewing the nodes, namespaces, and system pods&#13;&#10;" width="968" height="639"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.17 â€“ Viewing the nodes, namespaces, and system pods</p>
			<p>Next, let's launch the <code>hello-node</code> application, this time using the following commands:</p>
			<pre>$ kubectl create deployment hello-node --image=k8s.gcr.io/
echoserver:1.4
$ kubectl expose deployment hello-node --type=LoadBalancer 
--port=8080
$ kubectl get services</pre>
			<p>This will give you the followi<a id="_idTextAnchor592"/>ng <a id="_idTextAnchor593"/>output, as you can see, we have any <code>EXTERNAL-IP</code> and <code>PORT(S)</code>:</p>
			<div><div><img src="img/image_00_0181.jpg" alt="Figure 12.18 â€“ Launching and exposing the hello-node application&#13;&#10;" width="1008" height="281"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.18 â€“ Launching and exposing the hello-node application</p>
			<p>Adding<a id="_idIndexMarker938"/> the IP address with the second port <a id="_idIndexMarker939"/>should give you the URL to access the application, for example, I went to <a href="http://192.168.64.19:31846/">http://192.168.64.19:31846/</a> and was presented with the <code>hello-node</code> application.</p>
			<p>Next up, let's launch our <code>cluster</code> application with the following commands:</p>
			<pre>$ kubectl create deployment cluster --image=russmckendrick/cluster:latest
$ kubectl expose deployment cluster --type=LoadBalancer --port=80
$ kubectl get services</pre>
			<p>You <a id="_idTextAnchor594"/>mig<a id="_idTextAnchor595"/>ht notice that this time, for <code>EXTERNAL-IP</code>, it says <code>&lt;pending&gt;</code>:</p>
			<div><div><img src="img/image_00_0191.jpg" alt="Figure 12.19 â€“ Launching and exposing the cluster application&#13;&#10;" width="1035" height="309"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.19 â€“ Launching and exposing the cluster application</p>
			<p>You shouldn't have to worry about that; just use the second port and the<a id="_idTextAnchor596"/> <a id="_idTextAnchor597"/>external IP exposed for the other server. This gave me a URL of <a href="http://192.168.64.19:32042/">http://192.168.64.19:32042/</a> to access the <code>cluster</code> application on. </p>
			<p>You can also scale the cluster application using the following commands:</p>
			<pre>$ kubectl get deployment/cluster
$ kubectl scale --replicas=3 deployment/cluster
$ kubectl get deployment/cluster</pre>
			<p>If you are <a id="_idIndexMarker940"/>not following along, the Terminal<a id="_idIndexMarker941"/> output for this looks like the following:</p>
			<div><div><img src="img/image_00_0201.jpg" alt="Figure 12.20 â€“ Scaling the cluster application&#13;&#10;" width="663" height="281"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.20 â€“ Scaling the cluster application</p>
			<p>Before we finish up, as we have more nodes to explore, let's install the Kubernetes dashboard. To do this, run the following commands from your host machine. The first thing that is needed is to get the current version of the dashboard. To do this, run the following:</p>
			<pre>$ GITHUB_URL=https://github.com/kubernetes/dashboard/releases
$ VERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s 
-S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')</pre>
			<p>Now that we have the current version as an environment variable, we can run the following commands to launch the dashboard, add a user, and configure the user's access:</p>
			<pre>$ kubectl create -f 'https://raw.githubusercontent.com/
kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/
recommended.yaml'
$ kubectl create -f 'https://raw.githubusercontent.com/
PacktPublishing/Mastering-Docker-Fourth-Edition/master/
chapter12/k3s/dashboard.admin-user.yml'
$ kubectl create -f 'https://raw.githubusercontent.com/
PacktPublishing/Mastering-Docker-Fourth-Edition/master/
chapter12/k3s/dashboard.admin-user-role.yml'</pre>
			<p>With the<a id="_idIndexMarker942"/> dashboard installed and the user <a id="_idIndexMarker943"/>configured, we can grab the access token we will need to log in to the dashboard by running the following:</p>
			<pre>$ kubectl -n kubernetes-dashboard describe secret admin-user-
token | grep ^token</pre>
			<p>Make a note of the token as we will need it in a second. The last thing we need to do before accessing the dashboard is to start the Kubernetes proxy service by running the following:</p>
			<pre>$ kubectl proxy</pre>
			<p>With the proxy server running, open <a href="http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</a> in your pr<a id="_idTextAnchor598"/>e<a id="_idTextAnchor599"/>f<a id="_idTextAnchor600"/>erred browser, enter the token you made a note of, and sign in:</p>
			<div><div><img src="img/image_00_0211.jpg" alt="Figure 12.21 â€“ Opening the Kubernetes dashboard on our K3s node cluster&#13;&#10;" width="1106" height="737"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.21 â€“ Opening the Kubernetes dashboard on our K3s node cluster</p>
			<p>Once you <a id="_idIndexMarker944"/>have finished with you<a id="_idTextAnchor601"/>r<a id="_idTextAnchor602"/> K3s <a id="_idIndexMarker945"/>cluster, you can remove it using <code>the multipass delete --purge k3smaster k3snode1 k3snode2</code> command.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor603"/>One more thing â€“ K3d</h2>
			<p>Finally, Rancher <a id="_idIndexMarker946"/>also provides K3d. Like Kind, this is the entire Kubernetes distribution in a single container, which means that not only can you use K3s as your local development environment but it is also straightforward to introduce to your CI/CD pipelines.</p>
			<p>Before we summarize K3s, let's take a very quick look at how you can get K3d up and running on a macOS and Linux host, starting with Linux. To install the <code>k3d</code> command, run the following:</p>
			<pre>$ curl -s https://raw.githubusercontent.com/rancher/k3d/master/
install.sh | bash</pre>
			<p>Or, if you are using macOS, you can use Homebrew to install it using the following:</p>
			<pre>$ brew install k3d</pre>
			<p>Once K3d is installed (I ins<a id="_idTextAnchor604"/><a id="_idTextAnchor605"/>talled version 1.7), there are four commands we are going to look at:</p>
			<ul>
				<li><code>k3d cluster create k3s-default</code>: This comm<a id="_idTextAnchor606"/><a id="_idTextAnchor607"/>and will create a K3d-powered Kubernetes cluster called <strong class="bold">k3s-default</strong>. </li>
				<li><code>k3d kubeconfig merge k3s-default --<a id="_idTextAnchor608"/>s<a id="_idTextAnchor609"/>witch-context</code>: This will configure your local <code>kubectl</code> to talk to the <strong class="bold">k3s-default</strong> cluster.</li>
			</ul>
			<p>Now that you have a<a id="_idIndexMarker947"/> cluster up and running, you can interact with it like <a id="_idTextAnchor610"/><a id="_idTextAnchor611"/>any other Kubernetes cluster, for example, by running the following:</p>
			<pre>$ kubectl get nodes
$ kubectl get namespaces
$ kubectl cluster-info</pre>
			<p>This gives us the follo<a id="_idTextAnchor612"/><a id="_idTextAnchor613"/>wing output:</p>
			<div><div><img src="img/image_00_0221.jpg" alt="Figure 12.22 â€“ Running commands against our K3d cluster&#13;&#10;" width="1258" height="529"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.22 â€“ Running commands against our K3d cluster</p>
			<ul>
				<li><code>k3dcluster delete k3s-default</code> is the final command of the four we are going to look at, and as you might have guessed, this deletes the cluster.</li>
			</ul>
			<p>K3d is very much in active development. In fact, a complete rewrite of the K3d wrapper has  just been completed so if the preceding commands, which cover the new version 3, do no<a id="_idTextAnchor614"/><a id="_idTextAnchor615"/>t work, then try the following commands, which cover the old version:</p>
			<pre>$ k3d create
$ export KUBECONFIG='$(k3d get-kubeconfig --name='k3s-
default')': This will configure your local kubectl to talk to 
the k3s-default cluster.
$ k3d delete</pre>
			<p>For more up-to-date <a id="_idIndexMarker948"/>news on the development of K3d, see the project's GitHub page, which is linked in the <em class="italic">Further reading</em> section of this chapter.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor616"/>K3s summary</h2>
			<p>Like MicroK8s, K3s <a id="_idIndexMarker949"/>delivers on its promise of a lightweight Kubernetes distribution.</p>
			<p>Personally, I find K3s to be the better of the two as it feels more like a fully formed Kubernetes distribution than MicroK8s does.</p>
			<p>The other thing K3s has going for it is that deploying a local multi-node cluster is a relatively painless experience. This should give your local development environment a much more production-like feeling, and while Minikube does allow you to launch a multi-node cluster, the functionality is still in its infancy and is not really ready for public consumption yet.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor617"/>Summary</h1>
			<p>In this chapter, we looked at four different tools for launching both single-node and multi-node Kubernetes clusters. We discovered that while the method of launching each of the clusters is slightly different, once they are up and running, you get a mostly consistent experience once you start to interact with them using standard Kubernetes tools like <code>kubectl</code>.</p>
			<p>At this point, I should probably confess something: two of the four tools we have covered in this chapter do not actually u<a id="_idTextAnchor618"/>s<a id="_idTextAnchor619"/>e Docker in the traditional sense â€“ both MicroK8s and K3s actually use <strong class="bold">containerd</strong>.</p>
			<p>As you may recall from <a href="B15659_01_Final_JM_ePub.xhtml#_idTextAnchor046"><em class="italic">Chapter 1</em></a>, <em class="italic">Docker Overview</em>, <strong class="bold">containerd</strong> is an easily embeddable container runtime. It started life at Docker Inc., but the project was donated to the <strong class="bold">Cloud <a id="_idTextAnchor620"/>N<a id="_idTextAnchor621"/>ative Computing Foundation</strong> (<strong class="bold">CNCF</strong>) â€“ it is the container runtime of the Moby project, which Docker uses as its upstream<a id="_idTextAnchor622"/> <a id="_idTextAnchor623"/>project.</p>
			<p>It is not only small and lightweight, but it also offers full OCI Image and OCI Runtime specification support, meaning that it is 100% compatible with Docker images and also the way that Docker runs containers.</p>
			<p>In the next chapter, we are going to move away from running Kubernetes locally and take our clusters to the cloud.</p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor624"/>Questions</h1>
			<ol>
				<li value="1">True or false: Kind is recommended for production use.</li>
				<li>Name at least two tools we have looked at in this chapter that are able to run inside a Docker container.</li>
				<li>If you had an ARM-powered IoT device, which two Kubernetes distributions that we have covered in this chapter could you use?</li>
			</ol>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor625"/>Further reading</h1>
			<p>Some of the Google tools, presentations, and white papers mentioned at the start of the chapter can be found here:</p>
			<ul>
				<li>Minikube: <a href="https://minikube.sigs.k8s.io/">https://minikube.sigs.k8s.io/</a> </li>
				<li>Kind: <a href="https://kind.sigs.k8s.io/">https://kind.sigs.k8s.io/</a> </li>
				<li>MicroK8s: <a href="https://microk8s.io/">https://microk8s.io/</a> </li>
				<li>K3s: <a href="https://k3s.io/">https://k3s.io/</a> </li>
				<li>K3d: <a href="https://github.com/rancher/k3d">https://github.com/rancher/k3d</a> </li>
				<li>containerd: <a href="https://containerd.io/">https://containerd.io/</a> </li>
				<li>OCI Image and Runtime Specification: <a href="https://www.opencontainers.org/">https://www.opencontainers.org/</a> </li>
				<li>Certified Kubernetes offerings: <a href="https://www.cncf.io/certification/software-conformance/">https://www.cncf.io/certification/software-conformance/</a></li>
			</ul>
		</div>
	</div>



  </body></html>