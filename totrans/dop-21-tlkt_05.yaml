- en: Continuous Delivery and Deployment with Docker Containers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker容器的持续交付和部署。
- en: In software, when something is painful, the way to reduce the pain is to do
    it more frequently, not less.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件中，当某个过程变得痛苦时，减少痛苦的方法是更加频繁地执行它，而不是减少频率。
- en: – David Farley
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: – 大卫·法利（David Farley）
- en: At the time, we could not convert the **Continuous Integration** (**CI**) into
    the **Continuous Delivery** (**CD**) process because we were missing some essential
    knowledge. Now that we understand the basic principles and commands behind Docker
    Swarm, we can go back to the end of the [Chapter 1](9730e69c-a698-40c2-91ce-b92e48328a93.xhtml),
    *Continuous Integration with Docker Containers*. We can define the steps that
    will let us perform the full CD process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，我们无法将**持续集成**（**CI**）转化为**持续交付**（**CD**）过程，因为我们缺少一些关键知识。现在我们已经理解了Docker Swarm背后的基本原理和命令，我们可以回到[第一章](9730e69c-a698-40c2-91ce-b92e48328a93.xhtml)，*使用Docker容器进行持续集成*。我们可以定义出可以让我们执行完整CD过程的步骤。
- en: I won't go into Continuous Delivery details. Instead, I'll pitch it as a single
    sentence. *Continuous Delivery is a process applied to every commit in a code
    repository and results in every successful build being ready for deployment to
    production.*
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会深入讨论持续交付的细节。相反，我将用一句话来概括它。*持续交付是一个应用于每次提交到代码库的过程，每次成功构建都会准备好部署到生产环境。*
- en: CD means that anyone, at any time, can click a button, and deploy a build to
    production without the fear that something will go wrong. It means that the process
    is so robust that we have full confidence that "almost" any problem will be detected
    before the deployment to production. Needless to say, CD is an entirely automated
    process. There is no human involvement from the moment a commit is sent to a code
    repository, all the way until a build is ready to be deployed to production. The
    only manual action is that someone needs to press the button that will run a script
    that performs the deployment.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: CD意味着任何人随时可以点击一个按钮，将构建部署到生产环境，而不必担心出现问题。这意味着该过程非常稳健，我们有充分的信心，"几乎"所有的问题都会在部署到生产之前被发现。毫无疑问，CD是一个完全自动化的过程。从提交到代码库的那一刻起，一直到构建准备好部署到生产环境，过程中没有人工干预。唯一的手动操作是有人需要按下一个按钮，运行一个脚本执行部署。
- en: '**Continuous Deployment** (**CDP**) is one step forward. It is Continuous Delivery
    without the button. *Continuous Deployment is a process applied to every commit
    in a code repository and results with every successful build being Deployed to
    production.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**持续部署**（**CDP**）是向前迈出的一步。它是没有按钮的持续交付。*持续部署是一个应用于每次提交到代码库的过程，每次成功构建都会被部署到生产环境。*'
- en: No matter which process you choose, the steps are the same. The only difference
    is whether there is a button that deploys the release to production.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种过程，步骤都是一样的。唯一的区别是是否有一个按钮用于将版本部署到生产环境。
- en: At this point, it is safe to assume that we'll use Docker whenever convenient
    and that we'll use Swarm clusters to run the services in production and production-like
    environments.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以安全地假设我们将尽可能方便地使用Docker，并且我们将使用Swarm集群在生产和类似生产的环境中运行服务。
- en: 'Let''s start by specifying the steps that could define one possible implementation
    of the CD/CDP process:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从指定一些步骤开始，这些步骤可以定义CD/CDP过程的一个可能实现：
- en: Check out the code.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看代码。
- en: Run unit tests.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行单元测试。
- en: Build binaries and other required artifacts.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建二进制文件和其他所需的构件。
- en: Deploy the service to the staging environment.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将服务部署到预生产环境。
- en: Run functional tests.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行功能测试。
- en: Deploy the service to the production-like environment.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将服务部署到类似生产环境的环境中。
- en: Run production readiness tests.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行生产就绪性测试。
- en: Deploy the service to the production environment.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将服务部署到生产环境中。
- en: Run production readiness tests.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行生产就绪性测试。
- en: Now, let's get going and set up the environment we'll need for practicing the
    CD flow.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始配置练习CD流程所需的环境。
- en: Defining the Continuous Delivery environment
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义持续交付环境。
- en: A minimum requirement for a Continuous Delivery environment is two clusters.
    One should be dedicated to running tests, building artifacts and images, and all
    other CD tasks. We can use it for simulating a production cluster. The second
    cluster will be used for deployments to production.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付环境的最基本要求是两个集群。一个应专门用于运行测试、构建构件和镜像，以及执行所有其他CD任务。我们可以将其用作模拟生产集群。第二个集群将用于生产部署。
- en: Why do we need two clusters? Can't we accomplish the same with only one?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要两个集群？难道只用一个就能完成同样的事情吗？
- en: While we certainly could get away with only one cluster, having two will simplify
    quite a few processes and, more importantly, provide better isolation between
    production and non-production services and tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们完全可以仅用一个集群，但拥有两个集群将简化许多流程，更重要的是，提供更好的生产环境与非生产环境之间的隔离。
- en: The more we minimize the impact on the production cluster, the better. By not
    running non-production services and tasks inside the production cluster, we are
    reducing the risk. Therefore, we should have a production cluster separated from
    the rest of the environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们越是减少对生产集群的影响，就越好。通过不在生产集群内运行非生产服务和任务，我们减少了风险。因此，我们应该将生产集群与环境的其他部分隔离开来。
- en: Now let's get started and fire up those clusters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始，启动这些集群。
- en: Setting up Continuous Delivery clusters
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置持续交付集群
- en: What is a minimum number of servers required for a production-like cluster?
    I'd say two. If there's only one server, we would not be able to test whether
    networking and volumes work across nodes. So, it has to be plural. On the other
    hand, I don't want to push your laptop too much so we'll avoid increasing the
    number unless necessary.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个类似生产的集群，最低需要多少台服务器？我认为是两台。如果只有一台服务器，我们将无法测试节点之间的网络和存储卷是否正常工作。所以，必须是多个节点。另一方面，我不希望给你的笔记本电脑造成过大负担，所以除非必要，我们将避免增加节点数量。
- en: For the production-like cluster, two nodes should be enough. We should add one
    more node that we'll use for running tests and building images. The production
    cluster should probably be a bit bigger since it will have more services running.
    We'll make it three nodes big. If needed, we can increase the capacity later on.
    As you already saw, adding nodes to a Swarm cluster is very easy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类似生产的集群，两台节点应该足够了。我们应该再增加一个节点，用于运行测试和构建镜像。生产集群可能需要稍微大一点，因为它将运行更多的服务。我们将它设为三节点。如果需要，稍后可以增加容量。如你所见，向
    Swarm 集群中添加节点非常简单。
- en: By now, we set up a Swarm cluster quite a few times so we'll skip the explanation
    and just do it through a script.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经多次设置了一个 Swarm 集群，因此我们将跳过解释，直接通过脚本来完成。
- en: All the commands from this chapter are available in the `05-continuous-delivery.sh` ([https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe](https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe))
    Gist.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令都可以在 `05-continuous-delivery.sh` ([https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe](https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe))
    Gist 中找到。
- en: 'Let''s go back to the cloud-provisioning directory we created in the previous
    chapter and run the `scripts/dm-swarm.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh))
    script. It will create the production nodes and join them into a cluster. The
    nodes will be called `swarm-1`, `swarm-2`, and `swarm-3`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到前一章创建的云资源配置目录，并运行 `scripts/dm-swarm.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh))
    脚本。它将创建生产节点并将它们加入到集群中。节点将被命名为 `swarm-1`、`swarm-2` 和 `swarm-3`：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the `**node ls**` command is as follows (IDs are removed for
    brevity):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`**node ls**` 命令的输出如下（为了简洁，ID 已移除）：'
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we'll create the second cluster. We'll use it for running CD tasks as
    well as a simulation of a production environment. Three nodes should be enough,
    for now. We'll call them `swarm-test-1, swarm-test-2,` and `swarm-test-3.`
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建第二个集群。我们将用它来运行持续交付任务，以及模拟生产环境。现在，三个节点应该足够了。我们将它们命名为 `swarm-test-1`、`swarm-test-2`
    和 `swarm-test-3`。
- en: 'We''ll create the cluster by executing the `scripts/dm-test-swarm.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh))
    script:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过执行 `scripts/dm-test-swarm.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh))
    脚本来创建集群：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the `node ls` command is as follows (IDs were removed for brevity):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`node ls` 命令的输出如下（为了简洁，ID 已移除）：'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The only thing left, for now, is to create Docker registry services. We'll create
    one in each cluster. That way, there will be no direct relation between them,
    and they will be able to operate independently one from another. For registries
    running on separate clusters to share the same data, we'll mount the same host
    volume to both services. That way, an image pushed from one cluster will be available
    from the other, and vice versa. Please note that the volumes we're creating are
    still a workaround. Later on, we'll explore better ways to mount volumes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的工作就是创建Docker注册中心服务。我们将在每个集群中创建一个。这样，它们之间就没有直接的关联，并且可以相互独立操作。为了让运行在不同集群上的注册中心共享相同的数据，我们将把相同的主机卷挂载到两个服务上。这样，从一个集群推送的镜像将能够在另一个集群中访问，反之亦然。请注意，我们正在创建的卷仍然是一种临时解决方案。稍后我们将探索更好的挂载卷的方法。
- en: Let's start with the production cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从生产集群开始。
- en: We've already run the registry in the [Chapter 1](44df5a4c-1e47-4de0-9442-660034287e66.xhtml), *Continuous
    Integration with Docker Containers* . Back then, we had a single node, and we
    used Docker Compose to deploy services. Registry was not an option.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第一章](44df5a4c-1e47-4de0-9442-660034287e66.xhtml)中运行过注册中心，*使用Docker容器进行持续集成*。当时，我们有一个单一节点，并且使用Docker
    Compose来部署服务。注册中心并不是一个选项。
- en: '**A note to Windows users** Git Bash has a habit of altering file system paths.
    To stop this, execute the following before running the code block:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**Windows用户注意** Git Bash有修改文件系统路径的习惯。为了避免这种情况，在运行代码块之前执行以下命令：'
- en: '`export MSYS_NO_PATHCONV=1`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`export MSYS_NO_PATHCONV=1`'
- en: 'This time, we''ll run the registry as a Swarm service:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将作为Swarm服务运行注册中心：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We exposed port `5000` and reserved `100` MB of memory. We used the `--mount`
    argument to expose a volume. This argument is, somewhat, similar to the Docker
    Engine argument `--volume` or the volumes argument in Docker compose files. The
    only significant difference is in the format. In this case, we specified that
    the current host directory `source=$PWD` should be mounted inside the container
    `target=/var/lib/registry`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们暴露了端口`5000`并保留了`100` MB的内存。我们使用了`--mount`参数来暴露一个卷。这个参数与Docker引擎的`--volume`参数或Docker
    Compose文件中的volumes参数有些相似。唯一显著的区别在于格式。在这种情况下，我们指定当前的主机目录`source=$PWD`应该挂载到容器内部`target=/var/lib/registry`。
- en: Please note that, from now on, we'll always run specific versions. While, until
    now, the latest was alright as a demonstration, now we're trying to simulate CD
    processes we'll run in "real" clusters. We should always be explicit which version
    of a service we want to run. That way, we can be sure that the same service is
    tested and deployed to production. Otherwise, we could run into a situation where
    one version was deployed and tested in a production-like environment, but a different
    one was deployed to production.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从现在开始，我们将始终运行特定版本的服务。直到现在，使用最新版本作为演示是可以的，但现在我们尝试模拟CD过程，并且这些过程将在“真实”的集群中运行。我们应该始终明确指定要运行哪个版本的服务。这样，我们可以确保测试和部署到生产环境的是相同的服务。否则，我们可能会遇到一种情况，即在类似生产环境中部署和测试了一个版本，但生产环境中却部署了另一个版本。
- en: The benefits behind specific versions are even more apparent when we use images
    from Docker Hub. For example, if we just run the latest release of the registry,
    there is no guarantee that, later on, when we run it in the second cluster, the
    latest release will not be updated. We could, easily, end up getting different
    releases of the registry in different clusters. That could lead to some really
    hard-to-detect bugs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特定版本的好处在我们使用Docker Hub的镜像时变得更加明显。例如，如果我们只运行注册中心的最新版本，就无法保证在以后运行时，在第二个集群中，最新版本不会被更新。我们可能会轻易地在不同的集群中得到不同版本的注册中心，这可能会导致一些非常难以检测的bug。
- en: I won't bother you more with versioning. I'm sure you know what's it for and
    when to use it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会再多提版本控制的问题了。我相信你知道它的用途以及何时使用。
- en: 'Let''s get back to the `registry` service. We should create it inside the second
    cluster as well:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到`registry`服务。我们应该在第二个集群中也创建它：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we have the `registry` service running inside both clusters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在两个集群中都运行了`registry`服务。
- en: '![](img/cd-environment-registry-1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-registry-1.png)'
- en: 'Figure 5-1: CD and production clusters with the registry service'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-1：带有注册中心服务的CD和生产集群
- en: At the moment, we do not know in which servers the registries are running. All
    we know is that there is an instance of the service in each cluster. Normally,
    we'd have to configure Docker Engine to treat the registry service as insecure
    and allow the traffic. To do that, we'd need to know the IP of the server the
    registry is running in. However, since we ran it as a Swarm service and exposed
    port `5000`, the routing mesh will make sure that the port is open in every node
    of the cluster and forward requests to the service. That allows us to treat the
    registry as localhost. We can pull and push images from any node as if the registry
    is running in each of them. Moreover, Docker Engines default behavior is to allow
    only localhost traffic to the registry. That means that we do not need to change
    its configuration.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们不知道注册表正在哪些服务器上运行。我们只知道每个集群中都有该服务的一个实例。通常，我们需要配置Docker引擎，将注册表服务视为不安全的并允许流量通过。为了做到这一点，我们需要知道注册表运行的服务器的IP地址。然而，由于我们将其作为Swarm服务运行并暴露了端口`5000`，路由网格将确保端口在集群中的每个节点上都开放，并将请求转发到服务。这使我们能够将注册表视为本地主机。我们可以从任何节点拉取和推送镜像，就好像注册表在每个节点上运行一样。此外，Docker引擎的默认行为是只允许本地主机流量访问注册表。这意味着我们不需要更改它的配置。
- en: Using node labels to constrain services
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用节点标签来约束服务
- en: Labels are defined as key-value sets. We'll use the key `env` (short for environment).
    At the moment, we don't need to label the nodes used for CD tasks since we are
    not yet running them as services. We'll change that in one of the chapters that
    follow. For now, we only need to label the nodes that will be used to run our
    services in the production-like environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 标签被定义为键值对集合。我们将使用键`env`（即环境的缩写）。目前，我们不需要给用于持续交付（CD）任务的节点打标签，因为我们还没有将它们作为服务运行。我们将在接下来的章节中更改这一点。目前，我们只需要给将在生产环境中运行我们服务的节点打上标签。
- en: We'll use the nodes `swarm-test-2` and `swarm-test-3` as our production-like
    environment so we'll label them with the key `env` and the value `prod-like`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`swarm-test-2`和`swarm-test-3`节点作为我们的生产环境，所以我们将它们标记为键`env`，值`prod-like`。
- en: 'Let''s start with the node `swarm-test-2`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从节点`swarm-test-2`开始：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can confirm that the label was indeed added by inspecting the node:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查节点来确认标签确实已被添加：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the node `inspect` command is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 节点`inspect`命令的输出如下：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, one of the labels is `env` with the value `prod-like`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，其中一个标签是`env`，值为`prod-like`。
- en: 'Let''s add the same label to the second node:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将相同的标签添加到第二个节点：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/cd-environment-labels.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-labels.png)'
- en: 'Figure 5-2: CD cluster with labeled nodes'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-2：带标签节点的持续交付（CD）集群
- en: Now that we have a few nodes labeled as production-like, we can create services
    that will run only on those servers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有几个节点被标记为生产环境，我们可以创建只会在这些服务器上运行的服务。
- en: 'Let''s create a service with the `alpine` image and constrain it to one of
    the `prod-like` nodes:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个使用`alpine`镜像的服务，并将其约束到其中一个`prod-like`节点：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can list the processes of the `util` service and confirm that it is running
    on one of the `prod-like` nodes:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以列出`util`服务的进程，并确认它正在其中一个`prod-like`节点上运行：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of the `service ps` command is as follows (IDs are removed for brevity):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ps`命令的输出如下（为了简洁，ID已被移除）：'
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, the service is running inside the `swarm-test-2` node which
    is labeled as `env=prod-like`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，服务正在`swarm-test-2`节点中运行，该节点被标记为`env=prod-like`。
- en: That, in itself, does not prove that labels work. After all, two out of three
    nodes are labeled as production-like, so there was a 66% chance that the service
    would run on one of them if labels did not work. So, let's spice it up a bit.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这本身并不能证明标签起作用。毕竟，三个节点中有两个被标记为生产环境，所以如果标签不起作用，服务有66%的概率会运行在其中一个节点上。那么，我们来稍微增加点难度。
- en: 'We''ll increase the number of instances to six:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把实例数增加到六个：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let us take a look at the `util` processes:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`util`进程：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，ID已被移除）：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, all six instances are running on nodes labeled `env=prod-like`
    (nodes `swarm-test-2` and `swarm-test-3`).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有六个实例都在标记为`env=prod-like`的节点上运行（`swarm-test-2`和`swarm-test-3`节点）。
- en: 'We can observe a similar result if we would run the service in the global mode:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在全局模式下运行服务，我们可以观察到类似的结果：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s take a look at the `util-2` processes:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`util-2`进程：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，ID已被移除）：
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since we told Docker that we want the service to be global, the desired state
    is `Running` on all nodes. However, since we specified the constraint `node.labels.env
    == prod-like`, replicas are running only on the nodes that match it. In other
    words, the service is running only on nodes `swarm-test-2` and `swarm-test-3`.
    If we would add the label to the node `swarm-test-1`, Swarm would run the service
    on that node as well.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们告诉 Docker 我们希望服务是全局性的，因此期望的状态是在所有节点上都处于`Running`状态。然而，由于我们指定了约束`node.labels.env
    == prod-like`，副本仅在与该约束匹配的节点上运行。换句话说，服务仅在节点`swarm-test-2`和`swarm-test-3`上运行。如果我们将标签添加到节点`swarm-test-1`，Swarm也会在该节点上运行服务。
- en: 'Before we move on, let''s remove the `util` services:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们删除`util`服务：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that we know how to constrain services to particular nodes, we must create
    a service before proceeding with the Continuous Delivery steps.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道如何将服务限制到特定节点，我们必须在继续进行持续交付步骤之前先创建一个服务。
- en: Creating services
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建服务
- en: Before we continue exploring the Continuous Delivery steps, we should discuss
    a deployment change introduced with Docker Swarm. We thought that each release
    means a new deployment. That is not true with Docker Swarm. Instead of deploying
    each release, we are now updating services. After building Docker images, all
    we have to do is update the service that is already running. In most cases, all
    there is to do is to run the `docker service update --image <IMAGE> <SERVICE_NAME>`
    command. The service already has all the information it needs and all we have
    to do is to change the image to the new release.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续探索持续交付步骤之前，我们应该讨论一下 Docker Swarm 引入的部署变化。我们曾经认为每次发布意味着一次新的部署，但在 Docker Swarm
    中并非如此。我们现在更新的是服务，而不是每次都进行部署。在构建 Docker 镜像后，我们所要做的就是更新已经运行的服务。在大多数情况下，我们所需要做的仅仅是运行`docker
    service update --image <IMAGE> <SERVICE_NAME>`命令。服务已经拥有它所需的所有信息，我们要做的就是将镜像更改为新版本。
- en: For service update to work, we need to have a service. We need to create it
    and make sure that it has all the information it needs. In other words, we create
    a service once and update it with each release. That greatly simplifies the release
    process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使服务更新生效，我们需要有一个服务。我们需要创建它并确保它拥有所需的所有信息。换句话说，我们只需要创建一次服务，并在每次发布时进行更新。这大大简化了发布过程。
- en: Since a service is created only once, the **Return On Investment** (**ROI**)
    is too low for us to automate this step. Remember, we want to automate processes
    that are done many times. Things that are done once and never again are not worth
    automating. One of those things is the creation of services. We are still running
    all the commands manually so consider this as a note for the next chapter that
    will automate the whole process.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于服务只创建一次，因此**投资回报率**（**ROI**）太低，我们不打算自动化这一步骤。记住，我们希望自动化的是那些需要做多次的过程。那些只做一次就不再做的事情是没有自动化价值的。其中之一就是服务的创建。我们仍然手动运行所有命令，所以请将此作为下一章的说明，下一章将自动化整个过程。
- en: Let us create the services that form the *go-demo* application. We'll need the
    `proxy`, the `go-demo` service, and the accompanying database. As before, we'll
    have to create the `go-demo` and the `proxy` networks. Since we already did that
    a couple of times, we'll run all the commands through the `scripts/dm-test-swarm-services.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh))
    script. It creates the services in almost the same way as before. The only difference
    is that it uses the `prod-like` label to restrict services only to the nodes that
    should be utilized for production-like deployments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建构成*go-demo*应用程序的服务。我们需要`proxy`、`go-demo`服务以及随附的数据库。和之前一样，我们需要创建`go-demo`和`proxy`网络。由于我们已经做过几次了，我们将通过`scripts/dm-test-swarm-services.sh`（[https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh)）脚本运行所有命令。它几乎以与之前相同的方式创建服务。唯一的区别是，它使用`prod-like`标签将服务限制仅限于那些应该用于生产类似部署的节点。
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output of the `service ls` command is as follows (IDs are removed for brevity):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ls`命令的输出如下（为了简洁起见，已删除 ID）：'
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/cd-environment-prod-like-services.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-prod-like-services.png)'
- en: 'Figure 5-3: CD cluster with services running in nodes labeled as prod-like'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-3：带有在标记为 prod-like 节点上运行的服务的 CD 集群
- en: Please note that the proxy reconfiguration port was set to `8090` on the localhost.
    We had to differentiate it from the port `8080` that we'll use when running the
    `go-demo` service in the staging environment.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，代理重新配置端口已设置为`8090`，并且在本地主机上。我们必须将其与我们在暂存环境中运行`go-demo`服务时将使用的`8080`端口区分开来。
- en: On one hand, we want the services in the production-like cluster to resemble
    those in the production cluster. On the other hand, we do not want to waste resources
    in replication of the full production environment. For that reason, we are running
    two instances (replicas) of the `proxy` and `go-demo` services. Running only one
    would deviate too much from the idea that services should be scaled in production.
    Two of each gives us the ability to test that scaled services work as expected.
    Even if we run many more instances in production, two is just enough to replicate
    scaled behavior. Since we still did not manage to set up database replication,
    MongoDB is, for now, running as only one instance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，我们希望生产环境类似集群中的服务与生产集群中的服务相似。另一方面，我们又不希望浪费资源去复制完整的生产环境。因此，我们运行了`proxy`和`go-demo`服务的两个实例（副本）。仅运行一个实例会偏离服务在生产环境中应扩展的目标。每个服务有两个实例，使我们能够测试扩展后的服务是否按预期工作。即便在生产环境中运行更多实例，两个副本也足以复制扩展行为。由于我们仍未能设置数据库复制，MongoDB目前仅运行一个实例。
- en: 'We can confirm that all services were indeed created and integrated successfully
    by sending a request to `go-demo`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向`go-demo`发送请求来确认所有服务确实已成功创建和集成：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We''ll also create the same services in the production cluster. The only difference
    will be in the number of replicas (we''ll have more) and that we won''t constrain
    them. Since there is no significant difference from what we did before, we''ll
    use `scripts/dm-swarm-services.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh))
    script to speed up the process:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将在生产集群中创建相同的服务。唯一的区别是副本数量（我们会有更多副本）以及我们不会限制它们。由于与之前的操作没有显著区别，我们将使用`scripts/dm-swarm-services.sh`（[https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh)）脚本来加速这个过程：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output of the `service ls` is as follows (IDs are removed for brevity):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`service ls`的输出如下（为了简洁，已移除 IDs）：'
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/cd-environment-services.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-services.png)'
- en: 'Figure 5-4: CD and production clusters with services'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-4：CD 和生产集群中的服务
- en: Now that we have the services created in both clusters, we can start working
    on the Continuous Delivery steps.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在两个集群中创建了服务，可以开始进行持续交付步骤。
- en: Walking through Continuous Delivery steps
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按照持续交付步骤进行操作
- en: We already know all the steps required for the Continuous Delivery process.
    We did each of them at least once. We got introduced to some of them in the [Chapter
    1](44df5a4c-1e47-4de0-9442-660034287e66.xhtml), *Continuous Integration with Docker
    Containers*. After all, Continuous Delivery is Continuous Integration "extended".
    It's what Continuous Integration would be if it would have a clear objective.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了连续交付过程所需的所有步骤。我们至少完成过一次每个步骤。在[第 1 章](44df5a4c-1e47-4de0-9442-660034287e66.xhtml)《使用
    Docker 容器进行持续集成》中，我们介绍了一些步骤。毕竟，持续交付是持续集成的“扩展”。如果持续集成有明确的目标，那它就是持续交付的样子。
- en: We ran the rest of the steps throughout the chapters that lead to this point.
    We know how to create, and, more importantly, update a service inside a Swarm
    cluster. Therefore, I won't go into many details. Consider this sub-chapter a
    refreshment of everything we did by now.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在各章中完成了剩余的步骤，直到达到了这一点。我们知道如何在 Swarm 集群中创建服务，更重要的是，如何更新服务。因此，我不会详细讲解。可以将这一小节视为对我们到目前为止所做工作的一次回顾。
- en: 'We''ll start by checking out the code of a service we want to move through
    the CD flow:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先检查一个我们希望通过 CD 流程迁移的服务的代码：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we should run the `unit` tests and compile the service binary:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该运行`unit`测试并编译服务二进制文件：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/cd-environment-unit.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-unit.png)'
- en: 'Figure 5-5: Unit tests run inside the swarm-test-1 node'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-5：在 swarm-test-1 节点内运行单元测试
- en: Please note that we used the `swarm-test-1` node. Even though it belongs to
    the Swarm cluster, we used it in the "traditional" mode.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是`swarm-test-1`节点。尽管它属于 Swarm 集群，但我们以“传统”模式使用它。
- en: 'With the binary compiled, we can build Docker images:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 编译好二进制文件后，我们可以构建 Docker 镜像：
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/cd-environment-build.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-build.png)'
- en: 'Figure 5-6: Build run inside the swarm-test-1 node'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-6：在 swarm-test-1 节点中运行的构建
- en: 'With the image built, we can run `staging` dependencies, functional tests,
    and tear-down everything once we''re done:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 构建了镜像后，我们可以运行 `staging` 依赖、功能测试，并在完成后销毁所有内容：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/cd-environment-staging.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-staging.png)'
- en: 'Figure 5-7: Staging or functional tests run inside the swarm-test-1 node'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-7：在 swarm-test-1 节点中运行的暂存或功能测试
- en: 'Now that we are confident that the new release is likely to work as expected,
    we can push the result to the registry:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有信心新版本很可能会按预期工作，我们可以将结果推送到注册表：
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We ran the unit tests, built the binary, built the images, ran functional tests,
    and pushed the images to the registry. The release is very likely to work as expected.
    However, the only true validation is whether the release works correctly in production.
    There is no more reliable or worthy criteria than that. On the other hand, we
    want to reach production with as much confidence as we can. We'll balance those
    two needs by using the `swarm-test` cluster that is as close to production as
    we can reasonably get.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行了单元测试，构建了二进制文件，构建了镜像，运行了功能测试，并将镜像推送到注册表。这个版本很可能会按预期工作。然而，唯一真正的验证是该版本在生产环境中是否能正常工作。没有比这更可靠或更值得信赖的标准了。另一方面，我们希望尽可能有信心地进入生产环境。我们将通过使用尽可能接近生产环境的
    `swarm-test` 集群来平衡这两者需求。
- en: 'Right now, the `go-demo` service is running the release 1.0 inside the `swarm-test`
    cluster. We can confirm that by observing the output of the `service ps` command:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`go-demo` 服务正在 `swarm-test` 集群中运行版本 1.0。我们可以通过观察 `service ps` 命令的输出确认这一点：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows (IDs are removed for brevity):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（为了简洁，已删除 ID）：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s update the currently running release to the version we just built that
    is 1.1:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将当前正在运行的版本更新为我们刚刚构建的版本 1.1：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Please note that the service was initially created with the `--update-delay
    5s` argument. That means that each update will last for five seconds on each replica
    set (plus a few moments to pull the image and initialize containers).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，服务最初是通过 `--update-delay 5s` 参数创建的。这意味着每次更新将在每个副本集上持续五秒钟（加上一些时间来拉取镜像和初始化容器）。
- en: 'After a few moments (approximately 6 seconds), the output of the `service ps`
    command should be as follows (IDs are removed for brevity):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 经过片刻（大约 6 秒钟），`service ps` 命令的输出应如下所示（为了简洁，已删除 ID）：
- en: '[PRE33]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: If the output on your laptop is different, please wait for a few moments and
    repeat the `service ps` command.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在笔记本电脑上的输出不同，请稍等片刻并重复 `service ps` 命令。
- en: As you can see, the image changed to `localhost:5000/go-demo:1.1` indicating
    that the new release is indeed up and running.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，镜像已经更改为 `localhost:5000/go-demo:1.1`，表明新版本确实已经启动并运行。
- en: Please note that, since the service was created with the `--constraint 'node.labels.env
    == prod-like'` argument, new releases are still running only in the nodes marked
    as `prod-like`. That shows one of the big advantages Docker Swarm provides. We
    create a service with all the arguments that define its complete behavior. From
    there on, all we have to do is update the image with each release. Things will
    get more complicated later on when we start scaling and doing a few other operations.
    However, the logic is still essentially the same. Most of the arguments we need
    are defined only once through the service creation command.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于该服务是通过 `--constraint 'node.labels.env == prod-like'` 参数创建的，因此新的版本仍然仅在标记为
    `prod-like` 的节点上运行。这显示了 Docker Swarm 提供的一个大优势。我们使用所有定义其完整行为的参数创建一个服务。从那时起，我们要做的就是在每次发布时更新镜像。稍后当我们开始扩展和进行其他操作时，事情会变得更复杂。然而，逻辑本质上还是一样的。我们需要的大多数参数仅在服务创建命令中定义一次。
- en: '![](img/cd-environment-prod-like-update.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-prod-like-update.png)'
- en: 'Figure 5-8: The service is updated inside the prod-like nodes in the CD cluster'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5-8：服务在 CD 集群中的 prod-like 节点内更新
- en: Now we are ready to run some production tests. We are still not confident enough
    to run them against the production environment. First, we want to see whether
    they will pass when executed against the production-like cluster.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好进行一些生产环境测试。我们仍然不够自信直接在生产环境中进行测试。首先，我们想看看它们在类似生产环境的集群中执行时是否能通过。
- en: We'll run production tests like other types we've already run. Our Docker client
    is still pointing to the `swarm-test-1` node so anything we run with Docker Compose
    will continue being executed inside that server.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像之前执行其他类型测试一样运行生产测试。我们的Docker客户端仍然指向`swarm-test-1`节点，因此我们通过Docker Compose运行的任何内容都会继续在该服务器内执行。
- en: 'Let''s take a quick look the definition of the production service inside the `docker-compose-test-local.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml))
    file:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看`docker-compose-test-local.yml`文件中的生产服务定义（[https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml)）：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `production` service `extends` the `unit` service. That means that it inherits
    all the properties of the `unit` service, allowing us to avoid repeating ourselves.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`production` 服务`extends`了`unit`服务。这意味着它继承了`unit`服务的所有属性，从而避免了我们重复编写相同的内容。'
- en: Further on, we are adding the environment variable `HOST_IP.` The tests we are
    about to run will use that variable to deduce the address of the service under
    test `go-demo`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加环境变量`HOST_IP`。我们即将运行的测试将使用该变量来推断正在测试的服务`go-demo`的地址。
- en: Finally, we are overwriting the command used in the `unit` service. The new
    command downloads *go* dependencies `go get -d -v -t` and executes all the tests
    tagged as integration `go test --tags integration -v`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们正在覆盖`unit`服务中使用的命令。新命令下载*go*依赖项`go get -d -v -t`并执行所有标记为集成测试的测试`go test
    --tags integration -v`。
- en: 'Let''s see whether the service indeed works inside the `swarm-test` cluster:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看该服务是否确实在`swarm-test`集群中运行：
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We specified that the IP of the service under test is localhost. Since the node
    where tests are running `swarm-test-1` belongs to the cluster, the ingress network
    will forward the request to the `proxy` service which, in turn, will forward it
    to the `go-demo` service.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了正在测试的服务的IP为localhost。由于测试运行的节点`swarm-test-1`属于集群，入口网络将把请求转发到`proxy`服务，后者将请求转发给`go-demo`服务。
- en: 'The last lines of the output are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后几行如下：
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: All integration tests passed, and the whole operation took less than 0.2 seconds.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所有集成测试通过，整个操作花费不到0.2秒。
- en: '![](img/cd-environment-prod-like-tests.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd-environment-prod-like-tests.png)'
- en: 'Figure 5-9: Production tests are run against the updated service inside the
    CD cluster'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图5-9：在CD集群中运行更新后的服务时执行生产测试
- en: From now on, we should be fairly confident that the release is ready for production.
    We ran the pre-deployment unit tests, built the images, ran staging tests, updated
    the production-like cluster, and ran a set of integration tests.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我们应该相当自信地认为发布已经准备好进入生产环境。我们已经进行了预部署单元测试，构建了镜像，运行了暂存测试，更新了生产环境类似的集群，并执行了一套集成测试。
- en: Our Continuous Delivery steps are officially done. The release is ready and
    waiting for someone to make a decision to update the service running in production.
    In other words, at this point, the Continuous Delivery is finished, and we would
    be waiting for someone to press the button to update the service in the production
    cluster.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的持续交付步骤已经正式完成。发布已准备好，等待有人做出决定来更新正在生产环境中运行的服务。换句话说，此时，持续交付已经完成，我们将等待有人按下按钮来更新生产集群中的服务。
- en: There is no reason to stop now. We have all the knowledge we would need to convert
    this process from Continuous Delivery to Continuous Deployment. All we have to
    do is repeat the last few commands inside the production cluster.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在没有理由停下。我们已经掌握了将此过程从持续交付转为持续部署所需的所有知识。我们所需要做的只是重复在生产集群中执行的最后几个命令。
- en: Walking the extra mile from Continuous Delivery to Continuous Deployment
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从持续交付到持续部署的额外努力
- en: If we do have a comprehensive set of tests that give us confidence that each
    commit to the code repository is working as expected and if there is a repeatable
    and reliable Deployment process, there is no real reason for not taking that extra
    mile and automatically deploying each release to production.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一套全面的测试，可以让我们确信每次提交到代码仓库的代码都按预期工作，并且有一个可重复和可靠的部署过程，那么就没有理由不走那一步，自动将每个发布版本部署到生产环境。
- en: You might choose not to do Continuous Deployment. Maybe your process requires
    us to cherry pick features. Maybe our marketing department does not want new features
    to be available before their campaign starts. There are plenty of reasons why
    one would choose to stop at Continuous Delivery. Nevertheless, from the technical
    perspective, the process is the same. The only difference is that Continuous Delivery
    requires us to press the button that deploys the selected release to production
    while Continuous Deployment does the deployment as part of the same automated
    flow. In other words, the steps we are about to run are the same, with or without
    a button in between.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'This will probably be the shortest sub-chapter in the book. We are only a few
    commands short of converting the Continuous Delivery process into Continuous Deployment.
    We need to update the service in the production cluster (swarm) and go back to
    the `swarm-test-1` node and execute another round of tests. Since we already did
    all that, there is no strong reason to go into details. We''ll just do it:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/cd-environment-prod-update.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-10: The service is updated inside the production cluster'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the service is updated inside the production cluster, we can execute
    the last round of tests:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/cd-environment-prod-tests.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-11: Production tests are run against the updated service inside the
    production cluster'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: We updated the release running inside the production cluster and ran another
    round of integration tests. Nothing failed, indicating that the new release is,
    indeed, running in production correctly.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are we done with Continuous Deployment? The answer is no. We did not create
    the automated Continuous Deployment flow, but defined the steps that will help
    us run the process automatically. For the process to be fully automated and executed
    on each commit, we need to use one of the CD tools.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: We'll use Jenkins to transform manual steps into a fully automated Continuous
    Deployment flow. For the whole process to work, we'll need to set up Jenkins master,
    a few agents, and a deployment pipeline job.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is the time to take a break before diving into the next chapter. As before,
    we''ll destroy the machines we created and start fresh:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
