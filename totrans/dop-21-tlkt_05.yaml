- en: Continuous Delivery and Deployment with Docker Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software, when something is painful, the way to reduce the pain is to do
    it more frequently, not less.
  prefs: []
  type: TYPE_NORMAL
- en: – David Farley
  prefs: []
  type: TYPE_NORMAL
- en: At the time, we could not convert the **Continuous Integration** (**CI**) into
    the **Continuous Delivery** (**CD**) process because we were missing some essential
    knowledge. Now that we understand the basic principles and commands behind Docker
    Swarm, we can go back to the end of the [Chapter 1](9730e69c-a698-40c2-91ce-b92e48328a93.xhtml),
    *Continuous Integration with Docker Containers*. We can define the steps that
    will let us perform the full CD process.
  prefs: []
  type: TYPE_NORMAL
- en: I won't go into Continuous Delivery details. Instead, I'll pitch it as a single
    sentence. *Continuous Delivery is a process applied to every commit in a code
    repository and results in every successful build being ready for deployment to
    production.*
  prefs: []
  type: TYPE_NORMAL
- en: CD means that anyone, at any time, can click a button, and deploy a build to
    production without the fear that something will go wrong. It means that the process
    is so robust that we have full confidence that "almost" any problem will be detected
    before the deployment to production. Needless to say, CD is an entirely automated
    process. There is no human involvement from the moment a commit is sent to a code
    repository, all the way until a build is ready to be deployed to production. The
    only manual action is that someone needs to press the button that will run a script
    that performs the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous Deployment** (**CDP**) is one step forward. It is Continuous Delivery
    without the button. *Continuous Deployment is a process applied to every commit
    in a code repository and results with every successful build being Deployed to
    production.*'
  prefs: []
  type: TYPE_NORMAL
- en: No matter which process you choose, the steps are the same. The only difference
    is whether there is a button that deploys the release to production.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it is safe to assume that we'll use Docker whenever convenient
    and that we'll use Swarm clusters to run the services in production and production-like
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by specifying the steps that could define one possible implementation
    of the CD/CDP process:'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run unit tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build binaries and other required artifacts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the service to the staging environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run functional tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the service to the production-like environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run production readiness tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the service to the production environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run production readiness tests.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's get going and set up the environment we'll need for practicing the
    CD flow.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Continuous Delivery environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A minimum requirement for a Continuous Delivery environment is two clusters.
    One should be dedicated to running tests, building artifacts and images, and all
    other CD tasks. We can use it for simulating a production cluster. The second
    cluster will be used for deployments to production.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need two clusters? Can't we accomplish the same with only one?
  prefs: []
  type: TYPE_NORMAL
- en: While we certainly could get away with only one cluster, having two will simplify
    quite a few processes and, more importantly, provide better isolation between
    production and non-production services and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The more we minimize the impact on the production cluster, the better. By not
    running non-production services and tasks inside the production cluster, we are
    reducing the risk. Therefore, we should have a production cluster separated from
    the rest of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's get started and fire up those clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Continuous Delivery clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a minimum number of servers required for a production-like cluster?
    I'd say two. If there's only one server, we would not be able to test whether
    networking and volumes work across nodes. So, it has to be plural. On the other
    hand, I don't want to push your laptop too much so we'll avoid increasing the
    number unless necessary.
  prefs: []
  type: TYPE_NORMAL
- en: For the production-like cluster, two nodes should be enough. We should add one
    more node that we'll use for running tests and building images. The production
    cluster should probably be a bit bigger since it will have more services running.
    We'll make it three nodes big. If needed, we can increase the capacity later on.
    As you already saw, adding nodes to a Swarm cluster is very easy.
  prefs: []
  type: TYPE_NORMAL
- en: By now, we set up a Swarm cluster quite a few times so we'll skip the explanation
    and just do it through a script.
  prefs: []
  type: TYPE_NORMAL
- en: All the commands from this chapter are available in the `05-continuous-delivery.sh` ([https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe](https://gist.github.com/vfarcic/5d08a87a3d4cb07db5348fec49720cbe))
    Gist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go back to the cloud-provisioning directory we created in the previous
    chapter and run the `scripts/dm-swarm.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm.sh))
    script. It will create the production nodes and join them into a cluster. The
    nodes will be called `swarm-1`, `swarm-2`, and `swarm-3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `**node ls**` command is as follows (IDs are removed for
    brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll create the second cluster. We'll use it for running CD tasks as
    well as a simulation of a production environment. Three nodes should be enough,
    for now. We'll call them `swarm-test-1, swarm-test-2,` and `swarm-test-3.`
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll create the cluster by executing the `scripts/dm-test-swarm.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm.sh))
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `node ls` command is as follows (IDs were removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The only thing left, for now, is to create Docker registry services. We'll create
    one in each cluster. That way, there will be no direct relation between them,
    and they will be able to operate independently one from another. For registries
    running on separate clusters to share the same data, we'll mount the same host
    volume to both services. That way, an image pushed from one cluster will be available
    from the other, and vice versa. Please note that the volumes we're creating are
    still a workaround. Later on, we'll explore better ways to mount volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the production cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We've already run the registry in the [Chapter 1](44df5a4c-1e47-4de0-9442-660034287e66.xhtml), *Continuous
    Integration with Docker Containers* . Back then, we had a single node, and we
    used Docker Compose to deploy services. Registry was not an option.
  prefs: []
  type: TYPE_NORMAL
- en: '**A note to Windows users** Git Bash has a habit of altering file system paths.
    To stop this, execute the following before running the code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '`export MSYS_NO_PATHCONV=1`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we''ll run the registry as a Swarm service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We exposed port `5000` and reserved `100` MB of memory. We used the `--mount`
    argument to expose a volume. This argument is, somewhat, similar to the Docker
    Engine argument `--volume` or the volumes argument in Docker compose files. The
    only significant difference is in the format. In this case, we specified that
    the current host directory `source=$PWD` should be mounted inside the container
    `target=/var/lib/registry`.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, from now on, we'll always run specific versions. While, until
    now, the latest was alright as a demonstration, now we're trying to simulate CD
    processes we'll run in "real" clusters. We should always be explicit which version
    of a service we want to run. That way, we can be sure that the same service is
    tested and deployed to production. Otherwise, we could run into a situation where
    one version was deployed and tested in a production-like environment, but a different
    one was deployed to production.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits behind specific versions are even more apparent when we use images
    from Docker Hub. For example, if we just run the latest release of the registry,
    there is no guarantee that, later on, when we run it in the second cluster, the
    latest release will not be updated. We could, easily, end up getting different
    releases of the registry in different clusters. That could lead to some really
    hard-to-detect bugs.
  prefs: []
  type: TYPE_NORMAL
- en: I won't bother you more with versioning. I'm sure you know what's it for and
    when to use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get back to the `registry` service. We should create it inside the second
    cluster as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the `registry` service running inside both clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd-environment-registry-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1: CD and production clusters with the registry service'
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, we do not know in which servers the registries are running. All
    we know is that there is an instance of the service in each cluster. Normally,
    we'd have to configure Docker Engine to treat the registry service as insecure
    and allow the traffic. To do that, we'd need to know the IP of the server the
    registry is running in. However, since we ran it as a Swarm service and exposed
    port `5000`, the routing mesh will make sure that the port is open in every node
    of the cluster and forward requests to the service. That allows us to treat the
    registry as localhost. We can pull and push images from any node as if the registry
    is running in each of them. Moreover, Docker Engines default behavior is to allow
    only localhost traffic to the registry. That means that we do not need to change
    its configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Using node labels to constrain services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labels are defined as key-value sets. We'll use the key `env` (short for environment).
    At the moment, we don't need to label the nodes used for CD tasks since we are
    not yet running them as services. We'll change that in one of the chapters that
    follow. For now, we only need to label the nodes that will be used to run our
    services in the production-like environment.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the nodes `swarm-test-2` and `swarm-test-3` as our production-like
    environment so we'll label them with the key `env` and the value `prod-like`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the node `swarm-test-2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can confirm that the label was indeed added by inspecting the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the node `inspect` command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, one of the labels is `env` with the value `prod-like`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add the same label to the second node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-labels.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-2: CD cluster with labeled nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a few nodes labeled as production-like, we can create services
    that will run only on those servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a service with the `alpine` image and constrain it to one of
    the `prod-like` nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can list the processes of the `util` service and confirm that it is running
    on one of the `prod-like` nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `service ps` command is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the service is running inside the `swarm-test-2` node which
    is labeled as `env=prod-like`.
  prefs: []
  type: TYPE_NORMAL
- en: That, in itself, does not prove that labels work. After all, two out of three
    nodes are labeled as production-like, so there was a 66% chance that the service
    would run on one of them if labels did not work. So, let's spice it up a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll increase the number of instances to six:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us take a look at the `util` processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, all six instances are running on nodes labeled `env=prod-like`
    (nodes `swarm-test-2` and `swarm-test-3`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe a similar result if we would run the service in the global mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the `util-2` processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Since we told Docker that we want the service to be global, the desired state
    is `Running` on all nodes. However, since we specified the constraint `node.labels.env
    == prod-like`, replicas are running only on the nodes that match it. In other
    words, the service is running only on nodes `swarm-test-2` and `swarm-test-3`.
    If we would add the label to the node `swarm-test-1`, Swarm would run the service
    on that node as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on, let''s remove the `util` services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know how to constrain services to particular nodes, we must create
    a service before proceeding with the Continuous Delivery steps.
  prefs: []
  type: TYPE_NORMAL
- en: Creating services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we continue exploring the Continuous Delivery steps, we should discuss
    a deployment change introduced with Docker Swarm. We thought that each release
    means a new deployment. That is not true with Docker Swarm. Instead of deploying
    each release, we are now updating services. After building Docker images, all
    we have to do is update the service that is already running. In most cases, all
    there is to do is to run the `docker service update --image <IMAGE> <SERVICE_NAME>`
    command. The service already has all the information it needs and all we have
    to do is to change the image to the new release.
  prefs: []
  type: TYPE_NORMAL
- en: For service update to work, we need to have a service. We need to create it
    and make sure that it has all the information it needs. In other words, we create
    a service once and update it with each release. That greatly simplifies the release
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Since a service is created only once, the **Return On Investment** (**ROI**)
    is too low for us to automate this step. Remember, we want to automate processes
    that are done many times. Things that are done once and never again are not worth
    automating. One of those things is the creation of services. We are still running
    all the commands manually so consider this as a note for the next chapter that
    will automate the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: Let us create the services that form the *go-demo* application. We'll need the
    `proxy`, the `go-demo` service, and the accompanying database. As before, we'll
    have to create the `go-demo` and the `proxy` networks. Since we already did that
    a couple of times, we'll run all the commands through the `scripts/dm-test-swarm-services.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-test-swarm-services.sh))
    script. It creates the services in almost the same way as before. The only difference
    is that it uses the `prod-like` label to restrict services only to the nodes that
    should be utilized for production-like deployments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `service ls` command is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-prod-like-services.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-3: CD cluster with services running in nodes labeled as prod-like'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the proxy reconfiguration port was set to `8090` on the localhost.
    We had to differentiate it from the port `8080` that we'll use when running the
    `go-demo` service in the staging environment.
  prefs: []
  type: TYPE_NORMAL
- en: On one hand, we want the services in the production-like cluster to resemble
    those in the production cluster. On the other hand, we do not want to waste resources
    in replication of the full production environment. For that reason, we are running
    two instances (replicas) of the `proxy` and `go-demo` services. Running only one
    would deviate too much from the idea that services should be scaled in production.
    Two of each gives us the ability to test that scaled services work as expected.
    Even if we run many more instances in production, two is just enough to replicate
    scaled behavior. Since we still did not manage to set up database replication,
    MongoDB is, for now, running as only one instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can confirm that all services were indeed created and integrated successfully
    by sending a request to `go-demo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also create the same services in the production cluster. The only difference
    will be in the number of replicas (we''ll have more) and that we won''t constrain
    them. Since there is no significant difference from what we did before, we''ll
    use `scripts/dm-swarm-services.sh` ([https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh](https://github.com/vfarcic/cloud-provisioning/blob/master/scripts/dm-swarm-services.sh))
    script to speed up the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `service ls` is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-services.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-4: CD and production clusters with services'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the services created in both clusters, we can start working
    on the Continuous Delivery steps.
  prefs: []
  type: TYPE_NORMAL
- en: Walking through Continuous Delivery steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already know all the steps required for the Continuous Delivery process.
    We did each of them at least once. We got introduced to some of them in the [Chapter
    1](44df5a4c-1e47-4de0-9442-660034287e66.xhtml), *Continuous Integration with Docker
    Containers*. After all, Continuous Delivery is Continuous Integration "extended".
    It's what Continuous Integration would be if it would have a clear objective.
  prefs: []
  type: TYPE_NORMAL
- en: We ran the rest of the steps throughout the chapters that lead to this point.
    We know how to create, and, more importantly, update a service inside a Swarm
    cluster. Therefore, I won't go into many details. Consider this sub-chapter a
    refreshment of everything we did by now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by checking out the code of a service we want to move through
    the CD flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we should run the `unit` tests and compile the service binary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-unit.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-5: Unit tests run inside the swarm-test-1 node'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we used the `swarm-test-1` node. Even though it belongs to
    the Swarm cluster, we used it in the "traditional" mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the binary compiled, we can build Docker images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-build.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-6: Build run inside the swarm-test-1 node'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the image built, we can run `staging` dependencies, functional tests,
    and tear-down everything once we''re done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-staging.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-7: Staging or functional tests run inside the swarm-test-1 node'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are confident that the new release is likely to work as expected,
    we can push the result to the registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We ran the unit tests, built the binary, built the images, ran functional tests,
    and pushed the images to the registry. The release is very likely to work as expected.
    However, the only true validation is whether the release works correctly in production.
    There is no more reliable or worthy criteria than that. On the other hand, we
    want to reach production with as much confidence as we can. We'll balance those
    two needs by using the `swarm-test` cluster that is as close to production as
    we can reasonably get.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right now, the `go-demo` service is running the release 1.0 inside the `swarm-test`
    cluster. We can confirm that by observing the output of the `service ps` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s update the currently running release to the version we just built that
    is 1.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the service was initially created with the `--update-delay
    5s` argument. That means that each update will last for five seconds on each replica
    set (plus a few moments to pull the image and initialize containers).
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few moments (approximately 6 seconds), the output of the `service ps`
    command should be as follows (IDs are removed for brevity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If the output on your laptop is different, please wait for a few moments and
    repeat the `service ps` command.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the image changed to `localhost:5000/go-demo:1.1` indicating
    that the new release is indeed up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, since the service was created with the `--constraint 'node.labels.env
    == prod-like'` argument, new releases are still running only in the nodes marked
    as `prod-like`. That shows one of the big advantages Docker Swarm provides. We
    create a service with all the arguments that define its complete behavior. From
    there on, all we have to do is update the image with each release. Things will
    get more complicated later on when we start scaling and doing a few other operations.
    However, the logic is still essentially the same. Most of the arguments we need
    are defined only once through the service creation command.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd-environment-prod-like-update.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-8: The service is updated inside the prod-like nodes in the CD cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to run some production tests. We are still not confident enough
    to run them against the production environment. First, we want to see whether
    they will pass when executed against the production-like cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We'll run production tests like other types we've already run. Our Docker client
    is still pointing to the `swarm-test-1` node so anything we run with Docker Compose
    will continue being executed inside that server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look the definition of the production service inside the `docker-compose-test-local.yml` ([https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml](https://github.com/vfarcic/go-demo/blob/master/docker-compose-test-local.yml))
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `production` service `extends` the `unit` service. That means that it inherits
    all the properties of the `unit` service, allowing us to avoid repeating ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Further on, we are adding the environment variable `HOST_IP.` The tests we are
    about to run will use that variable to deduce the address of the service under
    test `go-demo`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we are overwriting the command used in the `unit` service. The new
    command downloads *go* dependencies `go get -d -v -t` and executes all the tests
    tagged as integration `go test --tags integration -v`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see whether the service indeed works inside the `swarm-test` cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We specified that the IP of the service under test is localhost. Since the node
    where tests are running `swarm-test-1` belongs to the cluster, the ingress network
    will forward the request to the `proxy` service which, in turn, will forward it
    to the `go-demo` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last lines of the output are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: All integration tests passed, and the whole operation took less than 0.2 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd-environment-prod-like-tests.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-9: Production tests are run against the updated service inside the
    CD cluster'
  prefs: []
  type: TYPE_NORMAL
- en: From now on, we should be fairly confident that the release is ready for production.
    We ran the pre-deployment unit tests, built the images, ran staging tests, updated
    the production-like cluster, and ran a set of integration tests.
  prefs: []
  type: TYPE_NORMAL
- en: Our Continuous Delivery steps are officially done. The release is ready and
    waiting for someone to make a decision to update the service running in production.
    In other words, at this point, the Continuous Delivery is finished, and we would
    be waiting for someone to press the button to update the service in the production
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There is no reason to stop now. We have all the knowledge we would need to convert
    this process from Continuous Delivery to Continuous Deployment. All we have to
    do is repeat the last few commands inside the production cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Walking the extra mile from Continuous Delivery to Continuous Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we do have a comprehensive set of tests that give us confidence that each
    commit to the code repository is working as expected and if there is a repeatable
    and reliable Deployment process, there is no real reason for not taking that extra
    mile and automatically deploying each release to production.
  prefs: []
  type: TYPE_NORMAL
- en: You might choose not to do Continuous Deployment. Maybe your process requires
    us to cherry pick features. Maybe our marketing department does not want new features
    to be available before their campaign starts. There are plenty of reasons why
    one would choose to stop at Continuous Delivery. Nevertheless, from the technical
    perspective, the process is the same. The only difference is that Continuous Delivery
    requires us to press the button that deploys the selected release to production
    while Continuous Deployment does the deployment as part of the same automated
    flow. In other words, the steps we are about to run are the same, with or without
    a button in between.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will probably be the shortest sub-chapter in the book. We are only a few
    commands short of converting the Continuous Delivery process into Continuous Deployment.
    We need to update the service in the production cluster (swarm) and go back to
    the `swarm-test-1` node and execute another round of tests. Since we already did
    all that, there is no strong reason to go into details. We''ll just do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-prod-update.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-10: The service is updated inside the production cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the service is updated inside the production cluster, we can execute
    the last round of tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cd-environment-prod-tests.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-11: Production tests are run against the updated service inside the
    production cluster'
  prefs: []
  type: TYPE_NORMAL
- en: We updated the release running inside the production cluster and ran another
    round of integration tests. Nothing failed, indicating that the new release is,
    indeed, running in production correctly.
  prefs: []
  type: TYPE_NORMAL
- en: What now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are we done with Continuous Deployment? The answer is no. We did not create
    the automated Continuous Deployment flow, but defined the steps that will help
    us run the process automatically. For the process to be fully automated and executed
    on each commit, we need to use one of the CD tools.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use Jenkins to transform manual steps into a fully automated Continuous
    Deployment flow. For the whole process to work, we'll need to set up Jenkins master,
    a few agents, and a deployment pipeline job.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now is the time to take a break before diving into the next chapter. As before,
    we''ll destroy the machines we created and start fresh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
