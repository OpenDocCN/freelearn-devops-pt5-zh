<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-195"><a id="_idTextAnchor194"/>9</h1>
<h1 id="_idParaDest-196"><a id="_idTextAnchor195"/>Learning about Distributed Application Architecture</h1>
<p>This chapter introduces the concept of distributed application architecture and discusses the various patterns and best practices that are required to run a distributed application successfully. It will also discuss the additional requirements that need to be fulfilled to run such an application in production. You might be wondering, what does this have to do with Docker containers? And you are right to ask. At first glance, these are not related to each other. But as you will soon see, when introducing containers that host an application or application service, your application will quickly consist of several containers that will be running on different nodes of a cluster of computers or VMs; and voilà – you are dealing with a distributed application. We thought that it makes sense to provide you with a sense of the complexity that distributed applications introduce and help you avoid the most common pitfalls.</p>
<p>Here is the list of topics we are going to discuss:</p>
<ul>
<li>What is a distributed application architecture?</li>
<li>Patterns and best practices</li>
<li>Running in production</li>
</ul>
<p>After reading this chapter, you will be able to do the following:</p>
<ul>
<li>Draft a high-level architecture diagram of a distributed application while pointing out key design patterns</li>
<li>Identify the possible pitfalls of a poorly designed distributed application</li>
<li>Name commonly used patterns for dealing with the problems of a distributed system</li>
<li>Name at least four patterns that need to be implemented for a production-ready distributed application</li>
</ul>
<p>Let’s get started!</p>
<h1 id="_idParaDest-197"><a id="_idTextAnchor196"/>What is a distributed application architecture?</h1>
<p>In this section, we are going to explain<a id="_idIndexMarker768"/> what we mean when we talk about distributed application architecture. First, we need to make sure that all the words or acronyms we use have a meaning and that we are all talking in the same language.</p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor197"/>Defining the terminology</h2>
<p>In this and subsequent<a id="_idIndexMarker769"/> chapters, we will talk a lot about concepts that might not be familiar to everyone. To make sure we are all talking the same language, let’s briefly introduce and describe the most important of these concepts or words:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Keyword</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Description</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>VM</p>
</td>
<td class="No-Table-Style">
<p>A <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) is a software simulation of a physical computer that runs on a host computer. It provides a separate operating system and resources, allowing multiple operating systems to run on a single physical machine.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Cluster</p>
</td>
<td class="No-Table-Style">
<p>A cluster is a group of connected servers that work together as a single system to provide high availability, scalability, and increased performance for applications. The nodes in a cluster are connected through a network and share resources to provide a unified, highly available solution.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Node</p>
</td>
<td class="No-Table-Style">
<p>A cluster node is a single server within a cluster computing system. It provides computing resources and works together with other nodes to perform tasks as a unified system, providing high availability and scalability for applications.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Network</p>
</td>
<td class="No-Table-Style">
<p>A network is a group of interconnected devices that can exchange data and information. Networks can be used to connect computers, servers, mobile devices, and other types of devices and allow them to communicate with each other and share resources, such as printers and storage.</p>
<p>More specifically in our case, these are physical and software-defined communication paths between individual nodes of a cluster and programs running on those nodes.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Port</p>
</td>
<td class="No-Table-Style">
<p>A port is a communication endpoint in a network-attached device, such as a computer or server. It allows the device to receive and send data to other devices on the network through a specific network protocol, such as TCP or UDP. Each port has a unique number that is used to identify it, and different services and applications use specific ports to communicate.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Service</p>
</td>
<td class="No-Table-Style">
<p>Unfortunately, this is a very overloaded term and its real meaning depends on the context that it is used in. If we use the term service in the context of an application, such as an application service, then it usually means that this is a piece of software that implements a limited set of functionalities that are then used by other parts of the application. As we progress through this book, other types of services that have slightly different definitions will be discussed.</p>
</td>
</tr>
</tbody>
</table>
<p>Naively said, a distributed application architecture is the opposite of a monolithic application architecture, but it is not unreasonable to look at this monolithic architecture first. Traditionally, most business applications are written in such a way that the result can be seen as a single, tightly coupled program that runs on a named server somewhere in a data center. All its code is compiled into a single binary, or a few very tightly coupled binaries that need to be co-located when running the application. The fact that the server – or more generally – the host, that the application is running on has a well-defined name or static IP address<a id="_idIndexMarker770"/> is also important in this context. Let’s look at the following diagram, which illustrates this type of application architecture a bit more precisely:</p>
<div><div><img alt="Figure 9.1 – Monolithic application architecture" height="358" src="img/B19199_09_01.jpg" width="341"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Monolithic application architecture</p>
<p>In the preceding diagram, we can<a id="_idIndexMarker771"/> see a server named <code>blue-box-12a</code> with an IP address of <code>172.52.13.44</code> running an application called <code>pet-shop</code>, which is a monolith consisting of a main module and a few tightly coupled libraries.</p>
<p>Now, let’s look at the following diagram:</p>
<div><div><img alt="Figure 9.2 – Distributed application architecture" height="458" src="img/B19199_09_02.jpg" width="491"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Distributed application architecture</p>
<p>Here, all of a sudden, we do<a id="_idIndexMarker772"/> not have just a single named server anymore; instead, we have a lot of them, and they do not have human-friendly names, but rather<a id="_idIndexMarker773"/> some unique IDs that can be something such as a <code>pet-api</code>, <code>pet-web</code>, and <code>pet-inventory</code>. Furthermore, each service runs in multiple instances in this cluster of servers or hosts.</p>
<p>You might be wondering why we are discussing this in a book about Docker containers, and you are right to ask. While all the topics we’re going to investigate apply equally to a world where containers do not (yet) exist, it is important to realize that containers and container orchestration engines help address all these problems in a much more efficient and straightforward way. Most of the problems that used to be very hard to solve in a distributed application<a id="_idIndexMarker774"/> architecture become quite simple in a containerized world.</p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor198"/>Patterns and best practices</h1>
<p>A distributed application architecture has many compelling benefits, but it also has one very significant drawback compared to a monolithic application architecture – the former is way more complex. To tame this complexity, the industry has come up with some important best practices and patterns. In the following sections, we are going to investigate some of the most important ones in more detail.</p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor199"/>Loosely coupled components</h2>
<p>The best way to address<a id="_idIndexMarker775"/> a complex subject<a id="_idIndexMarker776"/> has always been to divide it into smaller subproblems that are more manageable. As an example, it would be insanely complex to build a house in a single step. It is much easier to build a house from simple parts that are then combined into the final result.</p>
<p>The same also applies to software development. It is much easier to develop a very complex application if we divide this application into smaller components that interoperate and make up the overall application. Now, it is much easier to develop these components individually if they are loosely coupled with each other. What this means is that component A makes no assumptions about the inner workings of, say, components B and C, and is only interested in how it can communicate with those two components across a well-defined interface.</p>
<p>If each component has a well-defined and simple public interface through which communication with the other components in the system and the outside world happens, then this enables us to develop each component individually, without implicit dependencies on other components. During the development process, other components in the system can easily be replaced by stubs or mocks to allow us to test our components.</p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor200"/>Stateful versus stateless</h2>
<p>Every meaningful business application<a id="_idIndexMarker777"/> creates, modifies, or uses data. In IT, a synonym<a id="_idIndexMarker778"/> for data is <strong class="bold">state</strong>. An application service that creates or modifies persistent data is called a <strong class="bold">stateful component</strong>. Typical stateful components <a id="_idIndexMarker779"/>are database services or services that create files. On the other hand, application components<a id="_idIndexMarker780"/> that do not create or modify persistent data are called <strong class="bold">stateless components</strong>.</p>
<p>In a distributed application <a id="_idIndexMarker781"/>architecture, stateless components are much simpler to handle than stateful components. Stateless components can easily be scaled up and down. Furthermore, they can be quickly and painlessly torn down and restarted on a completely different node of the cluster – all because they have no persistent data associated with them.</p>
<p>Given this, it is helpful to design a system in a way that most of the application services are stateless. It is best to push all the stateful components to the boundaries of the application and limit how many are used. Managing stateful components is hard.</p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor201"/>Service discovery</h2>
<p>As we build applications<a id="_idIndexMarker782"/> that consist of many individual components <a id="_idIndexMarker783"/>or services that communicate with each other, we need a mechanism that allows the individual components to find each other in the cluster. Finding each other usually means that you need to know on which node the target component is running, and on which port it is listening for communication. Most often, nodes are identified by an <strong class="bold">IP address</strong> and a <strong class="bold">port</strong>, which is just a number in a well-defined range.</p>
<p>Technically, we could tell <strong class="bold">Service A</strong>, which wants to communicate with a target, <strong class="bold">Service B</strong>, what the IP address and port of the target are. This could happen, for example, through an entry in a configuration file:</p>
<div><div><img alt="Figure 9.3 – Components are hardwired" height="228" src="img/B19199_09_03.jpg" width="401"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Components are hardwired</p>
<p>While this might work very well in the context of a monolithic application that runs on one or only a few well-known and curated servers, it falls apart in a distributed application architecture. First of all, in this scenario, we have many components, and keeping track of them manually becomes a nightmare. This is not scalable. Furthermore, typically, Service A should or will never know on which node of the cluster the other components run. Their location may not even be stable as component B could be moved from node <em class="italic">X</em> to another node, <em class="italic">Y</em>, due to various reasons external to the application. Thus, we need another way in which Service A can locate Service B, or any other service, for that matter. Commonly, an external authority that is aware of the topology of the system at any given time is used.</p>
<p>This external authority<a id="_idIndexMarker784"/> or service knows all the nodes and their IP addresses <a id="_idIndexMarker785"/>that currently pertain to the cluster; it knows about all the services<a id="_idIndexMarker786"/> that are running and where they are running. Often, this kind of service is called a <strong class="bold">DNS service</strong>, where DNS stands for <strong class="bold">Domain Name System</strong>. As we will see, Docker has a DNS<a id="_idIndexMarker787"/> service implemented as part of its underlying engine. Kubernetes – the number one container orchestration system, which we’ll discuss in <a href="B19199_13.xhtml#_idTextAnchor276"><em class="italic">Chapter 13</em></a>, <em class="italic">Introducing Container Orchestration</em> – also uses a DNS service to facilitate communication between components running in a cluster:</p>
<div><div><img alt="Figure 9.4 – Components consulting an external locator service" height="236" src="img/B19199_09_04.jpg" width="401"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Components consulting an external locator service</p>
<p>In the preceding diagram, we can see how Service A wants to communicate with Service B, but it can’t do this directly. First, it has to query the external authority, a registry service (here, this is called <strong class="bold">DNS Service</strong>), about the whereabouts of Service B. The registry service will answer with the requested information and hand out the IP address and port number that Service A can use to reach Service B. Service A then uses this information and <a id="_idIndexMarker788"/>establishes communication with<a id="_idIndexMarker789"/> Service B. Of course, this is a naive picture of what’s happening at a low level, but it is a good picture to help us understand the architectural pattern of service discovery.</p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor202"/>Routing</h2>
<p>Routing is the mechanism<a id="_idIndexMarker790"/> of sending packets<a id="_idIndexMarker791"/> of data from a source component to a target component. Routing is categorized into different<a id="_idIndexMarker792"/> types. The so-called OSI model (see the reference to this in the <em class="italic">Further reading</em> section at the end of this chapter for more information) is used to distinguish between different types of routing. In the context of containers and container orchestration, routing at layers 2, 3, 4, and 7 are relevant. We will look at routing in more detail in subsequent chapters. For now, let’s just say that layer 2 routing is the most low-level type of routing, which connects a MAC address to another MAC address, while layer<a id="_idIndexMarker793"/> 7 routing, which is also called application-level routing, is the most high-level one. The latter is, for example, used to route requests that have a target identifier – that is, a URL such as <a href="https://acme.com/pets">https://acme.com/pets</a> – to the appropriate target<a id="_idIndexMarker794"/> component<a id="_idIndexMarker795"/> in our system.</p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor203"/>Load balancing</h2>
<p>Load balancing is used whenever <strong class="bold">Service A</strong> needs to communicate with <strong class="bold">Service B</strong>, such as in a request-response<a id="_idIndexMarker796"/> pattern, but the latte<a id="_idIndexMarker797"/>r is running in more than one instance, as shown in the following diagram:</p>
<div><div><img alt="Figure 9.5 – The request of Service A is being load balanced to Service B" height="241" src="img/B19199_09_05.jpg" width="591"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – The request of Service A is being load balanced to Service B</p>
<p>If we have multiple instances of a service such as Service B running in our system, we want to make sure that every one of those instances gets an equal amount of workload assigned to it. This task is a generic one, which means that we don’t want the caller to have to do the load balancing but, rather, an external service that intercepts the call and takes over the role of deciding which<a id="_idIndexMarker798"/> of the target service instances to forward the call to. This external service is called a load balancer. Load balancers can use different algorithms to decide how to distribute incoming calls to target service instances. The most common<a id="_idIndexMarker799"/> algorithm that’s used is called round-robin. This algorithm assigns requests repetitively, starting with instance 1, then 2, until instance <em class="italic">n</em>. After the last instance has been served, the load balancer starts over with instance number 1.</p>
<p>In the preceding example, a load balancer also facilitates high availability since a request from Service A will be forwarded to a healthy instance of Service B. The load balancer also takes the role of periodically<a id="_idIndexMarker800"/> checking the health of each instance<a id="_idIndexMarker801"/> of B.</p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor204"/>Defensive programming</h2>
<p>When developing<a id="_idIndexMarker802"/> a service for a distributed<a id="_idIndexMarker803"/> application, it is important to remember that this service is not going to be standalone and that it’s dependent on other application services or even on external services provided by third parties, such as credit card validation services or stock information services, to just name two. All these other services are external to the service we are developing. We have no control over their correctness or their availability at any given time. Thus, when coding, we always need to assume the worst and hope for the best. Assuming the worst means that we have to deal with potential failures explicitly.</p>
<h3>Retries</h3>
<p>When there is a possibility that an external <a id="_idIndexMarker804"/>service might be temporarily unavailable or not responsive enough, then the following procedure can be used. When the call to the other service fails or times out, the calling code should be structured in such a way that the same call is repeated after a short wait time. If the call fails again, the wait should be a bit longer before the next trial. The calls should be repeated up to a maximum number of times, each time increasing the wait time. After that, the service should give up and provide a degraded service, which could mean returning some stale cached data or no data at all, depending on the situation.</p>
<h3>Logging</h3>
<p>Important operations that are performed<a id="_idIndexMarker805"/> on a service should always be logged. Logging information needs to be categorized to be of any real value. A common list of categories includes <em class="italic">debug</em>, <em class="italic">info</em>, <em class="italic">warning</em>, <em class="italic">error</em>, and <em class="italic">fatal</em>. Logging information should be collected by a central log aggregation service and not stored on an individual node of the cluster. Aggregated logs are easy to parse and filter for relevant information. This information is essential to quickly pinpoint the root cause of a failure or unexpected behavior in a distributed system consisting of many moving parts, running in production.</p>
<h3>Error handling</h3>
<p>As we mentioned earlier, each application<a id="_idIndexMarker806"/> service in a distributed application is dependent on other services. As developers, we should always expect the worst and have appropriate error handling in place. One of the most important best practices is to fail fast. Code the service in such a way that unrecoverable errors are discovered as early as possible and, if such an error is detected, have the service fail immediately. But don’t forget to log meaningful information to <code>STDERR</code> or <code>STDOUT</code>, which can be used by developers or system operators later to track malfunctions in the system. Also, return a helpful error to the caller, indicating as precisely as possible why the call failed.</p>
<p>One sample of fail fast is always checking the input values provided by the caller. Are the values in the expected ranges and complete? If not, then do not try to continue processing; instead, immediately abort the operation.</p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor205"/>Redundancy</h2>
<p>A mission-critical system<a id="_idIndexMarker807"/> has to be available<a id="_idIndexMarker808"/> at all times, around the clock, 365 days a year. Downtime is not acceptable since it might result in a huge loss of opportunities or reputation for the company. In a highly distributed application, the likelihood of a failure of at least one of the many involved components is non-neglectable. We can say that the question is not whether a component will fail, but rather when a failure will occur.</p>
<p>To avoid downtime when one of the many components in the system fails, each part of the system needs to be redundant. This includes the application components, as well as all infrastructure parts. What that means is that if we have a payment service as part of our application, then we need to run this service redundantly. The easiest way to do that is to run multiple instances of this very service on different nodes of our cluster. The same applies, say, to an edge router or a load balancer. We cannot afford for these to ever go down. Thus, the router or load balancer must be redundant.</p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor206"/>Health checks</h2>
<p>We have mentioned<a id="_idIndexMarker809"/> various times that in a distributed<a id="_idIndexMarker810"/> application architecture, with its many parts, the failure of an individual component is highly likely and that it is only a matter of time until it happens. For that reason, we must run every single component of the system redundantly. Load balancers then distribute the traffic across the individual instances of a service.</p>
<p>But now, there is another problem. How does the load balancer or router know whether a certain service instance is available? It could have crashed, or it could be unresponsive. To solve this problem, we can use so-called <strong class="bold">health checks</strong>. The load balancer, or some other system service on behalf of it, periodically polls all the service instances and checks their health. The questions are basically, <em class="italic">Are you still there? Are you healthy?</em> The answer to each question is either <em class="italic">Yes</em> or <em class="italic">No</em>, or the health check times out if the instance is not responsive anymore.</p>
<p>If the component answers with <em class="italic">No</em> or a timeout occurs, then the system kills the corresponding instance and spins up a new instance in its place. If all this happens in a fully automated way, then we say that we have an auto-healing system in place. Instead of the load balancer periodically polling the status of the components, responsibility can also be turned around. The components could be required to periodically send live signals to the load balancer. If a component fails <a id="_idIndexMarker811"/>to send live signals over a predefined, extended period, it is assumed<a id="_idIndexMarker812"/> to be unhealthy or dead.</p>
<p>There are situations where either of the described ways is more appropriate.</p>
<h2 id="_idParaDest-208"><a id="_idTextAnchor207"/>Circuit breaker pattern</h2>
<p>A circuit breaker<a id="_idIndexMarker813"/> is a mechanism that is used<a id="_idIndexMarker814"/> to avoid a distributed application going down due to the cascading failure of many essential components. Circuit breakers help us avoid one failing component tearing down other dependent services in a domino effect. Like circuit breakers in an electrical system, which protect a house from burning down due to the failure of a malfunctioning plugged-in appliance by interrupting the power line, circuit breakers in a distributed application interrupt the connection from <strong class="bold">Service A</strong> to <strong class="bold">Service B</strong> if the latter is not responding or is malfunctioning.</p>
<p>This can be achieved by wrapping a protected service call in a circuit breaker object. This object monitors for failures. Once the number of failures reaches a certain threshold, the circuit breaker trips. All subsequent calls to the circuit breaker will return with an error, without the protected call being made at all:</p>
<div><div><img alt="Figure 9.6 – Circuit breaker pattern" height="512" src="img/B19199_09_06.jpg" width="572"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Circuit breaker pattern</p>
<p>In the preceding diagram, we have a circuit breaker<a id="_idIndexMarker815"/> that tips over after<a id="_idIndexMarker816"/> the second timeout is received when calling <strong class="bold">Service B</strong>.</p>
<h3>Rate limiter</h3>
<p>In the context<a id="_idIndexMarker817"/> of a circuit breaker, a rate limiter is a technique that’s used to control the rate at which requests are processed by a system or service. By limiting the number of requests allowed within a specific time window, rate limiters help prevent overloading and ensure the stability and availability of the service. This mechanism proves useful in mitigating the impact of sudden traffic spikes, protecting backend systems from being overwhelmed, and avoiding cascading failures throughout a distributed system. By integrating rate limiting with circuit breakers, systems can effectively maintain optimal performance and gracefully handle unexpected surges<a id="_idIndexMarker818"/> in demand.</p>
<h3>Bulkhead</h3>
<p>In addition to that, and still in the context<a id="_idIndexMarker819"/> of a circuit breaker, a bulkhead is a resilience pattern that’s used to isolate components or resources within a system, ensuring that a failure in one area does not cause a cascading effect on the entire system. By partitioning resources and segregating operations into separate, independent units, bulkheads help prevent a single point of failure from bringing down the entire service. This mechanism is useful in maintaining system stability, improving fault tolerance, and ensuring that critical operations can continue functioning, even in the event of localized failures. When combined with circuit breakers, bulkheads contribute to a more robust and resilient system, capable of handling failures and maintaining overall system performance.</p>
<h1 id="_idParaDest-209"><a id="_idTextAnchor208"/>Running in production</h1>
<p>To successfully run a distributed application<a id="_idIndexMarker820"/> in production, we need to consider a few more aspects beyond the best practices and patterns that were presented in the preceding sections. One specific area that comes to mind is introspection and monitoring. Let’s go through the most important aspects in detail.</p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor209"/>Logging</h2>
<p>Once a distributed application<a id="_idIndexMarker821"/> is in production, it is not possible to live debug it. But how can we then find out what the root cause of the application malfunctioning is? The solution to this problem is that the application produces abundant and meaningful logging information while running. We briefly discussed this topic in an earlier section. But due to its importance, it is worth reiterating. Developers need to instrument their application services in such a way that they output helpful information, such as when an error occurs or a potentially unexpected or unwanted situation is encountered. Often, this information is output to <code>STDOUT</code> and <code>STDERR</code>, where it is then collected by system daemons that write the information to local files or forward it to a central log aggregation service.</p>
<p>If there is sufficient information in the logs, developers can use those logs to track down the root cause of the errors in the system.</p>
<p>In a distributed application architecture, with its many components, logging is even more important than in a monolithic application. The paths of execution of a single request through all the components of the application can be very complex. Also, remember that the components are distributed across a cluster of nodes. Thus, it makes sense to log everything<a id="_idIndexMarker822"/> of importance and add things to each log entry, such as the exact time when it happened, the component in which it happened, and the node on which the component ran, to name just a few. Furthermore, the logging information should be aggregated in a central location so that it is readily available for developers and system operators to analyze.</p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor210"/>Tracing</h2>
<p>Tracing is used to find out how an individual request<a id="_idIndexMarker823"/> is funneled through a distributed application and how much time is spent overall on the request and in every individual component. This information, if collected, can be used as one of the sources for dashboards that show the behavior and health of the system.</p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor211"/>Monitoring</h2>
<p>Operation engineers<a id="_idIndexMarker824"/> like to have dashboards showing live key metrics of the system, which show them the overall health of the application at a glance. These metrics can be nonfunctional metrics, such as memory and CPU usage, the number of crashes of a system or application component, and the health of a node, as well as functional and, hence, application-specific metrics, such as the number of checkouts in an ordering system or the number of items out of stock in an inventory service.</p>
<p>Most often, the base data that’s used to aggregate the numbers that are used for a dashboard is extracted from logging information. This can either be system logs, which are mostly used for non-functional metrics, or application-level logs, for functional metrics.</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor212"/>Application updates</h2>
<p>One of the competitive advantages<a id="_idIndexMarker825"/> for a company<a id="_idIndexMarker826"/> is to be able to react promptly to changing market situations. Part of this is being able to quickly adjust an application to fulfill new and changed needs or to add new functionality. The faster we can update our applications, the better. Many companies these days roll out new or changed features multiple times per day.</p>
<p>Since application updates are so frequent, these updates have to be non-disruptive. We cannot allow the system to go down for maintenance when upgrading. It all has to happen seamlessly and transparently.</p>
<h3>Rolling updates</h3>
<p>One way of updating <a id="_idIndexMarker827"/>an application or an application service<a id="_idIndexMarker828"/> is to use rolling updates. The assumption here is that the particular piece of software that has to be updated runs in multiple instances. Only then can we use this type of update.</p>
<p>What happens is that the system stops one instance of the current service and replaces it with an instance of the new service. As soon as the new instance is ready, it will be served traffic. Usually, the new instance is monitored for some time to see whether it works as expected; if it does, the next instance of the current service is taken down and replaced with a new instance. This pattern is repeated until all the service instances have been replaced.</p>
<p>Since there are always a few instances running at any given time, current or new, the application is operational all the time. No downtime is needed.</p>
<h3>Blue-green deployments</h3>
<p>In blue-green<a id="_idIndexMarker829"/> deployments, the current version<a id="_idIndexMarker830"/> of the application service, called <strong class="bold">blue</strong>, handles all the application traffic. We then install the new version of the application service, called <strong class="bold">green</strong>, on the production system. This new service is not wired with the rest of the application yet.</p>
<p>Once the green service has been<a id="_idIndexMarker831"/> installed, we can execute <strong class="bold">smoke tests</strong> against this new service. If those succeed, the router can be configured to funnel all traffic that previously went to blue to the new service, green. The behavior of the green service is then observed closely and, if all success criteria are met, the blue service can be decommissioned. But if, for some reason, the green service shows some unexpected or unwanted behavior, the router can be reconfigured to return all traffic to the blue service. The green service can then be removed<a id="_idIndexMarker832"/> and fixed, and a new blue-green deployment can be executed<a id="_idIndexMarker833"/> with the corrected version:</p>
<div><div><img alt="Figure 9.7 – Blue-green deployment" height="332" src="img/B19199_09_07.jpg" width="573"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Blue-green deployment</p>
<p>Next, let’s look at canary releases.</p>
<h3>Canary releases</h3>
<p>Canary releases are releases<a id="_idIndexMarker834"/> where we have the current version of the<a id="_idIndexMarker835"/> application service and the new version installed on the system in parallel. As such, they resemble blue-green deployments. At first, all traffic is still routed through the current version. We then configure a router so that it funnels a small percentage, say 1%, of the overall traffic to the new version of the application service. Subsequently, the behavior of the new service is monitored closely to find out whether it works as expected. If all the criteria for success are met, then the router is configured to funnel more traffic, say 5% this time, through the new service. Again, the behavior of the new service is closely monitored and, if it is successful, more and more traffic is routed to it until we reach 100%. Once all the <a id="_idTextAnchor213"/>traffic has been routed to the new service and it has been stable for some time, the old version of the service can be decommissioned.</p>
<p>Why do we call this a canary release? It is named after coal miners who would use canary birds as an early <a id="_idIndexMarker836"/>warning system in mines. Canaries are particularly sensitive to toxic gas <a id="_idIndexMarker837"/>and if such a bird died, the miners knew they had to abandon the mine immediately.</p>
<h3>Irreversible data changes</h3>
<p>If part of our update<a id="_idIndexMarker838"/> process is to execute an irreversible change<a id="_idIndexMarker839"/> in our state, such as an irreversible schema change in a backing relational database, then we need to address this with special care. It is possible to execute such changes without downtime if we use the right approach. It is important to recognize that, in such a situation, we cannot deploy the code changes that require the new data structure in the data store at the same time as the changes to the data. Rather, the whole update has to be separated into three distinct steps. In the first step, we roll out a backward-compatible schema and data change. If this is successful, then we roll out the new code in the second step. Again, if that is successful, we clean up the schema in the third step and remove the backward compatibility:</p>
<div><div><img alt="Figure 9.8 – Rolling out an irreversible data or schema change" height="305" src="img/B19199_09_08.jpg" width="781"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Rolling out an irreversible data or schema change</p>
<p>The preceding diagram shows how the data and its structure are updated, how the application code is updated, and how the data and data structure are cleaned up.</p>
<h3>Changing the data structure at scale</h3>
<p>Over time, an application<a id="_idIndexMarker840"/> may produce an enormous amount of data. Changing the data structure at scale<a id="_idIndexMarker841"/> refers to the process of altering the format, organization, or layout of large amounts of data stored in a database or other type of data storage system. This can involve adding, removing, or modifying fields, tables, or other elements within the data structure. The goal is to optimize the data for a specific use case or business requirement while preserving the data’s accuracy and integrity. This process typically involves analyzing the existing data structure, planning and testing the changes, and then executing the update in a controlled manner. In large-scale data structure changes, it is important to have a well-defined strategy, a robust testing and validation process, and adequate resources, including technical expertise and backup systems, to minimize the risk of data loss or corruption during the migration process.</p>
<p>In a dynamic data migration scenario, data is constantly updated in real time as it is being used, making the migration process more complex and challenging. This type of migration requires a more sophisticated approach to ensure data consistency and integrity throughout the migration process. The solution should be able to keep track of changes made to the data in the source system and replicate them in the target system while minimizing downtime and data loss. This may involve using specialized tools, such as data replication or mirroring software, or employing a multi-step process that includes data synchronization and reconciliation. Additionally, it is essential to have robust testing and validation<a id="_idIndexMarker842"/> procedures in place, as well as a clear rollback plan, to minimize the risk of data loss or corruption during the migration process.</p>
<h3>Rollback and roll forward</h3>
<p>If we have frequent updates<a id="_idIndexMarker843"/> for our application services that run<a id="_idIndexMarker844"/> in production, sooner or later, there will be a problem<a id="_idIndexMarker845"/> with one of those updates. Maybe a developer, while fixing a bug, introduced<a id="_idIndexMarker846"/> a new one, which was not caught by all the automated, and maybe manual, tests, so the application is misbehaving. In this case, we must roll back the service to the previous good version. In this regard, a rollback is a recovery from a disaster.</p>
<p>Again, in a distributed application architecture, it is not a question of whether a rollback will ever be needed, but rather when a rollback will have to occur. Thus, we need to be sure that we can always roll back to a previous version of any service that makes up our application. Rollbacks cannot be an afterthought; they have to be a tested and proven part of our deployment process.</p>
<p>If we are using blue-green deployments to update our services, then rollbacks should be fairly simple. All we need to do is switch the router from the new green version of the service back to the previous blue version.</p>
<p>If we adhere to continuous delivery and the main branch of our code is always in a deployable state, then we can also consider rolling forward instead of rolling back. Often, it is faster to fix a production issue and roll out the fix immediately instead of trying to roll back our system<a id="_idIndexMarker847"/> to a previous state. The technique of rolling forward<a id="_idIndexMarker848"/> is of particular<a id="_idIndexMarker849"/> interest if the previous<a id="_idIndexMarker850"/> change introduced some backward incompatibility.</p>
<h1 id="_idParaDest-214"><a id="_idTextAnchor214"/>Summary</h1>
<p>In this chapter, we learned what a distributed application architecture is and what patterns and best practices are helpful or needed to successfully run a distributed application. We also discussed what is needed to run such an application in production.</p>
<p>In the next chapter, we will dive into networking limited to a single host. We are going to discuss how containers living on the same host can communicate with each other and how external clients can access containerized applications if necessary.</p>
<h1 id="_idParaDest-215"><a id="_idTextAnchor215"/>Further reading</h1>
<p>The following articles provide more in-depth information regarding what was covered in this chapter:</p>
<ul>
<li><em class="italic">Circuit </em><em class="italic">breakers</em>: <a href="http://bit.ly/1NU1sgW">http://bit.ly/1NU1sgW</a></li>
<li><em class="italic">The OSI model </em><em class="italic">explained</em>: <a href="http://bit.ly/1UCcvMt">http://bit.ly/1UCcvMt</a></li>
<li><em class="italic">Blue-green </em><em class="italic">deployments</em>: <a href="http://bit.ly/2r2IxNJ">http://bit.ly/2r2IxNJ</a></li>
</ul>
<h1 id="_idParaDest-216"><a id="_idTextAnchor216"/>Questions</h1>
<p>Please answer the following questions to assess your understanding of this chapter’s content:</p>
<ol>
<li>When and why does every part in a distributed application architecture have to be redundant? Explain this in a few short sentences.</li>
<li>Why do we need DNS services? Explain this in three to five sentences.</li>
<li>What is a circuit breaker and why is it needed?</li>
<li>What are some of the important differences between a monolithic application and a distributed or multi-service application?</li>
<li>What is a blue-green deployment?</li>
</ol>
<h1 id="_idParaDest-217"><a id="_idTextAnchor217"/>Answers</h1>
<p>Here are the possible answers to this chapter’s questions:</p>
<ol>
<li>In a distributed application architecture, every piece of the software and infrastructure needs to be redundant in a production environment, where the continuous uptime of the application is mission-critical. A highly distributed application consists of many parts and the likelihood of one of the pieces failing or misbehaving increases with the number of parts. It is guaranteed that, given enough time, every part will eventually fail. To avoid outages of the application, we need redundancy in every part, be it a server, a network switch, or a service running on a cluster node in a container.</li>
<li>In highly distributed, scalable, and fault-tolerant systems, individual services of the application can move around due to scaling needs or due to component failures. Thus, we cannot hardwire different services with each other. Service A, which needs access to Service B, should not have to know details about Service B, such as its IP address. It should rely on an external provider for this information. TheDNS is such a provider of location information. Service A just tells it that it wants to talk to Service B and the DNS service will figure out the details.</li>
<li>A circuit breaker is a means to avoid cascading failures if a component in a distributed application is failing or misbehaving. Similar to a circuit breaker in electric wiring, a software-driven circuit breaker cuts the communication between a client and a failed service. The circuit breaker will directly report an error back to the client component if the failed service is called. This allows the system to recover or heal from failure.</li>
<li>A monolithic application is easier to manage than a multi-service application since it consists of a single deployment package. On the other hand, a monolith is often harder to scale to account for increased demand in one particular area of the application. In a distributed application, each service can be scaled individually and each service can run on optimized infrastructure, while a monolith needs to run on infrastructure that is OK for all or most of the features implemented in it. However, over time, this has become less of a problem since very powerful servers and/or VMs are made available by all major cloud providers. These are relatively cheap and can handle the load of most average line-of-business or web applications with ease.</li>
</ol>
<p>Maintaining and updating a monolith, if not well modularized, is much harder than a multi-service application, where each service can be updated and deployed independently. The monolith is often a big, complex, and tightly coupled pile of code. Minor modifications can have unexpected side effects. (Micro) Services, in theory, are self-contained, simple components that behave like black boxes. Dependent services know nothing about the inner workings of the service and thus do not depend on it.</p>
<p class="callout-heading">Note</p>
<p class="callout">The reality is often not so nice – in many cases, microservices are hard coupled and behave like distributed monoliths. Sadly, the latter is the worst place a team or a company can be in as it combines the disadvantages of both worlds, with the monolith on one side and the distributed application on the other.</p>
<ol>
<li value="5">A blue-green deployment is a form of software deployment that allows for zero downtime deployments of new versions of an application or an application service. If, say, Service A needs to be updated with a new version, then we call the currently running blue version. The new version of the service is deployed into production, but not yet wired up with the rest of the application. This new version is called green. Once the deployment succeeds and smoke tests have shown it’s ready to go, the router that funnels traffic to the blue version is reconfigured to switch to the green version. The behavior of the green version is observed for a while and if everything is OK, the blue version is decommissioned. On the other hand, if the green version causes difficulties, the router can simply be switched back to the blue version, and the green version can be fixed and later redeployed.</li>
</ol>
</div>
</div></body></html>