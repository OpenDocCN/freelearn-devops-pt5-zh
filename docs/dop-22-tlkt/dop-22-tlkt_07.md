## 部署和配置 Prometheus

初看之下，部署 Prometheus 很简单。创建一个 Compose 文件并执行`docker stack deploy`命令。复杂性出现在我们开始将服务与 Prometheus 集成时。很快，你将亲身体验到集成问题。

像任何一个好故事一样，本章将从一个愉快的开始。对工程师来说，愉快意味着简单且有效。让我们看看实践中简单是怎样的。

### 部署 Prometheus 堆栈

我们将从克隆[vfarcic/docker-flow-monitor](https://github.com/vfarcic/docker-flow-monitor)仓库开始。它包含了本章中我们将使用的所有脚本和 Docker 堆栈。

```
`1` git clone `\`
`2 `    https://github.com/vfarcic/docker-flow-monitor.git
`3` 
`4` `cd` docker-flow-monitor 
```

`````````````````` Before we create a Prometheus service, we need to have a cluster. It will consist of three nodes created with Docker Machine.    ``` `1` chmod +x scripts/dm-swarm.sh `2`  `3` ./scripts/dm-swarm.sh `4`  `5` `eval` `$(`docker-machine env swarm-1`)`  ```   ````````````````` The `dm-swarm.sh` script created the nodes and joined them into a Swarm cluster.    Now we can create the first Prometheus service. We’ll start small and slowly move toward a more robust solution.    We’ll deploy the stack defined in `stacks/prometheus.yml`. It is as follows.    ``` `1` version: "3" `2`  `3` services: `4`  `5 `  prometheus: `6 `    image: prom/prometheus `7 `    ports: `8 `      - 9090:9090  ```   ```````````````` As you can see, it is as simple as it can get. It specifies the image and the port that should be opened.    Let’s deploy the stack.    ``` `1` docker stack deploy `\` `2 `    -c stacks/prometheus.yml `\` `3 `    monitor  ```   ``````````````` Please wait a few moments until the image is pulled and deployed. You can monitor the status by executing the `docker stack ps monitor` command.    Let’s confirm that Prometheus service is indeed up-and-running.    ``` `1` open `"http://``$(`docker-machine ip swarm-1`)``:9090"`  ```   `````````````` You should see the Prometheus Graph screen.    Let’s take a look at the configuration.    ``` `1` open `"http://``$(`docker-machine ip swarm-1`)``:9090/config"`  ```   ````````````` You should see the default config that does not define much more than intervals and internal scraping. In its current state, Prometheus is not very useful, so we’ll have to spice it up a bit.  ![Figure 3-1: Prometheus with the default configuration](img/00008.jpeg)  Figure 3-1: Prometheus with the default configuration    We should start fine tuning Prometheus. There are quite a few ways we can do that.    We can create a new Docker image that would extend the one we used and add our own configuration file. That solution has a distinct advantage of being immutable and, hence, very reliable. Since Docker image cannot be changed, we can guarantee that the configuration is exactly as we want it to be no matter where we deploy it. If the service fails, Swarm will reschedule it and, since the configuration is baked into the image, it’ll be preserved. The problem with that approach is that it is not suitable for microservices architecture. If Prometheus has to be reconfigured with every new service (or at least those that expose metrics), we would need to build it quite often and tie that build to CD processes executed for the services we’re developing. This approach is suitable only for a relatively static cluster and monolithic applications. Discarded!  ![Figure 3-2: Creating a new image every time Prometheus config change](img/00009.jpeg)  Figure 3-2: Creating a new image every time Prometheus config change    What would be the alternative approach?    We can enter a running Prometheus container, modify its configuration, and reload it. While this allows a higher level of dynamism, it is not fault-tolerant. If Prometheus fails, Swarm will reschedule it, and all the changes we made will be lost. Besides fault tolerance, modifying a config in a running container poses additional problems when running it as a service inside a cluster. We need to find out the node it is running in, SSH into it, figure out the ID of the container, and, only then, we can `exec` into it, modify the config, and send a reload request. While those steps are not overly complicated and can be scripted, they will pose an unnecessary operational complexity. Discarded!  ![Figure 3-3: Updating Prometheus configuration inside a container](img/00010.jpeg)  Figure 3-3: Updating Prometheus configuration inside a container    Among other reasons, we discarded the previous solution because it is not fault-tolerant.    We could mount a network volume to the service. That would solve persistence, but would still leave the problem created by a dynamic nature of a cluster. We still, potentially, need to change the configuration and reload Prometheus every time a new service is deployed or updated.    From the operational perspective, this solution is simpler than the previous solution we discussed. We do not need to find out the node it is running in, SSH into it, figure out the ID of the container, `exec` into it, and modify the config. Instead, we can alter the file on the network drive and send a reload request to Prometheus. While network drive simplifies the process, it does not make it as dynamic and independent from the services as it should be. We would need to make sure that the deployment pipeline of each of the services has the required steps that will reconfigure Prometheus. By doing that we would break one of our objectives. That is, our services would not contain all the information about themselves. Instead, we’d need to create a different pipeline for each and specify the targets, alerts, and other information we might need before reconfiguring Prometheus. We’ll discard this solution as well.  ![Figure 3-4: Updating Prometheus configuration stored on a network drive](img/00011.jpeg)  Figure 3-4: Updating Prometheus configuration stored on a network drive    What other options do we have? If we’re looking for an out-of-the-box solution that uses the official Prometheus image, all our options are exhausted. But we are engineers. We are used to extending other people solutions and adapting them to suit our needs. Let’s not limit our options and try to design a solution that would suit us well.    ### Designing A More Dynamic Monitoring Solution    How can we improve Prometheus design to suit our purposes better? How can we make it more dynamic and more scheduler friendly?    One improvement we can make is the usage of environment variables. That would save us from having to create a new image every time we need to change its configuration. At the same time, environment variables would remove the need to use a network drive (at least for configuration).    We can make a generic solution that will transform any environment variable into a Prometheus configuration entry or an initialization argument.    To enable Prometheus configuration through environment variables, we need to distinguish those that should be used as command line arguments from those that will serve to create the configuration file. We’ll define a naming convention stating that every environment argument with a name that starts with `ARG_` is a startup argument.    The code can be as follows.    ```  `1` `func` `Run``()` `error` `{`  `2` 	`cmdString` `:=` `"prometheus"`  `3` 	`for` `_``,` `e` `:=` `range` `os``.``Environ``()` `{`  `4` 		`if` `key``,` `value` `:=` `getArgFromEnv``(``e``,` `"ARG"``);` `len``(``key``)` `>` `0` `{`  `5` 			`cmdString` `=` `fmt``.``Sprintf``(``"%s -%s=%s"``,` `cmdString``,` `key``,` `value``)`  `6` 		`}`  `7` 	`}`  `8` 	`cmd` `:=` `exec``.``Command``(``"/bin/sh"``,` `"-c"``,` `cmdString``)`  `9` 	`return` `cmdRun``(``cmd``)` `10` `}`  ```   ```````````` It is a very simple function. It iterates through all the environment variables. If their names start with `ARG`, they will be added as arguments of the executable `prometheus`. Once the iteration is done, binary is launched with arguments.    We made Prometheus more *Docker-friendly* with only a few lines of code that sits on top of it.    The full source code can be found in the [run.go](https://github.com/vfarcic/docker-flow-monitor/blob/master/prometheus/run.go) file.    We should do something similar with the configuration file. Specifically, we can make the global section of the configuration use environment variables prefixed with `GLOBAL_`.    The logic of the code is similar to the `Run` function we explored. Please go through [config.go](https://github.com/vfarcic/docker-flow-monitor/blob/master/prometheus/config.go) for more details. The `GetGlobalConfig` function returns `global` section of the config while the `WriteConfig` function writes the configuration to the file.    Please consult [Prometheus Configuration](https://prometheus.io/docs/operating/configuration/) for more information about the available options.    By using environment variables, we managed to get rid of the network drive. As far as configuration is concerned, it will be fault tolerant. If the service fails and gets rescheduled with Swarm, it will not lose its configuration since it is part of the service definition. There is a downside though. Every time we want to change the configuration, we’ll need to execute `docker service update` command or modify the stack file, and re-execute `docker stack deploy`. As a result, Docker will stop the currently running replica and start a new one thus producing a short downtime. However, since we are, at the moment, only dealing with global configuration and startup arguments, changes will be very uncommon. We’ll deal with more dynamic parts of the configuration later.  ![Figure 3-5: Prometheus configuration defined through environment variables](img/00012.jpeg)  Figure 3-5: Prometheus configuration defined through environment variables    I have the code compiled and available as [vfarcic/docker-flow-monitor/](https://hub.docker.com/r/vfarcic/docker-flow-monitor/). Let’s give it a spin.    ### Deploying Docker Flow Monitor    Deploying *Docker Flow Monitor* is easy (as almost all Docker services are). We’ll start by creating a network called `monitor`. We could let Docker stack create it for us, but it is useful to have it defined externally so that we can easily attach it to services from other stacks.    ``` `1` docker network create -d overlay monitor  ```   ``````````` The stack is as follows.    ```  `1` `version``:` `"3"`  `2` `services``:`  `3`  `monitor``:`  `4`    `image``:` `vfarcic``/``docker``-``flow``-``monitor``:``$``{``TAG``:-``latest``}`  `5`    `environment``:`  `6`      `-` `GLOBAL_SCRAPE_INTERVAL``=``10``s`  `7`    `networks``:`  `8`      `-` `monitor`  `9`    `ports``:` `10 `      `-` `9090``:``9090` `11` `networks``:` `12 `    `monitor``:` `13 `       `external``:` `true`  ```   `````````` The environment variable `GLOBAL_SCRAPE_INTERVAL` shows the first improvement over the “original” Prometheus service. It allows us to define entries of its configuration as environment variables. That, in itself, is not a significant improvement but is a good start. More powerful additions will be explored later on.    Now we’re ready to deploy the stack.    ``` `1` docker stack rm monitor `2`  `3` docker stack deploy `\` `4 `    -c stacks/docker-flow-monitor.yml `\` `5 `    monitor  ```   ````````` Please wait a few moments until Swarm pulls the image and starts the service. You can monitor the status by executing `docker stack ps monitor` command.    Once the service is running, we can confirm that the environment variable indeed generated the configuration.    ``` `1` open `"http://``$(`docker-machine ip swarm-1`)``:9090/config"`  ```  ```````` ![Figure 3-6: Prometheus configuration defined through environment variables](img/00013.jpeg)  Figure 3-6: Prometheus configuration defined through environment variables    we are going to expose services url in pretty format, therefore we must get rid of port number (9090) in above url.    ### Integrating Docker Flow Monitor With Docker Flow Proxy    Having a port opened (other than `80` and `443`) is, often, not a good idea. If for no other reason, at least it’s not user-friendly to remember a different port for each service. In general service might need to be accessible on its own subdomain, it might need SSL certificate, it might require some URL rewriting, it might need a basic authentication, and so on and so forth. I won’t go into details since you probably already know all that and you are probably already using some proxy in your organization.    We’ll integrate [Docker Flow Monitor](http://monitor.dockerflow.com/) with [Docker Flow Proxy (DFP)](http://proxy.dockerflow.com/). If you haven’t used DFP before, please visit the [official documentation](http://proxy.dockerflow.com/) for tutorials, setup, and configuration.    Before we apply the knowledge about new ways to configure Prometheus, we need to run the proxy.    ``` `1` docker network create -d overlay proxy `2`  `3` docker stack deploy `\` `4 `    -c stacks/docker-flow-proxy.yml `\` `5 `    proxy  ```   ``````` We created the `proxy` network and deployed the `docker-flow-proxy.yml` stack. We won’t go into details how *Docker Flow Proxy* works. The essence is that it will configure itself with each service that has specific labels. For any deeper explanation, please visit [Docker Flow Proxy Stack Tutorial](http://proxy.dockerflow.com/swarm-mode-stack/) or any other tutorial available.    With the proxy up and running, we should redeploy our monitor.    We’ll replace the current monitor stack with a new one with in order to achieve this. The major difference is that this time we’ll define startup arguments as well as the labels that will allow the proxy to reconfigure itself to enable access to monitor. You’ll also notice that we will not expose port *9090*. It’ll be accessible through the proxy on port *80*, so there’s no reason to open any other port.    The stack is as follows.    ```  `1`  `monitor``:`  `2`    `image``:` `vfarcic``/``docker``-``flow``-``monitor``:``$``{``TAG``:-``latest``}`  `3`    `environment``:`  `4`      `-` `GLOBAL_SCRAPE_INTERVAL``=``10``s`  `5`      `-` `ARG_WEB_ROUTE``-``PREFIX``=/monitor`  `6`      `-` `ARG_WEB_EXTERNAL``-``URL``=``http``:``//``$``{``DOMAIN``:-``localhost``}``/``monitor`  `7`    `networks``:`  `8`      `-` `proxy`  `9`      `-` `monitor` `10 `    `deploy``:` `11 `      `labels``:` `12 `        `-` `com``.``df``.``notify``=``true` `13 `        `-` `com``.``df``.``distribute``=``true` `14 `        `-` `com``.``df``.``servicePath=/monitor` `15 `        `-` `com``.``df``.``serviceDomain=``$``{``DOMAIN``:-``localhost``}` `16 `        `-` `com``.``df``.``port``=``9090` `17`  `18`  `19 `  `swarm``-``listener``:` `20 `    `image``:` `vfarcic``/``docker``-``flow``-``swarm``-``listener` `21 `    `networks``:` `22 `      `-` `monitor` `23 `    `volumes``:` `24 `      `-` `/``var``/``run``/``docker``.``sock``:``/``var``/``run``/``docker``.``sock` `25 `    `environment``:` `26 `      `-` `DF_NOTIFY_CREATE_SERVICE_URL``=``http``:``//``monitor``:``8080``/``v1``/``docker``-``flow``-``monitor/\` `27` `reconfigure` `28 `      `-` `DF_NOTIFY_REMOVE_SERVICE_URL``=``http``:``//``monitor``:``8080``/``v1``/``docker``-``flow``-``monitor/\` `29` `remove` `30 `    `deploy``:` `31 `      `placement``:` `32 `        `constraints``:` `[``node``.``role` `==` `manager``]` `33`  `34` `networks``:` `35 `  `monitor``:` `36 `    `external``:` `true` `37 `  `proxy``:` `38 `    `external``:` `true`  ```   `````` This time we added a few additional environment variables. They will be used instead Prometheus’ default startup arguments.    We are specifying the route prefix (`ARG_WEB_ROUTE-PREFIX`) as well as the full external URL (`ARG_WEB_EXTERNAL-URL`).    > Please visit [ARG Variables](http://monitor.dockerflow.com/config/#arg-variables) section of the documentation for more information about environment variables that can be used as startup arguments.    We also used the `com.df.*` service labels that will tell the proxy how to reconfigure itself so that Prometheus is available through the path `/monitor`.    The second service is [Docker Flow Swarm Listener](http://swarmlistener.dockerflow.com/) that will listen to Swarm events and send reconfigure and remove requests to the monitor. You’ll see its usage later on. For now, just remember that we deployed it alongside the `monitor` service.    Let us deploy the new version of the monitor stack.    ``` `1` docker stack rm monitor `2`  `3` `DOMAIN``=``$(`docker-machine ip swarm-1`)` `\` `4 `    docker stack deploy `\` `5 `    -c stacks/docker-flow-monitor-proxy.yml `\` `6 `    monitor  ```   ````` Please execute, `docker stack ps monitor` to check the status of the stack. Once it’s up-and-running, we can confirm that the monitor is indeed integrated with the proxy.    ``` `1` open `"http://``$(`docker-machine ip swarm-1`)``/monitor/flags"`  ```   ```` By opening the *flags* screen, not only that we confirmed that the integration with *Docker Flow Proxy* worked but also that the arguments we specified as environment variables are properly propagated. You can observe that through the values of the `web.external-url` and `web.route-prefix` flags.  ![Figure 3-7: Prometheus flags screen with values passed through environment variables](img/00014.jpeg)  Figure 3-7: Prometheus flags screen with values passed through environment variables    Please note that we did not specify the port of the `monitor` service. As soon as the service was created, `swarm-listener` detected it and sent a request to the proxy to reconfigure itself. The information the proxy needs was obtained through the labels (e.g. `com.df.servicePath`).    There was a hidden reason behind the integration of the two. Apart from the need to have a proxy, I wanted to show you an existing implementation of the logic we are exploring. There was no need for a manual configuration of the proxy, nor we had to define the data proxy needs anywhere but inside the service definition itself. The `monitor` service contains all the information, and any other part of the system can fetch it. Everything related to the service is in a single location. By everything, I mean everything that we need for now. Later on, we’ll extend the definition of this and many other services.    ### What Now?    Soon we’ll start exploring *exporters* and their integration with *Prometheus* and *Docker Flow Monitor*.    We’ll take a break and remove the machines we created. Every chapter will start from scratch. Don’t be scared. It’ll take only a couple of minutes to get back to the previous state.    ``` `1` docker-machine rm -f `\` `2 `    swarm-1 swarm-2 swarm-3  ``` ```` ````` `````` ``````` ```````` ````````` `````````` ``````````` ```````````` ````````````` `````````````` ``````````````` ```````````````` ````````````````` ``````````````````
